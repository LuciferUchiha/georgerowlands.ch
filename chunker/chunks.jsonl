{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "What is a Digital Garden?", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "A digital garden is a mix between a notebook and a blog, it is a place to share thoughts and cultivate them into a garden.\nIt also allows me to have a place where I can store my notes/summaries/tutorials for my studies.  \nThe main difference to a blog is that a blog has articles and publication dates and never changes after it has been\npublished, whereas a digital garden is a place where the written content can be continuously edited and refined. The\nnotes are also very free flowing they can span from just a short cheatsheet to a full set of notes on an entire subject\nwhere you go into every nitty-gritty detail.  \nAnother key difference is the navigation. A blog is usually read in chronological order but a digital garden can be read\nin any order you want and uses lots of internal links to connect all the notes into a Network (although this can be\nquite hard to diligently do).  \nIf you are interested in learning more about digital gardens I can recommend the following\n[article by Maggie Appleton](https://maggieappleton.com/garden-history).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "How is my Garden Built?", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "The current iteration of my digital garden is built using [Nextra](https://nextra.site/). Nextra is a static site\ngenerator that is built on top of Next.js and MDX. This allows me to write my notes in markdown and also use the MDX\nformat to write JSX in my markdown files. These markdown files are then converted into static HTML files using Next.js\nand can be hosted on any static site hosting service such as [Vercel](https://vercel.com/).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "The Features", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "In this section I briefly go over some of the features that are supported by my digital garden and how to use them.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "The Features", "Header 3": "Markdown", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "Markdown is supported out of the box. Anything that is supported by markdown can be used in the notes. This includes but\nis not limited to:  \n- Headers\n- Lists\n- Links\n- Images\n- Code Blocks\n- Tables\n- Blockquotes  \nFor a full list of markdown features check out the [Markdown Guide](https://www.markdownguide.org/).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "The Features", "Header 3": "MDX", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "In addition to the normal markdown format, Nextra also supports the MDX format which allows you to write JSX, i.e. react code in a\nmarkdown file. To find out more about MDX check out the [official MDX documentation](https://mdxjs.com/).  \n#### Admonitions / Callouts  \nAdmonitions aren't included in standard markdown but have become very popular. Recently GitHub has also added support for\nadmonitions in markdown FileSystem, however they call them alerts.  \nAdmonitions are very useful to highlight certain text and add a category to the text. I have added a custom component that\nbuilds on nextra's callouts to be able to add custom callout types. To use callouts in a MDX file you can use the following syntax:  \n```\n<Callout type=\"warning\">\nThis Is a big scary warning.\n</Callout>\n```  \nRenders to:  \n<Callout type=\"warning\">\nThis Is a big scary warning.\n</Callout>  \nYou can also change the title of the banner:  \n```\n<Callout type=\"info\" title=\"The following types are supported\">\ninfo, warning, error, example, todo\n</Callout>\n```  \n<Callout type=\"info\" title=\"The following types are supported\">\ninfo, warning, error, example, todo\n</Callout>  \nThe default callout type uses the websites primary color, a rocket icon and has no title:  \n<Callout>\nThis is a default callout.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "The Features", "Header 3": "Jupyter Notebooks", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "<Callout type=\"todo\">\nTODO add how the hound works and how to use it.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "The Features", "Header 3": "LaTeX", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "It has recently become very popular to write LaTeX equations in markdown. Nextra supports this by using [KaTeX](https://katex.org/).\nYou can render LaTeX content either inline between `$\\LaTeX$` $\\LaTeX$ or as a block between `$$I = \\int_0^{2\\pi} \\sin(x)\\,dx$$`:  \n$$\nI = \\int_0^{2\\pi} \\sin(x)\\,dx\n$$  \nAnnoyingly Jupyter Notebooks use MathJax to render LaTeX content in the same way instead of KaTeX. This means that KaTeX\nsupports some things and MathJax supports other things. Importantly however is that the Jupyter Notebooks get converted\nto Markdown and therefore in the end it will only be rendered in KaTeX.  \nTherefore, if something is written that is supported in MathJax but not in KaTeX it might look okay but in the end,\nit will not be rendered by KaTeX. This leads to [my LaTeX Notation Guideline](./maths/latexGuidelines) to avoid\nconflicts whilst still keeping nice Formulas.  \nYou can see what is supported by KaTeX [here,](https://katex.org/docs/supported.html) and you can see what is supported\nby MathJax [here](https://docs.mathjax.org/en/latest/input/tex/macros/index.html).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "The Features", "Header 3": "PlantUML", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "If you ever need to create diagrams and especially UML diagrams, PlantUML is the way to go. I started with Mermaid\nto create UML diagrams but swapped to PlantUML for the additional features and the ability to create custom themes\n(so everything can be minimalist and purple :D).  \nTo render PlantUML diagrams the [Remark plugin Simple PlantUML](https://github.com/akebifiky/remark-simple-plantuml) is\nused which uses the official PlantUML server to generate an image and then adds it.  \nAn Example can be seen below, on the [official website](https://plantuml.com/) and also on [REAL WORLD PlantUML](https://real-world-plantuml.com/?type=class).  \n```plantuml\n@startuml\n\ninterface Command {\nexecute()\nundo()\n}\nclass Invoker{\nsetCommand()\n}\nclass Client\nclass Receiver{\naction()\n}\nclass ConcreteCommand{\nexecute()\nundo()\n}\n\nCommand <|-down- ConcreteCommand\nClient -right-> Receiver\nClient --> ConcreteCommand\nInvoker o-right-> Command\nReceiver <-left- ConcreteCommand\n\n@enduml\n```  \nTo use my custom theme you can use the following line at the beginning of the PlantUML file:  \n```\n@startuml\n!theme purplerain from http://raw.githubusercontent.com/LuciferUchiha/georgerowlands.ch/main\n\n...\n\n@enduml\n```  \nHowever, it seems like when using a custom theme There can not be more then one per page? My custom theme also has some processes built in for simple text coloring for example in cases of success, failure etc.  \n```plantuml\n@startuml\n!theme purplerain from http://raw.githubusercontent.com/LuciferUchiha/georgerowlands.ch/main\n\nBob -> Alice :  normal\nBob <- Alice :  $success(\"success: Hi Bob\")\nBob -x Alice :  $failure(\"failure\")\nBob ->> Alice : $warning(\"warning\")\nBob ->> Alice : $info(\"finished\")\n\n@enduml\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "My Digital Garden", "Header 2": "How can I Contribute?", "path": "../pages/digitalGarden/index.mdx"}, "page_content": "Do you enjoy the content and want to contribute to the garden by adding some new plants or watering the existing ones?\nThen feel free to make a pull request. There are however some rules to keep in mind before adding or changing content.  \n- Markdown filenames and folders are written in camelCase.\n- Titles should follow the\n[IEEE Editorial Style Manual](https://www.ieee.org/content/dam/ieee-org/ieee/web/org/conferences/style_references_manual.pdf).\nThey should also be added to the markdown file and specified in the `_meta.json` which maps files to titles and is also\nresponsible for the ordering.\n- LaTeX should conform with my notation and guideline, if something is not defined there you can of course add it.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computer Systems", "path": "../pages/digitalGarden/cs/computerArchitecture/computerSystems.mdx"}, "page_content": "This Page is meant as a brief introduction to the types of computer systems there are and how they are made and the issues we have faced with making improvements to our computer systems over the years.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computer Systems", "Header 2": "Types of Computer Systems", "path": "../pages/digitalGarden/cs/computerArchitecture/computerSystems.mdx"}, "page_content": "Let us first answer the question of what is a computer. What is the difference between a computer and just any other machine. A Computer is a programmable machine. Meaning that it can change its behavior and functionality, unlike other simple machines.  \nMost commonly Computers are split up into the following types:  \n- Personal Computer, short PC. The PC is the type of computer most people use and think of when talking about a computer. It serves a very general purpose and offers a wide variety of software to solve problems in our day-to-day life. In more recent years this type has also seen the addition of personal mobile devices (PMD) or more commonly known as smartphones and tablets. These devices are meant for the average consumer which makes subjects them to cost/performance tradeoffs.\n- Server computers or also just servers are computers that are usually accessed only over a network (Internet or LAN). Servers are built from the same basic technology as personal computers but with more performance and storage capabilities. Since they are also used by multiple people and are used to communicate between different applications and/or networks they have to be reliable to mitigate downtime.\n- Supercomputers, these computers represent the peak of what can be done with computers and are mainly used for research and academic purposes. You can find out more about the top supercomputers [here](https://www.top500.org/).\n- Embedded computers are the most used computers but people would never think so as they are usually hidden. They have a very wide range of applications and performances for example being part of your car to optimize fuel efficiency down to controlling the temperature in your coffee machine.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computer Systems", "Header 2": "Components of a Computer", "path": "../pages/digitalGarden/cs/computerArchitecture/computerSystems.mdx"}, "page_content": "<Callout type=\"todo\">\nCPU = Control + Datapath\nMemory\nIO\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computer Systems", "Header 2": "How are Chips made", "path": "../pages/digitalGarden/cs/computerArchitecture/computerSystems.mdx"}, "page_content": "blbabla silicon and moores law. yield etc.  \n<Image\nsrc=\"/cs/archSandToChips.png\"\nwidth={300}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computer Systems", "Header 2": "The Power Wall", "path": "../pages/digitalGarden/cs/computerArchitecture/computerSystems.mdx"}, "page_content": "As transistors get smaller, their power density stays constant. power wall and denard scaling\ncant reduce voltage because of noise => bits getting flipped and cant cool  \nlead to hift to Multicore Processors  \nblabla amhdals law, cant infinetly speeedup there is some limit.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computer Systems", "Header 2": "Programming a Computer", "path": "../pages/digitalGarden/cs/computerArchitecture/computerSystems.mdx"}, "page_content": "blabla high level, compiler assembler instruciton sets", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "Working with numbers on computer systems is slightly more complex then one would think due to the fact that computers work only with the binary numbers 1 and 0.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Integers", "Header 3": "Unsigned Integers", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "with n bits we can represent $2^n$ things. Encoding unsigned integers, i.e integers with no sign so positive numbers is pretty simple. The first bit called the LSB corresponds to $2^0$, the second one $2^1$. If that bit is set to 1 we add the value corresponding to that bit and receive the result. So if we have 32 bits we can represent $2^32$ things, so if we start at 0 we can represent the range from 0 to $2^32-1$.  \nThis can also be described mathematically as followed if we denote our binary representation as $B=b_{n-1},b_{n-2},..,b_0$ and the function $D(B)$ which maps the Binary representation to its corresponding value.  \n$$\nD(B)= \\sum_{i=0}^{n-1}{b_i \\cdot 2^i}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Signed Integers", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "When we involve signed integers it gets a bit more complex since now we also want to deal with negative numbers. In history there have been a few representations for encoding signed integers which often get forgotten.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Signed Integers", "Header 3": "Sign Magnitude", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "The idea for the sign and magnitude representation is a very simple one. You have a bit (the MSB) that represents the sign, 1 for negative, 0 for positive. All the other bits are the magnitude i.e. the value.  \n$$\nD(B)= (-1)^{b_{n-1}} \\cdot \\sum_{i=0}^{n-2}{b_i \\cdot 2^i}\n$$  \n<Image\nsrc=\"/cs/archSignMagnitude.png\"\nwidth={300}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n0000\\,1010_2 &= 10 \\\\\n1000\\,1010_2 &= -10\n\\end{align*}\n$$\n</Callout>  \nSeems pretty simple. However, there are two different representations for 0 which isn't good since computers often make comparisons with 0. This could potentially double the number of comparisons needed to be made which is one reason why this sign magnitude representation is not optimal.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Signed Integers", "Header 3": "One's Complement", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "The idea of the one's complement is also very simple, it is that we want to quickly find the negative number of the same positive value by just flipping all the bits. In other words:  \n$$\n-B=\\,\\sim B\n$$  \nAnd mathematically defined:  \n$$\nD(B)= -b_{n-1}(2^{n-1}-1) + \\sum_{i=0}^{n-2}{b_i \\cdot 2^i}\n$$  \n<Image\nsrc=\"/cs/archOnesCompliment.png\"\nwidth={300}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n0000\\,1010_2 &= 10 \\\\\n1111\\,0101_2 &= -10\n\\end{align*}\n$$\n</Callout>  \nhowever just like the sign magnitude representation the one's complement has the issue of having 2 representations for 0.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Signed Integers", "Header 3": "Two's Complement", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "Finally, we have the representation that is used nowadays, the two's complement. This representation solves the issue of the double representation of 0 whilst still being able to quickly tell if a number is positive or negative. It does however lead to there not being a positive value corresponding to the lowest negative value.  \n$$\nD(B)= -b_{n-1}(2^{n-1}) + \\sum_{i=0}^{n-2}{b_i \\cdot 2^i}\n$$  \n<Image\nsrc=\"/cs/archTwosCompliment.png\"\nwidth={300}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n0000\\,1010_2 &= 10 \\\\\n1111\\,0110_2 &= -10\n\\end{align*}\n$$\n</Callout>  \nAny easy way to calculate the negative value of a given value with the two's complement representation is the following:  \n$$\n\\sim B + 1 \\Leftrightarrow -B\n$$  \n#### Sign Extension  \nWhen using the two's complement we do need be aware of something when converting a binary number with $n$ bits to a binary number with $n+k$ bits and it is called sign extension. Put simply for the value of binary number to stay the same we need to extend the sign bit.  \n<Image\nsrc=\"/cs/archSignExtension.png\"\nwidth={500}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n10:&\\, 0000\\,1010_2 \\Rightarrow 0000\\,0000\\,0000\\,1010_2 \\\\\n-10:&\\, 1111\\,0110_2 \\Rightarrow 1111\\,1111\\,1111\\,0110_2\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "Representing real numbers can be pretty hard as you can imagine since real numbers can be infinite numbers such as $\\pi = 3.14159265358979323846264338327950288...$ but we only have finite resources and bits to represent them for example 4 or 8 bytes. Another problem is that often times when working with real numbers we find ourselves using very small or very large numbers such as $1$ Lightyear $=9'460'730'472'580.8\\,km$ or the radius of a hydrogen atom $0.000000000025\\,m$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "Header 3": "Binary Fractions", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "One way, but not a very good way to represent real numbers is to use binary fractions. Binary fractions are a way to extend the unsigned integer representation by adding a so-called binary/zero/decimal point. To the left of the binary point, we have just like with the unsigned representation the powers of 2. To the right, we now also use the powers of 2 with negative numbers to get the following structure:  \n$$\nB = b_{i},b_{i-1},..,b_0\\,.\\,b_{-1},...,b_{-j+1},b_{-j}\n$$  \nAnd Formula:  \n$$\nD(B) = \\sum_{k=-j}^{i}{b_k \\cdot 2^k}\n$$  \n<Image\nsrc=\"/cs/archBinaryFractions.png\"\nwidth={500}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n5 \\frac{3}{4} &= 0101.1100_2 \\\\\n2 \\frac{7}{8} &= 0010.1110_2 \\\\\n\\frac{63}{64} &= 0.1111110_2\n\\end{align*}\n$$\n</Callout>  \nFrom the above examples we can make 3 key observations the first 2 might already know if you have been programming for a long time.  \n- Dividing by powers of 2 can be done with shifting right $x / 2^y \\Leftrightarrow x >> y$\n- Multiply with powers of 2 can be done with shifting left $x \\cdot 2^y \\Leftrightarrow x << y$  \nThis representations does have its limits since we can only represent numbers of the form $\\frac{x}{s^k}$ other numbers such as $\\frac{1}{3}$ have repeating bit representations.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "Header 3": "Fixed Points", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "The fixed-point representation or also called $p.q$ fixed-point representation extends the idea of binary fractions by adding a sign bit making the left part of the binary point the same as the two's complement. The right part is the same fractional part. The number of bits for the integer part (including the sign) bit corresponds to $p$ the number of bits for the fractional part corresponds to $q$, 17.14 being the most popular format.  \n$$\nD(P)=-b_p \\cdot 2^p + \\sum_{k=-q}^{p-1}{b_k \\cdot 2^k}\n$$  \n<Image\nsrc=\"/cs/archFixedPoint.png\"\n/>  \nThis representation has many pros, it is simple we can use simple arithmetic operations and don't need special floating-point hardware which is why it is commonly used in many low-cost embedded processors. The only con is that we can not represent a wide range of numbers which we will fix with the next and last representation.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "Header 3": "Floating Points", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "In 1985 the [IEEE standard 754](https://standards.ieee.org/ieee/754/993/) was released and quickly adapted as the standard for so-called floating-point arithmetic. In 1989, William Kahan, one of the primary architects even received the Turing Award, which is like the noble prize for computer science. The floating-point representation builds on the ideas of the fixed-point representation and [scientific notation](../../Mathematik/scientificNotation).  \nFloating-point representation consists of 3 parts, the sign bit, and like the scientific notation an exponent and mantissa.  \n<Image\nsrc=\"/cs/archFloatingPoint.png\"\nwidth={600}\n/>  \nWe most commonly use the following sizes for the exponent and mantissa:  \n- Single precision: 8 bits for the exponent, 23 bits for the mantissa making a total of 32 bits with the sign bit.\n- Double precision: 11 bits for the exponent, 52 bits for the mantissa making a total of 64 bits. It doesn't offer much of a wider range then the single precision however, it does offer more precision, hence the name.  \nIn 2008 the IEEE standard 754 was revised with the addition of the following sizes:  \n- Half precision: 5 bits for the exponent, 10 bits for the mantissa making a total of 16 bits.\n- Quad precision: 15 bits for the exponent, 112 bits for the mantissa making a total of 32 bits.  \nWith the rise of artificial intelligence and neural networks, smaller representations have gained popularity for quantization. This popularity introduced the following so-called minifloats consisting of 8 bits in total:  \n- E4M3: as the name suggests 4 bits for the exponent and 3 bits for the mantissa.\n- E5M2: 5 bits for the exponent and 2 bits for the mantissa.  \nThe brain floating point which was developed by Google Brain is also very popular for AI as it has the same range as single precision due to using the same amount of bits for the exponent but with less precision.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "Header 3": "Floating Points", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "The brain floating point which was developed by Google Brain is also very popular for AI as it has the same range as single precision due to using the same amount of bits for the exponent but with less precision.  \nThe floating-point representation used however normalized values just like the scientific notation. Meaning the mantissa is normalized to the form of  \n$$\n1.000010010...110_2\n$$  \nSo, in reality, we are not actually storing the mantissa but only the fraction part which is why it is also commonly referred to as the fraction. This leads to two things, we get an extra bit for free since we imply that the first bit is 1, but we can no longer represent the value 0. We will however see later how we can solve the problem of representing 0.  \nWe also do not store the exponent using the two's complement. Instead, we use the so-called biased notation for the simple reason of wanting to compare values quickly with each other. To do this we want a form where the exponent with all zeros $0000\\,0000$ is smaller than the exponent with all ones $1111\\,1111$ which wouldn't be the case when using the two's complement. Instead, we use a bias. To calculate the bias we use the number of bits used to represent the exponent $k$. For single precision $k=8$, the bias for single precision is $127$ calculated using the formula:  \n$$\nbias = 2^{k-1}-1\n$$  \n<Callout type=\"example\">\nNow that we understand the form of the floating-point representation let us look at an example. We want to store the value $2022$ using single precision floating-point. First, we set the sign bit in this case $0$. Then we convert the value to a binary fraction. Then we normalize it whilst keeping track of the exponent. Then lastly we store the fraction part and the exponent + the bias.  \n$$\n\\begin{align}\n2022 &= 11111100110._2 \\cdot 2^0 & \\text{Convert to binary fraction} \\\\\n&= 1.1111100110_2 \\cdot 2^{10} & \\text{Shift binary point to normalize} \\\\\nM &= 1.1111100110_2 & \\text{Mantissa} \\\\\nFraction &= 1111100110_2 & \\text{Fraction} \\\\", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "Header 3": "Floating Points", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "$$\n\\begin{align}\n2022 &= 11111100110._2 \\cdot 2^0 & \\text{Convert to binary fraction} \\\\\n&= 1.1111100110_2 \\cdot 2^{10} & \\text{Shift binary point to normalize} \\\\\nM &= 1.1111100110_2 & \\text{Mantissa} \\\\\nFraction &= 1111100110_2 & \\text{Fraction} \\\\\nE &= 10 & \\text{Exponent} \\\\\nExp &= E + bias = 10 + 127 = 1000\\,1001_2 & \\text{Biased Exponent}\n\\end{align}\n$$  \n| Sign | Exponent  | Fraction                     |\n| ---- | --------- | ---------------------------- |\n| 0    | 1000 1001 | 1111 1001 1000 0000 0000 000 |\n</Callout>  \n#### Denormalized values  \nAs mentioned above we can't represent the value $0$ using the normalized values. For this, we need to use denormalized values or also often called subnormal. For this, in the case of single precision, we reserve the exponent that consists of only zeros so has the biased value $0$ and therefore the exponent $1-bias$, for single precision this would be $-126$. If the fraction also consists of all zeros then we have a representation for the value $0$. If it is not zero then we just have evenly distributed values close to 0.  \n<Callout type=\"example\">\n| Value                                             | Sign | Exponent  | Fraction                     |\n| ------------------------------------------------- | ---- | --------- | ---------------------------- |\n| 0                                                 | 0    | 0000 0000 | 0000 0000 0000 0000 0000 000 |\n| -0                                                | 1    | 0000 0000 | 0000 0000 0000 0000 0000 000 |\n| $0.5 \\cdot 2^{-126} \\approx 5.877 \\cdot 10^{-39}$ | 0    | 0000 0000 | 1000 0000 0000 0000 0000 000 |\n| $0.99999 \\cdot 2^{-126}$                          | 0    | 0000 0000 | 1111 1111 1111 1111 1111 111 |\n</Callout>  \n#### Special Numbers", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "Header 3": "Floating Points", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "| $0.5 \\cdot 2^{-126} \\approx 5.877 \\cdot 10^{-39}$ | 0    | 0000 0000 | 1000 0000 0000 0000 0000 000 |\n| $0.99999 \\cdot 2^{-126}$                          | 0    | 0000 0000 | 1111 1111 1111 1111 1111 111 |\n</Callout>  \n#### Special Numbers  \nFor some cases we want to be able to store some special values such as $\\infty$ if we do $1.0 / 0.0$ or $NaN$ when doing $\\sqrt{-1}$ or $\\infty - \\infty$. Just like with solving the issue of representing $0$, to represent special values we can reserve an exponent, in the case of single precision this is the exponent consisting of only ones. If the fraction only consists of zeros then it represents the value $\\infty$ otherwise if the fraction is not all zeros it represents $NaN$.  \n| Value     | Sign | Exponent  | Fraction                     |\n| --------- | ---- | --------- | ---------------------------- |\n| $\\infty$  | 0    | 1111 1111 | 0000 0000 0000 0000 0000 000 |\n| $-\\infty$ | 1    | 1111 1111 | 0000 0000 0000 0000 0000 000 |\n| $NaN$     | 0    | 1111 1111 | 1000 0000 0000 0000 0000 000 |\n| $NaN$     | 1    | 1111 1111 | 1111 1111 1111 1111 1111 111 |  \nFor other representations such as the E4M3, E5M2 or bfloat16 the handling of special numbers can be different. This comes down to there being less bits and therefore each bit having more meaning so reserving an entire exponent range just to represent $NaN$ would be a big waste:  \n|| E4M3             | E5M2                     |\n| ------------------ | ---------------- | ------------------------ |\n| $-\\infty / \\infty$ | N/A              | $S\\,11111\\,00_2$         |\n| $NaN$              | $S\\,1111\\,111_2$ | $S\\,11111\\,{01,10,11}_2$ |\n| $-0/0$             | $S\\,0000\\,000_2$ | $S\\,00000\\,00_2$         |  \n#### Precision  \nAs mentioned at the beginning of the floating-point section Real numbers are in theory infinite however we can not represent an infinite amount of numbers with a finite number of bits. Below you can see an estimated visualization of what values can actually be represented.  \n<Image", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "Header 3": "Floating Points", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "<Image\nsrc=\"/cs/archFloatingPointPrecision.png\"\n/>  \nAt a closer look, we can also see how the representations are distributed with the values close to zero being very precise.  \n<Image\nsrc=\"/cs/archFloatingPointRange.png\"\n/>  \nThis issue can however cause problems of imprecision if a certain number can not be represented and is rounded to the closest number that can be represented. For example in C we can do the following:  \n```c\n#include <stdio.h>\nint main ()\n{\ndouble d;\nd = 1.0 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1;\nprintf (“d = %.20f\\n”, d); // Not 2.0, outputs 2.00000000000000088818\n}\n```  \n#### Rounding  \nThe IEEE standard 754 defines four rounding modes:  \n- Round-up\n- Round-down\n- Round-toward-zero, i.e truncate, which is commonly done when converting from integer to floating point.\n- Round-to-even, the most common but also the most complicated of the four modes.  \nI will not go into detail of the first three modes as they are self-explanatory. Let us first look at why we need to round-to-even. The reason is actually pretty simple, normal rounding is not very fair.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n& &0.5+1.5+2.5+3.5 &= 8 \\\\\n\\text{Rounded: }& &1+2+3+4 &= 10 \\\\\n\\text{Round-to-even: }& &0 + 2 + 2 + 4 &= 8\n\\end{align*}\n$$\n</Callout>  \n<Callout type=\"todo\">\nThis part is not correct.\n</Callout>  \nWhen working with round-to-even we need to keep track of 3 things:  \n- Guard bit: The LSB that is still part of the fraction.\n- Round bit: The first bit that exceeds the fraction.\n- Sticky bit: A bitwise OR of all the remaining bits that exceed the fraction.  \nSo if we only have a mantissa of 4 bits, i.e a fraction with 3 bits then it could look like this:  \n<Image\nsrc=\"/cs/archGRSBits.png\"\nwidth={500}\n/>  \nNow we have 3 cases:  \n- If $GRS=0xx$ we round down, i.e do nothing since the LSB is already $0$.\n- If $GRS=100$ this is a so-called tie, if the bit before the guard bit is $1$ we round the mantissa up otherwise we round down i.e set the guard bit to $0$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Working with Numbers", "Header 2": "Real Numbers", "Header 3": "Floating Points", "path": "../pages/digitalGarden/cs/computerArchitecture/workingWithNumbers.mdx"}, "page_content": "Now we have 3 cases:  \n- If $GRS=0xx$ we round down, i.e do nothing since the LSB is already $0$.\n- If $GRS=100$ this is a so-called tie, if the bit before the guard bit is $1$ we round the mantissa up otherwise we round down i.e set the guard bit to $0$\n- For all other cases $GRS=110$, $GRS=101$ and $GRS=111$ we round up.  \n<Callout type=\"warning\">\nAfter rounding, you might have to normalize and round again for example if we have $1.1111\\,1111|11$ with $GRS=111$ and Biased exponent $128$, i.e $2^1$. We have to round up and get $11.0000\\,0000$ therefore we need to increase the exponent by $1$ to normalize again. This also means that after rounding we can produce a over or underflow to infinity.\n</Callout>  \n#### Addition/Subtraction  \n#### Multiplication", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Memory Hierarchy", "path": "../pages/digitalGarden/cs/computerArchitecture/memoryHierarchy.mdx"}, "page_content": "<Callout type=\"todo\">\nTo do about caches registers misses etc.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is RISC-V?", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/riscV.mdx"}, "page_content": "RISC-V is an open standard instruction set architecture that has been developed at the University of California, Berkeley since 1981 and is based on the established RISC principles which we will see when diving deeper.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is RISC-V?", "Header 2": "CISC vs RISC", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/riscV.mdx"}, "page_content": "Up until about 1986 most chip manufacturers were using the CISC (Complex Instruction Set Computers) Architecture, the most common example of this being the Intel x86 ISA which is widely used nowadays. However, they realized that it makes building the chips more complicated and slows down potential improvements. This brought on the switch to RISC (Reduced Instruction Set Computers) which focuses on having a small number of simple instructions and then letting the compilers resolve complexity. Some of the most common examples of RISC are MIPS, AMD ARM and the open-source RISC-V which is what we will be looking at.  \nHowever, you might have realized that most computers that you interact with use x86, doesn't that mean that you aren't getting the best performance that you could? This is actually not true, almost all chips nowadays use the RISC architecture, including x86 chips. But I just said that x86 uses CISC? This is true for the early x86 chips, the x86 chips nowadays are hybrid chips. They support CISC instructions for backward compatibility as a lot of devices were already using CISC, however, inside the chips they convert the CISC instructions to RISC instructions and execute them, which makes them have a \"RISC\" core.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is RISC-V?", "Header 2": "Extensions", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/riscV.mdx"}, "page_content": "RISC-V aims to be as lightweight as possible which is why it allows for extensions to be added for certain functionalities. This allows chip manufacturers to only add what they need and not have instructions that they never intend to use or support. We will mainly be focusing on the `RV32IG` variant which is equivalent to `RV32IMAFD`.  \n<Image\nsrc=\"/cs/archRiscVExtensions.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is RISC-V?", "Header 2": "Register Layout", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/riscV.mdx"}, "page_content": "RISC-V has 32 (or 16 in the embedded variant) integer registers and with the floating-point extension another separate 32 floating-point registers. Since our focus is on the 32-bit variation each register can store 32 bits. These registers are essential to the CPU as it can only work with data that is in a register it can not work on data in main memory. So if we want to manipulate data that is in the main memory we need to first transfer the data from the main memory to a register.  \nCertain registers have restrictions or should be used in a certain way. Most notable is that the first register will always store the value 0.  \n<Image\nsrc=\"/cs/archRiscVRegisters.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Procedure Calls", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/procedureCalls.mdx"}, "page_content": "A procedure or function is one of the key building blocks for programmers as it allows them to create understandable and reusable code. It also a way to add abstraction and simplify a program. In simple a procedure works as follows:  \n1. Put the parameters in a place the procedure (callee) can access them.\n2. Transfer control to the procedure.\n3. Acquire the storage resources needed for the procedure.\n4. Perform the task.\n5. Put the result of the task in a place the caller can access them.\n6. Return control to the caller.  \nFor the first and fifth point, we have the registers `x10-x17`. So that we know where to return to in step 6 we store the caller address in `x1`, this would be done when transferring control to the procedure with the `jal x1, Label` instruction.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Procedure Calls", "Header 2": "Using More Registers", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/procedureCalls.mdx"}, "page_content": "But what if the 8 registers for the arguments are not enough to complete our task. We can use other registers as long as we clean up after ourselves, meaning we can spill registers to memory and then restore the registers before returning control. This leads us to the idea of the stack. In RISC-V we keep track of a stack pointer in `x2` and push and pop data to the stack. Important here is however that the stack grows from high to low addresses meaning when updating the stack pointer we need to subtract.  \nFor this we also remember that the registers `x5-x7` and `x28-x31` are temporary registers and do not need to be restored before returning control but the registers `x8-x9` and `x18-x27` are saved registers and do need to be restored.  \n<Callout type=\"example\">\nIn the below example we could just use the temporary registers to store the temporary values but instead, we will spill some registers to the stack to demonstrate how this could be done.  \n```c\n// g in x10, h in x11, i in x12, j in x13\nint leaf( int g, int h, int i, int j) {\nint f;\nf = (g + h) – (i + j);\nreturn f;\n}\n```  \n```assembly\nleaf:\naddi sp, sp, -12        # make space on stack for 12 bytes 3x 32 bits\nsw x5,8(sp)             # save x5\nsw x6,4(sp)             # save x6\nsw x7,0(sp)             # save x7\nadd x5,x10,x11          # x5 <- g + h\nadd x6,x12,x13          # x6 <- i + j\nsub x7,x5,x6            # x7 <- x5 - x6\naddi x10,x7,0           # write result to x10 <- x7\nlw x7,0(sp)             # restore x7\nlw x6,4(sp)             # restore x6\nlw x5,8(sp)             # restore x5\naddi sp,sp,12           # adjust stack\njalr x0,0(x1)           # return to caller\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Procedure Calls", "Header 2": "Using More Registers", "Header 3": "Nested Procedures", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/procedureCalls.mdx"}, "page_content": "Procedures that do not call other procedures are called leaf procedures. But these are very rarely seen in programs, much more often we see nested procedures or even recursive procedures which need a lot of care when working with registers. For example, imagine the Procedure $A$ is called and the argument 3 is stored in `x10` and return address in `x1`. If $A$ then wants to call the procedure $B$ the argument in `x10` and return address in `x1` must be overwritten. So to prevent these collisions we must carefully push data to the stack and retrieve it again at a later time.  \nTo aid this tricky task of keeping track of the local data of a procedure some RISC-V compilers use a frame pointer `fp` which is stored in the register `x8`. As the stack pointer can always change the frame pointer offers a stable base register for local memory references.  \n<Image\nsrc=\"/cs/archFramePointer.png\"\nwidth={600}\n/>  \n<Callout type=\"example\">\n```c\nint fact(int n)\n{\nif (n < 1)\nreturn 1;\nelse\nreturn n * fact(n-1);\n}\n```  \n```assembly\nfact:\naddi sp, sp, -8     # make space for 8 bytes\nsw x1, 4(sp)        # save return address\nsw x10, 0(sp)       # save n\naddi x11, x10, -1   # x11 <- n - 1\nbge x11, zero, L1   # if (x11 >= 0), goto L1\naddi x10, zero, 1   # x10 <- 1 (retval)\naddi sp, sp, 8      # adjust stack\njalr zero, 0(x1)    # return\nL1:\naddi x10, x10, -1   # x10 <- n - 1\njal x1, fact        # call fact(n-1)\naddi t1, x10, 0     # t1 <- fact(n-1)\nlw x10, 0(sp)       # restore n\nlw x1, 4(sp)        # restore return address\naddi sp, sp, 8      # adjust stack pointer\nmul x10, x10, t1    # x10 <- n * t1 (retval)\njalr zero, 0(x1)    # return\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pseudo Instructions", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/pseudoInstructions.mdx"}, "page_content": "As I have already mentioned multiple times, some RISC-V implementations also offer pseudo instructions which are like aliases for other instructions but make the assembly code easier to read and understand.  \n<Image\nsrc=\"/cs/archPseudoInstructions.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Data Transfer Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/dataTransfer.mdx"}, "page_content": "Just using registries to store data is not enough which is why we also have main memory and secondary memory. Main memory is especially useful when working with composite data such as data structures or dynamic data.  \nAs mentioned previously we can not directly work on data that is stored in memory, the CPU can only work on data that is in a registry. This leads us to load and store data between the registries and the main memory.  \nEach byte in memory has an address. For composite data, RISC-V uses the little endian byte ordering meaning that the LSB byte is at the smallest address.  \nRISC-V defines a word as data that consists of 32 bits this corresponds to the size of the registry and is the most common size to read and write to and from memory. However, we can also only read a byte which is useful since ASCII only uses a byte. RISC-V also supports reading a so-called halfword which corresponds to 16 bits which is useful when working with Unicode characters.  \nWe do however need to keep in mind that in memory we only store the value, no context. So if we want a word to be handled like an unsigned integer we also need to specify that otherwise, it will treat it by default as a signed integer.  \n| Instruction            | Type | Example              | Meaning                                          |\n| ---------------------- | ---- | -------------------- | ------------------------------------------------ |\n| Load word              | I    | `lw rd, imm12(rs1)`  | `R[rd] = Mem4[R[rs1] + SignExt(imm12)]`          |\n| Load halfword          | I    | `lh rd, imm12(rs1)`  | `R[rd] = SignExt(Mem2[R[rs1] + SignExt(imm12)])` |\n| Load byte              | I    | `lb rd, imm12(rs1)`  | `R[rd] = SignExt(Mem1[R[rs1] + SignExt(imm12)])` |\n| Load word unsigned     | I    | `lwu rd, imm12(rs1)` | `R[rd] = ZeroExt(Mem4[R[rs1] + SignExt(imm12)])` |\n| Load halfword unsigned | I    | `lhu rd, imm12(rs1)` | `R[rd] = ZeroExt(Mem2[R[rs1] + SignExt(imm12)])` |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Data Transfer Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/dataTransfer.mdx"}, "page_content": "| Load word unsigned     | I    | `lwu rd, imm12(rs1)` | `R[rd] = ZeroExt(Mem4[R[rs1] + SignExt(imm12)])` |\n| Load halfword unsigned | I    | `lhu rd, imm12(rs1)` | `R[rd] = ZeroExt(Mem2[R[rs1] + SignExt(imm12)])` |\n| Load byte unsigned     | I    | `lbu rd, imm12(rs1)` | `R[rd] = ZeroExt(Mem1[R[rs1] + SignExt(imm12)])` |\n| Store word             | S    | `sw rs2, imm12(rs1)` | `Mem4[R[rs1] + SignExt(imm12)] = R[rs2]`         |\n| Store halfword         | S    | `sh rs2, imm12(rs1)` | `Mem2[R[rs1] + SignExt(imm12)] = R[rs2](15:0)`   |\n| Store byte             | S    | `sb rs2, imm12(rs1)` | `Mem1[R[rs1] + SignExt(imm12)] = R[rs2](7:0)`    |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Data Transfer Operations", "Header 2": "Loading With Pointers", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/dataTransfer.mdx"}, "page_content": "Pointers in C are nothing else but memory addresses which means we can also load data from and to them. The most simple use of pointers is to swap to values:  \n```c\n// x in a0, y in a1\nvoid swap(int *x, int *y)\n{\nint temp_x = *x;\nint temp_y = *y;\n*x = temp_y;\n*y = temp_x;\n}\n```  \nAnd as we can see we can use addresses stored in registries to load and write data:  \n```assembly\nlw a4, 0(a0)\nlw a5, 0(a1)\nsw a5, 0(a0)\nsw a4, 0(a1)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Data Transfer Operations", "Header 2": "Loading Sequential Data", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/dataTransfer.mdx"}, "page_content": "When reading sequential data we do need to keep in mind that each address only corresponds to a byte. This leads us to make \"jumps\" of size 4.  \n<Callout type=\"example\">\n```c\n// h in x21, base address of A in x22\nA[9] = h + A[8]\n```  \n```assembly\nlw x9, 32(x22)\nadd x9, x21, x9\nsw x9, 46(x22)\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Transfer Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/controlTransfer.mdx"}, "page_content": "When programming we often find ourselves using control structures like if and else this creates branches in our program where we either go down one or the other branch. RISC-V offers so-called branch instructions which in most cases take 2 operands and a Label to jump to after checking the condition. Labels are not some magic keywords, they are just an offset off the program counter, PC that is automatically handled by the assembler.  \n| Instruction                           | Type | Example                | Meaning                                                 |\n| ------------------------------------- | ---- | ---------------------- | ------------------------------------------------------- |\n| Branch equal                          | SB   | `beq rs1, rs2, imm12`  | `if (R[rs1] == R[rs2]) pc = pc + SignExt(imm12 << 1)`   |\n| Branch not equal                      | SB   | `bne rs1, rs2, imm12`  | `if (R[rs1] != R[rs2]) pc = pc + SignExt(imm12 << 1)`   |\n| Branch greater than or equal          | SB   | `bge rs1, rs2, imm12`  | `if (R[rs1] >= R[rs2]) pc = pc + SignExt(imm12 << 1)`   |\n| Branch greater than or equal unsigned | SB   | `bgeu rs1, rs2, imm12` | `if (R[rs1] >=u R[rs2]) pc = pc + SignExt(imm12 << 1)`  |\n| Branch less than                      | SB   | `blt rs1, rs2, imm12`  | `if (R[rs1] < R[rs2]) pc = pc + SignExt(imm12 << 1)`    |\n| Branch less than unsigned             | SB   | `bltu rs1, rs2, imm12` | `if (R[rs1] < u R[rs2]) pc = pc + SignExt(imm12 << 1)`  |  \nIn RISC-V you might notice that there is no greater then or less than or equal. This is because we can emulate these by just switching the operands, however, most CPUs have pseudo instructions to make the assembly code more readable.  \n<Callout type=\"example\">\n```c\n// i in x22, j in x23, f in x19, g in x20, h in x21\nif (i == j)\nf = g + h;\nelse\nf = g – h;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Transfer Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/controlTransfer.mdx"}, "page_content": "<Callout type=\"example\">\n```c\n// i in x22, j in x23, f in x19, g in x20, h in x21\nif (i == j)\nf = g + h;\nelse\nf = g – h;\n```  \nIn the code below we can also see a so-called unconditional branch meaning we always jump to the given Label. This unconditional branch makes us of the register `x0` always holding the value 0.  \n```assembly\nbne x22, x23, L1\nadd x19, x20, x21\nbeq x0, x0, Exit            # unconditional", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Transfer Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/controlTransfer.mdx"}, "page_content": "L1:\nsub x19, x20, x21\nExit:\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Transfer Operations", "Header 2": "Basic Blocks", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/controlTransfer.mdx"}, "page_content": "A basic block is a small building block for a program. It is a sequence of instructions that has no branch calls except for at the end and no has no branch target apart from at the beginning. A goal of the compiler to make as many big basic blocks as it can as this is better for optimization and reusability.  \n<Callout type=\"example\">\nLet us compare to different assembler outputs for the same code and look at their basic blocks. Our code does the following:  \n```c\nint fact_while (int x) {\nint result = 1;\nwhile (x > 1) {\nresult *= x;\nx = x – 1;\n}\nreturn result;\n}\n```  \nIt is common to rewrite loops as goto commands when trying to convert high-level code to assembler code.  \n```c\nint fact_while (int x) {\nint result = 1;\nLoop:\nif (x <= 1) goto Exit;\nresult = result * x;\nx = x – 1;\ngoto Loop;\nExit:\nreturn result;\n}\n```  \n```assembly\nfact_while:\naddi a5, a0, 0          # a5 = x (x)\naddi a0, zero, 1        # a0 = 1 (result)\nLoop:\naddi a4, zero, 1        # a4 = 1\nble a5, a4, Exit        # if (x <= 1) goto Exit\nmul a0, a0, a5          # result *= x\naddi a5, a5, -1         # x = x – 1\nbeq zero, zero, Loop    # goto Loop\nExit:\n```  \nThe assembly code above has 3 small basic blocks but if we convert the C code to this structure we can decrease the amount of basic blocks and increase their size.  \n```c\nint fact_while2 (int x) {\nint result = 1;\nif (x <= 1) goto Exit;\nLoop:\nresult = result * x;\nx = x – 1;\nif (x != 1) goto Loop;\nExit:\nreturn result;\n}\n```  \n```assembly\nfact_while2:\naddi a5, a0, 0          # a5 = x (x)\naddi a4, zero, 1        # a4 = 1\naddi a0, zero, 1        # a0 = 1 (result)\nble a5, a4, Exit        # if (x <= 1) goto Exit\nLoop:\nmul a0, a0, a5          # result *= x\naddi a5, a5, -1         # x = x – 1\nbne a5, a4, Loop        # if (x != 1) goto Loop\nExit:\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Transfer Operations", "Header 2": "Target Adressing", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/controlTransfer.mdx"}, "page_content": "When jumping using the branch instructions most jumps are not very far. As mentioned before the label is like an immediate offset meaning it can be up to 12 bits long. If we want to jump further we can use one of the commands below. The `jal` instruction stands for jump and link, we jump using the passed offset which can now be 20 bits long. We also store the current PC i.e the return address into the corresponding `rd` register. If we want to jump even further then we can load a large immediate into a temporary register using the `lui` instruction and then add the remaining 12 bits and jump at the same time using the `jalr` instructions which also lets us read the offset from a register.  \n| Instruction                           | Type | Example                | Meaning                                                 |\n| ------------------------------------- | ---- | ---------------------- | ------------------------------------------------------- |\n| Jump and link                         | UJ   | `jal rd, imm20`        | `R[rd] = pc + 4; pc = pc + SignExt(imm20 << 1)`         |\n| Jump and link register                | I    | `jalr rd, imm12(rs1)`  | `R[rd] = pc + 4; pc = (R[rs1] + SignExt(imm12)) & (~1)` |  \n<Callout type=\"info\">\nWe can also use the `jal` instruction as an unconditional branch by using the zero register as the return address, which is the same as discarding it:  \n```assembly\njal x0, Label\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arithmetic and Logical Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/arithmeticLogical.mdx"}, "page_content": "Arithmetic and logical operations are some of the key building blocks for writing any program as almost any functionality boils down to them.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arithmetic and Logical Operations", "Header 2": "Arithmetic Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/arithmeticLogical.mdx"}, "page_content": "In RISC-V all arithmetic operations have the same form, two sources (b and c) and one destination (a). Later on, we will learn more forms of operations and also how these operations are encoded to machine code, i.e to binary digits.  \n```assembly\nadd x20, x21, x20\n```  \nThis is in aid of the first design principle of RISC-V  \n> Simplicity favors regularity.  \n<Callout type=\"example\">\nIf we have the following C code  \n```c\n// f in x19, g in x20, h in x21\n// i in x22, j in x23\nf = (g + h) – (i + j);\n```  \nand we compile it we can expect that the following RISC-V code will be assembled.  \n```assembly\nadd x5, x20, x21\nadd x6, x22, x23\nsub x19, x5, x6\n```  \nHere we make use of the temporary registers `x5` and `x6`.\n</Callout>  \nWe will see what the immediate instructions are for further down.  \n| Instruction                      | Type | Example                | Meaning                                     |\n| -------------------------------- | ---- | ---------------------- | ------------------------------------------- |\n| Add                              | R    | `add rd, rs1, rs2`     | `R[rd] = R[rs1] + R[rs2]`                   |\n| Subtract                         | R    | `sub rd, rs1, rs2`     | `R[rd] = R[rs1] – R[rs2]`                   |\n| Add immediate                    | I    | `addi rd, rs1, imm12`  | `R[rd] = R[rs1] + SignExt(imm12)`           |\n| Set less than                    | R    | `slt rd, rs1, rs2`     | `R[rd] = (R[rs1] < R[rs2])? 1 : 0`          |\n| Set less than immediate          | I    | `slti rd, rs1, imm12`  | `R[rd] = (R[rs1] < SignExt(imm12))? 1 : 0`  |\n| Set less than unsigned           | R    | `sltu rd, rs1, rs2`    | `R[rd] = (R[rs1] <u R[rs2])? 1 : 0`         |\n| Set less than immediate unsigned | I    | `sltiu rd, rs1, imm12` | `R[rd] = (R[rs1] <u SignExt(imm12))? 1 : 0` |\n| Load upper immediate             | U    | `lui rd, imm20`        | `R[rd] = SignExt(imm20 << 12)`              |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arithmetic and Logical Operations", "Header 2": "Arithmetic Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/arithmeticLogical.mdx"}, "page_content": "| Set less than immediate unsigned | I    | `sltiu rd, rs1, imm12` | `R[rd] = (R[rs1] <u SignExt(imm12))? 1 : 0` |\n| Load upper immediate             | U    | `lui rd, imm20`        | `R[rd] = SignExt(imm20 << 12)`              |\n| Add upper immediate to PC        | U    | `auipc rd, imm20`      | `R[rd] = PC + SignExt(imm20 << 12)`         |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arithmetic and Logical Operations", "Header 2": "Arithmetic Operations", "Header 3": "Immediate Operands", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/arithmeticLogical.mdx"}, "page_content": "We often find ourselves using constants when working with operations, research actually estimates that 80% of the instructions have some constant value. For example, incrementing is just adding 1 etc. As this is the most common case RISC-V offers so-called immediate operands that work with a constant as a source instead of a registry address. This is also in aid of one of their design principles.  \n> Make the common case fast.  \nImmediate operands are faster as they avoid a load instruction. However, due to the way instructions are encoded we can only use constants that use up to 12 bits. However, later on, we will see how we can work with larger constants.  \n```assembly\naddi x22, x22, 4\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arithmetic and Logical Operations", "Header 2": "Logical Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/arithmeticLogical.mdx"}, "page_content": "We also often find ourselves manipulating or working with bits which is what the logical operations are for. They are in most high-level programming languages the same with the most common exception being for the arithmetic or logical shift right operation. The key difference between these 2 is that the arithmetic version fills the left with zeros where as the logical version fills it with the sign bit resulting in a sign-extension to preserve the decimal value.  \n| Instruction                      | Type | Example               | Meaning                                 |\n| -------------------------------- | ---- | --------------------- | --------------------------------------- |\n| AND                              | R    | `and rd, rs1, rs2`    | `R[rd] = R[rs1] & R[rs2]`               |\n| OR                               | R    | `or rd, rs1, rs2`     | `R[rd] = R[rs1] | R[rs2]`               |\n| XOR                              | R    | `xor rd, rs1, rs2`    | `R[rd] = R[rs1] ^ R[rs2]`               |\n| AND immediate                    | I    | `andi rd, rs1, imm12` | `R[rd] = R[rs1] & SignExt(imm12)`       |\n| OR immediate                     | I    | `ori rd, rs1, imm12`  | `R[rd] = R[rs1] | SignExt(imm12)`       |\n| XOR immediate                    | I    | `xori rd, rs1, imm12` | `R[rd] = R[rs1] ^ SignExt(imm12)`       |\n| Shift left logical               | R    | `sll rd, rs1, rs2`    | `R[rd] = R[rs1] << R[rs2]`              |\n| Shift right arithmetic           | R    | `sra rd, rs1, rs2`    | `R[rd] = R[rs1] >> R[rs2] (arithmetic)` |\n| Shift right logical              | R    | `srl rd, rs1, rs2`    | `R[rd] = R[rs1] >> R[rs2] (logical)`    |\n| Shift left logical immediate     | I    | `slli rd, rs1, shamt` | `R[rd] = R[rs1] << shamt`               |\n| Shift right logical immediate    | I    | `srli rd, rs1, shamt` | `R[rd] = R[rs1] >> shamt (logical`      |\n| Shift right arithmetic immediate | I    | `srai rd, rs1, shamt` | `R[rd] = R[rs1] >> shamt (arithmetic)`  |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arithmetic and Logical Operations", "Header 2": "Logical Operations", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/arithmeticLogical.mdx"}, "page_content": "| Shift right logical immediate    | I    | `srli rd, rs1, shamt` | `R[rd] = R[rs1] >> shamt (logical`      |\n| Shift right arithmetic immediate | I    | `srai rd, rs1, shamt` | `R[rd] = R[rs1] >> shamt (arithmetic)`  |  \nIf we look at the RISC-V logical operations there isn't anything special apart from there not being a NOT operation. This is because it can be simply implemented by using the XOR operation which sets a bit to 1 if the bits are different and otherwise a 0. To be more precise we XOR with the value that only consists of positive bits to simulate a NOT operation. However, we will come across pseudo instructions where there will be a NOT operation.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arithmetic and Logical Operations", "Header 2": "Operations With Large Constants", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/arithmeticLogical.mdx"}, "page_content": "If we want to work with constants larger than 12 bits we need to do use the following instruction:  \n```assembly\nlui x19, 0x003D0\n```  \nThis instruction stands for load upper immediate and allows us to load the 20 most significant bits into a registry. The 12 remaining bits will be set to 0 but we can also set these by either adding or using an OR operation.  \n<Image\nsrc=\"/cs/archLoadUpperImmediate.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arithmetic and Logical Operations", "Header 2": "Assembly Optimization", "path": "../pages/digitalGarden/cs/computerArchitecture/riscV/arithmeticLogical.mdx"}, "page_content": "One of the main goals of a compiler is to optimize the program when writing the assembly code.  \n```c\n// x in a0, y in a1\nint logical (int x, int y) {\nint t1 = x ^ y;\nint t2 = t1 >> 17;\nint mask = (1 << 8) – 7;\nint rval = t2 & mask;\nreturn rval;\n}\n```  \n```assembly\nxor a0, a0, a1      # a0 = x ^ y (t1)\nsrai a0, a0, 17     # a0 = t1 >> 17 (t2)\nandi a0, a0, 249    # a0 = t2 & ((1 << 8) – 7)\n```  \nIn the above example, we can see that a few simple optimizations have been made:  \n- Because x is only needed once we can use its registry to store the result of the first line instead of having to use a separate temporary registry\n- The calculation of the mask only consists of constants, which means it can be calculated at runtime. This results in the last two statements being combined into one instruction.  \n```c\n// x in a0, y in a1, z in a2\nint arith (int x, int y, int z) {\nint t1 = x + y;\nint t2 = z + t1;\nint t3 = x + 4;\nint t4 = y * 48;\nint t5 = t3 + t4;\nint rval = t2 - t5;\nreturn rval;\n}\n```  \n```assembly\nadd a5, a0, a1      # a5 = x + y (t1)\nadd a2, a5, a2      # a2 = t1 + z (t2)\naddi a0, a0, 4      # a0 = x + 4 (t3)\nslli a5, a1, 1      # a5 = y * 2\nadd a1, a5, a1      # a1 = a5 + y\nslli a5, a1, 4      # a5 = a1 * 16 (t4)\nadd a0, a0, a5      # a0 = t3 + t4 (t5)\nsub a0, a2, a0      # a0 = t2 – t5 (rval)\n```  \nIn this example the assembly code is actually longer then the C code. However, it has been optimzed, length of code does not correspond to efficiency. To be more precise the multiplication has been optimized because multiplicaitons are very slow. So instead of multiplying the compiler tries to make use of bit shifts which are much fast. So `y * 48` becomes `(3y) << 4`. Another example of this would be replacint `7 * x` with `8 * x - x` which can be translated to `(x << 3) - x`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Template method", "path": "../pages/digitalGarden/cs/patterns/templateMethod.mdx"}, "page_content": "The intent of the template method pattern is to define a skeleton of an algorithm in the superclass but lets subclasses override specific steps of the algorithm without changing itsstructure.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Template method", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/templateMethod.mdx"}, "page_content": "```mermaid\nclassDiagram\nAbstractClass <|-- ConcreteClass1\nAbstractClass <|-- ConcreteClass2\nAbstractClass <|-- ConcreteClass3\nclass AbstractClass{\n+algorithm()\n+step1()\n+step2()\n}\nclass ConcreteClass1{\n+step1()\n+step2()\n}\nclass ConcreteClass2{\n+step1()\n+step2()\n}\nclass ConcreteClass3{\n+step2()\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Composite", "path": "../pages/digitalGarden/cs/patterns/composite.mdx"}, "page_content": "The intent of the composite pattern is to represent a recursive tree like structures where individual objects or compositions of objects should be treated uniformly. The most common example is your file structure you have folders and files and folders with files inside other folders etc. You can the perform operations like delete or move on either a single file or folder.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Composite", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/composite.mdx"}, "page_content": "```mermaid\nclassDiagram\nComponentInterface <--o Composite\nComponentInterface <|-- Leaf\nComponentInterface <|-- Composite\nclass ComponentInterface{\n+execute()\n}\nclass Leaf{\n+execute()\n}\nclass Composite{\n-children: Component[]\n+add()\n+remove()\ngetChildren()\nexecute() // delegates all work to children\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Composite", "Header 2": "Things to be aware of", "Header 3": "Placement of managing functions", "path": "../pages/digitalGarden/cs/patterns/composite.mdx"}, "page_content": "There are 2 possibilities to place the managing functions(add, remove etc.) either in the Component or in the composite. If you place them in the Composite (Safe) there is a clear separation of tasks and are only defined where they are usable, however you might have to make type casts. If you place them in the Component (Transparent) you have a unified look however have to provide the functionality for all components which might not necessarly make sense.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Composite", "Header 2": "Things to be aware of", "Header 3": "No cycles", "path": "../pages/digitalGarden/cs/patterns/composite.mdx"}, "page_content": "To make sure that one element is not in 2 composites or that there are cycles (a composite is in a higher composite but the higher composite is also in the lower composite). We can add a flag to the abstract class Component to solve this. When adding to a composite we can then make the following checks:  \n```java\npublic void addFigure(Figure f) {\nif (f.contained)\nthrow new IllegalArgumentException();\nif (contains(f, this)) {\nthrow new IllegalArgumentException();\n}\nfigures.add(f);\nf.contained = true;\n}\n\nprivate boolean contains(Figure g1, GroupFigure g2) {\nif (g1 == g2) {\nreturn true;\n} else if (g1 instanceof GroupFigure) {\nfor (Figure f : ((GroupFigure) g1).figures) {\nif (contains(f, g2))\nreturn true;\n}\n} return false;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strategy", "path": "../pages/digitalGarden/cs/patterns/strategy.mdx"}, "page_content": "The intent of the strategy pattern is to be able to define a family of algorithms that are interchangable and we can easly add more algorithms if needed. So we also want to be able to change behavior just like in the state pattern. For example we want to support multiple different de/encryption methods. If the algorithm only changes based in its parameters we are not speaking of the strategy pattern.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strategy", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/strategy.mdx"}, "page_content": "We can see that the Class Diagram is very similiar to that of the state pattern. Importantly here is that the interface is powerful enough to support all current algorithms and also those in the future.  \n```mermaid\nclassDiagram\nStrategyInterface <--o Context\nStrategyInterface <|-- VariantA\nStrategyInterface <|-- VariantB\nclass Context{\n}\nclass StrategyInterface{\nalgorithm()\n}\nclass VariantA{\nalgorithm()\n}\nclass VariantB{\nalgorithm()\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strategy", "Header 2": "Example", "path": "../pages/digitalGarden/cs/patterns/strategy.mdx"}, "page_content": "```java\npublic class SecureChannel{\npublic interface Algorithm{\npublic int[] encrypt(byte[] key, int[] plain);\npublic int[] decrypt(byte[] key, int[] encrypted);\n}\nprivate Algorithm algorithm;\npublic void setAlgorithm(Algorithm algorithm) {\nif (algorithm == null) throw new IllegalArgumentException();\nthis.algorithm= algorithm;\n}\npublic void send(byte[] key, int[] plain) {\nwrite(algorithm.encrypt(key, plain));\n}\npublic int[] receive(byte[] key) {\nreturn algorithm.decrypt(key, read());\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Prototype", "path": "../pages/digitalGarden/cs/patterns/prototype.mdx"}, "page_content": "The intent of the prototype pattern is to be able to create new objects by cloning/copying prototype objects.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Prototype", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/prototype.mdx"}, "page_content": "```mermaid\nclassDiagram\nPrototypeInterface <|-- ConcretePrototype\nConcretePrototype <|-- SubClassPrototype\nclass PrototypeInterface{\nclone()\n}\nclass ConcretePrototype{\nclone()\n}\nclass SubClassPrototype{\nclone()\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Prototype", "Header 2": "Solutions", "Header 3": "Cloning based on Object.clone()", "path": "../pages/digitalGarden/cs/patterns/prototype.mdx"}, "page_content": "In java the following clone method is defined.  \n```java\nclass Object {\nprotected Object clone() throws CloneNotSupportedException\n...\n}\n}\n```  \nThe protected visibility means it can not be invoked on objects of static type and can only be invoked if clone is overridden in a subclass with a suitable visibility.  \n1. It is checked whether the class implements interface Cloneable (which is only a marker interface). If this is not the case, a CloneNotSupportedException is thrown.\n2. A new instance is created, i.e. as much memory as used by the original object is allocated however no constructor is invoked.\n3. Instead, the memory of the original object is copied byte by byte into the new instance (a so called memory copy) this means that all attributes are copied over into the new instance if there are fields that are not value types like int etc. but Objects like string etc. then the references are copied resulting in a shallow copy.  \nIf you wish for it not to be a shallow copy you have to override the clone method and clone all the object attributes correctly. When overriding the clone method you can strengthen the result type i.e not returning Object but the correct type. Final fields will just be copied they can not be changed if this is needed then you need to use copy constructors.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Prototype", "Header 2": "Solutions", "Header 3": "Cloning based on copy constructors", "path": "../pages/digitalGarden/cs/patterns/prototype.mdx"}, "page_content": "Copy constructors are constructors that receive an instance of their own type as parameter. They then initialize the new instance with the same values as the prototype passed.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Prototype", "Header 2": "Solutions", "Header 3": "Cloning based on serialization", "path": "../pages/digitalGarden/cs/patterns/prototype.mdx"}, "page_content": "If all the classes to be cloned are serializable (implement the Serializable interface), cloning can also be implemented with the help of Java serialization. The clone method which follows this approach looks as follows  \n```java\nObject clone() {\nByteArrayOutputStream baos = new ByteArrayOutputStream();\nObjectOutputStream oos = new ObjectOutputStream(baos);\noos.writeObject(this);\noos.close();\nbyte buf[] = baos.toByteArray();\nByteArrayInputStream bais = new ByteArrayInputStream(buf);\nObjectInputStream ois = new ObjectInputStream(bais);\nObject c = ois.readObject();\nreturn c;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "SOLID", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "The SOLID principles are a set of five design principles that help software developers create more maintainable,\nflexible, and robust code. They were frist introduced by Robert C. Martin \"Uncle Bob\" in his book \"Agile Software\nDevelopment, Principles, Patterns, and Practices\".", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "SOLID", "Header 3": "Single Responsibility Principle", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "A class/method should only have a single purpose/responsibility and therefore only one reason to change.  \n<Image\nsrc=\"/cs/patternsSolidSingleResponsibility.png\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "SOLID", "Header 3": "Open/Closed Principle", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "Classes should be open for extension but closed for modification. To extend the behavior, new code should be added however old code should not have to be modified. This then prevents situations in which a change to classes also requires adaption of all depending classes. This is achieved with interfaces which allow different implementations by keep the same API.  \n<Image\nsrc=\"/cs/patternsSolidOpenClose.png\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "SOLID", "Header 3": "Liskov Substitution Principle", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "Subtypes should be substitutable for their base types.  \n<Image\nsrc=\"/cs/patternsSolidLiskov.png\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "SOLID", "Header 3": "Interface Segregation Principle", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "Make fine-grained interfaces that are client specific instead of general purpose interfaces (Which slightly contradicts the strategy pattern). Clients should not be forced to depend on methods that they do not use.  \n<Image\nsrc=\"/cs/patternsSolidInterface.png\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "SOLID", "Header 3": "Dependency Inversion Principle", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "Depend on Abstractions not on concrete classes etc.  \n<Image\nsrc=\"/cs/patternsSolidDependencyInversion.png\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "Other Good Coding Principles", "Header 3": "Favor Composition over Inheritance", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "By using composition for example in the strategy pattern instead of inheritance it allows us to be flexible at runtime.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "Other Good Coding Principles", "Header 3": "Program to an interface, not and implementation", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "Avoid referencing concrete classes, declare interfaces instead as then implementations are easily switched out.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "Other Good Coding Principles", "Header 3": "Encapsulate what varies", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "By encapsulating/hiding the parts that can vary for example implementations behind interfaces we can minimize the impact\nof that code because thanks to the interface we have a unified API.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "OOP Design Principles", "Header 2": "Other Good Coding Principles", "Header 3": "KISS", "path": "../pages/digitalGarden/cs/patterns/oopDesignPrinciples.mdx"}, "page_content": "Keep it simple stupid. The simpler the code the easier it is to maintain and understand.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Factory", "path": "../pages/digitalGarden/cs/patterns/factory.mdx"}, "page_content": "Factory patterns are so called creational patterns meaning their intent is to abstract/hide how objects are created. Which in return allows the client to be in independent of how its objects are created.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Factory", "Header 2": "Factory method", "path": "../pages/digitalGarden/cs/patterns/factory.mdx"}, "page_content": "The factory method pattern delegates the instantiation of objects to a method in either subclasses or a static method. The disadvantage of having the factory method as a static method is it can not be subclassed to change the behavior however you don't need to create an object to make use of the method.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Factory", "Header 2": "Factory method", "Header 3": "Structure", "path": "../pages/digitalGarden/cs/patterns/factory.mdx"}, "page_content": "```mermaid\nclassDiagram\nCreator <|-- ConcreteCreator\nCreator --> ProductInterface\nProductInterface <|-- ConcreteProduct\nclass Creator{\n+someOperation()\n+createProduct(): Product\n}\nclass ProductInterface{\n}\nclass ConcreteProduct{\n}\nclass ConcreteCreator{\n+createProduct(): Product\nreturns a ConcreteProduct\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Factory", "Header 2": "Factory method", "Header 3": "Example", "path": "../pages/digitalGarden/cs/patterns/factory.mdx"}, "page_content": "<Image\nsrc=\"/cs/patternsFactoryMethod.png\"\ncaption=\"An example of the factory method pattern for a pizza store.\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Factory", "Header 2": "Abstract factory", "path": "../pages/digitalGarden/cs/patterns/factory.mdx"}, "page_content": "The factory method pattern delegates the instantiation of object familys to a another object.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Factory", "Header 2": "Abstract factory", "Header 3": "Structure", "path": "../pages/digitalGarden/cs/patterns/factory.mdx"}, "page_content": "<Image\nsrc=\"/cs/patternsAbstractFactoryStructure.png\"\ncaption=\"Structure of the abstract factory pattern.\"\nwidth={800}\n/>  \nA big question here is where is the concrete Factory so they can all have acces to it. Often this is done in it's own class  \n```java\npublic class CurrentFactory {\nprivate CurrentFactory() { }; // prevents instantiation\nprivate static Factory fac = null;\npublic static Factory getFactory() { return fac; }\npublic static void setFactory(Factory f) {\nif (f == null) throw new NullPointerException();\nfac = f;\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Factory", "Header 2": "Abstract factory", "Header 3": "Example", "path": "../pages/digitalGarden/cs/patterns/factory.mdx"}, "page_content": "<Image\nsrc=\"/cs/patternsAbstractFactory.png\"\ncaption=\"An example of the abstract factory pattern for a pizza store.\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Iterators", "path": "../pages/digitalGarden/cs/patterns/iterator.mdx"}, "page_content": "Often times you find yourself traversing/iterating over a [collection](../algorithmsDataStructures/collections). The iterator pattern encapsulates this functionality without exposing its underlying representation of the collection. In java this pattern is used to implement the enhanced for or also called for-each loop. Under the hood all the for each loop does is get an iterator instance and iterate over it step by step, this is also why you can only use the for-each loop on collections that actually implement the [`Iterable<T>`](https://docs.oracle.com/javase/8/docs/api/java/lang/Iterable.html) interface.  \nWhy is this pattern useful?  \n- Because depending on the collection there might be different ways to traverse it, for example [binary trees can be traversed in many orders](../algorithmsDataStructures/trees/binaryTrees#traversal-orders) but the methods you call to traverse the collection should be the same. We want a common interface for traversing different collections in different ways.\n- Also, because the iterator object holds all the details regarding the traversal of the collection, several iterators can go through the same collection at the same time, independently of each other as long as they don't change the underlying collection, if they do it gets a bit complicated with [mutexes](../concurrentProgramming/locking#locks).  \n<Image\nsrc=\"/cs/patternsIteratorTreeExample.png\"\ncaption=\"How different iterators can traverse a binary tree.\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Iterators", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/iterator.mdx"}, "page_content": "The `Iterable<T>` interface defines the method `Iterator<T> iterator()`, or `Iterator<T> createIterator()`which is responsible for creating and returning the [iterator interface](https://docs.oracle.com/javase/8/docs/api/java/util/Iterator.html) that can then be used to traverse the collection containing elements of type `T`.  \nAn iterator always holds the value of the next element, apart from at the beginning of an iteration, where it holds a reference to the first element. In java the iterator instance returned must implement the `Iterator<T>` interface which declares the operations required for traversing the collection.  \n<Callout type=\"todo\">\nAn animation of the iterator moving through a linked list would be cool.\n</Callout>  \n<Callout type=\"info\">\nWhen implementing an iterator it is recommended to do so in an internal private final class in the collection class as you then have access to the internal structure of the collection without having to make all the details public.\n</Callout>  \nThe pattern then has an overall structure that can look something like this:  \n```plantuml\n@startuml\n!theme purplerain from https://raw.githubusercontent.com/LuciferUchiha/nextra-garden/main\ninterface \"Iterable<T>\" as Iterable {\nIterator<T> iterator()\n}\n\ninterface \"Iterator<T>\" as Iterator {\nT next()\nboolean hasNext()\n}\nclass \"ConcreteIterator<T>\" as ConcreteIterator {\nT next()\nboolean hasNext()\n}\nclass \"Collection<T>\" as Collection {\nIterator<T> iterator()\n}\n\nCollection --|> Iterable\nConcreteIterator --|> Iterator\nCollection --> ConcreteIterator\nIterable --> Iterator\n@enduml\n```  \n- The `T next()` method returns the element the iterator is currently pointing to and advances the iterator to the next element.\n- The `boolean hasNext()` method returns false once the iterator has reached the end of the collection, otherwise true.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Iterators", "Header 2": "ListIterator", "path": "../pages/digitalGarden/cs/patterns/iterator.mdx"}, "page_content": "In Java, there is also the [`ListIterator<T>`](https://docs.oracle.com/javase/8/docs/api/java/util/ListIterator.html) interface which extends the `Iterator<T>` interface. This interface adds functionality that allows for iteration in both directions with `T next()` and `T previous()`. Matchingly it also offers a `boolean hasPrevious()` to the `boolean hasNext()` method. One can imagine that ListIterator has no current element, its position is always between the element that would be returned by a call to `previous()` and the element that would be returned by a call to `next()`.  \nBecause the iterator traverses a collection like a list it also works and assigns an index to each element which can be fetched.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Iterators", "Header 2": "Removing Elements", "path": "../pages/digitalGarden/cs/patterns/iterator.mdx"}, "page_content": "The iterator interface also defines an optional `void remove()` method, that removes the most recently returned element from the iterator.  \n<Image\nsrc=\"/cs/patternsIteratorRemove.png\"\ncaption=\"How the remove method works in an iterator.\"\nwidth={700}\n/>  \nHowever, when removing elements whilst other iterators are also traversing the collection, for example in a concurrent program some problems can occur, mainly in consistent state and iterator pointers getting cut of from iterating further. One way of solving these problems is by using a modification counter (modCount) which is incremented whenever the underlying collection is changed, for example when adding or removing an element. When an iterator is instantiated the modCount is copied and continuously checked if it is the same as the underlying modCount of the collection if not then a `ConcurrentModificationException` is thrown. How this works for different collections in java is [explained here](https://stackoverflow.com/a/5847949/10994912).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Iterators", "Header 2": "Implementation", "path": "../pages/digitalGarden/cs/patterns/iterator.mdx"}, "page_content": "A possible implementation for an iterator over a [linked list](../algorithmsDataStructures/linkedLists) could look like this:  \n```java\n\nclass SomeCollection<T> implements Iterable<T> {\n\n//implementation of other functions and internal state\n\n@Override\npublic Iterator <T> iterator() {\nreturn new MyIterator();\n}\n\nprivate final class MyIterator implements Iterator<T> {\n// p and pp keeps track of the previous elements to be able to remove\nprivate Node<T> next = first, p = null, pp = null;\nprivate int myModCount = modCount; // copy the modCount of the collection\nprivate boolean mayRemove = false;\n\n@Override\npublic boolean hasNext() {\nreturn next != null;\n}\n\n@Override\npublic T next() {\nif (modCount != myModCount)\nthrow new ConcurrentModificationException();\nif (next == null)\nthrow new NoSuchElementException();\nT elem = next.elem;\nif (p != null) pp = p;\np = next;\nnext = next.next;\nmayRemove = true;\nreturn elem;\n}\n\n@Override\npublic void remove() {\nif (modCount != myModCount)\nthrow new ConcurrentModificationException();\nif (!mayRemove)\nthrow new IllegalStateException();\nif (pp != null) pp.next = next;\nelse first = next;\nif (next == null) last = pp;\np = pp;\nmayRemove = false;\nsize--;\nmodCount++;\nmyModCount = modCount;\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Singleton", "path": "../pages/digitalGarden/cs/patterns/singleton.mdx"}, "page_content": "The intent of the singleton pattern is that a class only has a single instance which is accessed over a global point. For example config classes should only exist once. You can do this by making the class final to stop inheritance, by making the constructor private so no instance can be created and adding a static method that creates the one instance if not already and returns it. It should either not support cloning or return the same instance(this).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Singleton", "Header 2": "Things to be aware of", "Header 3": "Eager and lazy initialization", "path": "../pages/digitalGarden/cs/patterns/singleton.mdx"}, "page_content": "Eager means the instance is created as soon as the class is first initialized form for example other methods in the class which could use a lot of memory allthough it is not needed.  \n```java\npublic final class Singleton {\nprivate Singleton(){}\nprivate static Singleton instance = new Singleton();\npublic static Singleton getInstance(){}\nreturn instance;\n}\n}\n```  \nLazy means it is created when the getInstance functions is accessed this can however cause issues with multithreading which is why you need to synchronize the method.  \n```java\npublic final class Singleton {\nprivate Singleton(){}\nprivate static Singleton instance = null;\npublic static synchronized Singleton getInstance(){\nif(instance == null) instance = new Singleton();\nreturn instance;\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Singleton", "Header 2": "Things to be aware of", "Header 3": "Garbage collection", "path": "../pages/digitalGarden/cs/patterns/singleton.mdx"}, "page_content": "Instance cannot be reclaimed by the garbage collector as they are static. So you should either use `WeakReference<Singelton>`(removed when not referenced by strong references) or `SoftReference<Singelton>`(removed when system is short of memory).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Singleton", "Header 2": "Things to be aware of", "Header 3": "Serialization", "path": "../pages/digitalGarden/cs/patterns/singleton.mdx"}, "page_content": "Deserialization of a serialized singleton instance may lead to several singleton instances to avoid this we can do the following  \n```java\npublic final class Singleton implements Serializable {\nprivate Singleton(){ }\nprivate static Singleton instance = null;\npublic static synchronized Singleton getInstance(){\nif(instance == null) instance = new Singleton();\nreturn instance;\n}\npublic Object readResolve(){\nreturn getInstance();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Singleton", "Header 2": "Things to be aware of", "Header 3": "Singelton with Demand Holder Idiom", "path": "../pages/digitalGarden/cs/patterns/singleton.mdx"}, "page_content": "You can also create a Singelton the following way. This solution is thread-safe and is lazy. Important is that the construction of Singelton does not fail (exceptions).  \n```java\npublic class Singleton {\nprivate Singleton() {}\n\nprivate static class LazyHolder {\nstatic final Singleton INSTANCE = new Singleton();\n}\n\npublic static Singleton getInstance() {\nreturn LazyHolder.INSTANCE;\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Singleton", "Header 2": "Things to be aware of", "Header 3": "Singelton with Enum", "path": "../pages/digitalGarden/cs/patterns/singleton.mdx"}, "page_content": "You can also create a Singelton with an Enum. It is easy, thread safe and provides the Serialization for free. However it can not be extended to multiple instances and the fields can not be serialized.  \n```java\npublic enum SingletonDriver implements Driver {\nINSTANCE;\npublic String toString () { return \"Singleton \";\npublic void playSong(File file ) { ... }\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Command", "path": "../pages/digitalGarden/cs/patterns/command.mdx"}, "page_content": "The intent of the command pattern is to turn commands into stand-alone objects so that the object invoking the command does not need to worry about how the command is done. By doing so you can delay, queue, undo commands.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Command", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/command.mdx"}, "page_content": "```plantuml\n@startuml\n!theme purplerain from https://raw.githubusercontent.com/LuciferUchiha/georgerowlands.ch/main\n\nCommand <-- Invoker  : calls\nCommand <|-- CommandImpl\nReceiver <-- CommandImpl : calls\n\nclass Invoker {\nvoid setCommand(Command c)\nvoid execute()\n}\n\ninterface Command{\nvoid execute()\nvoid undo()\n}\n\nclass Receiver{\naction()\n}\n\n@enduml\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Command", "Header 2": "Example", "path": "../pages/digitalGarden/cs/patterns/command.mdx"}, "page_content": "```java\n// Client\npublic class CommandPatternDemo {\npublic static void main(String[] args) {\nStock abcStock = new Stock();\n\nBuyStock buyStockOrder = new BuyStock(abcStock);\nSellStock sellStockOrder = new SellStock(abcStock);\n\nBroker broker = new Broker();\nbroker.takeOrder(buyStockOrder);\nbroker.takeOrder(sellStockOrder);\n\nbroker.placeOrders();\n}\n}\n// Command Interface\npublic interface Order {\nvoid execute();\n}\n// Receiver\npublic class Stock {\nprivate String name = \"ABC\";\nprivate int quantity = 10;\n\npublic void buy(){\nSystem.out.println(\"Stock [ Name: \"+name+\",\nQuantity: \" + quantity +\" ] bought\");\n}\npublic void sell(){\nSystem.out.println(\"Stock [ Name: \"+name+\",\nQuantity: \" + quantity +\" ] sold\");\n}\n}\n// ConcreteCommands\npublic class BuyStock implements Order {\nprivate Stock abcStock;\n\npublic BuyStock(Stock abcStock){\nthis.abcStock = abcStock;\n}\n\npublic void execute() {\nabcStock.buy();\n}\n}\npublic class SellStock implements Order {\nprivate Stock abcStock;\n\npublic SellStock(Stock abcStock){\nthis.abcStock = abcStock;\n}\n\npublic void execute() {\nabcStock.sell();\n}\n}\n// Invoker\npublic class Broker {\nprivate List<Order> orderList = new ArrayList<Order>();\n\npublic void takeOrder(Order order){\norderList.add(order);\n}\n\npublic void placeOrders(){\nfor (Order order : orderList) {\norder.execute();\n}\norderList.clear();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decorator", "path": "../pages/digitalGarden/cs/patterns/decorator.mdx"}, "page_content": "The intent of the decorator pattern is to give objects new responsibilities without overusing inheritance and therefore creating a bunch of classes. Instead Components get decorated to further enhance objects.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decorator", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/decorator.mdx"}, "page_content": "```mermaid\nclassDiagram\nComponent <|-- IDecorator\nComponent <|-- ConcreteComponent\nIDecorator <|-- ConcreteDecoratorA\nIDecorator <|-- ConcreteDecoratorB\nComponent <-- IDecorator\nclass Component {\n+operationA()\n+operationB()\n}\nclass IDecorator {\nComponent wrappedObj\n+operationA()\n+operationB()\n}\nclass ConcreteDecoratorA{\nObject newState // can extend state\n+operationA()\n+operationB()\n}\n\nclass ConcreteDecoratorB{\n+operationA()\n+operationB()\n+newBehavior() // can add new\n}\n\nclass ConcreteComponent {\n+operationA()\n+operationB()\n}\n```  \nWith this structure you can do things in the decorator before after calling wrappedObj.operationA().", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decorator", "Header 2": "Example", "path": "../pages/digitalGarden/cs/patterns/decorator.mdx"}, "page_content": "```java\npublic abstract class Beverage {\nString description = \"Unknown Beverage\";\n\npublic String getDescription() {\nreturn description;\n}\n\npublic abstract double cost();\n}\npublic abstract class CondimentDecorator extends Beverage {\npublic abstract String getDescription();\n}\npublic class Espresso extends Beverage {\n\npublic Espresso() {\ndescription = \"Espresso\";\n}\n\npublic double cost() {\nreturn 1.99;\n}\n}\npublic class Milk extends CondimentDecorator {\nBeverage beverage;\n\npublic Milk(Beverage beverage) {\nthis.beverage = beverage;\n}\n\npublic String getDescription() {\nreturn beverage.getDescription() + \", Milk\";\n}\n\npublic double cost() {\nreturn .10 + beverage.cost();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Observer", "path": "../pages/digitalGarden/cs/patterns/observer.mdx"}, "page_content": "The intent of the Observer Pattern is that there a dependant objects of another object and that these dependent objects can be notified of changes without the other object knowing of the dependent objects and the connection between the the cooperating objects being to tight. The Observer Pattern is also commonly know as listener or Publish-Subscribe pattern.  \nYou can find a good detailed description [here](https://refactoring.guru/design-patterns/observer)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Observer", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/observer.mdx"}, "page_content": "```mermaid\nclassDiagram\nSubjectInterface <|-- ConcreteSubject\nSubjectInterface --> ObserverInterface\nObserverInterface <|-- ConcreteObserver\nConcreteObserver --> ConcreteSubject\nclass SubjectInterface{\n+registerObserver()\n+removeObserver()\n+notifyObservers()\n}\nclass ConcreteSubject{\n-state\n+registerObserver()\n+removeObserver()\n+notifyObserver()\n+getState()\n+setState()\n}\nclass ObserverInterface{\n+update()\n}\nclass ConcreteObserver{\n-observedObject\n+update()\n}\n```  \nOften instead of a interface for subject abstract classes are used as the behaviour for the 3 functions is always the same and you can add the data structure for storing the observers.  \nIn java you can use the provided `java.util.Observable` and `java.util.Observer` but is often not recommended.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Observer", "Header 2": "Things to be aware of", "Header 3": "How to notify", "path": "../pages/digitalGarden/cs/patterns/observer.mdx"}, "page_content": "There are a few ways to notify the observers.  \nFirstly there is the question of when to notify observers. Either they are notified when the `setState` function is called which is in most cases the way to go and the easiest to implement however it can lead to lots of updates. Or you can explicitly call the update function when you think it is needed, you just need to make sure you don't forget.  \nThen there is the question of what should a notification look like. We can differentiate between 2 models, push and pull. With pull the object is just notified that there has been a change and then has to get (pull) the actually changes itself.  \n- `update()` without any parameters.  \nOr there is the push option where you tell the observer what and where has been changed. This can be done in multiple ways.  \n- `update(Subject s, Color c)` with sender and/or the exact data that changed. Is easy but can be hard when multiple things changed.\n- `update(Subject s, Object args)` with sender and/or Object containing the the changes or the new state, as often seen in C#.\n- `update(Event e)` an event object that contains everything as seen in JavaFx or Swing.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Observer", "Header 2": "Things to be aware of", "Header 3": "Other small things", "path": "../pages/digitalGarden/cs/patterns/observer.mdx"}, "page_content": "There is clearly loose coupling between the subject and the observer as the subject does not know any concrete observers only the interface.  \nA notification is broadcast, meaning it is sent to all registered observers. It is then up to the observer how it handles notifications.  \nA simple change of the subjects state can cause a cascade of updates and therefore then multiple notifications.  \nYou must be aware careful that you do not create infinite loops by changing the state in the update function as this will cause the observer to be notified again.  \nIt is good practice to make sure the state really has changed before notifying all the observers and to also not allow the same listener to be added multliple times.  \nIn java you can use a `CopyOnWriteArrayList` as the data structure as it can happen that observers detach in the update method which would change the loop that you iterate over in `notifyObservers()`", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Observer", "Header 2": "Example", "path": "../pages/digitalGarden/cs/patterns/observer.mdx"}, "page_content": "This example is without the Subject interface however if could be added especially if there will be multiple subjects.  \n```java\npublic class NewsAgency{\nprivate String news;\nprivate List<Observer> channels = new ArrayList<>();\n\npublic void addObserver(Observer channel) {\nthis.channels.add(channel);\n}\n\npublic void removeObserver(Observer channel) {\nthis.channels.remove(channel);\n}\n\npublic void setNews(String news) {\nthis.news = news;\nfor (Observer channel : this.channels) {\nchannel.update(this.news);\n}\n}\n}\n\npublic class NewsChannel implements Observer {\nprivate String news;\n\n@Override\npublic void update(Object news) {\nthis.setNews((String) news);\n}\n}\n\npublic interface Observer {\npublic void update(Object o);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "State", "path": "../pages/digitalGarden/cs/patterns/state.mdx"}, "page_content": "The intent of the state pattern is that we can alter a programs behaviour (just like the strategy pattern) when its internal state changes and outsource the state-dependent behavior. A common example is some sort of dispenser machine that has multiple different states and depending on actions changes its state. For example a ticketmachine can be in the state \"INIT\" and when the action \"enterMoney\" is executed the machine changes to the state \"MONEY_ENTERED\".", "type": "Document"}
{"id": null, "metadata": {"Header 1": "State", "Header 2": "Structure", "path": "../pages/digitalGarden/cs/patterns/state.mdx"}, "page_content": "```mermaid\nclassDiagram\nStateInterface <-- Context\nStateInterface <|-- ConcreteStateA\nStateInterface <|-- ConcreteStateB\nclass Context{\nrequest() = currentState.handle()\n}\nclass StateInterface{\nhandle()\n}\nclass ConcreteStateA{\nhandle()\n}\nclass ConcreteStateB{\nhandle()\n}\n```  \nThe hardest part of implementing the state pattern is defining the state interface. The easiest way to do so is to draw a state diagram of the system. All the actions are then the methods in the interface and all the states are the concreteStates.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "State", "Header 2": "Things to be aware of", "Header 3": "State Transition", "path": "../pages/digitalGarden/cs/patterns/state.mdx"}, "page_content": "There are a few ways how state transition can be done.  \nDecentralized = may be initiated by state objects. For that the state must know its succesors, needs access to a state transition method in the context `context.setState(s);` or return the new state which is then set by the context.  \nParameterized = may be signaled by state, executed by context i.e by returning a key e.g a string or int. Association between, keys and states is held in the context.  \nCentralized = initiated by the context, state should be informed if it is activated or deactivated.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "State", "Header 2": "Creation of state objects", "path": "../pages/digitalGarden/cs/patterns/state.mdx"}, "page_content": "Created when needed = `c.setState(new StateB());` when state changes are rare.\nCreation ahead of time = `c.setState(c.STATE_B);` states have to be stored in context.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "State", "Header 2": "Example", "path": "../pages/digitalGarden/cs/patterns/state.mdx"}, "page_content": "```java\npublic class TicketMachine{\nprivate int destination;\nprivate boolean firstClass, dayTicket, halfPrice;\nprivate double price, enteredMoney;\nprivate interface State {\nvoid setDestination(intdestination);\nvoid setFirstClass(booleanfirstClass);\nvoid setDayTicket(booleandayTicket);\nvoid setHalfPrice(booleanhalfPrice);\nvoid enterMoney(double amount);\nvoid cancel();\n}\nprivate final State INIT = new StateInit();\nprivate final State DEST_SELECTED = new StateDestSelected();\nprivate final State MONEY_ENTERED = new StateMoneyEntered();\nprivate State state = INIT;\n\npublic void enterMoney(double amount) {\nstate.enterMoney(amount);\n}\n// etc...\nabstract class AbstractState implements State {\npublic void setDestination(intdestination) {\nthrow new IllegalStateException(); }\npublic void setFirstClass(booleanfirstClass) {\nthrow new IllegalStateException(); }\npublic void setDayTicket(booleandayTicket) {\nthrow new IllegalStateException(); }\npublic void setHalfPrice(booleanhalfPrice) {\nthrow new IllegalStateException(); }\npublic void enterMoney(double amount) {\nthrow new IllegalStateException(); }\npublic void cancel() { state = INIT; }\n}\nclass StateDestSelected extends AbstractState{\npublic void setFirstClass(boolean fc) {\nfirstClass= fc;\nprice = calculatePrice(destination, firstClass);\n}\npublic void enterMoney(double amount) {\nstate= MONEY_ENTERED; state.enterMoney(amount);\n}\n}\nclass StateMoneyEntered extends AbstractState{\npublic void enterMoney(double amount) {\nenteredMoney += amount;\nif (enteredMoney>= price) {\nprintTicketWithChange(destination, price, firstClass);\nstate = INIT;\n}\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fluent in Python", "path": "../pages/digitalGarden/cs/python/fluent.mdx"}, "page_content": "Some key notes and takeaways from the book Fluent Python by Luciano Ramalho.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fluent in Python", "Header 2": "Chapter 1: The Python Data Model", "path": "../pages/digitalGarden/cs/python/fluent.mdx"}, "page_content": "- Make use of it, it's there for a reason. It make implementing collections and classes easier.\n- Implement the special/magic/dunder methods to make your classes behave like built-in types.\n- Don't call special methods directly, use built-in functions instead like `len()`, `iter()`, `str()`, etc. rather than\n`obj.__len__()`, `obj.__iter__()`, `obj.__str__()`, etc.\n- collections.namedtuple is a great way to create a class that is just a collection of attributes.\n- ABC = Abstract Base Class\n- `__repr__` is for developers, `__str__` is for end users. If you only implement one, implement `__repr__`.\n- By default custom classes are truthy, unless you implement `__bool__` or `__len__` and return `False` or `0` respectively.\n- For numpy and some built-in types like list or str that are implemented in C, `__len__` is a C function that returns\nthe value of the `ob_size` field in the `PyObject` struct that represents any variable in the CPython implementation.\nThis is done for performance reasons. Is it important to know this? Probably not, but it's interesting.  \n```python\nimport collections\n\nCard = collections.namedtuple('Card', ['rank', 'suit'])\n\nclass FrenchDeck:\nRANKS = [str(n) for n in range(2, 11)] + list('JQKA')\nSUIT_VALUES = {'Spades': 3, 'Hearts': 2, 'Diamonds': 1, 'Clubs': 0}\n\ndef __init__(self):\n# cartesian product of ranks and suits using listcomps\nself._cards = [Card(rank, suit) for suit in self.SUIT_VALUES\nfor rank in self.RANKS]\n\ndef __len__(self):\nreturn len(self._cards)\n\ndef __getitem__(self, position):\nreturn self._cards[position]\n\ndef card_value(self, card):\nrank_value = self.RANKS.index(card.rank)\nreturn rank_value * len(self.SUIT_VALUES) + self.SUIT_VALUES[card.suit]\n\ndeck = FrenchDeck()\nsorted_deck = sorted(deck, key=deck.card_value)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fluent in Python", "Header 2": "Chapter 2: An Array of Sequences", "path": "../pages/digitalGarden/cs/python/fluent.mdx"}, "page_content": "- Listcomps are faster than map and filter, and more readable than the equivalent for loop.\n- Generator expressions are memory efficient and can be used as function arguments. `tuple(ord(symbol) for\nsymbol in symbols)`\n- Tuples are immutable, but the objects they contain may be mutable!\n- Tuples don't need to be seen as immutable lists, they can be used as records with no field names.\n- Unpacking can be used to swap variables `a, b = b, a` or to discard values `_, b = (1, 2)` or to split a list into\nhead and tail `head, *tail = [1, 2, 3, 4]` or return multiple values from a function `return a, b`.\n- The `*` operator can be used to grab excess items, kind of a wild card.\n- Pattern matching is cool and generally simple. But some patterns can be complex and hard to read.\n- `collections.deque` is a great data structure for implementing a queue. It's a doubly linked list with O(1) time\ncomplexity for adding or removing items from either end. Where as a list needs to shift all the items.\n- `collections.deque` can be used to implement a bounded queue i.e. fixed size queue by passing a `maxlen` argument to\nthe constructor.\n- slices are objects and can be used as arguments to functions and stored in variables.\n- lists can be manipulated in place using slice assignment `l[2:5] = [20, 30]` or `del l[5:7]`.\n- Memory views seem complicated and I don't really understand them yet.\n- `array.array` is a great way to store a large number of numerical values. It's more efficient than a list of ints\nbut can only store one type of value.\n- there is also the bisect module which provides binary search and insertion into sorted sequences which can be handy.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "path": "../pages/digitalGarden/cs/distributedSystems/actorModel.mdx"}, "page_content": "The actor model is not just good for concurrent programming but also for distributed systems as the actors can be on different systems and still communicate with each other.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Java Akka", "path": "../pages/digitalGarden/cs/distributedSystems/actorModel.mdx"}, "page_content": "We will be using the Akka framework again but instead of using the [Scala version of Akka](../Concurrent%20Programming/13-actorModel.md) we will be using the java version which is very similar.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Java Akka", "Header 3": "Creating an Actor", "path": "../pages/digitalGarden/cs/distributedSystems/actorModel.mdx"}, "page_content": "In Java, actors extend the AbstractClass and must implement the `Receive createReceiver()` method. The following actor discards all received messages because no matching is done.  \n```java\npublic class PrintActor extends AbstractActor {\n@Override\npublic Receive createReceiver() {\nreturn receiveBuilder().build();\n}\n}\n```  \nTo react to messages we can use pattern matching like the example below:  \n```java\npublic class PrintActor extends AbstractActor {\nprivate int cnt = 0;\n@Override\npublic Receive createReceive() {\nreturn receiveBuilder().matchAny(t -> onReceive(t)).build();\n}\nprivate void onReceive(Object msg) {\ncnt++;\nif (msg instanceof String) {\nSystem.err.println(cnt + \": received message \" + msg);\n} else {\nSystem.err.println(cnt + \": received unknown message\");\n}\n}\n}\n```  \nThe actual actor objects are then created and started asychnronsly by using the actor system. Just like in Scala when trying to create and actor object using `new` an `ActorInitializationException` is thrown.  \n```java\npublic static void main(String[] args) throws Exception {\nActorSystem as = ActorSystem.create();\nActorRef actor = as.actorOf(\nProps.create(PrintActor.class),\n\"Printer\" // name is optional and must be unique\n); // returns an immutable reference\n// ActorRef print = as.actorOf(Props.create(PrintActor.class, \"Msg:\")); For non default constructor actors\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Java Akka", "Header 3": "Sending Messages", "path": "../pages/digitalGarden/cs/distributedSystems/actorModel.mdx"}, "page_content": "In the scala version of Akka messages could be sent using the tell operator `!`. However, this is not possible in Java for syntax reasons so instead, messages can be sent to an actor by calling a member method on the receiving actor. Just like in Scala the message is guaranteed to be delivered at most once.  \n```java\nreceivingActor.tell(msg, ActorRef.noSender()) // ActorRef.noSender() is same as null\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Java Akka", "Header 3": "Receiving Messages", "path": "../pages/digitalGarden/cs/distributedSystems/actorModel.mdx"}, "page_content": "In Java there are the following methods that can be used for pattern matching:  \n- `matchAny(UnitApply<Object> apply`) Matches any argument.\n– `match(Class<P> type, UnitApply<P> apply)` Matches an argument of a particular type.\n– `match(Class<P> type, TypedPredicate<P> p, UnitApply<P> app)` Matches an argument of a particular type that matches a given predicate.  \nFor example:  \n```java\npublic Receive createReceive() {\nreturn receiveBuilder()\n.match(String.class,\ns -> s.startsWith(\"MSG:\"),\nmsg -> System.err.println(cnt++ + \": received message \" + msg\n));\n}\n```  \nThe above functions also allow you to then do pattern matching very similarly to pattern matching in scala using case classes (in Java records).  \n```java\npublic Receive createReceive() {\nreturn receiveBuilder()\n.match(LoginMessage.class, msg -> {\nString username = msg.username();\nsessions.put(username, getSender());\nbroadcastMessage(username, \"I just logged in\");\n})\n.match(TextMessage.class, msg -> {\nbroadcastMessage(msg.username(), msg.message());\n})\n.match(LogoutMessage.class, msg -> {\nString username = msg.username();\nsessions.remove(username);\nbroadcastMessage(username, \"I just logged out\");\ngetSender().tell(msg, getSelf());\n})\n.matchAny(msg -> unhandled(msg))\n.build();\n}\n```  \nInside the Actor there are also the following methods that can be used just like in scala:  \n- `getSelf()` Returns the actor reference to itself.\n- `getSender()` Returns the actor reference to the sender of the currently processed message.\n- `getContext()` Returns this actors context, may be used to create child actors.\n- `forward(Object message, ActorContext context)` Forwards the message and passes the original sender actor as the sender. Same as `a.tell(msg, getSender())`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Java Akka", "Header 3": "Distributed Actors", "path": "../pages/digitalGarden/cs/distributedSystems/actorModel.mdx"}, "page_content": "To use Akka in a distributed system actors need to be configured. Configurations can include log levels, message serializers, protocol specifics etc.  \nYou can either define one big config file for all actors with the followin strucutre:  \n```yaml title=\"application.conf\"\nPrintConfig {\nakka {\nactor {\nprovider = remote\n}\nremote {\nartery {\ntransport = tcp\ncanonical.hostname = \"127.0.0.1\"\ncanonical.port = 2552\n}\n}\n}\n}\n\nOtherConfig {\nakka {\nactor {\n...\n```  \nAnd then use them like the following:  \n```java\npublic static void main(String[] args) {\nConfig config = ConfigFactory.load().getConfig(\"PrintConfig\"); // without getConfig just gets base.\nSystem.out.println(c.getInt(\"akka.remote.artery.canonical.port\")); // 25520\nActorSystem sys = ActorSystem.create(\"PrintApplication\", config);\nsys.actorOf(Props.create(PrintActor.class), \"PrintServer\");\nSystem.out.println(\"Started Print Application\");\n}\n```  \nOr save each config in a separate file like `foo.conf` and then read the config with `ConfigFactory.load(\"foo\")`.  \nIf you for some reason don't have an ActorRef to a distrubted actor or want to make use of wildcards you can do this with the `actorSelection()` function:  \n```java\nActorSelection actor = as.actorSelection(\"akka://PrintApplication@127.0.0.1:25520/user/PrintServer\"); // PrintApplication is the name of the system. user is always there for some reason.\nActorSelection actor = as.actorSelection(\"akka://PrintApplication@127.0.0.1:25520/user/*/PrintServer\"); // all printServer no matter the parent Actor\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Ask Pattern", "path": "../pages/digitalGarden/cs/distributedSystems/actorModel.mdx"}, "page_content": "In the Java version of Akka you can also use the ask pattern. This can be used when you expect an answer to a sent message. Instead of using the ask `?` operator, there is the `ask(ActorRef ref, Object msg, long timeout)` method defined in the `akka.pattern.Patterns` package as a static method which returns an **Akka Future** not to mixed up with the normal Java Future. The Future is either a Success Object containing the response message or a Failure containing an AskTimeoutException.  \n```java\nTimeout timeout = new Timeout(5, TimeUnit.SECONDS);\nFuture<Object> res = Patterns.ask(actor, message, timeout);\nreturn (String) Await.result(res, timeout.duration()); // blocking\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Websockets", "Header 2": "Bidirectional Communication", "path": "../pages/digitalGarden/cs/distributedSystems/websockets.mdx"}, "page_content": "What if we wanted a system that could send notifications to the client asynchronously. This is exactly what WebSockets do, they set up a bidirectional channel using HTTP/TCP and enable server-driven, full-duplex messaging.  \n![webSocketsCommunication](/compSci/webSocketsCommunication.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Websockets", "Header 2": "Specification", "path": "../pages/digitalGarden/cs/distributedSystems/websockets.mdx"}, "page_content": "The WebSocket protocol specification defines `ws` (WebSocket) and `wss` (WebSocket Secure) as two new uniform resource identifier (URI) schemes[8] that are used for unencrypted and encrypted connections respectively. To start a WebSocket Channel there needs to be an initial handshake which is done over HTTPS using the `upgrade` header.  \n```cmd title=\"Request\"\nGET /examples/websocket/echoStream HTTP/1.1\nHost: server.example.com\nConnection: Upgrade\nUpgrade: websocket\nSec-Websocket-Key: mqn5Pm7wtXEX6BzqDInLjw==\nSec-Websocket-Version: 13\n```  \n```cmd title=\"Response\"\nHTTP/1.1 101 Switching Protocols\nServer: Apache-Coyote/1.1\nUpgrade: websocket\nConnection: upgrade\nSec-WebSocket-Accept: +TdGPOkAq62+toDOhVGj2QZWwg8=\nDate: Thu, 04 Apr 2021 19:21:39 GMT\n```  \nThe return key verifies, that the server understood the request and is calculated like the following:  \n```java\nString KEY_SUFFIX = \"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\";\nString computeReturnKey(String key) throws Exception {\nMessageDigest md = MessageDigest.getInstance(\"SHA-1\");\nbyte[] res = md.digest((key+KEY_SUFFIX).getBytes(Charset.forName(\"ascii\")));\nreturn Base64.encodeBytes(res);\n}\n```  \nA message has the following format:  \n![websocketMsgFormat](/compSci/websocketMsgFormat.png)  \nThe fields having the following meaning:  \n- FIN marks the final fragment in a message\n- RSV = 000\n- OPCODE, operation code\n- 0x0 continuation frame\n- 0x1 text frame\n- 0x2 binary frame\n- 0x8 close\n- 0x9 ping\n- 0xA pong\n– MASK indicates content obfuscation (XOR masking)  \nA message could look something like this:  \n```cmd\n0x81 1000 0001 Final Fragment | Text frame\n0x85 1 000 0101 Masked / length = 5\n0x96 1001 0110 Masking Key\n0xa7 1010 0111 Masking Key\n0x2b 0010 1011 Masking Key\n0x38 0011 1000 Masking Key\n0xde 1101 1110 xor 10010110 = 0100 1000 H\n0xc2 1100 0010 xor 10100111 = 0110 0101 e\n0x47 0100 0111 xor 00101011 = 0110 1100 l\n0x54 0101 0100 xor 00111000 = 0110 1100 l\n0xf9 1111 1001 xor 10010110 = 0110 1111 o\n```  \nAn implementation of this process could be:  \n```java", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Websockets", "Header 2": "Specification", "path": "../pages/digitalGarden/cs/distributedSystems/websockets.mdx"}, "page_content": "0xc2 1100 0010 xor 10100111 = 0110 0101 e\n0x47 0100 0111 xor 00101011 = 0110 1100 l\n0x54 0101 0100 xor 00111000 = 0110 1100 l\n0xf9 1111 1001 xor 10010110 = 0110 1111 o\n```  \nAn implementation of this process could be:  \n```java\nbyte[] maskingKey; // random masking key, 4 bytes\nbyte[] payloadData; // data to be transmitted\nbyte[] maskedData; // masked data to be generated\n// mask (on client)\nfor(int i = 0; i < payloadData.length; i++)\nmaskedData[i] = payloadData[i] ^ maskingKey[i%4];\n}\n//unmask (on server)\nfor(int i = 0; i < maskedData.length; i++)\npayloadData[i] = maskedData[i] ^ maskingKey[i%4];\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Websockets", "Header 2": "Specification", "Header 3": "Sub-Protocols", "path": "../pages/digitalGarden/cs/distributedSystems/websockets.mdx"}, "page_content": "WebSockets also offer the option for the client and server to agree on a protocol with which the transmitted data will be formatted and interpreted. Examples of sub-protocols are JSON, XML, MQTT, WAMP, STOMP, SOAP. These protocols can ensure agreement not only about the way the data is structured but also about the way communication must commence, continue and eventually terminate. As long as it is defined in the handshake with the `Sec-WebSocket-Protocol` header and both parties understand what the protocol entails, anything goes.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Websockets", "Header 2": "JSR 356 Example", "path": "../pages/digitalGarden/cs/distributedSystems/websockets.mdx"}, "page_content": "```java\n@ClientEndpoint\npublic class EchoClient {\n\nprivate static CountDownLatch latch = new CountDownLatch(1);\n\n@OnOpen\npublic void onOpen(Session session) throws IOException {\nSystem.out.println(\"onOpen \" + Thread.currentThread());\nsession.getBasicRemote().sendText(\"Hello\");\n//  session.getBasicRemote().sendBinary(ByteBuffer.wrap(new byte[]{'h', 'e', 'l', 'o'})); // sends a binary message\n\n//  session.getBasicRemote().sendText(\"Hello\", false);\n//  session.getBasicRemote().sendText(\"World\", true);\n}\n\n@OnMessage\npublic void onMessage(Session session, String message) throws IOException {\nSystem.out.println(\"onMessage \" + message + \" \" + Thread.currentThread());\nsession.close();\n}\n\n@OnClose\npublic void onClose(Session session, CloseReason closeReason) {\nSystem.out.printf(\"[%s] Session %s closed because of %s\\n\", Thread.currentThread(), session.getId(), closeReason);\nlatch.countDown();\n}\n\n@OnError\npublic void onError(Throwable exception, Session session) {\nSystem.out.println(\"an error occured on connection \" + session.getId() + \":\" + exception);\n}\n\npublic static void main(String[] args) throws Exception {\n// URI url = new URI(\"ws://86.119.38.130:8080/websockets/echo\");\nURI url = new URI(\"ws://localhost:2222/websockets/echo\");\n\n//System.out.println(Thread.currentThread());\nClientManager client = ClientManager.createClient();\nclient.connectToServer(EchoClient.class, url);\nlatch.await();\n}\n}\n```  \n```java\n@ServerEndpoint(\"/echo\")\npublic class EchoServer {\n\n{\nSystem.out.println(\"EchoServer created \" + this);\n}\n\npublic static void main(String[] args) throws Exception {\nServer server = new Server(\"localhost\", 2222, \"/websockets\", null, EchoServer.class);\nserver.start();\nSystem.out.println(\"Server started, press a key to stop the server\");\nSystem.in.read();\n}\n\n@OnOpen\npublic void onOpen(Session session) {\nSystem.out.printf(\"New session %s\\n\", session.getId());\n}\n\n@OnClose\npublic void onClose(Session session, CloseReason closeReason) {\nSystem.out.printf(\"Session %s closed because of %s\\n\", session.getId(), closeReason);\n}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Websockets", "Header 2": "JSR 356 Example", "path": "../pages/digitalGarden/cs/distributedSystems/websockets.mdx"}, "page_content": "@OnClose\npublic void onClose(Session session, CloseReason closeReason) {\nSystem.out.printf(\"Session %s closed because of %s\\n\", session.getId(), closeReason);\n}\n\n@OnMessage\npublic String onMessage(String message, Session session) {\nSystem.out.println(\"received message form \" + session.getBasicRemote() + \": \" + message);\nreturn \"echo \" + message;\n}\n\n@OnError\npublic void onError(Throwable exception, Session session) {\nSystem.out.println(\"an error occured on connection \" + session.getId() + \":\" + exception);\n}\n\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Networking", "Header 2": "TCP/IP and OSI Model", "path": "../pages/digitalGarden/cs/distributedSystems/networking.mdx"}, "page_content": "We define protocols as a means to standardize how computers interact with each other no matter the manufacturer or the parts inside. The most commonly used protocols are TCP, UDP and IP. The TCP/IP and OSI models splits protocols into four layers depending on their tasks.  \n![protocolModels](/compSci/protocolModels.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Networking", "Header 2": "IP Addressing", "path": "../pages/digitalGarden/cs/distributedSystems/networking.mdx"}, "page_content": "IP addresses are used to uniquely identify devices inside a network and are most commonly used in the IP protocol. There are IPv4 addresses which take up 32 bits and IPv6 addresses which take up 128 bits.  \n![ipAddresses](/compSci/ipAddresses.png)  \nUnicast addresses belong to a single network interface and a packet that is sent to a unicast address is delivered to the interface identified by that address.  \nLoopback addresses are the addresses assigned to the loopback interface. Anything sent to these IP addresses loops around and becomes IP input on the local host. These addresses are often used when testing a client.  \nMulticast addresses belong to a set of interfaces. A packet sent to a multicast address is delivered to all interfaces identified by\nthat address.  \nA broadcast address is used to target all systems on a specific subnet network instead of single hosts. In other words broadcast addresses allow information to be sent to all machines on a given subnet rather than to a specific machine so can also be classified as a multicast address.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Networking", "Header 2": "Sockets", "path": "../pages/digitalGarden/cs/distributedSystems/networking.mdx"}, "page_content": "Sockets are an abstraction through which applications can send and receive data through a network. A socket is one endpoint of a two-way communication link between two programs running on the network. A socket is bound to a port number so that the transport layer can identify the application that the data is destined to be sent to.  \nServers waits for requests on a particular port, when a client connects to the server service it discloses its own address and port so that the server knows where to send the response.  \nThere are stream Sockets which use the TCP protocol and provide a reliable byte stream between the two applications. Packages are delivered in the correct order and lost packages are retransmitted with help of the TCP protocol. The connection is also full duplex meaning data can be sent and received over one connection instead of needing one for each operation. When transmission is finished one or both parties close the connection.  \nDatagram Sockets use the UDP protocol and aren't as reliable as Stream Sockets.  \n![sockets](/compSci/sockets.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Messaging", "Header 2": "Asynchronous Communication", "path": "../pages/digitalGarden/cs/distributedSystems/messaging.mdx"}, "page_content": "We have seen synchronous communication already for example sockets or a telephone connection. However, there is also the asynchronous communication model which involves a Message Oriented Middleware (MOM) between the client and the server. This form of communication can be seen for example when sending an email where the Mail Server is the MOM. Asynchronous communication allows for applications to be loosely coupled as they only need to agree on the message format and not the API. It also means that the sender and receiver don't have to be active at the same time.  \nWhen it comes to protocols for asynchronous communication there is either the JMS (Java Message Service / Jakarta Messaging) Protocol or other protcols which use the TCP protocol such as:  \n- AMQP: Advanced Message Queuing Protocol, which is a Binary protocol with four messaging models (direct, topic, fanout and header).\n- STOMP: Simple (or Streaming) Text Oriented Messaging Protocol, which is a Text based protocol similar to HTTP.\n- MQTT: MQ Telemetry Transport, a lightweight protocol, intended to be used in IoT environments (small footprint) with a publish and subscribe pattern and no queues and supports the delivery guarantees: at least once, at most once, exactly once and last wish?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Messaging", "Header 2": "AMQP - Advanced Message Queuing Protocol", "path": "../pages/digitalGarden/cs/distributedSystems/messaging.mdx"}, "page_content": "As mentioned the AMQP protocol is a binary protocol that contains the following three key components:  \n- Exchanges: Endpoints of the broker that receives messages.\n- Queues: Endpoints that store messages from exchanges and are used by subscribers to retrieve messages.\n- Bindings/Routings: Rules that bind/route exchanges to queues.  \nThese concepts are programmable, meaning they can be created, modified and deleted. This also means that there can be multiple channels inside a single TCP connection which can save the overhead of having multiple connections. When a message is sent the sender needs to define the exchanger, the routing key and the payload.  \n![amqpComponents](/compSci/amqpComponents.png)  \nThere are four patterns when interacting with messages:  \n- Direct Exchange: Queues that are bound to an exchanger with the same key that is used to\npublish a message will receive the message\n- Fanout Exchange: Broadcast of the message to all queues that are bound to it (binding key is not used) which is suitable for the publish and subscribe pattern.\n- Topic Exchange: Routes messages to all queues that have a binding key that matches the routing key which is suitable for routing messages to different queues based on the type of message.\n- Headers Exchange: Messages are routed based on custom message headers  \n![amqpMessagePatterns](/compSci/amqpMessagePatterns.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Messaging", "Header 2": "RabbitMQ", "path": "../pages/digitalGarden/cs/distributedSystems/messaging.mdx"}, "page_content": "Is an open-source message broker written in Erlang that implements the AMQP protocol which can also be extended to MQTT, STOMP and HTTP.  \nFirst you need to connect to a broker:  \n```java\nConnectionFactory factory = new ConnectionFactory();\nfactory.setUsername(\"username\"); // Default: guest\nfactory.setPassword(\"password\"); // Default: guest\nfactory.setVirtualHost(\"myRabbit\"); // Default: /\nfactory.setHost(\"69.69.69.69\"); // Default: localhost\nfactory.setPort(5672); // Default: 5672\nConnection connection = factory.newConnection();\nChannel channel = connection.createChannel();\n```  \nThe next step would be to declare a queue. Durable queues survive server restarts, exclusive queues are restricted to a connection and auto-delete means the server can delete the queue when it is no longer used.  \n```java\nchannel.queueDeclare(QUEUE_NAME,\n/* durable: */ false,\n/* exclusive: */ false,\n/* autoDelete: */ false,\n/* arguments: */ null\n);\n```  \nTo then publish a message we can use the default exchange:  \n```java\nString message = \"Hello World at \" + LocalDateTime.now();\nchannel.basicPublish(\n/* exchange: */ \"\",\n/* routing key: */ QUEUE_NAME,\n/* props: */ null,\n/* body: */ message.getBytes(StandardCharsets.UTF_8));\n```  \nTo then finally receive a message callbacks can be registered:  \n```java\nDeliverCallback deliverCallback = (consumerTag, message) -> {\nString text = new String(message.getBody(), \"UTF-8\");\n};\nCancelCallback cancelCallback = consumerTag -> {\nSystem.out.println(\"Cancelled by the server\");\n}\nchannel.basicConsume(QUEUE_NAME,\n/* autoAck */ true, deliverCallback, cancelCallback);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Messaging", "Header 2": "RabbitMQ", "Header 3": "Echo Example", "path": "../pages/digitalGarden/cs/distributedSystems/messaging.mdx"}, "page_content": "```java\nchannel.queueDeclare(RPC_QUEUE_NAME,\n/* durable:    */ false,\n/* exclusive:  */ false,\n/* autoDelete: */ false,\n/* arguments:  */ null);\n\nfinal String corrId = UUID.randomUUID().toString();\n\nString replyQueueName = channel.queueDeclare().getQueue();\nSystem.out.println(replyQueueName);\n\nAMQP.BasicProperties props = new AMQP.BasicProperties\n.Builder()\n.correlationId(corrId)\n.replyTo(replyQueueName)\n.build();\n\nString message = String.format(\"Hello World from %s at %s\",\nSystem.getProperty(\"user.name\"),\nLocalDateTime.now());\nSystem.out.println(message);\n\nchannel.basicPublish(\n/* exchange:    */ \"\",   // Exchange: empty string is called \"default exchang\" which is a direct exchange.\n/* routing key: */ RPC_QUEUE_NAME,\n/* props:       */ props,\n/* body:        */ message.getBytes(StandardCharsets.UTF_8));\n\n\nfinal BlockingQueue<String> response = new ArrayBlockingQueue<>(1);\n\nString ctag = channel.basicConsume(replyQueueName, true, (consumerTag, delivery) -> {\nif (delivery.getProperties().getCorrelationId().equals(corrId)) {\nresponse.offer(new String(delivery.getBody(), \"UTF-8\"));\n}\n}, consumerTag -> {\n});\n\nString result = response.take();\nchannel.basicCancel(ctag);\n\nSystem.out.println(result);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Messaging", "Header 2": "RabbitMQ", "Header 3": "Publish and Subscribe", "path": "../pages/digitalGarden/cs/distributedSystems/messaging.mdx"}, "page_content": "```java\n// PUBLISHER\nchannel.exchangeDeclare(EXCHANGE_NAME, \"fanout\");\nString message = \"Current Date: \" + LocalDateTime.now();\nchannel.basicPublish(EXCHANGE_NAME,\n/* routing key: */ \"\", //!!!!!!\n/* properties: */ null,\n/* body: */ message.getBytes(StandardCharsets.UTF_8));\n\n// SUBSCRIBER\nchannel.exchangeDeclare(EXCHANGE_NAME, \"fanout\");\nString queueName = channel.queueDeclare().getQueue();\nchannel.queueBind(queueName, EXCHANGE_NAME, \"\");\nDeliverCallback deliverCallback = (consumerTag, delivery) -> {\nString message = new String(delivery.getBody(), \"UTF-8\");\nSystem.out.println(\"Received '\" + message + \"'\");\n};\nchannel.basicConsume(queueName,\n/* autoAck */ true, deliverCallback, consumerTag -> {});\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Internet", "Header 2": "IETF", "path": "../pages/digitalGarden/cs/distributedSystems/internet.mdx"}, "page_content": "IETF stands for Interent Engineering Task Force which is task force/organization with the goal to make the internet work better. They define internet standards and regulate the internets architecture with so called RFC specifications (Request for comments). Some of the most well known RFCs are the following  \n- RFC 791: [Interent Protocol](https://datatracker.ietf.org/doc/html/rfc791)\n- RFC 5322: [Email (SMTP)](https://datatracker.ietf.org/doc/html/rfc5322)\n- RFC 2549: [IP over Avian Carriers (IPoAC)](https://datatracker.ietf.org/doc/html/rfc2549)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Internet", "Header 2": "HTTP Protocol", "path": "../pages/digitalGarden/cs/distributedSystems/internet.mdx"}, "page_content": "The HTTP protocol stands for Hypertext transfer protocol and is used to access static or dynamic data on another computer and is based on a reliable transport layer protocols like TCP and IP.  \n![httpProtocol](/compSci/httpProtocol.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Internet", "Header 2": "HTTP Protocol", "Header 3": "Request", "path": "../pages/digitalGarden/cs/distributedSystems/internet.mdx"}, "page_content": "A HTTP request consists of a request line which holds the method of the request (GET, POST etc.), the url of the target and the version of the HTTP protocol to be used followed by a carriage return line feed (CR LF). You then have the headers which are key-value pairs separated by ':' and a CR LF at the end of each one. Last but not least you have the body which is a chunk of bytes  \n![httpRequest](/compSci/httpRequest.png)  \n#### Methods  \nIdempotent means that multiple identical requests will have the same outcome. So it does not matter if a request is sent once or multiple times. The following HTTP methods are idempotent: GET, HEAD, OPTIONS, TRACE, PUT and DELETE.  \n![httpRequestMethods](/compSci/httpRequestMethods.png)  \n#### Headers  \n| Key             | Description                                                                                                                    | Example                                                       |\n| --------------- | ------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------- |\n| Host            | specifies the host and port number of the server to which the request is being sent                                            | Host: developer.mozilla.org:8080                              |\n| Accept          | indicates which content types, expressed as MIME types, the client can understand                                              | Accept: text/html                                             |\n| Accept-Language | indicates the natural language and locale that the client prefers                                                              | Accept-Language: de-CH                                        |\n| Accept-Encoding | indicates the content encoding (usually a compression algorithm) that the client can understand                                | Accept-Encoding: gzip                                         |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Internet", "Header 2": "HTTP Protocol", "Header 3": "Request", "path": "../pages/digitalGarden/cs/distributedSystems/internet.mdx"}, "page_content": "| Accept-Encoding | indicates the content encoding (usually a compression algorithm) that the client can understand                                | Accept-Encoding: gzip                                         |\n| User-Agent      | lets servers and network peers identify the application, operating system, vendor, and/or version of the requesting user agent | User-Agent: Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:47.0) |\n| Referer         | contains an absolute or partial address of the page that makes the request                                                     | Referer: https://example.com/                                 |\n| Connection      | controls whether the network connection stays open after the current transaction finishes                                      | Connection: keep-alive, Connection: close                     |\n| Cookie          | contains stored HTTP cookies associated with the server                                                                        | Cookie: PHPSESSID=298zf09hf012fh2;                            |\n| Content-Length  | indicates the size of the message body, in bytes, sent to the recipient                                                        | Content-Length: 4                                             |\n| Content-Type    | indicates the original media type of the resource (prior to any content encoding applied for sending)                          | Content-Type: text/html; charset=UTF-8                        |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Internet", "Header 2": "HTTP Protocol", "Header 3": "Response", "path": "../pages/digitalGarden/cs/distributedSystems/internet.mdx"}, "page_content": "A HTTP reponse is built very similiar to a request but instead of a request line it has a status line which also holds the version of the HTTP protocol to be used followed by a status code and message.\n![httpResponse](/compSci/httpResponse.png)  \n#### Codes  \n![httpReponseCodes](/compSci/httpReponseCodes.png)  \n#### Headers  \n| Key               | Description                                                                                                                               | Example                                      |\n| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| Content-Length    | indicates the size of the message body, in bytes, sent to the recipient                                                                   | Content-Length: 4                            |\n| Content-Type      | indicates the original media type of the resource (prior to any content encoding applied for sending)                                     | Content-Type: text/html; charset=UTF-8       |\n| Content-Encoding  | lists any encodings that have been applied to the representation (message payload), and in what order                                     | Content-Encoding: gzip                       |\n| Location          | indicates the URL to redirect a page to. It only provides a meaning when served with a 3xx (redirection) or 201 (created) status response | Location: /index.html                        |\n| Date              | contains the date and time at which the message originated                                                                                | Date: Wed, 21 Oct 2015 07:28:00 GMT          |\n| Last-Modified     | contains a date and time when the origin server believes the resource was last modified                                                   | Last-Modified: Wed, 21 Oct 2015 07:28:00 GMT |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Internet", "Header 2": "HTTP Protocol", "Header 3": "Response", "path": "../pages/digitalGarden/cs/distributedSystems/internet.mdx"}, "page_content": "| Last-Modified     | contains a date and time when the origin server believes the resource was last modified                                                   | Last-Modified: Wed, 21 Oct 2015 07:28:00 GMT |\n| Expires           | contains the date/time after which the response is considered expired                                                                     | Expires: Wed, 21 Oct 2015 07:28:00 GMT       |\n| Server            | describes the software used by the origin server that handled the request                                                                 | Server: Apache/2.4.1 (Unix)                  |\n| Transfer-Encoding | specifies the form of encoding used to safely transfer the payload body to the user                                                       | Transfer-Encoding: chunked                   |\n| Cache-Control     | control caching in browsers and shared caches                                                                                             | Cache-Control: no-cache                      |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Internet", "Header 2": "HTTP Protocol", "Header 3": "MIME Types", "path": "../pages/digitalGarden/cs/distributedSystems/internet.mdx"}, "page_content": "The Content-Type or MIME (Multipurpose Internet Mail Extensions) type specifies type of the body, like text/javascript or something else like audio, video, etc. being sent between client and server. MIME types are not limited to HTTP, they are used in many other locations.  \n`Media-Type = type / subtype { “;” parameter }`  \nTypes: text / image / audio / video / application / message / multipart  \nSubtypes that start with x are non standard subtypes.  \nFor example:  \n- Media-Type: text/html;charset =ISO 8859 1\n- Media-Type: application/octet stream\n- Media-Type: image/jpeg", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Internet", "Header 2": "HTTP Protocol", "Header 3": "Enhancements", "path": "../pages/digitalGarden/cs/distributedSystems/internet.mdx"}, "page_content": "Over the years the HTTP protocol has been enhanced and newer versions have been released/specified.  \n#### Version 1.1  \n- Network connection management\n- Persistent connections were introduced, meaning that several requests can be sent over the same connection\n- Pipelining was introduced, meaning you can send a new request before the previous ones have even been answered  \n- Bandwidth optimization\n- Clients can request parts/ranges of documents for example to complete a previously interrupted request\n- Message transmission\n- Trailers were introduced, meaning message headers can be delivered at the end of the body which can also be similar to a checksum\n- Transfer encoding and content length. Clients reading a resposne need to know when they have reached the end. Servers can indicate the end of a message in four ways\n- Implied content length, for example certain response codes like 304 are defined to never have content, so the client can assume the response to terminate with a double CR LF\n- Content-length header, the length of the content is specified in the content-length attribute in bytes.\n- Chunked encoding, the content is broken down into a number of chunks each prefixed by its size in bytes, a zero size chunk then indicated the end of the message. For this to work the server must set the header to `transfer-encoding : chunked`.\n- Internet address conservation  \n#### Version 2.0  \n- Binary, packet-based protocol. With the switch to 2.0 all HTTP messages are split and sent in clearly defined frames. This also means that chunked transfer encoding must not be used with HTTP/2.0. This switch provides more mechanisms for data streaming but also allows for more efficiency.\n- Multiple requests can now be sent in parallel over a single TCP connections.\n- HPACK, headers are compressed and cached on the server\n- Servers can push resources together with a requested resources for example a script or css file along with a HTML page.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Euclidean Algorithm", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "The euclidean algorithm is used to find the GCD of two numbers. This can be done locally but also using a distributed algorithm:  \n```java\nint gcd(int a, int b) {\nassert a >= 0 && b >= 0;\nwhile(b != 0) {\nint tmp = b;\nb = a % b;\na = tmp;\n}\nreturn a;\n}\n```  \nIn the distributed algorithm each node has a reference to its left and right neighbor. The node first informs its neighbors of its own value. If a received number is smaller than its value then a node adjusts its value and shares its new value with its neighbors.  \n```java\npublic class GcdActor extends AbstractActor {\nprivate int n;\nprivate final Set<ActorRef> neighbours = new HashSet<>();\npublic GcdActor(int n) {\nthis.n = n;\nSystem.out.printf(\"%s Initial Value: %d%n\", getSelf(), n);\n}\n@Override\npublic Receive createReceive() {\nreturn receiveBuilder()\n.match(ActorRef.class, actor -> {\nneighbours.add(actor);\nif(neighbours.size() == 2) {\nneighbours.forEach(a -> a.tell(n, getSelf()));\n}})\n.match(Integer.class, value -> {\nif(value < n) {\nn = ((n-1) % value) + 1;\nneighbours.forEach(a -> a.tell(n, getSelf()));\nSystem.out.printf(\"%s Current Value: %d%n\",\ngetSelf(), n);\n}})\n.matchAny(msg -> unhandled(msg))\n.build();\n}\n}\npublic static void main(String[] args) throws Exception {\nActorSystem as = ActorSystem.create();\nList<Integer> values = List.of(108, 76, 12, 60, 36);\nList<ActorRef> actors = IntStream.range(0, values.size())\n.mapToObj(n -> as.actorOf(Props.create(GcdActor.class, values.get(n)), \"GCD\"+n))\n.collect(Collectors.toList());\nfinal int size = actors.size();\nfor(int i = 0; i < size; i++) {\nactors.get(i).tell(actors.get((i+1) % size), null);\nactors.get(i).tell(actors.get((size+i-1) % size), null);\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Echo Algorithm", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "The idea of the echo algorithm is to traverse an arbitrary graph by implicitly building a spanning tree. The algorithm is defined as followed:  \nThere are two types of messages: Explorer messages, which color the nodes red, and Echo messages, which color the nodes green. Before the algorithm is executed, all nodes are white.  \n1. An initiator turns red and sends an explorer to all of his neighbors.\n2. A white node that receives an explorer turns red\n3. A node that has received an explorer or an echo over all of its edges turns green\n4. A non-initiator node that has received an explorer or an echo over all of its edges sends an echo over the edge over which it received the first explorer\n5. The algorithm terminates when the initiator turns green  \nThe edges over which the echo messages have run result in a spanning tree. For a Graph with $E$ edges this algorithm uses 2 * E messages.  \n```java\npublic class EchoNode extends AbstractActor {\nprivate final Set < ActorRef > neighbours = new HashSet < > ();\nprivate ActorRef parent;\nprivate int counter = 0; // number of received tokens\n@Override\npublic Receive createReceive() {\nreturn receiveBuilder()\n.match(ActorRef.class, actor -> neighbours.add(actor))\n.match(Start.class, value -> {\nparent = getSender(); // initiator\nneighbours.forEach(a -> a.tell(new Token(), getSelf()));\n})\n.match(Token.class, msg -> {\ncounter++;\nif (parent == null) { // variant: if(counter == 1)\nparent = getSender();\nSystem.out.printf(\"Actor %s got informed by %s%n\",\ngetSelf(), getSender());\nneighbours.stream()\n.filter(a -> a != parent)\n.forEach(a -> a.tell(msg, getSelf()));\n}\nif (counter == neighbours.size()) {\nparent.tell(msg, getSelf());\n}\n})\n.matchAny(msg -> unhandled(msg))\n.build();\n}\npublic static void main(String[] args) throws Exception {\nActorSystem as = ActorSystem.create();\nList < ActorRef > actors = IntStream.range(0, 8)\n.mapToObj(n -> as.actorOf(Props.create(EchoNode.class), \"Node\" + n))\n.collect(Collectors.toList());\naddEdge(actors, 0, 1);\n...\naddEdge(actors, 7, 5);", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Echo Algorithm", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "ActorSystem as = ActorSystem.create();\nList < ActorRef > actors = IntStream.range(0, 8)\n.mapToObj(n -> as.actorOf(Props.create(EchoNode.class), \"Node\" + n))\n.collect(Collectors.toList());\naddEdge(actors, 0, 1);\n...\naddEdge(actors, 7, 5);\nTimeout timeout = new Timeout(5, TimeUnit.SECONDS);\nFuture < Object > f = Patterns.ask(actors.get(0), new Start(), timeout);\nObject result = Await.result(f, timeout.duration());\nSystem.out.println(result);\nas.terminate();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Election Algorithm", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "The idea of the election algorithm is to elect a leader among equal nodes for example to coordiante concurrency etc. For the alogrithm to work we need to assume that each node has a unique identifier that can be ordered. The node with the highest order is then the leader. At the end each node should know who the leader is.  \nInitially the value in each node is negative infinity.\nEvery node can start the election as long as it is not yet involved in an election, i.e. value is neg inf.\nUpon start, a node stores its id number in the value field and sends this value to the next node.\nIf a message arrives, its value is compared with the stored one. If it is greater than the stored value, the value is updated and the message is\nforwarded. If it is smaller, then the message is discarded.\nA node is leader if it receives its own message. The leader then may inform the other nodes about the election / termination of\nthe algorithm.  \n```java\npublic class ElectionNode extends AbstractActor {\nprivate ActorRef next; // ring references\nprivate ActorRef initiator; // initiator of the election\nprivate final int id; // id of this actor\nprivate int master = Integer.MIN_VALUE; // id of elected node\npublic ElectionNode(int id) {\nthis.id = id;\n}\n@Override\npublic Receive createReceive() {\nreturn receiveBuilder()\n.match(ActorRef.class, actor -> next = actor)\n.match(Start.class, value -> {\nif (master == Integer.MIN_VALUE) {\ninitiator = getSender();\nmaster = id;\nnext.tell(new Token(master), getSelf());\n}\n})\n.match(Token.class, token -> {\nif (token.value > master) {\nmaster = token.value;\nnext.tell(token, getSelf());\n} else if (token.value == id) {\nSystem.out.println(\"hurray, I got elected \" + getSelf());\nnext.tell(new Reset(id), getSelf());\n}\n})\n.match(Reset.class, token -> {\nmaster = Integer.MIN_VALUE;\nif (token.value == id) {\ninitiator.tell(\"\" + id, getSelf());\n} else {\nnext.tell(token, getSelf());\n}\n})\n.matchAny(msg -> unhandled(msg))\n.build();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Hash Tables", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "Distributed system that provides a lookup service like a hash table. Key Value pairs are stored in the nodes of a DHT\nAny participating node can efficiently retrieve the value associated with a given key. The tricky part is figuring out which node is responsible for which key? How do we handle changes to the network topology? Nodes can join or leave the network at any time.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Hash Tables", "Header 3": "Consistent Hashing", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "The goal of consistent hashing is to minimize the number of remaps if nodes are added or removed i.e. if the table is resized we only want $\\frac{n}{m}$keys need to be remapped on average with $n$ = number of keys and $m$ = number of nodes.\nKeys and Nodes are mapped to the same ID space (Integers) Nodes: hash(IP), Keys: hash(key). Hash Functions:  \n- SHA 1 => 160bit $2^160$ possible nodes\n- Java => 32bit $2^32$ possible nodes  \nEach object (keys and nodes) is mapped to a point on a circle for example: if we use 6bit objects then we have the ID space: 0 .. $2^6 - 1$ = 63). Each key is stored at its successor, i.e. in the node with the next higher or equal ID. This has the following advantages:  \n- All nodes store roughly the same number of keys if the hash function is uniform.\n- If a node joins or leaves, only a fraction of the keys need to be moved to a different node, i.e. only the successor\nof a node is involved.  \nThis technique can be implemented in different ways. Either we have a complete graph so each node knows the location of every other node which leads to a lookup complexity of $O(1)$ but storage of the routing table takes up $O(n)$ with $n$ being the number of nodes.  \n```java\nrecord Put(Object key, Object value) {} // used to initate a put\nrecord Put2(Object key, Object value) {} // used to store at dest node\nrecord Get(Object key) {} // used to initate a get\nrecord Get2(Object key) {} // used to get at dest node\nrecord Result(int id, Object value) {} // used to return result\nrecord AddNode(int id, ActorRef actor) {} // initiates an add node\nrecord Partition(int id) {} // used to partition a node\nrecord PartitionAnswer(Map < Object, Object > map) {}\n// answer to a partition request\nrecord Print() {} // debugging, i.e. print node info on console\npublic class HashNode extends AbstractActor {\nprivate final int id; // id of this node\n// references to all actors\nprivate final TreeMap < Integer, ActorRef > actors = new TreeMap < > ();\n// data stored in this node", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Hash Tables", "Header 3": "Consistent Hashing", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "public class HashNode extends AbstractActor {\nprivate final int id; // id of this node\n// references to all actors\nprivate final TreeMap < Integer, ActorRef > actors = new TreeMap < > ();\n// data stored in this node\nprivate final Map < Object, Object > values = new HashMap < > ();\npublic HashNode(int id) {\nthis.id = id;\n}\npublic Receive createReceive() {\nreturn receiveBuilder()\n.match(Map.class, actors -> {\nthis.actors.putAll(actors);\n})\n.match(Get.class, msg -> {\nvar keys = actors.navigableKeySet();\nvar key = keys.ceiling(msg.key().hashCode());\n// ceiling returns the least element in this set\n// greater than or equal to the given element,\n// or null if there is no such element.\nif (key == null) key = keys.first();\nactors.get(key).tell(new Get2(msg.key()), getSender());\n})\n.match(Get2.class, msg -> {\ngetSender().tell(new Result(id, values.get(msg.key())),\ngetSelf());\n})\n.matchAny(msg -> {\nunhandled(msg);\n})\n.build();\n}\n}\n```  \nOr we can have a cyclic graph so each node only knows the location of its successor, this leads to a lookup complexity of $O(n)$ but storage only uses $O(1)$.  \n```java\nrecord Put(Object key, Object value) {} // used to initate a put\nrecord Put2(Object key, Object value, int previousId) {}\n// used to distrbibute put in the ring\nrecord Get(Object key) {} // used to initate a get\nrecord Get2(Object key, int previousId, int counter) {}\n// used to distribute get in the ring\nrecord Result(int id, Object value, int counter) {}\nrecord SetNext(int nextId, ActorRef next) {}\nrecord AddNode(int newId, ActorRef newActor) {}\nrecord Partition(int id) {} // used to partition a node, i.e. return\n// all elements <= id\nrecord PartitionAnswer(Map < Object, Object > map) {}\nrecord Print(ActorRef start) {} // print node info on console\npublic class HashNode extends AbstractActor {\nprivate final int id; // id of this node\nprivate ActorRef next; // next node in the ring\nprivate int nextId; // id of next node in ring\nprivate Map < Object, Object > values = new HashMap < > (); // data\npublic HashNode(int id) {", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Hash Tables", "Header 3": "Consistent Hashing", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "private final int id; // id of this node\nprivate ActorRef next; // next node in the ring\nprivate int nextId; // id of next node in ring\nprivate Map < Object, Object > values = new HashMap < > (); // data\npublic HashNode(int id) {\nthis.id = id;\n}\npublic Receive createReceive() {\nreturn receiveBuilder()\n.match(SetNext.class, msg -> {\nnext = msg.next();nextId = msg.nextId();\n})\n.match(Put.class, msg -> {\nnext.tell(new Put2(msg.key(), msg.value(), this.id), null);\n})\n.match(Get.class, msg -> {\nnext.tell(new Get2(msg.key(), this.id, 1), getSender());\n})\n.match(Put2.class, msg -> {\nint hash = msg.key().hashCode();\nif (between(hash, msg.previousId(), this.id)) {\nvalues.put(msg.key(), msg.value());\n} else {\nnext.tell(new Put2(msg.key(), msg.value(), this.id), null);\n}\n})\n.match(Get2.class, msg -> {\nint hash = msg.key().hashCode();\nif (between(hash, msg.previousId(), this.id)) {\ngetSender().tell(new Result(id, values.get(msg.key()),\nmsg.counter()), getSelf());\n} else {\nnext.tell(new Get2(msg.key(), id, msg.counter() + 1),\ngetSender());\n}\n})\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Hash Tables", "Header 3": "Chord Algorithm", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "The chord algorithm and protocol implements a distributed hash table with a lookup time of log(N) and is based on Consistent Hashing. It uses so called finger tables. In these tables every node knows up to $m$ other nodes, and the distance of the nodes it knows increases exponentially (m is the bit length\nof the hash function). Meaning the The i-th entry (0..m-1) in the table of node n contains a reference to the successor $((n + 2^i ) \\mod 2^m)$ the first entry of the finger table is the immediate successor. Example: 16 node Chord network (m = 4).  \n#### Lookup  \nThe finger table is used to find the predecessor of the node which stores a given key.  \n1. Node 10 is asked to look up key 5 =\\> Finger table refers to node 43.\n2. Node 43 is asked to look up key 5 =\\> Finger table refers to node 1\n3. Node 1 is asked to look up 5 =\\> Key is between 2 and 10 (1 \\< 5 \\<= 10), so Node 10 contains the searched key and its associated value  \n```c\nn.find_successor(id)\nif id in (n, successor] then // n < id && id <= successor\nreturn successor // this is the node which contains key id\nelse\n// forward the query around the circle\nn0 = closest_preceding_node(id)\nreturn n0.find_successor(id)\n// search the local table for the highest predecessor of id\nn.closest_preceding_node(id)\nfor (int i = m - 1; i >= 0; i--)\ndo\nif (finger[i] in (n, id)) then\nreturn finger[i]\nreturn n\n```  \n#### Join  \nIf a new node joins, the following invariants must be maintained:  \n- Each node refers to its immediate successor =\\> ensures correctness\n- Each ( key,value ) pair is stored in successor(hash(key)) =\\> ensures correctness\n- The finger table of each node should be correct =\\> keeps query operation fast  \n```java\nrecord Put(Object key, Object value) {} // used to distribute in ring\nrecord Put2(Object key, Object value) {} // put in destination node\nrecord Get(Object key, int counter) {} // used to distribute in ring\nrecord Get2(Object key, int counter) {} // get in destination node\nrecord Result(int id, Object value, int counter) {}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Hash Tables", "Header 3": "Chord Algorithm", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "record Put2(Object key, Object value) {} // put in destination node\nrecord Get(Object key, int counter) {} // used to distribute in ring\nrecord Get2(Object key, int counter) {} // get in destination node\nrecord Result(int id, Object value, int counter) {}\nrecord Partition(int id) {} // used to partition a node, i.e. return\n// all elements <= id\nrecord PartitionAnswer(Map < Object, Object > map) {}\nrecord Print() {} // debugging, i.e. print node info on console", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Distributed Algorithms", "Header 2": "Distributed Hash Tables", "Header 3": "Chord Algorithm", "path": "../pages/digitalGarden/cs/distributedSystems/distributedAlgorithms.mdx"}, "page_content": "public class HashNode extends AbstractActor {\nprivate final int id; // id of this node\nprivate int next; // id of next node\nprivate TreeMap < Integer, ActorRef > fingerTable;\nprivate Map < Object, Object > values = new HashMap < > ();\npublic HashNode(int id) {\nthis.id = id;\n}\npublic Receive createReceive() {\nreturn receiveBuilder()\n.match(TreeMap.class, fingerTable -> {\nthis.fingerTable = fingerTable;\n})\n.match(Integer.class, next -> this.next = next)\n.match(Get.class, msg -> {\nint hash = msg.key().hashCode();\nif (between(hash, id, next)) {\nfingerTable.get(next).tell(\nnew Get2(msg.key(), msg.counter() + 1), getSender());\n} else {\nvar set = fingerTable.navigableKeySet();\nvar prev = set.lower(hash);\nif (prev == null) prev = set.last();\nfingerTable.get(prev).tell(\nnew Get(msg.key(), msg.counter() + 1), getSender());\n}\n})\n.match(Get2.class, msg -> {\ngetSender().tell(new Result(this.id,\nvalues.get(msg.key()), msg.counter()), getSelf());\n})\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "REST stands for \"Representational State Transfer\" and is a architecture for distributed systems. The term REST originated in Roy Fielding's PhD in 2000 who was one of the main authors of the HTTP protocol specification. REST does not enforce any rules regarding how it should be implemented however it does define some design guidelines/constraints that should be followed if the system is to be truly RESTful.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Constraints", "Header 3": "Uniform Interface", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "Resources (data) are the key abstraction in REST. The interface of the API should be uniform meaning there shouldn't be lots of different ways of doing the same thing which also means that a resource in the system should only have one logical URI. The resource should however not be too large but still contain everything in its representation. Whenever relevant, a resource should also contain links pointing to relative URIs to fetch related information (HATEOAS = Hypermedia as the Engine of Application State).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Constraints", "Header 3": "Client-Server", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "Client applications and server applications must be able to evolve separately without any dependency on each other. For this reason you often see versioning of APIs so that clients are reverse compatible.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Constraints", "Header 3": "Stateless", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "All client-server interactions should be stateless. This means the server does not store anything about the latest HTTP request that the client made and will treat every request as new.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Constraints", "Header 3": "Cacheable", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "Caching has large performance benefits for the client but also reduces the load of the server. So in REST, resources should be cached then declare themselves cacheable.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Constraints", "Header 3": "Layered System", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "REST allows you to use a layered system architecture where you deploy the APIs (Controllers) on server A, and store data on server B and authenticate requests in Server C (Services), for example. A client should not be able to tell whether it is connected directly to the end server or an intermediary along the way.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Common Patterns", "Header 3": "Collection and Element structure", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "Resources are often structure using urls for collections or singular elements. This especially helps to fullfil the constraint of having a uniform interface. In the below example a single product is an element and all products make up a collection.  \n| Request               | Description                                                                                                       |\n| --------------------- | ----------------------------------------------------------------------------------------------------------------- |\n| GET `/products`       | List all elements in the collection (products)                                                                    |\n| POST `/products`      | Add a product to the collection                                                                                   |\n| DELETE `/products`    | Remove the collection including all of its elements                                                               |\n| GET `/products/id`    | Read the element (product) with its unique identifier=`id`                                                        |\n| PUT `/products/id`    | Update the element (with the updated item in the body, often without the unique identifier as already in the URI) |\n| DELETE `/products/id` | Remove the element corresponding to the id                                                                        |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Common Patterns", "Header 3": "Put vs Patch", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "Their is often the discussion of what the differences are between the PATCH and the PUT methods. The biggest difference is that PUT is idempotent and PATCH isn't meaning it can cause side effects. The other key difference is that PUT sends the modified version of the resource whereas PATCH just sends instructions describing how a resource should be modified (most often just the to be modified fields).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Jakarta", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "What is jakarta?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Jakarta", "Header 3": "Client", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "Jakarta offers some packages that make submitting requests to an API very easy. The general process is as followed:  \n1. Obtain an instance of a client.\n2. Create and configure a WebTarget which represents the API.\n3. Create and configure a request from the WebTarget.\n4. Submit the request.  \n```java\npublic class RestClient {\n\nprivate static final String REST_URI = \"http://localhost:3001/api/v1/\";\n\npublic static void main(String[] args){\nClient client = ClientBuilder.newClient();\nWebTarget rootTarget = client.target(REST_URI); // immutable with respect to URI\nWebTarget productsTarget = rootTarget.path(\"products\"); // mutable with respect to configuration\n\nInvocation.Builder invocationBuilder = productsTarget.request(MediaType.APPLICATION_JSON);\nResponse getResponse = invocationBuilder.get(Product.class);\nProduct product = new Product(\"Logitech mouse\", 3); // title, amount\nResponse postResponse = invocationBuilder.post(Entity.entity(product, MediaType.APPLICATION_JSON);\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Jakarta", "Header 3": "RESTful API with JAX-RS", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "JAX-RS is a specification of annotations for server services. With them we can create a RESTful API. For this we create singletons that bind to a certain http method (method binding).  \n#### Injection  \nWith injections we can extract values from the request. These values can then also be automatically converted to the correct type. This automatic type conversion is possible from string to primitive types, to class `T` that either has a constructor with a single string parameter or a static method `T valueOf(String arg)`.  \nYou can also get various context objects:  \n- `@Context UriInfo`  \n```java\n@Singleton\n@Path(\"/products\")\npublic class ProductResource{\n\n@Context\nprivate UriInfo info; // also HttpHeaders, Request, SecurityContext, Providers etc.\n@GET\n@Path(\"{id}\")\npublic Response getProduct(@PathParam(\"id\") String id) {\n// For complexer responses\nString product = ...; // get from DB or whatever\nResponseBuilder builder = Response.ok(book); // body\nbuilder.language(\"en\").header(\"Some-Header\", \"some value\");\n\nreturn builder.build();\n}\n\n@POST\n@Path(\"{title}-{amount}\")\npublic String createProduct(\n@PathParam(\"title\") String title,\n@PathParam(\"amount\") int amount,\n@DefaultValue(10) @QueryParam(\"price\") int price,\n@HeaderParam(\"Referer\") String referer,\n@CookieParam(\"customerId\") Cookie customerId )\n) { ... }\n\n@PUT\n@Path(\"{id}\")\npublic String updateProduct(@PathParam(\"id\") String productID, String body) { ... }", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Jakarta", "Header 3": "RESTful API with JAX-RS", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "@PUT\n@Path(\"{id}\")\npublic String updateProduct(@PathParam(\"id\") String productID, String body) { ... }\n\n@DELETE\n@Path(\"{id}\")\npublic void deleteProduct(@PathParam(\"id\") String productID) { ... }\n}\n```  \n#### Content Negotiation  \nYou can also use the `@Produces` annotation to declare the type of result.  \n```java\n@Produces({\"text/plain\", \"text/html\"})\n@Produces({\"application/xml\", \"application/json\"})\n```  \nThe `@Consumes` annotation does something very similiar and declare the type which is accepted.  \n```java\n@Consumes(\"application/x-www-form-urlencoded\")\n@Consumes({\"application/xml\", \"application/json\"});\n```  \n#### Content Handlers  \nData binding or also often called marshalling is the process of converting data from or to the body. Above we have used string but you can also use a byte[], InputStream etc.. If a request sends data using `\"application/x-www-form-urlencoded\"` you can read it in with a `MultivalueMap<String,String`. These are all so called providers which you can also implement yourself by adding `@Provider` to a class and implementing `MessageBodyReader<T>` and/or `MessageBodyWriter<T>`. To be able to use the provider then in a client you need to register it.  \n```java\nClient c = ClientBuilder.newClient();\nc.register(XStreamProvider.class);\n```  \n```java\n@Provider\n@Consumes(\"application/xstream\")\n@Produces(\"application/xstream\")\npublic class XStreamProvider implements MessageBodyReader<Object>, MessageBodyWriter<Object> {\n\nprivate XStream xstream = new XStream (new DomDriver());\n\npublic boolean isReadable(Class<?> type, Type genericType,\nAnnotation[] annotations, MediaType mimeType) {\nreturn true;\n}\n\npublic Object readFrom(Class<Object> type, Type genericType,\nAnnotation[] annotations, MediaType mimeType,\nMultivaluedMap<String, String> httpHeaders, InputStream entityStream) {\nreturn xstream.fromXML(entityStream);\n}\n\npublic boolean isWriteable(Class<?> type, Type genericType,\nAnnotation[] annotations, MediaType mimeType) {\nreturn true;\n}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Jakarta", "Header 3": "RESTful API with JAX-RS", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "public boolean isWriteable(Class<?> type, Type genericType,\nAnnotation[] annotations, MediaType mimeType) {\nreturn true;\n}\n\npublic long getSize (Object object , Class<?> type,\nType genericType, Annotation[] annotations, MediaType mimeType) {\nreturn -1; // size not yet known\n}\n\npublic void writeTo(Object object, Class<Object> type, Type genericType,\nAnnotation[] annotations, MediaType mimeType,\nMultivaluedMap<String, String> httpHeaders, InputStream entityStream) {\nreturn xstream.toXML(object, entityStream);\n}\n}\n```  \n#### Conditional Get  \nFor performance reasons we don't want to transfer resources if they have not changed which is why we can do conditional GETs two different ways with the help of headers.  \nWhen sending a response we can add the \"Last-Modified\" header and then when sending a request for the same resource we can use the value in the \"If-Modified-Since\" Header. This can then either return with a modified value or with a 304, not modified status.  \nWe can also use the \"ETag\" and \"If-None-Match\" headers which work pretty much the same. The ETag (entity tag) value is an identifier which represents a specific version of the resource.  Common methods of ETag generation are using a hash of the resource's content or just hash of the last modification timestamp.  \n```java\nDate lastModifiedDate = ...\n// EntityTag eTag = ...\nResponse.ResponseBuilder responseBuilder = request.evaluatePreconditions(lastModifiedDate);\nif (responseBuilder == null) {//last modified date didn't match, send new content\nreturn Response.ok(\"dummy user list\")\n.lastModified(lastModifiedDate)\n//.tag(tag)\n.build();\n} else {\n\nreturn responseBuilder.build();  //sending 304 not modified\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Jakarta", "Header 3": "Deployment with Jersey", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "You need to register Jersey as the servlet dispatcher for REST requests in the `web.xml` file.  \n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\" id=\"WebApp_ID\" version=\"3.0\">\n<display-name>com.vogella.jersey.first</display-name>\n<servlet>\n<servlet-name>Jersey Web Application</servlet-name>\n<servlet-class>org.glassfish.jersey.servlet.ServletContainer</servlet-class>\n<!-- Register resources and providers under com.vogella.jersey.first package. -->\n<init-param>\n<param-name>javax.ws.rs.Application</param-name>\n<param-value>ch.georgerowlands.MyApplication</param-value>\n</init-param>\n</servlet>\n<servlet-mapping>\n<servlet-name>Jersey Web Application</servlet-name>\n<url-pattern>/*</url-pattern>\n</servlet-mapping>\n</web-app>\n```  \nAnd then add your services to the Application  \n```java\npublic class MyApplication extends Application {\nprivate Set<Object> singletons = new HashSet<Object>();\nprivate Set<Class<?>> classes = new HashSet<Class<?>>();\n\npublic MyApplication () {\nclasses.add(ProductResource.class);\nsingeltons.add(new ProductResource());\n}\n\n@Override\npublic Set<Class<?>> getClasses () { return classes; }\n\n@Override\npublic Set<Object> getSingletons () { return singletons; }\n}\n\npublic class Server {\npublic static void main(String[] args ) throws Exception {\nfinal URI BASE_URI = new URI(\"http://localhost:9998\");\nResourceConfig rc = ResourceConfig.forApplication(new MyApplication());\n// Resource config that scans for JAX RS resources so need for application\n// ResourceConfig rc = new ResourceConfig().packages(\"ch.georgerowlands.resources\");\nJdkHttpServerFactory.createHttpServer(BASE_URI, rc);\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "RESTful APIs", "Header 2": "Jakarta", "Header 3": "Documenting with OpenAPI", "path": "../pages/digitalGarden/cs/distributedSystems/restfulApis.mdx"}, "page_content": "The OpenAPI Specification (OAS) defines a standard interface description for REST APIs. Swagger is a set of open-source tools that are built around OpenAPI.  \n- Swagger Annotations: Annotations that can be added to Java implementations to generate OpenAPI specifications.\n- Swagger Editor: Editor for writing OpenAPI specifications.\n- Swagger UI: Renders OpenAPI specifications into an interactive API documentation with which REST services can be tested.\n- Swagger Codegen: Generates server and clients from an OpenAPI specification.  \n```java\n@Singleton\n@Path(\"/products\")\n@OpenAPIDefinition(\ninfo = @Info(\ntitle =\"Products\",\ndescription =\"Service to manage products\",\nversion = \"2022.05\"\n),\nservers = @Server(url = \"http://localhost:3001\")\n)\npublic class ProductResource{\n\n@GET\n@Path(\"{id}\")\n@Operation(\nsummary = \"Get product by id\",\ndescription = \"Returns a single product\",\nresponses = {\n@ApiResponse(responseCode = \"200\",\ndescription = \"Successful operation\",\ncontent = @Content(\nschema = @Schema(implementation = Product.class)\n)),\n@ApiResponse(responseCode = \"404\",\ndescription = \"Product not found\"\n),\n}\n)\npublic Response getProduct(@PathParam(\"id\") String id) { ... }\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Knowledge", "Header 2": "Definitions", "path": "../pages/digitalGarden/cs/distributedSystems/generalKnowledge.mdx"}, "page_content": "A distributed system is a set of interacting active components which are located in different locations and realize a common application. Each active component has its it's own independent set of instructions which means they can also run in parallel/concurrently. The location of an active component can be physically different to its other nodes in the system but it can also just be logically for example a different process.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Knowledge", "Header 2": "Advantages and disadvantages", "path": "../pages/digitalGarden/cs/distributedSystems/generalKnowledge.mdx"}, "page_content": "Distributed systems can be used by multiple users at the same time that can interact with each other.  \nDue to the concurrent nature of distributed systems it can also come with improvements in performance, scalability and use of idle resources.  \nDepending on your design of the system you can also achieve higher reliability, stability and fault tolerance. For example if you have two of the same component running on different machines in different locations and one goes down you still have the other one running as a backup. Between the two components you can also split up the load instead of having one component constantly overloaded.  \nDistributed systems do however come with a multitude of disadvantages.  \n- If the components are located in different locations or on different machines you are depending on several physical components however you also do not have a single point of failure.\n- Designing the system can be much more complex as you might need more complex algorithms to manage consistency problems between the components and also have to take extra security precautions.\n- You also need to make sure that deployment can be orchestrated cleanly and that versioning is done correctly.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Knowledge", "Header 2": "Client and server model", "path": "../pages/digitalGarden/cs/distributedSystems/generalKnowledge.mdx"}, "page_content": "Clients send requests to servers and therefore actively initialize communication with the server. In most cases Clients work with multiple servers at the same time.  \nServers provide some service/functionality and wait passively for requests from clients and can typically handle the requests concurrently or with queues. Server don't necessarily have to be on separate devices. One device can have multiple servers running at the same time.  \n![clientServerModel](/compSci/clientServerModel.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Knowledge", "Header 2": "Communication", "Header 3": "Synchronous and Asynchronous", "path": "../pages/digitalGarden/cs/distributedSystems/generalKnowledge.mdx"}, "page_content": "Synchronous communication happens when messages between sender and receiver are exchanged in real time. An example of synchronous communication is human communication like a phone call or video meeting.  \nAsynchronous communication happens when messages can be exchanged independent of time. It doesn’t require the receiver's immediate attention, allowing them to respond to the message at their convenience. Examples of asynchronous communication are emails, online forums etc.  \n![syncAndAsync](/compSci/syncAndAsync.png)  \nInteresting you can emulate asynchronous communication with synchronized calls and vice versa.  \n```java title=\"Emulation of async call\"\nid = service.submit(args);\n// do something else\nif(service.isReady(id)){\nres = service.getResult(id);\n}\n```  \n```java title=\"Emulation of sync call\"\nex.submit(task, handler);\nwhile(!handler.isReady(id)){\n// busy waiting\n}\nres = handler.getResult(id);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Knowledge", "Header 2": "Communication", "Header 3": "Styles", "path": "../pages/digitalGarden/cs/distributedSystems/generalKnowledge.mdx"}, "page_content": "#### Remote procedure calls - RPC  \nWhen using this style of communication one process calls a procedure (subroutine or service) to execute in a different address space than its own. The procedure may be on the same system or a different system connected on a network and in most cases are synchronous. System calls in the unix operating systems are an example of this.  \nYou can differentiate between Procedural RPC where the server provides a set of operations and is typically stateless for example GraphQL and Object oriented RPC where the server hosts a set of object and typically has it's own state for example RMI.  \n#### Massage based systems  \nIn message based systems like MQTT or RabbitMQ information is exchanged through messages. These messages can either be synchronous when exchanged over TCP or UDP, or they can be asynchronous for example in the case of MQTT where you have subscriptions.  \n![messageSystem](/compSci/messageSystem.png)  \n#### Shared repository  \nShared repository systems provide small interfaces which allow tuples to be created, read and deleted. REST would come under this category.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Records", "path": "../pages/digitalGarden/cs/java/records.mdx"}, "page_content": "With JDK 14 [record classes](https://docs.oracle.com/en/java/javase/18/language/records.html) were introduced, which are\na new kind of type declaration. They are especially useful for passing around immutable data containers. For example\nconsider the immutable class below.  \n```java filename=\"Rectangle.java\"\npublic final class Rectangle {\nprivate final double length;\nprivate final double width;\n\npublic Rectangle(double length, double width) {\nthis.length = length;\nthis.width = width;\n}\n\n// getters\ndouble length() {\nreturn this.length;\n}\n\ndouble width() {\nreturn this.width;\n}\n\n// Implementation of equals() and hashCode(), which specify\n// that two record objects are equal if they\n// are of the same type and contain equal field values.\npublic boolean equals(Object other) {...}\n\npublic int hashCode() {...}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Records", "path": "../pages/digitalGarden/cs/java/records.mdx"}, "page_content": "public int hashCode() {...}\n\n// An implementation of toString() that returns a string\n// representation of all the record class's fields,\n// including their names.\npublic String toString() {...}\n}\n```  \nThe following record is equivalent to the above.  \n```java filename=\"Rectangle.java\"\nrecord Rectangle(double length, double width) {}\n```  \nA record consists of a name and a list of components (length and width). A record automatically provides the following\nfunctionalities:  \n- A private final field for each of its components.\n- A public read accessor (Getter) method for each component with the same name and type of the component (without get,\nso length() not getLength()).\n- A public canonical constructor which initializes all components.\n- Implementations of the equals() and hashCode() methods, which specify that two records are equal if they are of the\nsame type and their components are equal.\n- An implementation of the toString() method that includes the string representation of all the record's components,\nwith their names. For example \"rectangle[length=12, width=10]\".  \nThere are however some restrictions when working with records:  \n- Records cannot extend any class (because they already extend the Record class just like enums extend the Enum class).\n- Records cannot declare instance fields (apart from the private final fields in the component list).\n- Records cannot extend other records and therefore also can't be abstract because they are implicitly final.\n- The components of a record are implicitly final, they can not be made mutable.  \nThere are however some things you can still do:  \n- You can declare a record inside a class however a nested record will be implicitly static.\n- You can create generic records\n- Records can implement interfaces\n- You can declare in a record's body static methods, static fields, static initializers, constructors, instance methods,\nand nested types\n- You can annotate records and a record's individual components", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Records", "Header 2": "Canonical and compact constructors", "path": "../pages/digitalGarden/cs/java/records.mdx"}, "page_content": "Records automatically generate a canonical constructor which initializes all components. You can however also define the\ncanonical constructor yourself for example if you want to add validation.  \n```java filename=\"Rectangle.java\"\nrecord Rectangle(double length, double width) {\npublic Rectangle(double length, double width) {\nif (length <= 0 || width <= 0) {\nthrow new java.lang.IllegalArgumentException(String.format(\"Invalid dimensions: %f, %f\", length, width));\n}\nthis.length = length;\nthis.width = width;\n}\n}\n```  \nHaving to rewrite the component list as parameters for the constructors can be tiresome and also very error-prone which\nis why compact constructors which were introduced whose signature is derived from the component list. At the end the\ncompact constructor also assigns parameters to the corresponding private fields.  \n```java filename=\"Rectangle.java\"\nrecord Rectangle(double length, double width) {\npublic Rectangle {\nif (length <= 0 || width <= 0) {\nthrow new java.lang.IllegalArgumentException(String.format(\"Invalid dimensions: %f, %f\", length, width));\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Dependency Injection and Beans", "path": "../pages/digitalGarden/cs/java/spring/dependencyInjectionBeans.mdx"}, "page_content": "Dependency Injection is a design pattern used in software development and is especially used in the Spring framework.\nDependency Injection allows objects to be created with their dependencies explicitly provided, instead of the objects\nthemselves having to be responsible for the creation of their dependencies. It is a way to achieve loose coupling\nbetween objects and their dependencies. This is also referred to as Inversion of Control, IoC which is another principle\nwhere instead of as in traditional programming, a component would create and control the objects it depends on in IoC the\nresponsibility is shifted to a container or framework, in Springs case the Spring Container.  \n![springIOC](/cs/springIoC.png)  \nWhen working with Spring Dependency Injection is controlled by the Spring Container which is responsible for creating\nand managing the lifecycle of objects, and injecting their dependencies. The objects are referred to as beans. These\nbeans are then kept track of in the Spring/Application Context which can be fetched as follows:  \n```java\n@SpringBootApplication\npublic class MyApplication {\npublic static void main(String[] args) {\nApplicationContext context = SpringApplication.run(MyApplication.class, args);\nfor (String beanName : context.getBeanDefinitionNames()) {\nSystem.out.println(beanName);\n}\n}\n}\n```  \nFor the Spring Container to know what is a bean you use to define them using XML but nowadays, you just annotate Java\nClasses. The parent annotation is `@Bean`, however most of the time you will be using different ones such as `@Entity`,\n`@Component`, `@Controller`, `@Service` which all inherit from it. Once the bean is defined, the container reads manages\nit and can inject it into its dependencies. Dependency injection is also commonly referred to as auto-wiring\nrelationships between beans, which is why in Spring you use the annotation `@Autowired` for a dependency to be injected.  \nBeans are found and created most via a component scan. Packages are scanned for annotated classes. Scan is started in", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Dependency Injection and Beans", "path": "../pages/digitalGarden/cs/java/spring/dependencyInjectionBeans.mdx"}, "page_content": "relationships between beans, which is why in Spring you use the annotation `@Autowired` for a dependency to be injected.  \nBeans are found and created most via a component scan. Packages are scanned for annotated classes. Scan is started in\npackage of main class and then down the tree so make sure no beans above main package! can also override basePackage to\nstart the scan.  \n```java\n@Component(\"fooFormatter\")\npublic class FooFormatter {\npublic String format() {\nreturn \"foo\";\n}\n}\n```  \nWe can then inject the dependency in many ways such as Constructor injection, Setter injection or Field injection.\nConstructor being the best practice and the other being heavily debated:  \n```java\n@Component\npublic class FooServiceField {\n@Autowired\nprivate FooFormatter fooFormatter;\n}\n@Component\npublic class FooServiceSetter {\nprivate FooFormatter fooFormatter;\n@Autowired\npublic void setFormatter(FooFormatter fooFormatter) {\nthis.fooFormatter = fooFormatter;\n}\n}\n@Component\npublic class FooServiceConstructor {\nprivate FooFormatter fooFormatter;\n@Autowired // optional\npublic FooServiceConstructor(FooFormatter fooFormatter) {\nthis.fooFormatter = fooFormatter;\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Dependency Injection and Beans", "Header 2": "Qualifiers and Primary Beans", "path": "../pages/digitalGarden/cs/java/spring/dependencyInjectionBeans.mdx"}, "page_content": "use qualifiers so that when interface is to be injected we can choose implementation. Or mark one as primary, i.e the\ndefault one to inject. Show example of in constructor", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Dependency Injection and Beans", "Header 2": "Spring Profiles", "path": "../pages/digitalGarden/cs/java/spring/dependencyInjectionBeans.mdx"}, "page_content": "Can setup different profiles for example in dev you want h2 database and matching services etc. and then for in prod use\npostgres or mysql. Or could also use for different languages, why would an api be language dependent? 2 beans with same\nname but then set different profiles and in application.properties set the active profile. there is a default profile and\nyou can have multiple profiles.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Dependency Injection and Beans", "Header 2": "Spring Bean Lifecycle", "path": "../pages/digitalGarden/cs/java/spring/dependencyInjectionBeans.mdx"}, "page_content": "all beans are made ready before application is considered ready for use. Can hook onto certain events to do certain thigns.\nRarely need to do anything with this stuff. PreDestroy annotated method much more lightly to use.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Dependency Injection and Beans", "Header 2": "Spring Stereotype", "path": "../pages/digitalGarden/cs/java/spring/dependencyInjectionBeans.mdx"}, "page_content": "@Component, Controller etc. A certain set of characteristics expected with a bean. Not always functional could also just\nbe for readability/documentation. thia ahould prob be mentioned somewhere above.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Dependency Injection and Beans", "Header 2": "Bean Scopes", "path": "../pages/digitalGarden/cs/java/spring/dependencyInjectionBeans.mdx"}, "page_content": "default is singelton, one isntance in the spring container. Other possibilites are prototype where new instance for each\nrequest and then more neach scopes such as request, session , global session, application??? and websocket. Could also\nmake custom scope why idk. Set using scope annotation, almsot always singelton is fine.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "From Java to Kotlin", "Header 2": "Kotlin and the JVM", "path": "../pages/digitalGarden/cs/kotlin/fromJava.mdx"}, "page_content": "Kotlin was created by JetBrains in 2011 as an improved alternative to Java. Kotlin is more modern, concise, and safer\nthan Java. Kotlin has gained a lot of popularity as it just like Java but better in many ways, Google even declared\nas the primary language for Android development.  \nKotlin was designed to be fully interoperable with Java. This means that you can use Java classes, methods and libraries\nin Kotlin and vice versa. This is possible because Kotlin code can be compiler into Java bytecode. The Kotlin compiler\ngenerates `.class` files just like the Java compiler that can then be executed on the Java Virtual Machine (JVM).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "From Java to Kotlin", "Header 2": "Entry Point", "path": "../pages/digitalGarden/cs/kotlin/fromJava.mdx"}, "page_content": "Just like Java, Kotlin uses a main function as an entry point. However in Java it always needs to be able to handle the program arguments by using either of these possibilities:  \n```java\npublic static void main(String args[]) {\nSystem.out.println(\"Hello World!\");\n}\n```  \nor  \n```java\npublic static void main(String... args) {\nSystem.out.println(\"Hello World!\");\n}\n```  \nIn Kotlin it is like in C you can either define the program arguements or not, if you don't define them they will just be ignored.  \n```kotlin\nfun main() {\nprintln(\"Hello world!\")\n}\n```  \nor  \n```kotlin\nfun main(args: Array<String>) {\nprintln(\"Hello world!\")\n}\n```  \nWhere Array is a wrapper with a lot of additional functioanlity for a normal Java array.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "From Java to Kotlin", "Header 2": "Semicolons", "path": "../pages/digitalGarden/cs/kotlin/fromJava.mdx"}, "page_content": "Unlike in Java where semicolons are mandatory to end a statement, in Kotlin semicolons are optional. However, there are\nsome situations where you may need to use semicolons in Kotlin to separate multiple statements on a single line.  \n<SideBySideBlock>\n<Block>\n```kotlin filename=\"Kotlin\"\nval a = 0\nval x = 5; val y = 10; println(x + y)\n```\n</Block>\n<Block>\n```java filename=\"Java\"\nfinal var a = 0;\nfinal var x = 5; final var y = 10; System.out.println(x + y);\n```\n</Block>\n</SideBySideBlock>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "From Java to Kotlin", "Header 2": "Standard Output", "path": "../pages/digitalGarden/cs/kotlin/fromJava.mdx"}, "page_content": "In Java the `java.lang` package is implicitly imported which contains amongst other things the classes for String, Integer\nand important in this section the System class which is used to output things to the standard output via\n`System.out.println()`. In Kotlin `java.lang` is implicitly imported but so is the `kotlin` package and more importantly\nfor this section the `kotlin.io` package (plus a few other packages). The `kotlin.io` package contains, as with a lot of\nthings in Kotlin the wrapper function `println()` which internally just calls `System.out.println()`. This then allows you\nto not always have to write such verbose code (you can still however of course use `System.out.println()` in kotlin, but don't).  \n<SideBySideBlock>\n<Block>\n```kotlin filename=\"Kotlin\"\nprintln(\"hello world!\")\n```\n</Block>\n<Block>\n```java filename=\"Java\"\nSystem.out.println(\"hello world!\");\n```\n</Block>\n</SideBySideBlock>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "From Java to Kotlin", "Header 2": "`val` and `var`", "path": "../pages/digitalGarden/cs/kotlin/fromJava.mdx"}, "page_content": "Type inference was introduced in Java 10 with the introduction of the `var` keyword. In Kotlin, you use `var` to define a\nmutable variable. By default, the type can be inferred, but it can also be explicitly defined after `:`. In Kotlin, you can\nalso use the `val` keyword which the same as writing `final var` in Java, i.e. the variable is then immutable (read-only).  \n<Callout type=\"warning\">\nThe final keyword exists in Kotlin, however can not be used in conjunction with variables, only to stop inheritance\nof\na class method. I know very confusing.\n</Callout>  \n<SideBySideBlock>\n<Block>\n```kotlin filename=\"Kotlin\"\nvar a = 1\nvar b // doesn't work, how much memory???\nvar c : Int;\nc = 4\nvar d : Int = 5\nval e = 10\ne = 15 // will fail\n```\n</Block>\n<Block>\n```java filename=\"Java\"\nvar a = 1; // java also has type inference\nvar b; // also doesn't work\nint c;\nc = 4;\nint d = 5;\nfinal int e = 10;\ne = 15; // will also fail\n```\n</Block>\n</SideBySideBlock>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Ridge Regression", "Header 2": "Bias and Variance", "path": "../pages/digitalGarden/cs/machineLearning/ridge.mdx"}, "page_content": "What exactly is overfitting? 2 factors play a role in overfitting. Bias and Variance. Bias is the difference between\nthe average prediction of our model and the correct value which we are trying to predict. Bias can determine if we\ngot the relationship correct, so we fit the curve correctly. Variance is the variability of model prediction for a\ngiven data point. If a model has no variance then it will fit badly to unseen data.  \nThere are a few ways to combat overfitting. One way is to use cross validation which will allow the model to\ngeneralize better to unseen data. Another way is to use regularization. Regularization is a technique that penalizes\ncomplexity. There is then also boosting and bagging. Bagging is used in random forests.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Backpropagation", "Header 2": "Forward Pass", "path": "../pages/digitalGarden/cs/machineLearning/backprop.mdx"}, "page_content": "The forward pass, sometimes also called forward propogration, is the process of calculating the output of a neural network given an input. This is done by running the input through the network layer by layer,\nand applying the activation function to the output of each layer. The output of the last layer is the output of the network.  \nSay we have a simple neural network with three linear layers, the input layer of size 2, a hidden layer of size 2 and an output layer of size 1. We will use the\nsigmoid activation function for the hidden layer and the output layer.  \n<Image src=\"/cs/mlSimpleNeuralNetwork.png\"\ncaption=\"A simple neural network.\"\n/>  \nEach linear layer is defined by a weight matrix $\\boldsymbol{W}$ and a bias vector\n$\\boldsymbol{b}$. The vector $\\boldsymbol{z}$ contains the pre-activations and the vector $\\boldsymbol{a}$ the activatiosn, i.e. outputs of the layer.  \n$$\n\\begin{align*}\n\\boldsymbol{z} &= \\boldsymbol{W}\\boldsymbol{x} + \\boldsymbol{b} \\\\\n\\boldsymbol{a} &= \\sigma(\\boldsymbol{z})\n\\end{align*}\n$$  \nLet's say we have the following input, weights and biases:  \n| Variable | Value |\n| -------- | ----- |\n| $x1$     | 0.888 |\n| $x2$     | -0.49 |\n| $w1$     | 1.76  |\n| $w2$     | 0.4   |\n| $w3$     | 0.97  |\n| $w4$     | 2.24  |\n| $w5$     | 1.86  |\n| $w6$     | -0.97 |\n| $b1$     | 0     |\n| $b2$     | 0     |\n| $b3$     | 0     |  \nThen we can calculate the output of the network as follows. First we calculate the output of the hidden layer.  \nEvaluation Trace or Wengert list??? https://pub.towardsai.net/a-gentle-introduction-to-automatic-differentiation-74e7eb9a75af  \n$$\n\\begin{align*}\na1 &= x1w1 + x2w2 + b1 \\\\\n&= 0.888 * 1.76 + -0.49 * 0.4 + 0 \\\\\n&= 1.367\n\\\\\nh1 &= \\sigma(a1) \\\\\n&= \\frac{1}{1 + e^{-a1}} \\\\\n&= \\frac{1}{1 + e^{-1.367}} \\\\\n&= 0.797\n\\end{align*}\n$$  \nIf we do the same for the other neuron we get $a2 = 0.888 * 0.97 + -0.49 * 2.24 + 0 = -0.236$ and $h2 = \\sigma(-0.236) = 0.441$. Now we have our hidden layer outputs,\nwe can calculate the output of the network.  \n$$\n\\begin{align*}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Backpropagation", "Header 2": "Forward Pass", "path": "../pages/digitalGarden/cs/machineLearning/backprop.mdx"}, "page_content": "\\end{align*}\n$$  \nIf we do the same for the other neuron we get $a2 = 0.888 * 0.97 + -0.49 * 2.24 + 0 = -0.236$ and $h2 = \\sigma(-0.236) = 0.441$. Now we have our hidden layer outputs,\nwe can calculate the output of the network.  \n$$\n\\begin{align*}\na3 &= h1w5 + h2w6 + b3 \\\\\n&= 0.797 * 1.86 + 0.441 * -0.97 + 0 \\\\\n&= 1.055\n\\\\\ny &= \\sigma(a3) \\\\\n&= \\frac{1}{1 + e^{-a3}} \\\\\n&= \\frac{1}{1 + e^{-1.055}} \\\\\n&= 0.741\n\\end{align*}\n$$  \n<Image src=\"/cs/mlForwardPass.gif\"\ncaption=\"The forward pass of the simple neural network.\"\n/>  \nWe can also write these calculations in matrix form which is more efficient and easier to generalise to larger networks.  \n<Callout type=\"todo\">\nDo matrix form\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Backpropagation", "Header 2": "Backpropagation", "path": "../pages/digitalGarden/cs/machineLearning/backprop.mdx"}, "page_content": "The backpropagation algorithm is the key to training neural networks. It is the process of calculating the gradients of the loss function with respect to the\nweights and biases of the network. These gradients are then used to update the weights and biases using gradient descent to minimise the loss function.  \nThe backpropagation algorithm is based on the chain rule from calculus. So lets start with a brief reminder of the chain rule.  \n<Callout type=\"info\" title=\"Chain Rule\">\nIf we have a the differentiable functions $f(x)$ and $g(x)$ and the composite function $h(x) = f(g(x))$, i.e where\nthe function $f$ is applied to the output of $g$, then the derivative of $h$ with respect to $x$ is given by:  \n$$\nh'(x) = f'(g(x))g'(x) \\text{ or } \\frac{dh}{dx} = \\frac{df}{dg}\\frac{dg}{dx}\n$$  \nNotice that the denominator $dg$ is the the same as the following numerator, this can be thougth of as \"the chain\". The chain rule also makes sense\nintuitively, if we think of $dg$ cancelling out in the numerator and denominator.  \nIt is a simple but powerful rule that allows us to calculate the derivative of a composite function. For example, if we have $h(x) = (x^2 + 1)^3$,\nthen we can write $h(x) = f(g(x))$ where $f(x) = x^3$ and $g(x) = x^2 + 1$. The derivative of $h$ is then given by:  \n$$\n\\begin{align*}\nh'(x) &= f'(g(x))g'(x) \\\\\n&= 3(x^2 + 1)^2 * 2x \\\\\n&= 6x(x^2 + 1)^2\n\\end{align*}\n$$  \nThis also works for more obvious composite functions such as $h(x) = \\sin(x^2 + 1)$.  \nThe key take away is that the derivative of a composite function can be calculated step by step, by first calculating the derivative of the most\ninner function, then the next inner function and so on. This is the key idea behind backpropagation as a neural network is just one big composite function with\nlots of variables and lots of inner functions.  \nTODO: Multiple variables\n</Callout>  \nshow the idea. The chain rule. then the full derivation.  \nShow nice reformulation of the loss function to make the calculation easier.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Backpropagation", "Header 2": "Backpropagation", "Header 3": "Vanishing and Exploding Gradients", "path": "../pages/digitalGarden/cs/machineLearning/backprop.mdx"}, "page_content": "The vanishing and exploding gradient problem is a problem that occurs when training very deep neural networks. It is caused by the chain rule and the fact that the\ngradient of the loss function is calculated by multiplying the gradients of each layer together. If the gradients are small, then multiplying them together will\nmake them even smaller. This is the vanishing gradient problem. If the gradients are large, then multiplying them together will make them even larger. This is the\nexploding gradient problem.  \nThere are many possible solutions to this problem. Some of the most common are:  \n- Different activation function such as ReLU or Leaky ReLU.\n- Batch Normalisation.\n- Residual connections, also known as skip connections.  \nWe can see the vanishing gradient problem pretty easily by looking at the derivative of the sigmoid function.  \n<div className=\"flex justify-center mt-5\">\n<iframe src=\"https://www.desmos.com/calculator/xs6tgfd48r?embed\" width=\"500\" height=\"500\"/>\n</div>  \nThe derivative of the sigmoid function is always less then 0.25, multiplying this together for each layer will make the gradient very small.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Backpropagation", "Header 2": "Automatic Differentiation", "path": "../pages/digitalGarden/cs/machineLearning/backprop.mdx"}, "page_content": "computational graph, show how it works. Why does plus copy the gradient?  \nSo far we have been calculating the gradients of the loss function with respect to the weights and biases by hand. This is fine for toy examples, but it is not\npractical for larger networks. Luckily, there is a technique called automatic differentiation that allows us to calculate the gradients automatically.  \nAutomatic differentiation is also based on the chain rule and the fact that every operation in a neural network uses elementary operations such as addition,\nmultiplication and elementary functions such as sine, cosine and the exponential function.  \nUsing these elementary operations, we can build a computational graph of the neural network. A computational graph is a directed acyclic graph where the nodes\nrepresent the operations and the edges represent the flow of data. The computational graph is then used to calculate the gradients of the loss function with\nrespect to the weights and biases.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "MLP - Multi Layer Perceptron", "path": "../pages/digitalGarden/cs/machineLearning/mlp.mdx"}, "page_content": "Combination of linear regressors with non-linear activation functions.  \nHave been proven to be universal function approximators.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "Week 1 - Neural Networks", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "NN are good for unstrucutred data but also strucuted  \ndeep learning has risen because of the amount of data and computational power. Graph of performance over time with model size and performance. also new algorithms like backpropagation and better activation functions.  \nInference is the process of predicting the output of the model given the input. Inference is also called prediction or testing.  \nTrying to mimic the human brain. The brain is made up of neurons. Neurons are connected to each other. Not really true tho?  \nduring 1980 and 90s trendy but then died out. Now it is back. minst dataset.  \nComputer vision 2012 Imagenet challenge. AlexNet. 2014 VGGNet. 2015 ResNet. 2016 Inception. 2017 Xception. 2018 NASNet. 2019 EfficientNet.  \nRecently NLP Chatbots, Google Duplex, GPT-3, etc.  \nNeurons take input and then send outputs/activations.  \nDemand prediction for a store. Input is the item and output is the demand. Depends on awaness, price, etc. combine neurons to make a model. Form Layers input layers, hidden and output  \nFully connected because can learn to ignore some features. NN automatically does feature engineering. Can learn to ignore some features. Can learn to combine features. Can learn to create new features. Can learn to do feature engineering.  \nCan turn a picture into a vector of numbers and then use that as input to the NN then classify, but not great. This works for MNST.  \nMatrix notation is nice. Can do matrix multiplication instead of for loops. Can do matrix multiplication in parallel. Can do matrix multiplication on a GPU.  \nSo called forward propagation if we download weights. If we train the model, then it is called back propagation.  \nTensorflow and numpy have incosisency, rows first then columns. if only one dimension i.e [200,17] this is 1d vector i.e 2 rows but technically just an array not [[200,17]] which is 1 row 2 columns.  \nSequential string Dense layers togehter.  \nmust implement the forward propagation and the back propagation by hand one time. Then can use a library.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "Week 1 - Neural Networks", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "Sequential string Dense layers togehter.  \nmust implement the forward propagation and the back propagation by hand one time. Then can use a library.  \ncan first do for loop an dot product then can do matrix multiplication to go faster. and make code cleaner.  \nWhy use a non-linear activation very important! otherwise just linear regression. Can use sigmoid function. Can use tanh function. Can use ReLU function. Can use leaky ReLU function. Can use softmax function.  \nshow the speed difference of using numpy with vectors rather than for loops. Probably already noticable with 1000 samples and a simple operation like sigmoid.  \nwhere is broadcasting used in a normal linear layer or activation function?  \nlog loss is actually just maximum likelihood estimation, which makes sense.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "Week 2 - Neural Network Training", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "BinaryCrossentropy is the same as log loss. CategoricalCrossentropy is the same as log loss but for multiple classes.\nepochs is the iterations of the training, i.e GD. batch size is the number of training samples to use to calculate the gradient.  \nkeras got merged into tf.  \nloss/cost function is the same thing.  \ncompute the gradient for the descent using backpropagation so we don't anallytically compute the gradient by the hand anymore.  \nlinear activation function is just the identity function or like no activation function.  \nproof of relu being differentiable???? is max(0,x) differentiable at x=0? no. but it is differentiable everywhere else. so it is differentiable almost???  \nDepending on what output u want the last layer has different activation functions. For example, if you want a binary output, then use sigmoid. If you want a multi-class output, then use softmax.\nIf you want a regression output, then use linear. if you only want positive values, then use ReLU.  \nRelu is faster than sigmoid and tanh. ReLU is also more stable than sigmoid and tanh, stable meaning that GD is less likely to saturate. ReLU is also more biologically plausible than sigmoid and tanh.  \nfor non final layers the tanh activation is almost always better then the sigmoid. tanh is just a shifted sigmoid. tanh is also zero centered which has some benefits, same as standardizing the input.  \nwhat are pros and cons of leaky relu and the derivative of these?  \nhow should we initialize the weights? random normal distribution with mean 0 and std 1. or random uniform distribution between -1 and 1. or xavier initialization. can have an impact on the training. can also have an impact on the final performance of the model.  \nfirst build a shallow neural network to learn and calucalte.  \n#### Multiclass  \nMNIST dataset. 10 classes. multiline decision boundary? softmax is generalization of logistic regression. softmax is the multiclass version of the sigmoid function.  \nsoftmax regression. 4 output neurons z1...z4. for 4 classes.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "Week 2 - Neural Network Training", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "#### Multiclass  \nMNIST dataset. 10 classes. multiline decision boundary? softmax is generalization of logistic regression. softmax is the multiclass version of the sigmoid function.  \nsoftmax regression. 4 output neurons z1...z4. for 4 classes.  \na4 = softmax(z4) = e^z4 / sum(e^z1 + e^z2 + e^z3 + e^z4) etc. for all 4 activations, tjese all sum to 1. so they can be interpreted as probabilities. the highest probability is the prediction.  \nthe loss then also becomes categorical cross entropy or just cross entropy a generalization of log loss. etc.  \nSparseCategoricalCrossentropy??? Sprase means only one class is true and the rest are false??? but apparently better version in tf???  \nBecause of the small numbers in cross entropy can lead to floating point errors so we use more numerically accurate version of cross entropy.\nSo for logistic regression we live it up to tenserflow of whether it wants to first compute the indermieate value after the linear layer, the so called logits or if it wants to\nuse the logits directly in the cross entropy function. so instead use from_logits=True in the cross entropy function and the linear activation function in the last layer.  \nThese errors are worse for softmax because the numbers are even smaller. So we use the same from_logits=True for softmax.  \nDoes pytorch have the same problem?  \n#### Multilabel  \nMultilabel classification is when there are multiple correct labels. For example, a picture of a dog and a cat. The correct labels are dog and cat. Another example is a picture of a dog and a person. The correct labels are dog and person.  \ncould build a network for each label but this is not good. So we use a single network for all the labels. The output layer has multiple neurons, one for each label. The activation function is sigmoid because we want the output to be between 0 and 1 for each label.  \nwhich loss is used????  \n#### GD and Backpropagation  \nbetter optimizer algorithms, like adam, rmsprop, etc. with momentum, explain all of these.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "Week 2 - Neural Network Training", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "which loss is used????  \n#### GD and Backpropagation  \nbetter optimizer algorithms, like adam, rmsprop, etc. with momentum, explain all of these.  \nAnother layer could be convultiotional etc. but we will get to that later. 1d conv layer does anything??? Cant the network find that out itself?  \nbackprop uses a computation graph, where for each operation we define the forward and the backwards prop, i.e how the gradient is calculated. Then we can just use the chain rule to calculate the gradient of the cost function with respect to the parameters.  \nauto diff is cool. can just define the forward prop and then it will automatically calculate the backward prop. can also use this to calculate the gradient of the cost function with respect to the parameters.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "Week 3 - Advice for Applying Machine Learning", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "Diagnostics are tests that you can run to gain insight into what is/isn't working with your model.  \ntraining and test set. This is useful to see if the model is overfitting or underfitting. We can also count the number of correct and wrong classifications and calculate metrics like recall etc.  \nCross validation is when you split the training set into multiple training and validation sets. This is used to tune the hyperparameters of the model such as degrees of polynomial, regularization parameter, etc.  \ncross valuidation or just validation/dev and then test set. use the validation results to pick the best model and then use the test set to get the final performance of the model.  \nWhy do we add the validation and not just use the test to select, this was explained but didn't really get it.\nK-fold cross validation is not mentioned.\nplotting the training loss and the validation loss of multiple models is a good way to see if the model is overfitting or underfitting. A sweet spot of hyperparameters is when the training loss is low and the validation loss is low.  \nIf the training loss is low and the validation loss is high, then the model is overfitting. If the training loss is high and the validation loss is high, then the model is underfitting. If the training loss is low and the validation loss is low, then the model is good.  \nestablishing baselines is important. For example, if we have a binary classification problem, then we can use a baseline of 50% accuracy, i.e. random guessing. We can also use other already existing models as baselines or human performance as a baseline.  \nHaving more data doesn't always help, especialy if the algorithm suffers from high bias.Can see this by looking at the impact of the training set size on the training and validation loss.  \nWhen improving models, look at bad examples, maybe you can see a pattern, like when classifying cats and dogs, maybe the model is bad at classifying black cats, etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "Week 3 - Advice for Applying Machine Learning", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "When improving models, look at bad examples, maybe you can see a pattern, like when classifying cats and dogs, maybe the model is bad at classifying black cats, etc.  \nWhen then adding more data can just add general data or can add more data of the bad examples. Maybe you can also find new feature from the bad examples.  \nData augmentation is when you generate new data from the existing data. For example, if you have a picture of a cat, then you can flip the picture horizontally to get a new picture of a cat. This is useful when you don't have enough data. or make it black and white, etc.\nThis is very useful for computer vision in helping the model generalize better. For speech recognition, you can add noise to the audio clip. For text classification, you can add spelling mistakes to the text etc.  \nTransfer learning is when you use a model that was trained on a different task as a starting point for your model. For example, you can use a model that was trained on ImageNet as a starting point for your model. This is useful when you don't have enough data. This is also useful when you don't have enough computational resources to train a model from scratch.\nThe idea is that the model has already learned some features that are useful for your task some general things. Just swap the last layer and then train the model on your task. This is called fine-tuning. This is also called transfer learning.  \nTransferLearning of Imagenet onto cat vs dog?  \nTrading of accuracy, recall and precision. Some data is skewd for example if you have 99% of one class and 1% of the other class. Then you can just predict the majority class and get 99% accuracy. But this is not a good model. So you can use recall and precision to evaluate the model.\nRecall is the percentage of the positive examples that are correctly classified. Precision is the percentage of the positive predictions that are correct.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "Week 3 - Advice for Applying Machine Learning", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "Recall is the percentage of the positive examples that are correctly classified. Precision is the percentage of the positive predictions that are correct.\nYou can also use the F1 score rather then taking the normal average of recall and precision. The F1 score is the harmonic mean of recall and precision.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Neural Networks and Deep Learning from ML Specialization", "Header 3": "other parts", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "show how to calculate the number of parameters in a NN. This is useful to see how many parameters you have to train. This is also useful to see how much memory the model will take up.\nwhy deep networks? Can think of layers as feature engineering. Can also think of layers as a hierarchy of concepts. For example, in computer vision, the first layer can learn edges, the second layer can learn shapes, the third layer can learn objects, etc.  \nfor the backwards prop, intermediate values are cached. So we can use these values to calculate the gradients using the chain rule and the computation graph.  \nparameters vs hyperparameters. parameters are the weights and biases of the model. hyperparameters are the learning rate, number of layers, number of neurons, etc.  \nsometimes can be a very empirical process to tune the hyperparameters. can use grid search or random search. can also use bayesian optimization. can also use gradient descent to tune the hyperparameters???", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "make sure training and dev and test set are from the same distribution. so not training from camara and testing on phone, this is for obvious reasons bad.  \nIf overfitting, then can use regularization. If underfitting, then can use a bigger network or more training data or different architecture etc.  \nin linear regression we use ridge and lasso regression or elastic net, a combination of both. The idea is to keep weights and biases small.  \nFor neural networks, we can use L2 regularization, which is the same as ridge regression. We can also use L1 regularization, which is the same as lasso regression. We can also use dropout regularization, which is a different type of regularization.  \nfor simplisity we can ignore regularization for the biases. lambda is the regularization parameter.  \nWhen we are using the matrix notation for the weights and samples, isntead we use the frobenius norm, which is the same as the L2 norm but for matrices. The frobenius norm is the square root of the sum of the squares of the elements of the matrix.\nIs this rly the same as the l2 norm? what About the l1 norm for matrices?  \nThis regularization is also called weight decay. This is because the weights are decayed over time.  \nWhy does regularaization work? forces entire network to be used and not just a few neurons. forces the weights to be small. forces the weights to be similar to each other. doesnt go into extremes of the activation functions.  \nA layer of dropout is a layer that randomly sets some of the activations to zero. This is done during training. During testing, all the activations are used. The idea is that the model can't rely on any one neuron. So it has to learn multiple ways of doing the same thing which can help with generalization.  \ndata augmention is also a form of regularization by adding more data or by adding noise to the data. Can help with generalization.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "data augmention is also a form of regularization by adding more data or by adding noise to the data. Can help with generalization.  \nearly stopping is also a form of regularization. The idea is that we stop training when the validation loss starts to increase. This is because the model is starting to overfit. Can be done manually and was a big thing for GANs.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera - Deep Learning Specialization", "Header 2": "Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization", "Header 3": "Vanishing and Exploding Gradients", "path": "../pages/digitalGarden/cs/machineLearning/courseraDL.mdx"}, "page_content": "Is especially the case with deep networks. The gradients can get very small or very big. This can cause the model to not train. This is because the gradients are used to update the weights and biases. If the gradients are very small, then the weights and biases won't change. If the gradients are very big, then the weights and biases will change too much.  \nThe gradients can get very small or very big because of the activation functions. For example, the sigmoid activation function can cause the gradients to get very small. The ReLU activation function can cause the gradients to get very big in the backpropagation.  \nthe weight initialization can also cause the gradients to get very small or very big. For example, if the weights are initialized to zero, then the gradients will be zero. If the weights are initialized to a very big number, then the gradients will be very big.  \nWe can use gradient checking to check if the gradients are correct, seems a bit weird??? mainly for debugging??? doesn't work for complex NN such as with dropout, etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Analysis", "Header 2": "Page Rank", "path": "../pages/digitalGarden/cs/machineLearning/linkAnalysis.mdx"}, "page_content": "very earily in times of internet how to organize web, first try was human curated web directoreis like yahoo, dmoz, look smart.  \nsecond try was web search, google. Wanted to find relevant docs from a small and trsuted set but web is huge full of untrusted documents and random pages.  \nChallanges wo to trust? The idea for this is that trustowrthy pages point to each other. What is best answer for query? The idea for this is that pages that know about a subject are pointing to many other pages", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Analysis", "Header 2": "Page Rank", "Header 3": "Flow Formulation", "path": "../pages/digitalGarden/cs/machineLearning/linkAnalysis.mdx"}, "page_content": "Idea is that links are votes, a page is more important if it has more links, do we look at in or out links? are all links equal?, links from important pages should count more => recursive question.  \nSimple recursive formulation is that each links vote is proportional to the importance of its source page. so if page j has important rj with n out link each link should get rj/n votes, js own importance rj is the sum of the votes of its inlinks. A vote from an important page is worth more this also fullfills the requirement of a page is important if it is pointed to by orther importange pages. this leads to n equations for n nodes and no constants so there is no unique solution. so to get a unique solution we add a constraint that all page ranks summed together equal 1. We can solve this by using gaussian elimation but this becomes very quickly very hard for large web graphs so we need a new formulation.  \nmatrix formulation, instead of the equations we write a column stochastic adjacency matrix, meaning each column sums up to 1. so if page i has di out links and there is a connection from i to j then Mji is 1/di otherwise 0.  \nwe then have the rank vector r which contains all the page rankings. so ri is the score for page i and all score summed = 1 as before. we can then write the flow equaiton as r=M*r.  \nIn other words the rank vector is an eigenvector of the adjacency matrix m with the eigenvalue 1. because the matrix is column stochstic the largest eigenvalue is 1.  \nNow to solve these flow equations we can use the power iteration method. Suppose there are N pages we initialize rank vector r0 with all values 1/N. we then iterate rt+1 = M*rt and stop when we no longer change the values much, i.e we converge. rt+1 -rt < epsilon. This is a method to find the dominant eigenvector i.e the vector corresponding to the largest eigenvalue which because it is column stochastic is 1.  \nr1 = M * r0\nr2 = M*r1 = M(M*r1) = M^2 * r0\netc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Analysis", "Header 2": "Page Rank", "Header 3": "Flow Formulation", "path": "../pages/digitalGarden/cs/machineLearning/linkAnalysis.mdx"}, "page_content": "r1 = M * r0\nr2 = M*r1 = M(M*r1) = M^2 * r0\netc.  \nWe can also interpret this using probailities on a random walk. the pagerank vector gives the stationary distribution for a random walk on a graph i.e", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Analysis", "Header 2": "Google Formulation", "path": "../pages/digitalGarden/cs/machineLearning/linkAnalysis.mdx"}, "page_content": "There is a problem with the above definition as it might not always converge. Google defined two types of problems that can occur in a network which stops the power iteration from converging.  \ndead ends i.e pages with no out links so a random walk can not leave so eventually importance leaks out to 0.  \nSpider traps all out links are within a group so random walk get stuck in a trap. these traps then slowly absorob all the importance.  \nThe soultion to both of these issues is teleports. To solve spider traps at each step the random walker/surfer has two options with a prob of beta it will follow a link at random with 1-beta it will teleport to some random page, common values for beta are 0.8 or 0.9.  \nTo solve the issue of dead ends we just adjust the adjacancy matrix by keeping the colum stochastic properity, so if a node has no out links it just becomes a unfirom distribution to all nodes.  \nThis slightly changes the page rank quation as now there is the additional probability of a teleport. this leads to the google matrix which can still be solved with the power iteration method.  \nBut what if we have 1 billion pages we can not really store this in main memory so we need to make use of the spasity of the matrix. we can do this by just storing the degree for each source node and its destination nodes. we can then store rold and M on the disk and rnew in memory and just calcualte row by row so we do not need to load the entire matrix into memory.  \nBut what if we can not even fit the entire rnew in memory, then we can use the so called block based update algorithm where we then just do block by block.  \nIn the above solution we are still scanning m and rold once for each block tho so we can do even better. which leads to block stripe update algorithm. Each strip contains only destination nodes corresponding to the blocks of rnew.  \nPage rank does however have its limitaitons. It only measures generic popularity of a page. The solution to this is topic specific page rank.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Analysis", "Header 2": "Google Formulation", "path": "../pages/digitalGarden/cs/machineLearning/linkAnalysis.mdx"}, "page_content": "Page rank does however have its limitaitons. It only measures generic popularity of a page. The solution to this is topic specific page rank.\nIt only uses a single measure of importance. Other models of importance are used in the hub and authorities extension.\nIt is susceptible to link spam, i.e artificial link topographies to boost page rank to solve the there is the trust rank.  \nTopic specific page rank is useful if we do not just want a generic popularity but a popularity within a topic. this allows search quieries to be answwered based on intersts of the users for example the query jaguar is it the animal or car?  \nThe idea here is a bias random walk so when teleporting instead of going to any page uniformly we only teleport to a topic-specific set. this gives pages closer to these topic specific pages a higher rank. to make this work all we need to do is update the teleportation part and give the ones with topic specific higher probability to be teleported to.  \nproximity on graphs i.e the relevance closeness or similarity of pages. shortest path is not good because slow and also does not take into account hubs. network flow is also not good as it does not punish long paths? dont know netowork flow lol.  \nRandom Walk with Restart. makes no sense  \nWhat is web spam? any deliberate action to boost web pages position. spam are web pages that are the result of spamming. Constant war, first search engines considered number of times query word appeared, prominece of word position. Add keywords hidden thousand of times, or links with high topic ranking. Googles solution is to look at what others say about u for example looking at anchor text that is linked to ur page.  \nspam farming. Inaccessible pages, accessible pages, owned pages. To combat spam the trsut rank was developed. which is topic specific page rank but with a teleport set of trusted pages like .edu or .gov pages.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Analysis", "Header 2": "Google Formulation", "path": "../pages/digitalGarden/cs/machineLearning/linkAnalysis.mdx"}, "page_content": "spam farming. Inaccessible pages, accessible pages, owned pages. To combat spam the trsut rank was developed. which is topic specific page rank but with a teleport set of trusted pages like .edu or .gov pages.  \nWe create a seed set by hand which contains good trusted pages. We can then use this as the teleport set. Trust attenuation is the degreee of trust which is spread across the graph. trust splitting across out links  \nspam mass = what fraction of a pages rank comes from spam pages. page rank p rp, rp+ page rank with trusted pages only in teleport page rank from spam pages is then the difference and the spam mass is the proportion.  \nHITS, hypertextinduced topic selection is a meassure of importance. we have hubs and authorities. Hubs are course bulleting, authoritizes hold usefull information. Authority each page start with hub score 1 authorities collect there votes, hubs collect authority scores. so it doesn't just infetly increase it is normalized that all hubs equal 1.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Convolution", "Header 2": "Of Vectors", "path": "../pages/digitalGarden/cs/machineLearning/convolution.mdx"}, "page_content": "For Deep learning.  \nIs just cross-correlation. Dot product of the 2 vectors after flattening the matrix (patch and kernel). Theorectically the one vector should be flipped upside down for\nit to not be a cross-correlation but nobody cares.  \nThis is associtiave in regards to applying filters? Where as cross-correlation isnt?  \nhttps://www.youtube.com/watch?v=4ERudRAxyGE this connects the vectors with the gradients  \nfor imag eprocessing we just flip the kernel not the image. Has impact on output size hence padding stride etc. IF we use convolution and not cross correlation i.e do flipping\nit allows us to do fft?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Convolution", "Header 2": "Of Functions", "path": "../pages/digitalGarden/cs/machineLearning/convolution.mdx"}, "page_content": "For Signal processing.  \nDiscrete vs continuos is very different.  \nof 2 functions is then another function where one of them is flipped on y axis and then slowly slid along the x axis over the other function.\nShould probably both be scaled to the same units then can interpret it better? But If resulting funciton is same as an input then the two inputs\nare the same??? How to interpret cross correlation.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cross Correlation", "Header 2": "Of Vectors", "path": "../pages/digitalGarden/cs/machineLearning/convolution.mdx"}, "page_content": "See above.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cross Correlation", "Header 2": "Of Functions", "path": "../pages/digitalGarden/cs/machineLearning/convolution.mdx"}, "page_content": "Same but the function isnt flipped.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cross Correlation", "Header 2": "Of Time Series", "path": "../pages/digitalGarden/cs/machineLearning/convolution.mdx"}, "page_content": "How is this interpretetd. Should say somethign about lags no? Because one is slid across the other it is correlation of one time series with the lagged time series of the other.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Auto Correlation", "path": "../pages/digitalGarden/cs/machineLearning/convolution.mdx"}, "page_content": "Is Cross Correlation but the two functions are the same so a cross correlation with itself. How is this interpretetd? Same goes here for time series is this interpreted?\nBecause it is slid across it is basically the correlation with the lagged version of it self. so each point is correlation of the series with the x lag of itself.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Auto Correlation", "Header 2": "Partial Auto Correltion", "path": "../pages/digitalGarden/cs/machineLearning/convolution.mdx"}, "page_content": "a, b, c. a with b are correlated and b with c, to measure true influence of c on a  need to take out the influence b had on a. Taking out inluence of other timespots on current.  \nAR model or MA model https://medium.com/@vaibhav1403/ar-model-vs-ma-model-427ee28587a#:~:text=How%20they%20differ%3A,white%20noise%20or%20error%20terms.  \n1 lag differencing is a way to detrend. subtract lag 1 values.  \nfirst order AR model is if lag 1 is used, second order if lag 2 etc. How many points are taken = order, simpler models prefered. Significance threhsold, how to get?  \nAR model uses partial AC, MA model uses full AC\nhttps://www.youtube.com/watch?v=5Q5p6eVM7zM  \nAlso known as Conditional correlation. Take out all intermediate effects.  \nhttps://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/  \ncorrelogram with Confidence intervals are drawn as a cone. By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation and not a statistical fluke.  \nThe partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.  \nThe autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.  \nIt is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation  \nhow it is calcualted: https://timeseriesreasoning.com/contents/partial-auto-correlation/", "type": "Document"}
{"id": null, "metadata": {"Header 1": "TF-IDF", "path": "../pages/digitalGarden/cs/machineLearning/tfidf.mdx"}, "page_content": "There are lots of use cases where we want to be able to find out which words are the most important to a document in a collection or corpus of documents. Or in other words, which words add the most value to the document. Some possible applications for these measurements are search engines like Google or DuckDuckGo.  \nThe most famous and widely used measurement for finding the importance of a word in a document is called TF-IDF. TF-IDF has two components the TF (term frequency) and IDF (inverse document frequency).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "TF-IDF", "Header 2": "Term Frequency", "path": "../pages/digitalGarden/cs/machineLearning/tfidf.mdx"}, "page_content": "The main idea of TF is that a word is important to a document if the word occurs frequently. So if we for example have a set of documents and want to find out which documents are the most relevant to the search query, \"the offside rule\" (i.e documents relating to football rules). A simple way to start would be to eliminate all documents that do not contain all three words \"the\", \"offside\" and \"rule\", however this still leaves many documents that might not be relevant. To further distinguish between relative and non-relative documents, we can count the number of times each word occurs in each document, this is the so-called term frequency. However, longer documents could then have a greater term frequency then other documents although that word/term might not be as relevant for the document as for others.  \nTo solve this the relative frequency is taken instead of the absolute frequency i.e the nominator is the raw count of the term $t$ in the document $d$ and the denominator is simply the total number of terms in document $d$.  \n$$\n\\mathrm{tf}(t,d) = \\frac{f_{t,d}}{\\sum_{t'\\in d}{f_{t',d}}}\n$$  \nThere are also some common alternatives for example using the highest frequency as the denominator.  \n$$\n\\mathrm{tf}(t,d) = \\frac{f_{t,d}}{\\mathrm{max}(f_{t',d}: t'd \\in d)}\n$$  \nWith the introduction of a hyperparameter $k$ you have the so-called augmented term frequency which is also commonly seen with $k=0.5$ for shorter documents.  \n$$\n\\mathrm{tf}(t,d) = k + (1-k) \\frac{f_{t,d}}{\\mathrm{max}(f_{t',d}: t'd \\in d)}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "TF-IDF", "Header 2": "Inverse Document Frequency", "path": "../pages/digitalGarden/cs/machineLearning/tfidf.mdx"}, "page_content": "The IDF measures how much information a word provides i.e. if it is common or rare across all documents. For example the word \"the\" is very common so TF alone might incorrectly rank documents which have the word \"the\" more frequently higher then other documents. So the word \"the\" is not a good keyword to distinguish relevant and non-relevant documents and words compared to \"offside\" and \"rules\". To avoid this an inverse document frequency factor is used which adds a higher weight to the words that occur rarely.  \nSo for the corpus of documents $D$ with $|D|=N$ being the number of documents in the corpus and $n_t$ being the number of documents the term $t$ occurs in the inverse document frequency can be defined as:  \n$$\n\\mathrm{idf}(t, D) = \\log{\\frac{N}{n_t}}\n$$  \nTo avoid division by zero also commonly changed to:  \n$$\n\\mathrm{idf}(t, D) = \\log{\\frac{N}{1 + n_t}}\n$$  \nWe can then finally combine the two components to form the TF-IDF score of a term $t$ in the document $d$ of corpus $D$ :  \n$$\n\\mathrm{tfidf}(t,d,D)=\\mathrm{tf}(t,d) \\cdot \\mathrm{idf}(t,D)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Main Ideas of Machine Learning", "path": "../pages/digitalGarden/cs/machineLearning/mainIdeas.mdx"}, "page_content": "Machine learning uses algorithms to automatically learn patterns and relationships from given data (training data), with\nthe goal of making quantitative predictions (regression) or identifying structures to then classify the data.  \n<Callout type=\"example\">  \nWe have a lot of data on houses, including their, size, location, number of rooms, etc. the so called features of\nour data. We can then use this data, machine learning and a given target variable (sometimes also called label) in this\ncase the price to predict the price of a new house given its features.  \n<Callout type=\"todo\">\nAdd image of house data\n</Callout>  \n</Callout>  \nMachine learning also has a lot of other applications such as clustering, dimensionality reduction and anomaly detection.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Main Ideas of Machine Learning", "Header 2": "Data", "path": "../pages/digitalGarden/cs/machineLearning/mainIdeas.mdx"}, "page_content": "In machine learning, data is typically split into three different subsets: training data, validation data, and test data.  \nThe training data is used to train the machine learning model to learn patterns and relationships between the features and the target variable.\nFeatures can be of different shapes, for example numerical, categorical, or text-based, and so can the target variable\ndepending on the model's goal. In the case of a house price it would be a continuous numerical value. But if we wanted to\nclassify the house on whether it is expensive or not, the target variable would be a binary value.  \nThe model is trained by using an algorithm to adjust its parameters to minimize the difference between its\npredicted output and the actual target values in the training data. This process is often referred to as \"fitting\" the\nmodel to the training data.  \nOnce the model is trained, it is evaluated on the validation data to determine how well it can generalize to new, unseen\ndata. The validation data is used to tune the hyperparameters of the model, which are parameters that are not learned\nfrom the training data but are set before training begins.  \nAfter the hyperparameters have been tuned using the validation data, the model is tested on the test data to get an\nunbiased estimate of its performance. The test data should be representative of the data that the model will encounter\nin the real world, and it should be completely independent of the training and validation data to avoid bias.  \n<Callout type=\"todo\">\nAdd image of data split\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Main Ideas of Machine Learning", "Header 2": "Data", "Header 3": "Overfitting and Underfitting", "path": "../pages/digitalGarden/cs/machineLearning/mainIdeas.mdx"}, "page_content": "Not sure if this belongs here.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Main Ideas of Machine Learning", "Header 2": "Classification vs Regression", "path": "../pages/digitalGarden/cs/machineLearning/mainIdeas.mdx"}, "page_content": "Binary classification vs multi-class classification", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Power Law", "path": "../pages/digitalGarden/cs/machineLearning/powerLaw.mdx"}, "page_content": "The power law defines a relationship between two quantities where a relative change in one quantity $a$ gives rise to a proportional relative change in the other quantity $b$. The most common example for this relationship is the area of a square in regard to the length of a side. If the length is doubled then the area is multiplied by a factor of four. A similar example where if the length of a side of a cube is doubled then the volume the cube is multiplied by a factor of eight.  \nIn general, $x$ and $y$ are in a power law relationship if $\\log{y}$ is linear to $\\log{x}$.  \n$$\n\\log{y} = b + a \\cdot log{x} \\Leftrightarrow y = e^b + x^a \\Leftrightarrow y = cx^a\n$$  \nPower laws are very frequent in our life some common examples are:  \n- If a person is popular on social network, he/she will get more popular in the future.\n- If a person is rich, he/she will get more rich in the future.  \nA power law can be turned into a linear relationship if the quantities are plotted on logarithmic axes.  \n<Image\nsrc=\"/cs/mlPowerLawLog.png\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decision Trees", "path": "../pages/digitalGarden/cs/machineLearning/decisionTrees.mdx"}, "page_content": "Trees with decision nodes and leaf nodes. Decision nodes ask a question and split the data based on the answer.\nLeaf nodes are the final nodes that predict the outcome either a class label or a value.  \nThe goal is to create a tree that asks questions that best split the data to make the most accurate predictions.  \nNeed to pick the first question to ask. The question should split the data into subsets that are as pure as possible\npure meaning that all the data in the subset belongs to the same class. The measure of purity can be entropy or gini impurity etc.\nalso minimize impurity, also want to minimize the number of questions asked to avoid overfitting. Can also define a max depth.\nOr when the number of samples in a node is below a certain threshold or the purity didnt improve by a certain threshold.  \nHow does this relate to cross-entropy?\nEntropy is a measure of impurity in a bunch of examples. It is 0 when all examples are of the same class. It is 1 when the examples are evenly split between classes.\nif x is 0.5 then the entropy is 1. If x is 0 or 1 then the entropy is 0. Hence the entropy is a measure of impurity. the fomula looks a lot like the log loss function so it is related.  \np1 =fraction of examples in class 1, p2 = fraction of examples in class 2, i.e. 1-p1  \n$$Entropy = -p1log2(p1) - p2log2(p2)$$  \nWhen choosing we want to minimize the entropy. but we also need to normalize it by the number of examples in the node so the entropy is not biased towards nodes with more examples.  \nSo we cant just add the entropys of the 2 branches and average them. We need to weight them by the number of examples in each branch. Does this also matter for binary classification?  \nCan then calculate the information gain by subtracting the weighted entropy of the branches from the entropy of the parent node. \"reductiion entropy\" which is like weve never made a split before.  \nWe do recursive splitting until good tree.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decision Trees", "path": "../pages/digitalGarden/cs/machineLearning/decisionTrees.mdx"}, "page_content": "Can then calculate the information gain by subtracting the weighted entropy of the branches from the entropy of the parent node. \"reductiion entropy\" which is like weve never made a split before.  \nWe do recursive splitting until good tree.  \nif we have categorical features we can split on them. If we have numerical features we can split on them by choosing a threshold and splitting on the threshold.\nWhen a categorical feature has many categories, we can use a binary/one-hot encoding to turn it into a numerical feature. k possible values can be encoded as k binary features.  \nfinding the threshold for a numerical feature is a bit more complex. You can sort the values and then try all midpoints between the values. so if you have 3 values you have 2 midpoints.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decision Trees", "Header 2": "Using multiple decision trees", "path": "../pages/digitalGarden/cs/machineLearning/decisionTrees.mdx"}, "page_content": "one tree can be overfitting i.e very sensitive to small changes in the data. so we can use multiple trees and average their predictions, this is called a Tree Ensemble.  \nSampling with replacement is called bootstrapping. We can use bootstrapping to create multiple datasets from the original dataset. We can then train a tree on each dataset.  \nreplacement because a sample gets put back in the dataset after it is picked. So the same sample can be picked multiple times.  \nRandom forest is a tree ensemble where each tree is trained on a different dataset. The predictions are then averaged this uses the wisdom of the crowd to make predictions.  \nwhat is difference of bagging and bootstrapping? both seem to do the same.  \nRandom forst can be further randomized to make the trees more different from each other. To avoid always having the same root decision for example.  \nFrom n features randomly select k features and then pick the best feature from those k features. k is usually sqrt(n) for classification.  \nin the random forest when building the dataset all samples have the same probability of being picked. In gradient boosting the probability of being picked is weighted by the error of the previous tree.\nSo we give missclassified samples a higher probability of being picked. This is called boosting.  \ndecsiion trees work great for tabular data structured data. They dont work well for unstructured data like images or text or audio.  \nsmall trees can be visualized and interpreted. They can be used for feature selection. They can handle missing values. They can handle categorical and numerical features.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decision Trees", "Header 2": "Classification Trees", "path": "../pages/digitalGarden/cs/machineLearning/decisionTrees.mdx"}, "page_content": "Can be used for classification, i.e. in the leaves we have the class labels.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decision Trees", "Header 2": "Classification Trees", "Header 3": "Building a Classification Tree", "path": "../pages/digitalGarden/cs/machineLearning/decisionTrees.mdx"}, "page_content": "#### Gini Impurity", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decision Trees", "Header 2": "Regression Trees", "path": "../pages/digitalGarden/cs/machineLearning/decisionTrees.mdx"}, "page_content": "Can be used for regression, i.e. in the leaves we then get a prediction of the target variable. if multiple examples end up in the same leaf we can take the average of the target variable.  \nInstead of using entropy or gini impurity we can use the variance of the target variable as a measure of impurity. we can then weight the variance of the branches by the number of examples in the branch and subtract the weighted variance of the branches from the variance of the parent node.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decision Trees", "Header 2": "Pruning a Tree", "path": "../pages/digitalGarden/cs/machineLearning/decisionTrees.mdx"}, "page_content": "Pruning is a technique to reduce the size of the tree by removing nodes that do not provide much information.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Decision Trees", "Header 2": "Problems with Decision Trees", "path": "../pages/digitalGarden/cs/machineLearning/decisionTrees.mdx"}, "page_content": "They can overfit the data. They are sensitive to small changes in the data. They can create biased trees if some classes dominate.\nThey can't handle unseen data well.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 1 - Introduction", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "Uses of ML without knowing it:\n- Instagram automatically tags images with labels or tags people in the background\n- Similiar movies to what you like on Netflix\n- Voice to text on your phone\n- Spam filter on your email  \n\"Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.\" - Arthur Samuel (1959)  \nMade a checkers program that could play thousands of games against itself and learn from its mistakes. The more it played, the better it got.  \nNot everything can be defined with algorithms. For example, recognizing a face. We can't write an algorithm to recognize a face. We can't write an algorithm to recognize spam. We can't write an algorithm to recognize handwritten digits, etc.  \nHence Machine learning started.  \nAGI - Artificial General Intelligence - AI that can do anything a human can do. We are not there yet. It can think itself. It can learn itself. It can do anything a human can do.  \n#### Supervised vs Unsupervised Learning  \nSupervised learning, learn a function that maps an input to an output based on example input-output pairs. So it learns $f(x) = y$, where $x$ is the input and $y$ is the output.  \nFor example:  \n- Email spam filter, where $x$ is the email and $y$ is spam or not spam.\n- Speech recognition, where $x$ is the audio clip and $y$ is the text transcript.\n- Image classification, where $x$ is the image and $y$ is the object in the image.\n- Machine translation, where $x$ is the sentence in one language and $y$ is the sentence in another language.\n- Online advertising, where $x$ is the user and $y$ is the probability of the user clicking on the ad.\n- Visual inspection, where $x$ is the image of a manufactured part and $y$ is the probability that the part is defective.  \nAfter learning the function, we can then use it to predict the output for new values of $x$, never seen before.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 1 - Introduction", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "- Visual inspection, where $x$ is the image of a manufactured part and $y$ is the probability that the part is defective.  \nAfter learning the function, we can then use it to predict the output for new values of $x$, never seen before.  \nPredict housing prices based on the size of the house. input $x$ is the size of the house, output $y$ is the price of the house. We can then predict the price of a new house based on its size.  \nRegression could fit a straight line or curve to the data to then predict the price of a new house. Perticular type of supervised learning. Task to predict a continuous value output.  \nClassification is another type of supervised learning. Task to predict a discrete value output. For example, predict if a tumor is malignant or benign based on its size. $x$ is the size of the tumor, $y$ is 0 or 1, where 0 is benign and 1 is malignant.  \nClassification allocates so-called classes, categories or labels to the input.  \nCan be hard if the data is not linearly separable. For example, predict if a tumor is malignant or benign based on just its size is hard. But if we add another input, like the age of the patient, then it becomes easier.  \nLearning algorithms tries to find some boundary line that separates the two classes. For example, a straight line that separates the two classes. This is called a classifier.  \nUnsupervised learning is more used. No labels are given. The learning algorithm tries to find some structure/patterns or somehting interesting in the data. For example, clustering. Given a set of data points, group them into clusters. For example grouping the cancer patients into different groups based on their age and tumor size.  \n- Google News uses clustering to group similar news articles together.\n- Anomaly detection is another example of unsupervised learning. Given a set of data points, find which ones are anomalies. For example, credit card fraud detection. Given a set of credit card transactions, find which ones are fraudulent.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 1 - Introduction", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "- Anomaly detection is another example of unsupervised learning. Given a set of data points, find which ones are anomalies. For example, credit card fraud detection. Given a set of credit card transactions, find which ones are fraudulent.\n- Dimensionality reduction is another example of unsupervised learning. Given a set of data points, find a lower-dimensional representation of the data that captures the most important attributes of the data.  \n#### Linear Regression with One Variable  \nFitting a straight line to the data. Simple example of house prices. $x$ is the size of the house, $y$ is the price of the house. We want to predict the price of a new house based on its size. We can expect the price to increase as the size of the house increases.  \nIs a Regression problem because we are predicting a continuous value output, i.e the price of the house.  \nTraining set is the data we use to train the model. We use the training set to learn the function $f(x) = y$. We use the training set to learn the parameters of the model.  \ninput/variable/feature is the size of the house, $x$. The output/target variable is the price of the house, $y$.  \nAn example of a training sample is $(x^{(i)}, y^{(i)})$, where $x^{(i)}$ is the size of the house and $y^{(i)}$ is the price of the house. The superscript $(i)$ denotes the $i$th training sample where if we have $m$ training samples, then $i = 1, 2, \\dots, m$.  \nThe prediction of the model is $\\hat{y}^{(i)}$, where $\\hat{y}^{(i)}$ is the predicted price of the house given the size of the house $x^{(i)}$, called the hypothesis, y-hat. Where the hypothesis function approximates the true function $f(x) = y$.  \nFor linear regression with one variable, the hypothesis function is $h_\\theta(x) = \\theta_0 + \\theta_1 x$, where $\\theta_0$ and $\\theta_1$ are the parameters of the model. The parameters are also called the weights of the model. The parameters are what we want to learn from the training data.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 1 - Introduction", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "The first parameter $\\theta_0$ is called the bias parameter, y-intercept for 1d. The second parameter $\\theta_1$ is called the slope parameter in linear regression.  \nunivariate linear regression because we have one input variable.  \nNeed some way to measure how well the model fits the data. We need some way to measure the error of the model. We need some way to measure the difference between the predicted value and the actual value.\nThis is called the cost function. The cost function measures the error of the model on the training data. We want to minimize the cost function. We want to minimize the error of the model on the training data.  \nThe cost function for linear regression is the mean squared error (MSE) cost function. We take the average because we dont want the cost function to depend on the number of training samples.\nWe square the error because we want to penalize large errors more than small errors, this also makes the cost function convex, which makes it easier to minimize but also removes the sign of the error, so we can't tell if the error is positive or negative.  \nThe cost function is $J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$.  \nWe take 2m because we want to cancel out the 2 when we take the derivative of the cost function. this makes it easier to minimize with gradient descent. can also put the 1/2 inside the sum.  \nThe error is the difference between the predicted value and the actual value. The error is $e^{(i)} = \\hat{y}^{(i)} - y^{(i)}$. which is also called the residual, because it is what is left over after the prediction.  \nsimplificiation by just minimizing w and where b is just 0. Showing the cost function is convex  is cool. Also show the contour plot of the cost function and how it can be interpreted as a bowl. The minimum is the bottom of the bowl.  \n#### Gradient Descent  \nGD thing from efalg was cool.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 1 - Introduction", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "#### Gradient Descent  \nGD thing from efalg was cool.  \nGradient descent is an optimization algorithm. It is an iterative algorithm. It is an algorithm that takes steps towards the minimum of the cost function. It can be used to minimize any differentiable function. It can be used to minimize the cost function of linear regression.  \nIf we have a function with one parameter, then we can use calculus to find the minimum of the function, however if we have a function with multiple parameters, then we can't use calculus to find the minimum of the function. We can use gradient descent to find the minimum of the function.  \nGD is an iterative method that calculates the gradient, i.e. the derivative, of the function at the current point and then takes a step in the direction of the negative gradient. The negative gradient is the direction of steepest descent. The step size is called the learning rate, $\\alpha$.\nThe learning rate controls how big the steps are. If the learning rate is too small, then it will take a long time to converge. If the learning rate is too big, then it might not converge at all. The learning rate is a hyperparameter of the model. LR can be constant or can be adaptive and\nvisualized. If the derivative is 0, then we have reached a minimum. If the derivative is positive, then we have to go left. If the derivative is negative, then we have to go right. The derivative is the slope of the tangent line at the current point. The derivative is the rate of change of the function at the current point.\nA saddle point is a point where the derivative is 0, but it is not a minimum.  \nThe gradient descent algorithm is $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta_0, \\theta_1)$, where $\\theta_j$ is the $j$th parameter of the model, $\\alpha$ is the learning rate and $J(\\theta_0, \\theta_1)$ is the cost function.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 1 - Introduction", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "GD however, is not guaranteed to converge to the global minimum. It can get stuck in a local minimum. It can also oscillate around the minimum. It can also take a long time to converge. It can also diverge. It can also converge to a saddle point.  \nWhen actually implementing this you need to use temporary variables for the parameters, so you don't update the parameters before you have calculated the gradient for all the parameters.  \nShow MSE with GD and the calculations, why the 2m is so much nicer. MSE only has one minimum, so GD will always converge to the global minimum. This is due to the convexity of the cost function.\nIf a function is convex, then it has only one minimum. If a function is not convex, then it can have multiple minima. If a function is not convex, then GD is not guaranteed to converge to the global minimum.  \n\"Batch GD\" is when we use all the training samples to calculate the gradient (this is the default setup). There are other types of GD, like \"Stochastic GD\" where we use one training sample to calculate the gradient. \"Mini-batch GD\" is when we use a subset of the training samples to calculate the gradient??? I thought this was just called \"Stochastic GD\".", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 2 - Regression with multiple input variables", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "We have a vector $x$ of input variables, $x = (x_1, x_2, \\dots, x_n)$, where $n$ is the number of input variables/features. The subscript is the feature the superscript the training sample.\nA sample is therefore a vector of features.  \nFor each feature we have a parameter $\\theta_j$, where $j$ is the feature. The parameters are also called the weights of the model. The parameters are what we want to learn from the training data. Dont forget the bias parameter $\\theta_0$ that is added at the end  \nThe hypothesis function is $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n = \\theta_0 + \\sum_{j=1}^n \\theta_j x_j = \\theta_0 + \\theta^T x$, where $\\theta = (\\theta_1, \\theta_2, \\dots, \\theta_n)$ is the vector of parameters and $x = (x_1, x_2, \\dots, x_n)$ is the vector of features.  \nBias can be interpreted as a base price and then you pay for faetures.  \nCan rewrite as dot product + bias.  \nCan rewrite this to be slightly nicer by adding a 1 to the feature vector, $x_0 = 1$, and then the hypothesis function is $h_\\theta(x) = \\theta^T x$, where $\\theta = (\\theta_0, \\theta_1, \\dots, \\theta_n)$ is the vector of parameters and $x = (x_0, x_1, \\dots, x_n)$ is the vector of features.  \nmultivariate regression because we have multiple input variables.  \nusing vectors is not just fancy, it makes it more readable but also faster to implement and faster to run.  \nGradient descent is pretty much the same just defined slightly different.  \nFor pretty much only the linear regression model, we can use the normal equation to find the parameters that minimize the cost function.  \n#### Improving GD  \nPlot contour plots per weight. Depending on the value range of a feature the contour plot can be very stretched out. i,e changing it can have a big effect on the cost function. This can make GD take a long time to converge. It can also make GD take a long time to converge depending on the learning rate, initial parameters and the feature values ranges.  \n-> Rescale features can sovle this problem.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 2 - Regression with multiple input variables", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "-> Rescale features can sovle this problem.  \nMaxScale Devide by the maximum value of the feature. This will make the feature values range from 0 to 1.  \nMean normalization subtract the mean from the feature values. This will make the feature values range from -1 to 1 i.e be around 0.  \nZ-score normalization subtract the mean and divide by the standard deviation. This will make the feature values have a mean of 0 and a standard deviation of 1.  \nWant all feature ranges to be in the same ballpark.  \nWhat if the new value is outside the range of the training data?  \nfeature engineering -> choosing or engineer approprirate features. Can have a large impact on the performance of the model. Can also have a large impact on the performance of the model if you choose the wrong features.  \nFor example for a house the area might be better then width and height.  \n#### Checking GD  \nCan look at cost curve over the iterations/epochs. If it is decreasing, then it is working. If it is increasing, then it is not working. If it is oscillating, then the learning rate is too big. If it is not decreasing, then the learning rate is too small.  \nHas converged when the cost function is not decreasing anymore.  \nCan also do a convergence test. If the difference between the cost function of the current iteration and the previous iteration is smaller than some threshold, then it has converged, i.e. $|J(\\theta^{(i)}) - J(\\theta^{(i-1)})| < \\epsilon$.  \n#### Polynomial Regression  \nnot just linear lines but also quadratic, cubic, etc. curves to ur data  \nif only have one feature, then can add more features by adding powers of the feature. For example, if we have one feature $x$, then we can add the features $x^2, x^3, \\dots, x^n$. to make it polynomial.  \nquadratic functions prob not optimal cause they go up and come down again.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 3 - Classification", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "only 2 outputs/classes -> Binary Classification  \npositive/negative -> spam/not spam, malignant/benign, etc.  \nLinear regression with a threshold value could be used for binary classification. If the output is greater than the threshold, then it is positive, otherwise it is negative. However, this is not a good idea.  \nDecison boundary can be effected by outliers because they can pull the line towards them.  \n#### Logistic Regression  \nName can be confussing becaues used for classification not regression. Was given out of historical reasons.  \nsigmoid function = logistic function = logistic activation function  \nsigmoid function is $g(z) = \\frac{1}{1 + e^{-z}}$, where $z$ is the input to the sigmoid function. The sigmoid function squashes the input to a value between 0 and 1, with the output being close to 1 if the input is positive and close to 0 if the input is negative.  \nLogistic regression is just doing linear regression and then applying the sigmoid function to the output of the linear regression. The hypothesis function is $h_\\theta(x) = g(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}}$, where $g(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.  \nThis fixes the previous issue. Can think of it as the probability of the output being positive. If the output is greater than 0.5, then it is positive, otherwise it is negative.  \nWhenever the linear regression is greater than 0, then the logistic regression is greater than 0.5. Whenever the linear regression is less than 0, then the logistic regression is less than 0.5. Where the linear regression is 0 is the decision boundary.  \nCan visualize the decision boundary for 2 features nicely.  \nIf the decision boundary is not linear, then we can add polynomial features , i.e use polynomial logistic regression. For example a circle decision boundary.  \n#### Cost Function for Logistic Regression  \nIf you use MSE as the cost function for logistic regression, then the cost function will be non-convex. Why? Example? Visulaize?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 3 - Classification", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "#### Cost Function for Logistic Regression  \nIf you use MSE as the cost function for logistic regression, then the cost function will be non-convex. Why? Example? Visulaize?  \nSo instead we use the logistic loss function also called the log loss function or cross entropy. for positive examples, the cost function is $-log(h_\\theta(x))$. For negative examples, the cost function is $-log(1 - h_\\theta(x))$.  \nif the prediction is close to the actual value, then the cost is low with 1/0 being the lowest cost. If the prediction is far from the actual value, then the cost is high. The log is negative, so we only get positve cost values.  \nThe nice thing about the log loss function is that it is convex. The function can also be rewritten to be easier to implememt, it is completly equivalent.  \n$-y log(h_\\theta(x)) - (1 - y) log(1 - h_\\theta(x))$. where y is the actual value, 0 or 1. If y is 1, then the cost function is $-log(h_\\theta(x))$. If y is 0, then the cost function is $-log(1 - h_\\theta(x))$.  \nWhen running it over all test samples the negative sign can be taken outside the sum making it $-\\frac{1}{m} \\sum_{i=1}^m y^{(i)} log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) log(1 - h_\\theta(x^{(i)}))$.  \nThis cost function is derived from maximum likelihood estimation. The cost function is the negative log likelihood function. The likelihood function is the probability of the data given the parameters. The maximum likelihood estimation is the process of finding the parameters that maximize the likelihood function.  \nLikelhoods never made sense to me.  \nLoss is the error of a single training sample. Cost is the average error of all the training samples.  \nAfter calcualting the gradient looks very similar to linear regression. The only difference is the hypothesis function.  \n#### Overfitting  \nUnderfitting, not fitting well -> high bias\nOverfitting, fitting too well -> high variance  \nnot sure about the definitions of bias and variance.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Supervised Learning", "Header 3": "Week 3 - Classification", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "#### Overfitting  \nUnderfitting, not fitting well -> high bias\nOverfitting, fitting too well -> high variance  \nnot sure about the definitions of bias and variance.  \nmore data can help with overfitting or feature engineering or regularization. Some features might not be useful or might be redundant.  \nTo avoid Overfitting, we can use regularization. Regularization is a technique to reduce the complexity of the model. Instead of removing features try to reduce the weights of the features.  \nfor the MSE we put lambda as a hyperparameter to the cost we add lambda/2m * sum of the weights squared. This will make the weights smaller. This is called L2 regularization or ridge regression.\nThe higher the lambda, the smaller the weights. The higher the lambda, the more the weights are penalized. If lambda is too high, then the weights will be too small and the model will underfit.  \nAfter taking the derivative of the cost function, the regularization just makes the weights smaller by subtracting lambda/m * theta_j from the gradient.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Advanced Learning Algorithms", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "see deep learning page.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Advanced Learning Algorithms", "Header 3": "Week 4 - Decison Trees", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "look at decision tree page.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Unsupervised Learning", "Header 3": "Week 1 - Unsupervised Learning", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "#### Clustering  \napplications of clustering. customer segmentation, market segmentation, social network analysis, group news articles, etc.  \n##### K-means  \nneed to define k, the number of clusters. need to define the distance metric. need to define the initial centroids. need to define the stopping criteria.  \ninitial centroids can be random or can be the first k data points. There are other ways to initialize the centroids.  \nthen go through each data point and assign it to the closest centroid. then update the centroids to be the mean of the data points assigned to that centroid. then repeat until convergence.  \nthe distance metric is usually the euclidean distance. but can also use other distance metrics.  \ndoes the mean in the name is because it uses the mean to update the centroids?  \nk means can also be used to assign sizes to tshirt sizes  \nproof of why k means is minimizing the cost function. the cost function is the sum of the squared distances from the data points to their centroids. the cost function is convex, so k means is guaranteed to converge to the global minimum.  \nthe initialization of the centroids can have a big impact on the final result. so it is common to run k means multiple times with different initializations and then pick the best result based on the cost function. distorsion is the cost function???  \nhow to choose k? elbow method. plot the cost function vs k and then pick the k where the cost function starts to flatten out. this is called the elbow method. Often u already know k tho for a known downstream task.  \n#### Anomaly Detection  \nfinding outliers or unusual data points. for example, credit card fraud detection, find broken machines in a factory, etc.  \nMost commonly use density estimation. The idea is that the normal data points will have a high density meaning a high probability of occuring. The anomalous data points will have a low density meaning a low probability of occuring.  \nany data point that has a probability lower than some threshold is considered an anomaly.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Unsupervised Learning", "Header 3": "Week 1 - Unsupervised Learning", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "any data point that has a probability lower than some threshold is considered an anomaly.  \nThe feature values are assumed to be independt and follow a gaussian distribution. We therefore get a probability for each feature value. We then multiply the probabilities to get the probability of the data point. We then compare the probability to the threshold to see if it is an anomaly or not.  \nThe gaussian distributions are calculated using the training data.  \nto evaulate the model we need some labeled data. We can use the labeled data with validation and test sets we can then finetuen the threshold.  \ndifference to supervised learning is we have very small positive examples and a lot of negative examples. anomaly detection learns to just detect the negative examples. supervised learning learns to detect both the positive and negative examples.  \nfeatures are very important for anomaly detection. if the features are not gaussian, then the model will not work well, can try and make them gaussian by applying a transformation to the features.¨", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Unsupervised Learning", "Header 3": "Week 2 - Recommender Systems", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "recommendation one of the most common applications of machine learning. for example, netflix, amazon, youtube, etc.  \nratings from 1 to 5. 1 is bad, 5 is good. can also have binary ratings, like/dislike. can also have implicit ratings, like views, clicks, etc.  \nmovies can have features like a weight of movie genres, like it is 50% action, 30% comedy, 20% romance. can also have features like the actors, director, etc.  \ncan use the features and a linear regression for each user to predict the rating of a movie. Cost is the MSE.  \nHowever, not realisitc to have a linear regression for each user, also less realistic to have the good features.  \n#### Collaborative Filtering  \nBased on the idea that similar users have similar ratings. Based on the idea that similar movies have similar ratings. I dont really like his explanation.  \nWe want to learn a vector that when multiplied with the weights of a person give the ratings of that person. This vector is then a sort of latent feature of the movie, the weights of the liner model are the latent features of the user. The latent features are not known, they are learned from the data.  \nSo we then have 2 cost functions which are based on each other. The cost function for the user latent features is based on the cost function for the movie latent features. The cost function for the movie latent features is based on the cost function for the user latent features. This is called alternating least squares.  \nThese 2 costs can then be combined. This semmes rly gross.  \nCan also use binary ratings like favorited, liked, etc. instead of using linear regression, we can use logistic regression. The cost function is then the log loss.  \nmean normlization is good idea for the ratings, i.e subtract the mean rating from the ratings per movie.  \nIs this based on the user or the movie????  \nmost similiar movies are closest using the latent vector of the movie.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Unsupervised Learning", "Header 3": "Week 2 - Recommender Systems", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "mean normlization is good idea for the ratings, i.e subtract the mean rating from the ratings per movie.  \nIs this based on the user or the movie????  \nmost similiar movies are closest using the latent vector of the movie.  \nProblems: Cold start problem, new users and new movies. Sparsity problem, most users have not rated most movies. Dont use other features of the movie, like the genre, actors, etc. or of the user like age etc.  \n#### Content Based Filtering  \nCollaborative filtering is similiar users.  \ncontent based filtering uses user features and movie features to predict the rating. We then have a vector of user features and a vector of movie features. We then use these vectors to predict the rating. The cost function is the MSE.  \nwe then multiply the user features with some weight vector and then do the same for the movie features(latent faeture/vector). We then use the dot product of the 2 vectors together to get the score.  \nwe can use deep learning to create these latent vectors, we then have 2 neural networks, one for the user features and one for the movie features. We then use the dot product of the 2 latent vectors to get the score.  \nFor big data we rather use retrieval and ranking. We first use a retrieval model to get a subset of the data. We then use a ranking model to rank the data. For example, we can use a retrieval model to get the top 100 movies and then use a ranking model to rank the top 100 movies.  \nlatent features can be precomputed and then used in the ranking model.  \n#### PCA  \nReducing the number of features, i.e dimensions in a vector. For example, if we have a 3d vector, then we can project it onto a 2d plane. We can then use the 2d vector instead of the 3d vector. This is called dimensionality reduction.  \nPCA is a technique for dimensionality reduction. PCA is an unsupervised learning algorithm.  \nIt extracts the most important features of the data. or combinations of which. like if we have width and height of a car and reduce it to just size.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Unsupervised Learning", "Header 3": "Week 2 - Recommender Systems", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "PCA is a technique for dimensionality reduction. PCA is an unsupervised learning algorithm.  \nIt extracts the most important features of the data. or combinations of which. like if we have width and height of a car and reduce it to just size.  \nnormally good idea to first to normalization and scaling.  \nwhen projecting onto an axis we want to minimize the distance between the data points and the axis. this is called the projection error. However we can't just project onto any axis, we want to project onto the axis that maximizes the variance of the data points. This is called the principal component.\nThe principal component is the axis that maximizes the variance of the data points. the variance contains the most information about the data.  \nPCA with multiple axis are all orthogonal to each other. The first principal component is the axis that maximizes the variance of the data points. The second principal component is the axis that maximizes the variance of the data points, but is orthogonal to the first principal component.\nThe third principal component is the axis that maximizes the variance of the data points, but is orthogonal to the first and second principal components. etc.  \napart from visulaizatin also useful for compression. can also use PCA for anomaly detection. can also use PCA for denoising. can also use PCA for feature engineering.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Unsupervised Learning", "Header 3": "Week 3 - Reinforcement Learning", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "cant be bothered.  \ngiven a state decide on an action. The action changes the state and gives a reward. The goal is to maximize the reward. rewards being positve and negative.  \nterminal states are the end. can be good or bad. can be multiple terminal states.  \nthe concept of return is the sum of the rewards. the goal is to maximize the return.  \nThere is also a discount factor, gamma, which is between 0 and 1. The discount factor is used to discount future rewards.  \nthere are lots of differenct policies which is the function that maps the state to the action. the goal is to find the best policy. often the policy is a neural network and deonted by pi.  \nMarkov decision process, mdp. Markov referes to the markov property. The markov property is that the future state only depends on the current state and action. The future state does not depend on the past states and actions. The markov property is not always true in the real world, but it is a good approximation.  \n#### state-action value function Q  \nQ(s,a) gives the return of taking action a in state s and then behave optimmaly after that. The goal is to find the optimal Q function. The optimal Q function is the Q function that maximizes the return.  \nIs this q-learning?  \nThe policy then becomes the action that maximizes the Q function. The policy is the argmax of the Q function.  \nBellmans equation??? just basically the same but nicely written?  \nRandom stachastic environment? slightly changes bellman equation the next state is not deterministic but random.  \ncontinous state spaces for example anywhere on a line rather then on a grid. state is then defined by a vector of features. Q function is then a neural network.  \nto learn the q function the state is contaconted with the action and then fed into the neural network. the output is the q value for that state and action. for the action we can use one hot encoding for a video game, but for a car we can use a continous action space.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coursera Machine Learning", "Header 2": "Unsupervised Learning", "Header 3": "Week 3 - Reinforcement Learning", "path": "../pages/digitalGarden/cs/machineLearning/courseraML.mdx"}, "page_content": "replay buffer, is used to store the state, action, reward, next state, done. then sample from the replay buffer to train the neural network.  \nwe can also isntead of conactoante the state and action, we can use the state as input and then output the q value for each action. this is called the dueling architecture, so an output activation for each possible action.  \ndont always want to take the best action, sometimes want to explore. epsilon greedy, with probability epsilon take a random action, otherwise take the best action.  \nmini batch gradient descent uses a small batch and then updates the neural network. this is more stable than updating the neural network after each sample.  \nsoft update???", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linear Classification & Regression", "path": "../pages/digitalGarden/cs/machineLearning/linear.mdx"}, "page_content": "also known as least squares LS", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Neural Networks", "Header 2": "Message Passing", "path": "../pages/digitalGarden/cs/machineLearning/gnn.mdx"}, "page_content": "Intermediate topic before moving to GNNs. Is this GraphSAGE?  \nGiven a network where some nodes have labels, how do we propagate the labels to the other nodes?  \nNetwork with scammers and trusted nodes. How do we find other scammers or trusted nodes? (red and green nodes)  \ncould just use node embeddings and then use a classifier on top of that.  \nBut we think of it here as a semi-supervised learning problem. We have some labels and we want to propagate them to non-labeled nodes, partially labeled.  \nkey concept is \"collective classification\", i.e nodes that are connected to each other are more likely to have the same label.\nnode edges/network structure correlates with node labels.  \n\"homophily\" - nodes that are connected to each other are more likely to have the same label, footbal fans are more likely to be friends with other football fans.\n\"birds of a feather flock together\"\n\"influence\" - connections between nodes can influence the labels of the nodes, e.g. a football fan can influence his friends to become football fans.  \n\"guilt by association\" - if a node is connected to a scammer, it is more likely to be a scammer.  \nlabel of v depends on the labels of its neighbors, its features and the features of its neighbors.  \nThe three approaches below are classic approaches to this problem.  \nUse a probabilistic model to model the probability of a node having a label given the labels of its neighbors. And we make a first order markov assumption,\ni.e. the label of a node depends only on the labels of its neighbors. $P(y_v | y_u, N(v))$  \nIterative process till it converges. That has three parts:  \n- local classifier: Will assign initial labels to the nodes based on their features. $P(y_v | x_v)$. Only applied once at the beginning.\n- relational classifier: Will assign labels to the nodes based on the labels and/or features of their neighbors. $P(y_v | y_u, N(v))$.\n- collective inference: Applying the relational classifier iteratively till it converges.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Neural Networks", "Header 2": "Message Passing", "Header 3": "Relational classification", "path": "../pages/digitalGarden/cs/machineLearning/gnn.mdx"}, "page_content": "Basic idea is to use a probabilistic model to model the probability of a node having a label given the labels of its neighbors.  \nSo the weighted average of class probabilities of the neighbors. Can for example intiialize the lables with ground-truth and if unlabled set the label to 0.5 for two classes.  \n$$P(y_v=c) = \\frac{1}{\\sum_{u \\in N(v)} w_{vu}} \\sum_{u \\in N(v)} w_{vu} P(y_v = c)$$  \nwhere $w_{vu}$ is the weight of the edge between $v$ and $u$ and $P(y_v = c)$ is the probability of node $v$ having label $c$.  \nProblem is not necessarily converging and don't use the features of the nodes.  \nThe order of updating the labels is important and can be done in different ways. For example in order of id. Or randomly. Or in order of degree. Not better to do\nthem all at the same time???  \nIf a label doesnt change, then it is not updated anymore, i.e converged. So we can stop the process when no label changes anymore.  \nThis process is just based on the network strucutre and their labels.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Neural Networks", "Header 2": "Message Passing", "Header 3": "Iterative classification", "path": "../pages/digitalGarden/cs/machineLearning/gnn.mdx"}, "page_content": "Will also use the features of the nodes and not just the labels.  \nEAch node has a feature vector $x_v$ and a label $y_v$. We train two classifiers, one that predicts the label of a node based on its features\nand one that predicts the label of a node based its features and the labels of its neighbors, $z_v$. This is a summary vector of the labels of the neighbors.\nCould be for example a histogram of the labels of the neighbors, lots of possiblities.  \nThe classifiers are trained using a training set in phase 1. In phase 2, we use the first classifier to predict the labels of the nodes and then use the second classifier\nto predict the labels of the nodes based on the features and the labels of the neighbors. We then update the labels of the nodes with the predictions of the second classifier.\nWe then repeat this process until convergence.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Neural Networks", "Header 2": "Message Passing", "Header 3": "Belief propagation", "path": "../pages/digitalGarden/cs/machineLearning/gnn.mdx"}, "page_content": "This is a message passing algorithm. We send messages from the nodes to their neighbors and then update the labels of the nodes based on the messages.  \n\"loopy belief propagation\" because graph can have cycles. Passing messages, updating beliefs, passing messages, updating beliefs, etc.  \nFirst example is a path graph and we want to count the number of nodes in a graph. Pass messages i.e count of neighbor + 1 etc. at the sink node we have the count.  \nCan also do this for a tree, when we start in the leaves and then move up to the root.  \n> a Node listens to its incoming messages from its neighbors, updates its belief and then sends messages to its neighbors.  \nWe have a label-lable potential matrix where the value $\\phi_{uv}(y_u, y_v)$ the probability of a the node u having label $y_u$ and node v having label $y_v$.  \nThe function \"prior belief\" is the probability of a node being in class y_i, $/\\psi(Y_i)$.  \nThe above is very weird. I don't understand it. But its the formal idea of the above.  \nIf the graph has cycles we have problems with convergence but it can still be run. messages just keep being passed around in the cycles.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Neural Networks", "Header 2": "Deep Learning on Graphs", "path": "../pages/digitalGarden/cs/machineLearning/gnn.mdx"}, "page_content": "problems with simple node embeddings:  \n- parameter number grows with the number of nodes. no sharing of parameters between nodes.\n- transductive, i.e. only works on the graph it was trained on. cannot generalize to new nodes.\n- do not incorporate node features.  \nuse deep graph encoders to solve these problems, i.e deeper models then just skipgram.  \nif a node does not have features, we can use a one-hot encoding of the node id or just a vector of ones or constants.  \nInitial idea could be to just take a row or column of the adjacency matrix as the node embedding and concatonate it with the node features.\nthis results in a lot input features for the first layer which is not good. or we cant use the same network for different graphs becuase of sizes.\nadjacency matrix also depends on ordering.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Neural Networks", "Header 2": "Deep Learning on Graphs", "Header 3": "Graph Convolutional Networks", "path": "../pages/digitalGarden/cs/machineLearning/gnn.mdx"}, "page_content": "GCN - Graph Convolutional Networks, instead of with images using patches, we use patches of the graph, i.e. the neighbors of a node. We want to generalize this idea.  \nSliding window is a bit tricky to define because neighborhoods can be different sizes.  \nthe center \"aggregates\" information from the neighbors to create a new type of message/information.  \nNeed to learn the transfromations of the messages and then the aggregation function.  \nEach node defines a different computation graph becuase they have different neighbors. So we need to share parameters between nodes.  \nmodel depth = number of layers. number of layers = number of hops, K. The input features of the first layer are just the node features. In the end each\nnode has a embedding/feature vector.  \nThe above seems to be very simple but dumb idea.  \nbecause the ndoe order is arbitrary, the aggregation function needs to be symmetric, i.e. the order of the neighbors does not matter. permutation/order invariance.  \nSimple idea is to just sum or average the messages. and then pass it through a FNN.  \n$$h_v^{(k+1)} = \\sigma \\left( W_k \\sum_{u \\in N(v)}{\\frac{h_u^{(k)}}{|N(v)|} + B_k h_v^{(k)}} \\right)$$  \nwhere $h_v^{(k)}$ is the embedding of node $v$ at layer $k$, $W_k$ is the weight matrix of layer $k$, $B_k$ is the bias matrix of layer $k$\nand $\\sigma$ is the activation function.  \nWhy do we add the node embedding to the sum? Residual connection, self-transformation? This can also be written in matrix form.  \nWe can then use the above to train it to predict labels in a supervised way.\nWe can also use it in a unsupervised way to learn the embeddings of the nodes.  \nBecause the parameters are shared between nodes, we can use the same network for different graphs. or on unseen nodes.  \nHow is this related to GraphSage????q", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "When working with data we often do not know the entire data set in advance. For example, think of the dataset of all Google queries, this dataset is constantly changing and becoming larger. This is what leads us to the idea of data streams. These data streams are potentially infinite and non-stationary i.e their distributions are constantly changing.  \nData Stream Model\nLet us take a closer look at what data streams look like. We have a system that is receiving input at a rapid rate from one or more input ports (each port being a separate stream). We also think of this input as elements or tuples of data.  \nNow we want to make decisions or calculations using these data streams. But we run into a few problems, the data is constantly changing and we can not fit all the data in memory, be it secondary or primary. So we need to develop some algorithms to either sample the data or make approximations.  \n<Callout type=\"example\">\nThink of Google as the system. They have multiple data streams, one could search queries another maybe from Gmail or Google Drive etc. If we take a look at the search query stream maybe google wants to find out which search queries are popular at the minute to create a trending page.\n</Callout>  \n<Image\nsrc=\"/cs/mlDataStreams.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Sampling a Data Stream", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "Since we can't store and use the entire data stream we want to take samples of a data stream. However, we want these samples to be fair and represent the entire data stream which can make things very complicated, for example, if we want a sample size of 100 elements we can not just keep track of the last 100 elements as this is not a fair representation of the entire data stream.  \nWhen working with samples we are interested in two types of samples either a fixed proportion of the data stream for example 10% of all elements. Or we can get a random sample of fixed size for example 100 elements.  \n<Image\nsrc=\"/cs/mlDataSample.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Sampling a Data Stream", "Header 3": "Fixed Proportion Sample", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "When working with a fixed proportion sample we, for example, want 1 in 10 elements, i.e 10% of the data. However, this does have some issues, if the data stream is not very large then 10% might not be enough data to do our task, be it a calculation, decision or prediction etc.  \nYou can also imagine the opposite, since the data stream is infinitely large this fixed proportion will grow infinitely which could lead to use not being able to store it in memory.  \n#### Naive Approach  \nLet us carry on with the example of Google search queries, we might imagine that Google receives the following tuples in a stream `(user, query, time)`. If we could get a 10% sample of the data stream we can estimate the answer to the question \"How often did a user query the same at least twice?\".  \nOur first naive approach might be that we have a 10-sided dice and every time we receive a new tuple we roll the die and if it hits 1 we add the tuple to the sample. If we would be programming this instead of throwing a dice we could generate a random number between 1 and 10, $r \\in [1,10]$ and if $r=1$ we store the sample.  \nHowever, this approach has a flaw. Imagine each user issues $s$ queries once and $d$ queries twice, the total number of queries executed is $s+2d$. So if we wanted to know the fraction of double queries made it would be $\\frac{d}{s+2d}$. However, with this uniform approach our sample will contain $s/10$ of the single queries, $2d/10$ of the duplicate queries at least once but only $d/100$ pairs of duplicates. So our naive approach comes to the solution of $d/(10s+20d)$ which is not correct.  \n#### Bucket Approach  \nA different approach would be to take all the queries of 10% of the users. This could be done by defining the user as the key of the tuple and then uniformly hashing the key into 10 buckets. If we then wanted a 20% sample we could just take all the users that were hashed to the buckets with value $h \\leq 2$.  \n<Callout type=\"todo\">\nSome probabilities would be interesting to see just as in the exercises.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Sampling a Data Stream", "Header 3": "Fixed Proportion Sample", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "<Callout type=\"todo\">\nSome probabilities would be interesting to see just as in the exercises.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Sampling a Data Stream", "Header 3": "Fixed Size Sample - Reservoir Sampling", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "Sometimes we only want to have a fixed sample $S$ of $s$ tuples because maybe we are only able to fit that amount into memory. For this to be a fair and representative sample we want each element to have an equal probability of being in the sample. So if we want a sample size of $s=2$ and at time $t=5$ we have seen 5 elements then we want each element to be in the sample with a probability of $s/t=2/5$.  \nWe can do this using the so-called Reservoir Sampling Algorithm:  \n1. We store the first $s$ elements of the stream $S$.\n2. Then suppose we have seen $t-1$ elements and the $t$-th element arrives.\n1. With a proability of $s/t$ we keep the $t$-th element. If we keep it we need to make space for it in our sample we do this by kicking out an element picked uniformly at random.\n2. With a probability of $1-s/t$ we discard it  \nYou can simply think of a water reservoir where if fresh water comes in some old water spills/overflows to make space for it.  \n<Image\nsrc=\"/cs/mlSamplingReservoir.png\"\nwidth={450}\n/>  \nWe can prove that this algorithm fits our requirements by using proof by induction:  \nWe assume that after seeing $t$ elements, the sample contains each element seen so far with the probability of $s/t$. For our induction step we need to show that after $t+1$ elements the sample still fulfills the same requirements, i.e the sample contains each element seen so far with the probability of $s/(t+1)$.  \nInductive step: For an element already in the sample the probability that it stays in the sample is:  \n$$\n(t+1 \\text{ discarded}) + \\Big((t+1 \\text{ not discarded}) (\\text{element not picked})\\Big) \\newline\n= (1- \\frac{s}{t+1})+\\Big((\\frac{s}{t+1})(\\frac{s-1}{s})\\Big) = \\frac{t}{t+1}\n$$  \nSo if time is now $t \\to t+1$ then the probability that the tuple is still in the sample is  \n$$\n\\frac{s}{t}\\cdot \\frac{t}{t+1} = \\frac{s}{t+1}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Queries Over a Sliding Window", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "We very often find ourselves querying a data stream about the most recent input elements. This can be further generalized to processing queries using a sliding window. This sliding window holds $N$ elements.  \n<Image\nsrc=\"/cs/mlQuerySlidingWindow.png\"\nwidth={600}\n/>  \nWe could solve this problem quite easily by just keeping track of the $N$ most recent elements. But what if the $N$ elements take up to much storage and therefore can not be stored in memory? Or we have multiple streams and therefore want to minimize the memory usage as much as possible.  \nFor this let us move on to our next example of Amazon being our system and that we have for every product $X$ a data stream that consists of 1 or 0 for if that specific item was sold at the time. We could easily come up with the query of how many times has the product $X$ been bought in the last $k$ sales.  \nThanks to our stream structure we can generalize this query to how many 1s are in the last $k$ elements where $k \\leq N$.  \nAs mentioned before we don't just want to keep track of the last $N$ elements. Instead, we might think of maintaining 2 counters, $A$ for the number of 1s from the beginning of the stream and $B$ for the number of 0s from the beginning of the stream. If we then assume that the distribution is uniform we would get the following formula:  \n$$\nN \\cdot \\frac{A}{A+B}\n$$  \nThis solution is very simple but not very accurate as the distribution is not necessarily as uniform as we assumed. To prove this we can just think of the sales of Ice cream, it might be sold a lot in summer but much less in winter.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Queries Over a Sliding Window", "Header 3": "DGIM Method", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "The DGIM method is another way to approximate answer the above question of how many 1s are in the last $k$ elements of a data stream. However, it does not assume uniformity and only uses $O(\\log^2 N)$ storage which is more than just 2 counters but it makes use of the sliding window instead of looking at the entire data stream. There is however a drawback to this approach, since it is an approximation there will be an error, and in the worst case this method will produce an error of 50%! You might think this is a lot which it is, however, we can reduce this error to a small fraction $> 0$ in exchange for using more memory.  \nThe main idea of the DGIM method is that we split the data stream into blocks with exponentially increasing sizes. Firstly we define that each element has a corresponding timestamp of when it was input into the stream, for example, the first element has the timestamp 1, the next 2 etc.  \nA block in the DGIM method consists of the following parts:  \n- The timestamp $t$ of its end. But the be able to represent relevant timestamps to the window and keep storage small we do $t \\mod N$. This uses only $\\log N$ bits.\n- The number of 1s between its beginning and end. This uses $\\log(\\log N)$ bits because we add the constraint that the number of 1s in a block must be of a power of 2.  \nWe also add a few more constraints to the algorithm apart from the amount of 1s having to be a power of 2:  \n- There can only be one or two Blocks with the same amount of 1s.\n- Blocks can not overlap.\n- Blocks are sorted by size, meaning that \"older\" blocks can not be smaller than \"newer\" blocks.\n- Blocks disappear when they are out of the window, i.e their end time is larger than $N$.  \n<Image\nsrc=\"/cs/mlDGIMBlocks.png\"\n/>  \nThis leads us to have at max $\\log N$ blocks, and each block needs $O(\\log N)$ bits so we have a total memory usage of $O(log^2 N)$.  \n#### Updating Blocks", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Queries Over a Sliding Window", "Header 3": "DGIM Method", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "<Image\nsrc=\"/cs/mlDGIMBlocks.png\"\n/>  \nThis leads us to have at max $\\log N$ blocks, and each block needs $O(\\log N)$ bits so we have a total memory usage of $O(log^2 N)$.  \n#### Updating Blocks  \nWhen new values come into the system we need to maintain our data structure. If the new element is 0 we can ignore it but if it is a 1 we need to do the following:  \n1. Create a new Block of size 1 containing just the new element with the end timestamp being the current time.\n2. If there are three blocks of size 1, we need to combine the two oldest blocks into a block of size 2.\n3. Recursively check if there are three blocks of the same size and combine the oldest blocks.  \n<Image\nsrc=\"/cs/mlDGIMBlocksUpdate.png\"\n/>  \n#### Querying  \nNow we just need to know how to query the DGIM data structure we have created to estimate the number of 1s in the sliding window. For this, we sum the sizes, i.e number of 1s of all the blocks apart from the last block. For the last block, we only add half the size. We only take half of the last block because we do not know how many 1s of the last black are actually still within the sliding window. This is also what leads us to our error of 50%. We can prove this by assuming the last block has the size $2^r$, then we assume that half i.e $2^{r-1}$ of its 1s are still in the window. We can also assume that there is at least one block of each size smaller than $2^r$ which sums up to $1+2+4+...+2^{r-1}= 2^r -1$. So our max error can be defined as the following:  \n$$\n\\frac{2^{r-1}}{2^r -1}\n$$  \n#### Reducing Error  \nWe can further reduce the error by changing the constraint of having either 1 or 2 blocks of the same size. Instead, we allow there to be either $x-1$ or $x$ blocks of the same size (excluding the largest size which can have between 1 and $x$ blocks). The error is then at most $1/x$. We can tradeoff between the maximum error and the amount of storage used.  \n<Callout type=\"todo\">\nExample of worst case with error 50%.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Queries Over a Sliding Window", "Header 3": "DGIM Method", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "<Callout type=\"todo\">\nExample of worst case with error 50%.  \nThis does not make a lot of sense with the end timestamp as it would already be gone as it is larger then $N$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Filtering a Data Stream", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "We often find ourselves wanting to filter data, for example, if we receive a search query we want to filter the results to only contain the search keyword. However the most common example to explain these algorithms is an email spam filter, when we receive a new email we want to filter it into the good or spam pile. In a more general form if we have a list of elements $S$ we want to determine which elements of a stream are in $S$. The most obvious solution would be a hash table, however as always when working with streams we most likely will not have enough memory to store all elements of $S$ in a hash table.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Filtering a Data Stream", "Header 3": "First Approaches", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "Our first approach could be a more streamlined hash table. We create a bit array $B$ with $n$ bits that are all initially set to 0. We then use the hash function $h$ to hash each element $s \\in S$ to one of the $n$ buckets and set that bit to 1 i.e $B[h(s)]=1$.  \nTo then find out if an element $e$ is in $S$ we just hash the element $e$ using the same hash function $h$ and check if the value is 1. The solution creates false positives due to collisions but no false negatives.  \n<Callout type=\"todo\">\nprobability of false negative\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Filtering a Data Stream", "Header 3": "Bloom Filter", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "The bloom filter is very commonly used and is heavily inspired by our first approach however it aims to minimize the false negative rate by using multiple hash functions.  \nWe have the same setup as in our first approach where we have \"training data\" $S$ with $m$ elements and a bit array $B$ with $n$ elements initialized at 1. However, now we have $k$ independent hash functions $h_1, h_2, ... h_k$. Now in our \"training\" phase we hash each element $s \\in S$ using each hash function and set the value of the bit array to 1. So $B[h_i(s)]= 1$ for $i \\in [1,k]$. Important to not get confused here is that we only have one bit array $B$ not multiple.  \nIf we now want to check to see if an element $e$ is in $S$ we again use all the hash functions but now check if all the values are 1. If even only one of the values isn't 1 then we can with 100% certainty say $e \\notin S$.  \n<Callout type=\"todo\">\nprobability of false negative and how to pick k.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Counting Distinct Elements", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "We have lots of use cases for wanting distinct elements for example Amazon might be interested to know how many distinct products they have sold in the last week. The most obvious approach would be to maintain a set of elements seen so far i.e use a hash table to keep track of all the distinct elements seen so far and their count. But as always we assume we do not have enough memory to store a hash table big enough so instead we want to be able to estimate the count fairly.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Counting Distinct Elements", "Header 3": "Flajolet-Martin algorithm", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "The Flajolet-Martin algorithm does exactly that. It is quite a simple algorithm we hash each element $e$ that comes into the system and use the output as in index of a bitstring and set it to 1, important to note is that the hash function is no longer uniform but exponential. In other words, $1/2$ of the elements map to the first bit, $1/4$ to the second bit etc.  \nOur intuition of this is then that the probability of the first bucket being 1 is $1/2$ the second $1/4$ the $k$-th bit then has a probability of $1/2^k$ of being 1. This means that if the $k$-th bit is set to 1 then an event with a probability of $1/2^k$ has happened which we would expect if we inserted $s^k$ distinct elements.  \nThe algorithm then says that if $R$ is the position of the least 0 bit then the number of distinct elements is $2^R/C$ where $C$ is some constant.  \nWe can further improve the accuracy of this algorithm by using $k$ different hash functions und bitstrings. We then compute the average position of the least 0 bit $b$.  \nThe number of distinct elements is then $2^b/C$ where $C$ is still some constant.  \nThe storage required for this algorithm is also very small. If we use $k$ hash functions and $k$ bitstrings and given a N distinct elements we only need $O(k log N)$ bits.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Moments", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "First, let us define what a moment is and then see what they can be used for. Suppose we have a data stream $S$ and the set $A$ contains all the distinct elements in the stream. We then let $m_i$ be the number of times the value $i$ occurs in the stream $S$. We can then define the $k$-th moment as:  \n$$\n\\sum_{i \\in A}{(m_i)^k}\n$$  \n<Callout type=\"example\">\nIf we have the stream $S=[x,y,x,y,z,z,z,x,z]$ then the 2nd moment is $3^2+2^2+4^2=29$\n</Callout>  \nIf we take a closer look at different moments we will notice the following:  \n- The 0th moment corresponds to the number of distinct elements, just as we saw below\n- The 1st moment corresponds to the number of elements in the stream $S$ i.e $|S|$.\n- The 2nd moment we can call the surprise number of $S$ which is a measure of how uneven the distribution of $S$ is (the lower the value the more even the distribution).  \n<Callout type=\"example\">\nIf we have the following values for $m_i: [10, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]$ then the surpise value is $910$ because the distribution is pretty even.  \nIf $m_i: [90, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]$ then the surprise value is $8110$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Moments", "Header 3": "AMS Method", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "Now we want a method to be able to give an unbiased estimate of the $k$-th moment. First, let us look at the 2nd moment  \n<Callout type=\"todo\">\nConfussed by the algorithm\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Mining Data Streams", "Header 2": "Counting Frequent Itemsets", "path": "../pages/digitalGarden/cs/machineLearning/miningDataStreams.mdx"}, "page_content": "We can imagine that Amazon or Google often ask themselves the question given a data stream how they can find recent frequent items, frequent items being defined by a threshold and recent meaning being in a sliding window.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Random Forests", "path": "../pages/digitalGarden/cs/machineLearning/randomForests.mdx"}, "page_content": "The idea of Random Forests is to build many decision trees for the same problem but that are slightly different. To then get the\nfinal prediction, we take the average of all the predictions of the different trees. This is called an ensemble method.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Random Forests", "Header 2": "Building a Random Forest", "path": "../pages/digitalGarden/cs/machineLearning/randomForests.mdx"}, "page_content": "We will build a random forest to predict the survival of passengers on the Titanic.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Random Forests", "Header 2": "Building a Random Forest", "Header 3": "Bagging - Bootstrap Aggregation", "path": "../pages/digitalGarden/cs/machineLearning/randomForests.mdx"}, "page_content": "The idea of bagging is to build many models on different subsets of the data and then to average the predictions. So for each\ndecision tree we build, we will use a different subset of the data.  \nwhere did the name come from?  \nThis should improve the performance and prevent overfitting.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "PageRank", "path": "../pages/digitalGarden/cs/machineLearning/pageRank.mdx"}, "page_content": "PageRank can be extended to matrix factorization. All related to graph embeddings.  \n2 standford students start of google search engine. PageRank is the algorithm that google uses to rank web pages in their search engine results.  \nweb as a graph, nodes are web pages, edges are hyperlinks. PageRank is a centrality measure. It is a measure of the importance of a node in a graph.  \njust think of pages as static. and hyperlinks are just navigational not transnational. i.e not to add or buy something etc.  \nWikipedia perfect example, directed graph.  \nPageRank is a centrality measure. It is a measure of the importance of a node in a graph.  \nlink analysis algorithm. PageRank, Personalized PageRank, Random Walk with Restart.  \nThink of links as votes, the more links to a page the more important it is, incoming links are more important than outgoing links because they are more difficult to get.  \nAre all incoming links equal? No, the importance of the page casting the vote matters. A page casting a vote is more important if it has more incoming links.\nThis becomes a recursive question and the idea of PageRank is to solve this recursively.  \nif page i has importance x_i and d_i outgoing links, then the importance of each link is x_i/d_i. The importance of page i, x_i is the sum of the importance\nof the pages casting votes for it, i.e incoming links. So x_i = sum_j x_j/d_j where j are the pages casting votes for page i.  \nimportance=rank  \nA small example can be shown as a system of linear equations. you might think that you can then just solve the system of linear equations. But the problem is that\nthis is not scalable. The matrix is very large and sparse. So we need to use an iterative method.  \nstochastic adjacency matrix M, each column sums to 1. Called column stochastic matrix. Think of it as a probability distribution. How can this be interpreted?  \nRank vector, each page has a component in the rank vector. The rank vector is a probability distribution, i.e sums to 1.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "PageRank", "path": "../pages/digitalGarden/cs/machineLearning/pageRank.mdx"}, "page_content": "Rank vector, each page has a component in the rank vector. The rank vector is a probability distribution, i.e sums to 1.  \nThe flow equation can then be written as r=M*r how can this be interpreted and an example.  \nCan think of it as a random web surfer. At each time step the surfer is at a page and then follows a link to another page. The probability of following a link is\nthe importance of the page casting the vote. The surfer is at page i with probability r_i. The probability of following a link from page i to page j is M_ij.  \np(t) is the probability distribution of the surfer at time t. p(t+1) is the probability distribution of the surfer at time t+1. p(t+1)=M*p(t). This is the flow equation.  \np(t) is a stationary distribution, i.e it has converged. p(t+1)=p(t)=p.  \nTherefore r is also a stationary distribution. r=M*r. This is the PageRank equation.  \nThis then is related to the eigenvalue problem. M*r=r. r is the eigenvector where the eigenvalue is 1. This is the Perron-Frobenius theorem???? Because 1*r=M*r=r  \nIf you repeatly multiply M by itself, you will converge to the eigenvector with eigenvalue 1. This is the power method. And can be thought of the random surfer\nrepeatly following links. Till it converges to the stationary distribution. principal eigenvector, eigenvector with largest eigenvalue which is 1 here, why?  \nWe first assign each page a the rank 1/n where n=number of pages => r0. Then we repeatly multiply M by the rank vector. This is the power method repeated till convergence.  \nIn general takes about 50 iterations to converge. The power method is a very simple algorithm. It is also very scalable. It is also very easy to implement.  \nWe have two problems, dead ends/sinks and spider traps. Dead ends are pages with no outgoing links. Spider traps are all outgoing links stay within a group of pages.  \nSinks lead to importance \"leaking\" out of the graph, i.e dissapears, random walker falls off cliff. What does this mean, visualize.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "PageRank", "path": "../pages/digitalGarden/cs/machineLearning/pageRank.mdx"}, "page_content": "Sinks lead to importance \"leaking\" out of the graph, i.e dissapears, random walker falls off cliff. What does this mean, visualize.  \nSpider traps lead to importance \"trapped\" in the system. What does this mean, visualize. Eventually they absorb all the importance. random walker is stuck.  \nTo solve this we can add beta which is a teleporting factor. This is the probability of following a link. 1-beta is the probability of teleporting to a random page.\nnormally beta is between 0.8 and 0.9.  \nIf a page is a dead end we can also make the teleporting factor 1, i.e if a columns sums to 0 we can make it sum to 1\nby settings all the values to 1/n. It could theoritcally teleport again to the same page. But this is very unlikely. These dead ends anyway violate column stochasticity.  \nThe above can be nicely written as a sum or as the \"Google\" matrix. The Google matrix is a column stochastic matrix. It is a linear combination of the stochastic matrix\nand the teleporting matrix. The teleporting matrix is a matrix with all values 1/n.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "PageRank", "Header 2": "Personalized PageRank / Topic-Specific PageRank", "path": "../pages/digitalGarden/cs/machineLearning/pageRank.mdx"}, "page_content": "Have users and items where edges are purchases. We want to recommend items to users. We can use PageRank to find similar items. We want to find items that are similar\nto the items that the user has already purchased. We can use Personalized PageRank to do this. We want a recommender system.  \nDoesn't teleport uniformly to all pages, but only to pages that are similar to the pages that the user has already visited i.e a subset denoted by S.\nIf S only contains one page then it is also called random walk with restart.  \nCan find out which pages are most similiar to the page in S.  \nPages are similiar because the random walker often walks over the same pages. This is the intuition. It meassures similiarity over a lot of things like:\nthe entire graph, direction of links and degree of pages.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "PageRank", "Header 2": "Matrix Factorization and Graph Embeddings", "path": "../pages/digitalGarden/cs/machineLearning/pageRank.mdx"}, "page_content": "Instead of saying nodes are similiar if they appear on the same random walk, we just say they are similiar if they are connected.  \nSo we try to approximate the adjacency matrix with the product of two matrices, the embedding matrix and its transpose.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Correlation", "path": "../pages/digitalGarden/cs/machineLearning/correlation.mdx"}, "page_content": "Correlation is a statistical measure that describes how related two random variables are, for computer scientists, this\nis usually how related two vectors or time series are.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Correlation", "Header 2": "Correlation Coefficients", "path": "../pages/digitalGarden/cs/machineLearning/correlation.mdx"}, "page_content": "Most often the correlation coefficient is defined as $\\rho$ (rho) or $r$ and is a value between -1 and 1, i.e.\n$-1 \\leq r \\leq 1$. The simplest form of correlation is a linear correlation, which is a linear relationship between\nthe two variables. The correlation coefficient can then be interpreted in the following way:  \n- $r = 1$ means that the two variables are perfectly correlated, i.e. if one variable increases, the other variable\nincreases as well.\n- $r = -1$ means that the two variables are perfectly negatively correlated, i.e. if one variable increases, the\nother variable decreases.\n- $r = 0$ means that the two variables are not correlated at all, i.e. if one variable increases or decreases, the\nother variable does not change.  \nTherefore, the absolute value of the correlation coefficient can be interpreted as the strength of the correlation.  \nThe standard interpretation of the correlation strength is:  \n- $0.8 \\leq |r| \\leq 1$ is a strong correlation.\n- $0.5 \\leq |r| \\leq 0.8$ is a moderate correlation.\n- $0.1 \\leq |r| \\leq 0.5$ is a weak correlation.\n- $0 \\leq |r| \\leq 0.1$ is no correlation.  \n<Image\nsrc=\"/cs/correlationLinear.png\"\ncaption=\"Visualizations of linear correlations, positive and negative.\"\nwidth={600}\n/>  \n<Callout type=\"example\">\nA simple example of a linear correlation is the air temperature and the number of ice creams sold. If it is hot\nthen more ice creams are sold, if it is cold then less ice creams are sold. This is a linear correlation, and the\ncorrelation coefficient is close to 1 as the two variables increase and decrease together.\n</Callout>  \nNon-linear correlations are also possible, but they are more difficult to interpret.  \n<Image\nsrc=\"/cs/correlationNonLinear.png\"\ncaption=\"Visualizations of linear and non-linear correlations. The top left is the linear correlation coeff.,\nthe top right is the non-linear correlation coeff.\"\nwidth={600}\n/>  \n<Callout type=\"warning\">\nThe correlation coefficient only measures if there is a correlation between the two variables, it does not say", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Correlation", "Header 2": "Correlation Coefficients", "path": "../pages/digitalGarden/cs/machineLearning/correlation.mdx"}, "page_content": "the top right is the non-linear correlation coeff.\"\nwidth={600}\n/>  \n<Callout type=\"warning\">\nThe correlation coefficient only measures if there is a correlation between the two variables, it does not say\nanything about cause and effect. For example if we have a positive correlation between the number of ice creams\nsold and the number of people who drown, it does not mean that eating ice cream causes people to drown. It is\nsimply that both variables increase during the summer months.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Correlation", "Header 2": "Correlation Coefficients", "Header 3": "Outliers", "path": "../pages/digitalGarden/cs/machineLearning/correlation.mdx"}, "page_content": "The correlation coefficient is just like regression sensitive to outliers. If there are outliers in the data, then\nthe correlation coefficient will be affected by them. So it is important to check for outliers before calculating the\ncorrelation coefficient.  \n<Image\nsrc=\"/cs/correlationOutlier.png\"\ncaption=\"An example of an outlier affecting the correlation coefficient.\"\nwidth=\"500\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Correlation", "Header 2": "Correlation Coefficients", "Header 3": "Pearson's Correlation Coefficient", "path": "../pages/digitalGarden/cs/machineLearning/correlation.mdx"}, "page_content": "Pearson's correlation coefficient is the most common correlation coefficient, and is a measure of the linear correlation\nbetween two variables. If we have the two variables $X$ and $Y$, which have $n$ values, then the Pearson's correlation\ncoefficient is defined as:  \n$$\n\\begin{align*}\nr_{X,Y} &= \\frac{n \\cdot \\sum_{i=1}^{n}{(x_i - y_i)} - \\sum_{i=1}^{n}{x_i} \\cdot \\sum_{i=1}^{n}{y_i}}\n{\\sqrt{n \\cdot \\sum_{i=1}^{n}{x_i^2} - (\\sum_{i=1}^{n}{x_i})^2} \\cdot \\sqrt{n \\cdot \\sum_{i=1}^{n}{y_i^2} - (\\sum_{i=1}^{n}{y_i})^2}} \\\\\n&= \\frac{\\sum_{i=1}^{n}{(x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}}\n{\\sqrt{\\sum_{i=1}^{n}{(x_i - \\bar{x})^2}} \\cdot \\sqrt{\\sum_{i=1}^{n}{(y_i - \\bar{y})^2}}}\n\\end{align*}\n$$  \nThis formula can look a bit scary, but it is actually quite simple. Let us start with a few quick reminders. The mean\nvalue of a variable $X$ is defined as:  \n$$\nE(X) = \\mu_X = \\bar{x} = \\frac{1}{n} \\cdot \\sum_{i=1}^{n}{x_i}\n$$  \nThe variance of a variable $X$ is defined as:  \n$$\n\\begin{align*}\nVar(X) &= E(X^2) - E(X)^2 \\\\\n&= \\frac{1}{n} \\cdot \\sum_{i=1}^{n}{x_i^2} - (\\frac{1}{n} \\cdot \\sum_{i=1}^{n}{x_i})^2\n\\end{align*}\n$$  \nThe standard deviation of a variable $X$ is defined as:  \n$$\n\\sigma(X) = \\sqrt{Var(X)}\n$$  \nAnd lastly the covariance between two variables $X$ and $Y$ is defined as:  \n$$\n\\begin{align*}\n\\text{cov}(X,Y) &= E((X - \\mu_X) \\cdot (Y - \\mu_Y)) \\\\\n&= \\frac{1}{n} \\cdot \\sum_{i=1}^{n}{(x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}\n\\end{align*}\n$$  \n<Callout type=\"todo\">\nThis can be linked to all the other articles about statistics.\n</Callout>  \nNow we can see that the nominator of the Pearson's correlation coefficient is the covariance between $X$ and $Y$ times\n$n$, and the denominator is the product of the standard deviation of $X$ and $Y$ times $n$. So the Pearson's\ncorrelation coefficient is the covariance between $X$ and $Y$ divided by the product of the standard deviation of $X$\nand $Y$. So we can rewrite the formula as:  \n$$\nr_{X,Y} = \\frac{\\text{cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}\n$$  \n<Callout type=\"example\">", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Correlation", "Header 2": "Correlation Coefficients", "Header 3": "Pearson's Correlation Coefficient", "path": "../pages/digitalGarden/cs/machineLearning/correlation.mdx"}, "page_content": "correlation coefficient is the covariance between $X$ and $Y$ divided by the product of the standard deviation of $X$\nand $Y$. So we can rewrite the formula as:  \n$$\nr_{X,Y} = \\frac{\\text{cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}\n$$  \n<Callout type=\"example\">  \nLet's say we have the hypothesis that students with a higher GPA also have a higher SAT score. We can then check\nif there is a correlation between the two variables. We have the following data:  \n| Student | GPA ($X$) | SAT ($Y$) |\n|---------|-----|-----|\n| 1 | 3.4 | 595 |\n| 2 | 3.2 | 520 |\n| 3 | 3.9 | 715 |\n| 4 | 2.3 | 405 |\n| 5 | 3.9 | 680 |\n| 6 | 2.5 | 490 |\n| 7 | 3.5 | 565 |  \nIf we plot the data we get the following plot, where we can see a clear correlation:  \n<div className=\"flex justify-center mt-5\">\n<iframe src=\"https://www.desmos.com/calculator/p6uua7noqe?embed\" width=\"400\" height=\"400\"/>\n</div>  \nBy extending the table slightly we can calculate the Pearson's correlation coefficient pretty quickly.  \n| Student | GPA ($X$) | SAT ($Y$) | $x_i y_i$ | $x_i^2$ | $y_i^2$ |\n|---------|-----|-----|-----|-----|-----|\n| 1 | 3.4 | 595 | 2023 | 11.56 | 354025 |\n| 2 | 3.2 | 520 | 1664 | 10.24 | 270400 |\n| 3 | 3.9 | 715 | 2789 | 15.21 | 511225 |\n| 4 | 2.3 | 405 | 932 | 5.29 | 164025 |\n| 5 | 3.9 | 680 | 2652 | 15.21 | 462400 |\n| 6 | 2.5 | 490 | 1225 | 6.25 | 240100 |\n| 7 | 3.5 | 565 | 1978 | 12.25 | 319225 |\n| **Sum** | 22.7 | 3970 | 13262 | 76.01 | 2322400 |  \nNow we can calculate the Pearson's correlation coefficient:  \n$$\n\\begin{align*}\nr_{X,Y} &= \\frac{7 \\cdot 13262 - 22.7 \\cdot 3970}{\\sqrt{7 \\cdot 76.01 - 22.7^2} \\cdot \\sqrt{7 \\cdot 2322400 - 3970^2}} \\\\\n&= \\frac{2715}{2864.22} \\approx 0.95\n\\end{align*}\n$$  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Correlation", "Header 2": "Correlation Coefficients", "Header 3": "Spearman's Rank Correlation Coefficient", "path": "../pages/digitalGarden/cs/machineLearning/correlation.mdx"}, "page_content": "Spearman's rank correlation coefficient measures the monotonic relationship between two variables. As a reminder, a\nmonotonic function is a function that is either strictly increasing or strictly decreasing. The word rank comes into\nplay because we first rank the values of the variables with the lowest value of the variable getting the rank 1 and\nthe highest value of the variable getting the rank $n$, we then have $R(X)$ and $R(Y)$ which are the ranked variables.  \nThen the Spearman's rank correlation coefficient is actually calculated just as the Pearson's correlation coefficient\nbetween the two ranked variables. Often the Spearman's rank correlation coefficient is denoted as $r_s$.  \n$$\nr_s = \\frac{\\text{cov}(R(X),R(Y))}{\\sigma_{R(X)} \\cdot \\sigma_{R(Y)}}\n$$  \nIf all the ranks are unique (which is often the case if there aren't any duplicates in the data), then the formula can\nbe simplified to:  \n$$\nr_s = 1 - \\frac{6 \\cdot \\sum_{i=1}^{n}{(R(X_i) - R(Y_i))^2}}{n \\cdot (n^2 - 1)} = 1 - \\frac{6 \\cdot \\sum_{i=1}^{n}{d_i^2}}{n \\cdot (n^2 - 1)}\n$$  \n<Image\nsrc=\"/cs/correlationSpearman.png\"\ncaption=\"Visualizations of Spearman's rank correlation coefficient compared to Pearson's correlation coefficient.\"\nwidth={300}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Correlation", "Header 2": "Correlation Matrix", "path": "../pages/digitalGarden/cs/machineLearning/correlation.mdx"}, "page_content": "Isn't this also used in image classification to see how close classes are?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Finding Similar Items", "path": "../pages/digitalGarden/cs/machineLearning/findingSimilarItems.mdx"}, "page_content": "Many problems relating to data can be expressed as finding similar items. For example, if we wanted to solve the scene completion problem we would want to find images that are similar to each other to complete the scene.  \n<Image\nsrc=\"/cs/mlSceneCompletionProblem.png\"\n/>  \nWe can generalize this problem to finding near-neighbors in a high dimensional space. We can pretty easily come up with a naive solution to solve this. For example, if vectorize the images and then define some distance function $d(x_1,x_2)$ which returns the distance between the vectors $x_1$ and $x_2$. We could then find all pairs of images $(x_i, x_j)$ that are within some distance threshold $d(x_i, x_j) \\leq t$. However, for $n$ images this naive solution takes $O(n^2)$. Luckily there is a method to do this in $O(n)$!", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Finding Similar Items", "Header 2": "Jaccard Similarity and Distance", "path": "../pages/digitalGarden/cs/machineLearning/findingSimilarItems.mdx"}, "page_content": "The jaccard similarity of two sets is the size of their intersection divided by the size of their union:  \n$$\nsim(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n$$  \nThe jaccard distance is then defined as  \n$$\nd(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\n$$  \nThis leads to the desired result of if the documents are similar then the distance is small.  \n<Callout type=\"todo\">\nexample and better use cases and prob separate into single page.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Finding Similar Items", "Header 2": "Overview of Finding Similar items", "path": "../pages/digitalGarden/cs/machineLearning/findingSimilarItems.mdx"}, "page_content": "Our goal is to find \"near duplicate\" pairs of a given large number documents. Some possible applications for this could be to find mirror websites, cluster articles or detect plagiarism. When faced with this problem to solve we can immediatily think of some issues:  \n- What will the similarity look like if two documents have the same content but in a different order?\n- What if there are too many documents to compare, $O(n^2)$?\n- What if the documents are to large to load into main memory?  \nSo to solve this problem we define a series of steps:  \n1. We convert documents into sets, this can be done by shingling.\n2. We convert large sets to short signatures while preserving their similarity, we call this process min-hashing.\n3. Lastly, we only want to focus on pairs of signatures that have a high similarity i.e we want to decrease the number of pairs to compare, this is done with locality sensitive-hashing, short LSH.  \n<Image\nsrc=\"/cs/mlFindingSimiliarItemsOverview.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Finding Similar Items", "Header 2": "Shingling", "path": "../pages/digitalGarden/cs/machineLearning/findingSimilarItems.mdx"}, "page_content": "Shingling is the process of converting a document into a set. We might start with some simple approaches such as using the set of words appearing in documents or the set of important words defined by the TF-IDF but these approaches do not really work well as they do not take something into account. That something is the ordering of the words.  \n<Callout type=\"example\">\nIn the two approaches above the following documents would have the same similarity:\n......TODODODODODODODO\n</Callout>  \nThe answer to this problem is a $k$-shingle or or also commonly referred to as $n$-gram. A $k$-shingle for a document is a sequence of $k$ tokens, a token could be characters or words or something else.  \n<Callout type=\"todo\">\nExample and possibly belongs on its own page.\n</Callout>  \nWe can then further compress long shingles by hashing them to for example 4 bytes ($s^32-1$ possible values small likelihood of collision).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Finding Similar Items", "Header 2": "Min-Hashing", "path": "../pages/digitalGarden/cs/machineLearning/findingSimilarItems.mdx"}, "page_content": "Our next step would be to encode sets as bit vectors, so we have one dimension per element in the universal set. We can then interpret the set intersection as a bitwise AND operation and the set union as a bitwise OR operation.  \n<Callout type=\"todo\">\nexample\n</Callout>  \nOur next step could be to go from vectors to matrices, where each row is a shingle and each column represents a document. This is very nice as the column similarity corresponds to the jaccard similarity of the 2 sets.  \nOur next goal is to compute for each column a small signature but to keep the properity where similar columns have similar signatures. The key idea here is that we want to find a hash function that if we hash a column C to the signature, the signature is small and we preserve the similarity. This can be done with so called min-hashing.  \n<Callout type=\"todo\">\nCan explain a lot of things for min hashing I think, as seems very popular\n</Callout>  \nThe process of min hashing can be defined in the following steps:  \n1. We have $k$ hash functions.\n2. For each Column C and hash function k we keep a slot for the min-hash value whihc is in the beggining infinity.\n3. We scan row for row looking for 1s and replace the value with the hashed value if it is smaller.  \n<Callout type=\"todo\">\nexample, code ?\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Finding Similar Items", "Header 2": "Locality-Sensitive Hashing", "path": "../pages/digitalGarden/cs/machineLearning/findingSimilarItems.mdx"}, "page_content": "The last step is to find signatures with a high similarity that we can compare. So the general idea is to have a function $f(x,y)$ that tells us whether $x$ and $y$ are a candidate pair given a certain threshhold $t$.  \nSo if $t=0.8$ then x and y are a candidate pair if their signatures agree on at least 80% of their rows. However this means we need to compare all pairs of columns which is $O(n^2)$ again. Instea we use a hash function again that only hashes similar columns to the same bucket, the candidate pairs are then those that hash to the same bucket.  \nSo we partition the signature matrix M into b bands which each have r rows. We then hash for each band the portion of columns to a hash table with k buckets where k is as large as possible. A candidate column is then if at least for one band they hashed to the same bucket. b and r are then hyperparameters to catch most similar pairs and the fewest non similar pairs.  \n<Callout type=\"todo\">\nthe s curve\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Copy and Move Semantic", "path": "../pages/digitalGarden/cs/cpp/copyMoveSemantic.mdx"}, "page_content": "When working with big objects for example an Image copying objects around can get very expensive. For example if create a factory function  \n```cpp\nstring factory() {\nstring t = \"text\";\nreturn t; // t gets copied using a temp object\n}\nstring s = factory(); // might get copy elision\ns = factory() // another temp is created\n```  \nTo improve this we need to understand a few things.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Copy and Move Semantic", "Header 2": "Lvalues and Rvalues", "path": "../pages/digitalGarden/cs/cpp/copyMoveSemantic.mdx"}, "page_content": "In C++ you can split values into groups.  \n- Lvalues are values whos adresses you can get and in most times last a long time. For example x here is a Lvalue `int x = 3;`.\n- Rvalues however are transient or temporary and are destroyed when no longer needed. For example (x+y) here is a Rvalue `int z = x+y` because it is temporary and as as soon as it is copied to z it is destroyed and we can't get its address. Some goes for the 3 above you can not do `&3` so it is an Rvalue.  \nRvalues can be split up into 2 further groups, pure-right values which are  \n- Literals: 42, true, nullptr\n- Arithmetic expressions: a + b, a < b\n- Function calls: str.substr(1, 2)  \nand Xvalues (eXpiring). Xvalues are functions calls with an Rvalue reference as return or array indexing or attribut calls on an Rvalue.  \nGL values that are lValues and Xvalues.  \n<Image\nsrc=\"/cs/cppLRValues.png\"\ncaption=\"Hierarchy of Lvalues and Rvalues in C++.\"\nwidth={400}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Copy and Move Semantic", "Header 2": "Lvalues and Rvalues", "Header 3": "Rvalue reference", "path": "../pages/digitalGarden/cs/cpp/copyMoveSemantic.mdx"}, "page_content": "You can prolong the life of an Rvalue by using a Rvalue reference  \n```cpp\n#include <iostream>\nusing namespace std;\nint main()\n{\nstring s1 = \"Test\";\n//string&& r1 = s1; Rvalue reference cant have Lvalue\nconst string& r2 = s1 + s1; // constant Lvalue referenz can have Rvalue, life prolonged\nstring&& r3 = s1 + s1; // Rvalue reference has Rvalue, life prolonged\nr3 += \"Test\";\n}\n```  \nYou can then use Rvalue references as parameters to make sure things do not get copied or wasted. You can also remember when you a returning an Rvalue reference you should always use move. [But with C++20 this becomes all irrelevant](https://stackoverflow.com/questions/17473753/c11-return-value-optimization-or-move).  \n```cpp\n#include <iostream>\nusing namespace std;\n\nstring foo(string&& s) { // s becomes Lvalue in function\ns += \"456\";\nreturn move(s); // if not move then it is return by value and gets copied\n}\nint main()\n{\nstring s = \"Test\";\nstring f = foo(s + \"abc\");\n// string t = foo(s); not possible because s is Lvalues\ncout << s <<endl;\ncout << f << endl;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Copy and Move Semantic", "Header 2": "Copying and moving", "path": "../pages/digitalGarden/cs/cpp/copyMoveSemantic.mdx"}, "page_content": "You often want to create copies of other objects (also commonly known as prototypes). You need to very careful with what you are doing then especially when you are working with pointers and don't want a shallow copy but a deep copy. Instead of creating a copy you can also create a move constructor which moves all the values from object to another. You can then also override the assignment operator to either copy or move the data depending on what is on the right side. All the move function from the standard library does is make an Rvalue out of an Lvalue.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nclass Person\n{\nprivate:\nstring last_name;\nstring first_name;\nint* age;\nvoid invalidate() {\nthis->last_name = \"\";\nthis->first_name = \"\";\nthis->age = nullptr;\n}\npublic:\nexplicit Person(const string& last_name_param = \"\", const string& first_name_param = \"\", int age_param = 0)\n: last_name(last_name_param), first_name(first_name_param), age(new int(age_param)) //  age(source_p.get_age()) would have 2 pointers to same value\n{}\n\n// copy constructor\nPerson(const Person& source)\n: last_name(source.last_name), first_name(source.first_name), age(new int(*source.age))\n{\ncout << \"Copy constructor called\" << endl;\n}\n\n// move constructor\nPerson(Person&& source)\n: last_name(source.last_name), first_name(source.first_name), age(source.age)\n{\ncout << \"Move constructor called\" << endl;\nsource.invalidate();\n}\n\n~Person() {\ndelete age; // Make sure that you are not leaking memory\n}\n\n// copy assignment operator\nvoid operator=(const Person& source) noexcept {\ncout << \"Copy assignment operator called\" << endl;\n\n// Check for self-assignment:\nif (this == &source)\nreturn;\n\nthis->set_last_name(source.get_last_name());\nthis->set_first_name(source.get_first_name());\nthis->set_age(*source.get_age());\n}\n\n// move assignment operator\nvoid operator=(Person&& source) noexcept {\ncout << \"Move assignment operator called\" << endl;\n\n// Check for self assignment\nif (this == &source)\nreturn;", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Copy and Move Semantic", "Header 2": "Copying and moving", "path": "../pages/digitalGarden/cs/cpp/copyMoveSemantic.mdx"}, "page_content": "// move assignment operator\nvoid operator=(Person&& source) noexcept {\ncout << \"Move assignment operator called\" << endl;\n\n// Check for self assignment\nif (this == &source)\nreturn;\n\nthis->set_last_name(source.get_last_name());\nthis->set_first_name(source.get_first_name());\nthis->age = source.get_age();\nsource.invalidate();\n}\n\nvoid set_first_name(const string& first_name) { this->first_name = first_name; }\nvoid set_last_name(const string& last_name) { this->last_name = last_name; }\nvoid set_age(int age) { *(this->age) = age; } // Remember to dereference\nconst string& get_first_name() const { return first_name; }\nconst string& get_last_name() const { return last_name; }\nint* get_age() const { return age; };\n\n//Utilities\nvoid print_info() {\ncout << \"Person object at : \" << this\n<< \" [ Last_name : \" << last_name\n<< \", First_name :  \" << first_name\n<< \" ,age : \" << *age\n<< \" , age address : \" << age\n<< \" ]\" << endl;\n}\n};\n\nint main()\n{\ncout << \"Testing copy constructor\" << endl;\nPerson  p1(\"John\", \"Snow\", 25);\np1.print_info();\n//Create a person copy\nPerson p2(p1);\np2.print_info();\ncout << \"Testing move constructor\" << endl;\nPerson p3(move(p2)); // moved, move() just makes p2 a Rvalue\np3.print_info();\n//p2.print_info(); now basically dead", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Copy and Move Semantic", "Header 2": "Copying and moving", "path": "../pages/digitalGarden/cs/cpp/copyMoveSemantic.mdx"}, "page_content": "cout << \"Testing copy operator\" << endl;\n// Person p4 = p3; will use the copy constructor because the object first needs to be constructed\nPerson p4;\np4 = p3;\np3.print_info();\np4.print_info();\ncout << \"Testing move operator\" << endl;\nPerson p5;\np5 = move(p4);\np5.print_info();\n//p4.print_info(); now basically dead\n}\n```  \n```bash title=\"Output\"\nTesting copy constructor\nPerson object at : 000000CDCD38F420 [ Last_name : John, First_name :  Snow ,age : 25 , age address : 000001DF4FBE9C70 ]\nCopy constructor called\nPerson object at : 000000CDCD38F4A0 [ Last_name : John, First_name :  Snow ,age : 25 , age address : 000001DF4FBF0FE0 ]\nTesting move constructor\nMove constructor called\nPerson object at : 000000CDCD38F520 [ Last_name : John, First_name :  Snow ,age : 25 , age address : 000001DF4FBF0FE0 ]\nTesting copy operator\nCopy assignment operator called\nPerson object at : 000000CDCD38F520 [ Last_name : John, First_name :  Snow ,age : 25 , age address : 000001DF4FBF0FE0 ]\nPerson object at : 000000CDCD38F5A0 [ Last_name : John, First_name :  Snow ,age : 25 , age address : 000001DF4FBEC190 ]\nTesting move operator\nMove assignment operator called\nPerson object at : 000000CDCD38F620 [ Last_name : John, First_name :  Snow ,age : 25 , age address : 000001DF4FBEC190 ]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Classes", "Header 2": "Constructors", "path": "../pages/digitalGarden/cs/cpp/classes.mdx"}, "page_content": "In C++ primitive types don't have constructors so you need to initialize them in the constructor.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nclass Point {\nprivate:\ndouble m_x;\ndouble m_y;\n\npublic:\n// default constuctor\nPoint() = default; // or just Point(){};\nPoint(double x, double y) { // Point object is already initialized\nm_x = x; // lots of copiessssss\nm_y = y;\n}\n};\n\nint main()\n{\nPoint p1(); // default constructor, very bad nothing is initialized\nPoint p2(1, 2);\n}\n```  \nHowever the above example is a bad way of creating a constructor as the object is already initialized and then the values are changed, this leads to lots of member-wise copying unnecessarily used memory. Even worse is the default constructor which leaves the attributes uninitialized because as mentioned the primitives don't have a default constructor.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Classes", "Header 2": "Constructors", "Header 3": "Initializer lists", "path": "../pages/digitalGarden/cs/cpp/classes.mdx"}, "page_content": "Instead in modern C++ we use initializer lists which stops the copying and everything bad mentioned above.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nclass Point {\nprivate:\ndouble m_x;\ndouble m_y;\n\npublic:\n// default constuctor\nPoint() : m_x(0), m_y(0) {};\nPoint(double x, double y): m_x(x), m_y(y) {};\n};\n\nint main()\n{\nPoint p1(); // default constructor x and y are 0\nPoint p2(1, 2);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Classes", "Header 2": "Constructors", "Header 3": "Default parameters", "path": "../pages/digitalGarden/cs/cpp/classes.mdx"}, "page_content": "We can improve the above constructor even more by using default parameter values. Because the arguments also don't change and we don't want them to we can add const.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nclass Point {\nprivate:\ndouble m_x;\ndouble m_y;\n\npublic:\nPoint(const double x = 0, const double y = 0): m_x(x), m_y(y) {};\n};\n\nint main()\n{\nPoint p1(); // x and y are 0\nPoint p2(1, 2);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Classes", "Header 2": "Constructors", "Header 3": "Explicit constructors", "path": "../pages/digitalGarden/cs/cpp/classes.mdx"}, "page_content": "You need to be very careful with creating constructors and often have to define a constructor as explicit otherwise something might just implicitly create an object of a certain type.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nclass Complex {\nprivate:\ndouble real;\ndouble imag;\n\npublic:\n// Default constructor\nexplicit Complex(double r = 0.0, double i = 0.0)\n: real(r), imag(i)\n{}\n\n// compare two Complex numbers\nbool operator==(Complex rhs) { return (real == rhs.real && imag == rhs.imag); }\n};\n\nint main()\n{\nComplex com1(3.0, 0.0);\ncout << com1 == 3.0 << endl; // if not explicit this will create a Complex object (3.0,0.0) and compare them\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Classes", "Header 2": "Anonymous/temporary objects", "path": "../pages/digitalGarden/cs/cpp/classes.mdx"}, "page_content": "These objects have no name and are thrown away after use.  \n```cpp\n#include <iostream>\nusing namespace std;\nclass Point {\ndouble m_x, m_y;\npublic:\nPoint(double x = 0, double y = 0)\n: m_x(x), m_y(y)\n{}\n\nvoid print() {\ncout << \"(\" << m_x << \"/\" << m_y << \")\" << endl;\n}\n};\n\nint main()\n{\nPoint(1, 3).print();\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Classes", "Header 2": "Constant member functions", "path": "../pages/digitalGarden/cs/cpp/classes.mdx"}, "page_content": "Constant member functions are denoted by writting `const` at the end of the definition. If in this class function changes a member variable of the class it will generate a compiler error, however, reading of a class variables is okay inside of the function.  \n```cpp\n#include<iostream>\nusing namespace std;\nclass Demo {\nint val;\npublic:\nDemo(int x = 0): val(x) {    }\nint getValue() const {\n// val++; would not work\nreturn val;\n}\n};\nint main() {\nDemo d(8);\ncout << \"The value using object d : \" << d.getValue();\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Enums and Enum Classes", "path": "../pages/digitalGarden/cs/cpp/enums.mdx"}, "page_content": "Just like in C you can use normal enums and they work the same way.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Enums and Enum Classes", "Header 2": "Enum classes", "path": "../pages/digitalGarden/cs/cpp/enums.mdx"}, "page_content": "Enum classes or so called scoped enumerations make enumerations both strongly typed and strongly scoped. Class enums don't allow implicit conversions to int, and also don't allow comparisons between different enumerations.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nint main()\n{\nenum class Color {\nRed,\nGreen,\nBlue\n};\nenum class State {\nGood,\nBad\n};\n\n// An enum value can be used as variable identifier\nint Green = 10;\n\n// Instantiating the Enum Class\nColor x = Color::Green;\n\n// Comparison now is completely type-safe\nif (x == Color::Red)\ncout << \"It's Red\\n\";\nelse\ncout << \"It's not Red\\n\";\n\nState p = State::Good;\n\nif (p == State::Bad)\ncout << \"Something went wrong\\n\";\nelse\ncout << \"All is good\\n\";\n\n// won't work because no implicit conversion to int\n// if(x == p)\n// cout<<\"red is equal to good\";\n// cout<< x;\ncout << int(x);\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "C++ allows you to specify more than one definition for a function name or an operator in the same scope, which is called function overloading and operator overloading.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "Header 2": "Function Overloading in C++", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "You can have multiple definitions for the same function name in the same scope. The definition of the function must differ from each other by the types and/or the number of arguments in the argument list. You cannot overload function declarations that differ only by return type.  \n```cpp\nclass PrintData {\npublic:\nvoid print(int i) {\ncout << \"Printing int: \" << i << endl;\n}\nvoid print(double  f) {\ncout << \"Printing float: \" << f << endl;\n}\nvoid print(char* c) {\ncout << \"Printing character: \" << c << endl;\n}\n};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "Header 2": "Operator Overloading", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "You can redefine or overload most of the built-in operators available in C++. Thus, a programmer can use operators with user-defined types as well.  \nOverloaded operators are functions with special names: the keyword \"operator\" followed by the symbol for the operator being defined. Like any other function, an overloaded operator has a return type and a parameter list.  \nThere are two types of operator overloading either the operator is an instance method or it is not. So x+y could be `operator+(x,y)` or `x.operator(y)` if you use the instance version then you can refer to x using the special `this`. By using the friend keyword you make the function no longer an instance method meaning you can not access the special `this`, but you can access the private attributes, even if you define the function in the class and implement it outside the class.  \n```cpp title=\"Box.h\"\n#pragma once\nclass Box {\ndouble m_length;\ndouble m_breadth;\ndouble m_height;\n\npublic:\nBox(const double length = 0, const double breadth = 0, const double height = 0)\n: m_length(length)\n, m_breadth(breadth)\n, m_height(height)\n{}\n\ndouble getVolume(void) {\nreturn m_length * m_breadth * m_height;\n}\nvoid setLength(double length) {\nm_length = length;\n}\nvoid setBreadth(double breadth) {\nm_breadth = breadth;\n}\nvoid setHeight(double height) {\nm_height = height;\n}\n\nBox operator+(const Box& b) {\nBox box;\nbox.m_length = this->m_length + b.m_length;\nbox.m_breadth = this->m_breadth + b.m_breadth;\nbox.m_height = this->m_height + b.m_height;\nreturn box;\n}\n};\n```  \n```cpp title=\"main.cpp\"\n#include <iostream>\n#include \"Box.h\"\n\nusing namespace std;\n\nint main()\n{\nBox box1(6, 7, 5);\nBox box2(12, 13, 10);\ndouble volume;\n\n\ncout << \"Volume of Box1 : \" << box1.getVolume() << endl;\ncout << \"Volume of Box2 : \" << box2.getVolume() << endl;\n\nBox box3 = box1 + box2;\n\ncout << \"Volume of Box3 : \" << box3.getVolume() << endl;\nreturn 0;\n}\n```  \n```bash title=\"Output\"\nVolume of Box1 : 180\nVolume of Box2 : 1440\nVolume of Box3 : 4860\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "Header 2": "Operator Overloading", "Header 3": "Not overloadable Operators", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "- ::\n- .*\n- .\n- ?:  \n<Callout type=\"warning\">\nIf you overload && or || you deactivate the Short-circuit evaluation.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "Header 2": "Operator Overloading", "Header 3": "Index operator", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "When overloading the index operator it is good practice to first implement an `T at(size_t)` function which makes the range check and then use that in your implementation. You also need to be aware of whether you return by value or by reference which can lead to different results down the road.  \n<Image\nsrc=\"/cs/cppIndexOperator.png\"\ncaption=\"The difference between returning by value and by reference in the index operator.\"\nwidth={700}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "Header 2": "Operator Overloading", "Header 3": "Increment/Decrement operator", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "```cpp\nclass Point\n{\nint m_x, m_y;\npublic:\nPoint& operator++();       // Prefix increment operator.\nPoint operator++(int);     // Postfix increment operator.\nPoint& operator--();       // Prefix decrement operator.\nPoint operator--(int);     // Postfix decrement operator.\n\nPoint(const int x = 0, const int y = 0)\n: m_x(x)\n, m_y(y)\n{}\n\nint x() { return m_x; }\nint y() { return m_y; }\n};\n\nPoint& Point::operator++()\n{\nm_x++;\nm_y++;\nreturn *this;\n}\n\nPoint Point::operator++(int)\n{\nPoint temp = *this;\n++(* this);\nreturn temp;\n}\n\nPoint& Point::operator--()\n{\nm_x--;\nm_y--;\nreturn *this;\n}\n\nPoint Point::operator--(int)\n{\nPoint temp = *this;\n--(*this);\nreturn temp;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "Header 2": "Operator Overloading", "Header 3": "Type conversion operator", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "When overloading type conversions you don't need to add any parameters or a return type as this can be implicitly done. If you want the type conversion to be explicit you can add explicit before the function definition.  \n```cpp\n#include <cmath>\n#include <iostream>\n\nclass Complex {\ndouble m_real;\ndouble m_imag;\n\npublic:\n// Default constructor\nComplex(double r = 0.0, double i = 0.0)\n: m_real(r)\n, m_imag(i)\n{\n}\n\n// function style\ndouble mag() { return getMag(); }\n\n// conversion operator\noperator double() { return getMag(); }\n\nprivate:\ndouble getMag() { return sqrt(m_real * m_real + m_imag * m_imag); }\n};\n\nint main()\n{\nComplex com1(3.0, 4.0);\nstd::cout << com1.mag() << std::endl;\n// Converts to double implicitly because double already has output operator\nComplex com2(4.0, 5.0);\nstd::cout << com2 << std::endl;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "Header 2": "Operator Overloading", "Header 3": "Literal operator", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "```cpp\n#include <iostream>\n#include <string>\n\nstruct Distance\n{\nprivate:\nlong double kilometers;\nexplicit Distance(long double val)\n: kilometers(val)\n{}\n\nfriend Distance operator\"\" _km(long double val);\nfriend Distance operator\"\" _mi(long double val);\n\npublic:\nconst static long double km_per_mile;\nlong double get_kilometers() { return kilometers; }\n\nDistance operator+(Distance other) { return Distance(get_kilometers() + other.get_kilometers()); }\n};\n\nconst long double Distance::km_per_mile = 1.609344L;\n\nDistance operator\"\" _km(long double val) { return Distance(val); }\n\nDistance operator\"\" _mi(long double val) { return Distance(val * Distance::km_per_mile); }\n\nint main()\n{\n// Must have a decimal point to bind to the operator we defined!\nDistance d(402.0_km); // construct using kilometers\nstd::cout << \"Kilometers in d: \" << d.get_kilometers() << std::endl; // 402\n\nDistance d2{ 402.0_mi }; // construct using miles\nstd::cout << \"Kilometers in d2: \" << d2.get_kilometers() << std::endl;  //646.956\n\n// add distances constructed with different units\nDistance d3 = 36.0_mi + 42.0_km;\nstd::cout << \"d3 value = \" << d3.get_kilometers() << std::endl; // 99.9364\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Overloading", "Header 2": "Operator Overloading", "Header 3": "Output operator", "path": "../pages/digitalGarden/cs/cpp/overloading.mdx"}, "page_content": "To output objects on the console you can override the so called output operator which works with Operating system streams.  \n```cpp\n#include <iostream>\n\nclass Date\n{\nint m_month, m_day, m_year;\npublic:\nDate(int m, int d, int y)\n: m_month(m)\n, m_day(d)\n, m_year(y)\n{\n}\nfriend std::ostream& operator<<(std::ostream& os, const Date& dt) {\nos << dt.m_month << '/' << dt.m_day << '/' << dt.m_year;\nreturn os;\n}\n};\n\nint main()\n{\nDate dt(5, 6, 92);\nstd::cout << dt << std::endl;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "Templates are the C++ version of Java's generics. Just like with Java generics you can write generic code (a template) that is not dependent on a type.  \n```cpp\ntemplate<typename T> T min(T a, T b) {\nreturn(a < b) ? a : b;\n}\n\ntemplate<class T> T min(T a, T b) {\nreturn(a < b) ? a : b;\n}\n```  \nThe two definitions are identical. Using class or typename makes no difference. You can also combine this with non generic types for example when writing a type for a generic static array that has a variable length.  \n```cpp\ntemplate<class T, int S>\nclass Array {\nT m_array[S];\npublic:\nArray() : Array(0) {}\ntemplate<typename E> Array(E val) {\nfor (auto& a : m_array) {\na = static_cast<T>(val);\n}\n}\nconst T& operator[](int pos) const { return m_array[pos]; }\nT& operator[](int pos) { return m_array[pos]; }\nvoid print() const {\ncout << '[';\ncout << m_array[0];\nfor (int i = 1; i < S; i++) cout << ',' << m_array[i];\ncout << ']' << endl;\n}\n};\nint main() {\nArray<int, 10> arr;\narr[3] = 4;\narr.print();\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Generic Classes", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "In C++ you can define generic classes and also define a default type for the generic class which you can not do in Java.  \n```cpp\ntemplate<typename T> class Vector {\nT* m_array;\n…\n};\ntemplate<typename T = char> class String { // default is char\nT* m_string;\n};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Generic Classes", "Header 3": "Explicit Instantiation", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "When a template is instantiated an entire function or class is built. This happens every time the code is compiled. To reduce compilation time you can use explicit instantiation.  \n```cpp\ntemplate<typename T> class A {\nT m_t;\npublic:\nA(T t): m_t(t) {}\nvoid f(T t);\n};\n// instead of\nint main() {\nA<double> a(3.5);\n}\n// add to header\nextern template class A<int>;\n// or cpp\ntemplate class A<int>;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Template of Templates", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "A template parameter list can take another template parameter list so we can have structures like this:  \n```cpp\n// function that takes a template that is specialized to a Container with type T and A.\ntemplate<template<typename, typename> class Container, class T, class A>\nstd::ostream& operator<<(std::ostream& os, const Container<T, A>& v) {\nos<<'[';\nauto it= v.begin();\nif(it != v.end()) os<< *it++;\nfor(; it != v.end(); it++) os<<\", \"<< *it;\nos<<']';\nreturn os;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Specialization", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "After the template definition, you can also add a specialization for a template method.  \n```cpp\ntemplate<typenameT> T min(T a, T b) {\nreturn(a < b) ? a : b;\n}\ntemplate<> char min<char>(char a, char b) {\na = tolower(a);\nb = tolower(b);\nreturn (a < b) ? a : b;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Specialization", "Header 3": "Partial Specialization", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "When there are multiple parameters for a generic class you can partially specialize some of them.  \n```cpp\ntemplate<typename T, class C> class MyClass{}; // generic\ntemplate<class C> class MyClass<char, C> {}; // partial\n\nMyClass<int, string> c1; // generic\nMyClass<char, string> c2; // partial\nMyClass<char, iostream> c3; // partial\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Specialization", "Header 3": "Specialization vs Overloading", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "It is important to remember that overloaded functions are always a better match than specialized template functions.  \n```cpp\ntemplate<typename T> void foo(T x) { cout<<\"Generic\"<< endl; }\ntemplate<> void foo(int* p) { cout<<\"Specialized\"<< endl; }\ntemplate<typename T> void foo(T* p) { cout<<\"Overloaded\"<< endl; }\nint i = 5;\nfoo(i); // Generic\nfoo(&i); // Overloaded\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Alias Templates", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "Generic classes and functions can have a very long definition especially when there are templates of templates involved. To shorten this you can either use `typedef` like in C for structs, or you can use the `using` keyword which is the more modern and flexible way of doing it.  \n```cpp\n// old\ntypedef array<vector<uint64_t>*, 50> AV50;\n// modern and flexible\ntemplate<typename T> using MA50 = array<T, 50>;\nusing MAV50 = MA50<vector<uint64_t>>;\n\nint main() {\n// long\narray<vector<uint64_t>*, 50> myArray;\nAV50 myShortArray;\nMAV50 myArray;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Variadic Templates", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "In C++ you can define templates that take a variadic number of arguments. This can be done by using `...`. Pattern-Matching is done at compilation.  \n```cpp\ntemplate<typename... Ts> class C {\nsize_t types = sizeof...(Ts); // number of params\n};\ntemplate<typename... Ts> void func(const Ts&... vs){\nsize_t params = sizeof...(vs); // number of params\n}\n```  \nYou can then also implement recursive templates like this:  \n```cpp\ntemplate<typename First, typename... Rest>\nvoid logging(const First& first, const Rest&... rest) {\ncout<< '[' << sizeof...(Rest) << \"] \";\ncout<< first << \", \";\nlogging(rest...); // recursion\n}\n// last call - anchor\ntemplate<typename T> voidlogging(constT& t) {\ncout << \"[0] \" << t << endl;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Perfect Forwarding", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "If a function template forwards its arguments without changing its lvalue or rvalue characteristics, we call it perfect forwarding. So for example if we have a function like `template<typename T> void f(T&& x) { ... }` then if a lvalue is passed to it stays a lvalue in the function body. But if an rvalue is passed to it becomes an lvalue in the function body. To avoid this we use the `std::forward<t>()` function which comes into use when working with universal/forwarding references (generic rvalues). The forward function forwards lvalues as lvalues and rvalues are moved with the move function to remain rvalues.  \n```cpp\ntemplate<typename First, typename... Rest>\nvoid logging(First&& first, Rest&&... rest) {\ncout << '[' << sizeof...(Rest) << \"] \";\ncout << forward<First>(first) << \", \";\nlogging(forward<Rest>(rest)...);\n}\n// anchor\ntemplate<typenameT> void logging(T&& t) {\ncout<< \"[0] \" << forward<T>(t)<< endl;\n}\n\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Template Meta Programming - TMP", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "Template meta programming is programming with types and not values.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Template Meta Programming - TMP", "Header 3": "Manipulating Types at Compile Time", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "You can also manipulate types at compile time with meta programming. This is for example how the move and forward functions are implemented.  \n```cpp\ntemplate<typename T >\nstruct removeConst {\ntypedef T type;\n};\ntemplate<typename T >\nstruct removeConst<const T> {\ntypedef T type;\n};\nint main() {\nstatic_assert(is_same<int, removeConst<int>::type>::value, \"Aren't the same\");\nstatic_assert(is_same<int, removeConst<const int>::type>::value, \"Aren't the same\");\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Template Meta Programming - TMP", "Header 3": "Calculating at Compile Time", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "Calculating the factorial of a number is the \"Hello World\" of template meta programming.  \n```cpp\ntemplate <int N>\nstruct Factorial{\nstatic int const value = N * Factorial<N-1>::value;\n};\n\n// anchor\ntemplate <>\nstruct Factorial<1>{\nstatic int const value = 1;\n};\n```  \n#### Integral Constant  \nWith integral constants you can also calculate values at compile time:  \n```cpp\n// Definition\ntemplate<classT, T v>\nstruct integral_constant{\nstatic constexpr T value = v;\ntypedef T value_type;\ntypedef integral_constant<T,v> type;\nconstexpr operator T() const noexcept{ return v; }\nconstexpr T operator()() const noexcept{ return v; }\n};\n// Use\ntemplate <int N>\nstruct Factorial: integral_constant<int, N*Factorial<N-1>::value> {};\ntemplate<>\nstruct Factorial<1> : integral_constant<int, 1> {};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Template Meta Programming - TMP", "Header 3": "Tag Dispatch", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "With tag dispatching, you can add tags to calls to be able to differentiate between types for a generic function.  \n```cpp\ntemplate<typename T>\nbool equals(T lh, T rh) {\nreturn equals(lh, rh, conditional_t<is_floating_point<T>::value, true_type, false_type>{});\n}\n\ntemplate<typename T>\nbool equals(T lh, T rh, true_type) { // floating\n// imprecision handling\nreturn lh == rh;\n}\n\ntemplate<typename T>\nbool equals(T lh, T rh, false_type) {\nreturn lh == rh;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Template Meta Programming - TMP", "Header 3": "SFINAE", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "SFINAE stands for \"Substitution Failure Is Not An Error\". To tell the compiler that a generic parameter is a type we use the `typename` keyword. This can also be further used to substitute types for example here:  \n```cpp\ntemplate<typename T>\nstruct Extrema{\nusing type = typename T::value_type; // substitution\ntype m_min, m_max;\nExtrema(const T& data)\n: m_min(*min_element(begin(data), end(data)))\n, m_max(*max_element(begin(data), end(data))) {}\n};\nExtrema<vector<int>> x({ 8, 3, 5, 6, 1, 3 });\n```  \nIf this substitution however doesn't work (if in the above example T does not have a `value_type` attribute) then there is no compiler error. We can then use the `enable_if` struct to be able to differentiate between generic methods just like with tag dispatching.  \n```cpp\ntemplate<typename T>\nenable_if<!is_floating_point<T>::value, bool> Equals(T lh, T rh) {\n// imprecision handling\ncout << \"blabla\" << endl;\nreturn lh == rh;\n}\n\ntemplate<typename T>\nenable_if<is_floating_point<T>::value, bool> Equals(T lh, T rh, false_type) { // floating\nreturn lh == rh;\n}\n```  \nNow we can also take a quick look at how `is_floating_point<T>` is implemented.  \n```cpp\ntemplate <class T>\nconstexpr bool is_floating_point_v= _Is_any_of_v<T, float, double, long double>;\n// true if T is in Ts\ntemplate <class T, class... Ts>\nconstexpr bool _Is_any_of_v= disjunction_v<is_same<T, Ts>...>;\n// implementation of is_same\ntemplate <class, class>\nconstexpr bool is_same_v = false; // determine whether arguments are the same type\ntemplate <class T>\nconstexpr bool is_same_v<T, T> = true;\n// implementation of disjunction\ntemplate <class First, class... Rest>\nstruct disjunction<First, Rest...> : _Disjunction<First::value, First, Rest...>::type {};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Templates", "Header 2": "Concepts", "path": "../pages/digitalGarden/cs/cpp/templates.mdx"}, "page_content": "In C++20 there are also concepts. Just like everything else up till now a concept is a set of constraints on template parameters evaluated at compile time.  \n```cpp\nstruct S1 {};\nstruct S2 { using type = int; };\n// old\ntemplate<class T>\nconstexpr bool has_type_member_f(T) { return has_type_member<T>; }\nstatic_assert(!has_type_member_f(S1()));\nstatic_assert(has_type_member_f(S2()));\n\n// new\ntemplate<typenameT>\nconcept has_type_member = requires{ typename T::type; };\nstatic_assert(!has_type_member<S1>);\nstatic_assert(has_type_member<S2>);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stream I/O", "Header 2": "Standard I/O", "path": "../pages/digitalGarden/cs/cpp/streamIO.mdx"}, "page_content": "In C the standard input and output work with the functions `scanf()` and `printf()`.  \nIn C++ this works over byte streams which are objects of the class `istream` for input and `ostream` for output.  \nThe objects `cin` and `cout` are such objects and are most commonly used. There are however also `cerr` for the standard error output and `clog` for buffered standard error output.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stream I/O", "Header 2": "Un/Formatted I/O", "path": "../pages/digitalGarden/cs/cpp/streamIO.mdx"}, "page_content": "Formatted I/O uses the `<<` and `>>` operators. These operators either read an object and write it to a stream as a string or the opposite way around. Unformatted uses like in C the `write()` and `read()` functions.  \n```cpp\n#include<iostream>\nusing namespace std;\nint main() {\nconstexpr int nColumns = 4;\ncout << \"ASCII-Tabelle\" << endl << endl;\nfor (int i = 32; i < 128; i++) {\ncout.width(3); // Numberwidth: 3\ncout.fill('0'); // fill with leading zeros\ncout << i << \" = 0x\";\ncout.setf(ios::hex, ios::basefield); // set to base 16\ncout.setf(ios::uppercase); // Hexnumbers in CAPS\ncout << i << \": \";\ncout.unsetf(ios::hex); // change back to decimal\ncout << (char)i << '\\t'; // output char\nif (i % nColumns == nColumns - 1)\ncout << endl;\n}\n}\n```  \nFormatted input of booleans:  \n```cpp\ncin.setf(ios::boolalpha);\ncin >> boolValue;\n```  \nError handling of formatted input:  \n```cpp\nif(cin.good()) { // short: if (cin)\ncout << \"Input is: \" << i << endl;\n} else{\ncin.clear();\ngetline(cin, s); // read entire line\nstringstream ss(s);\ncout << \"No number was entered instead the strings: \";\nwhile(ss.good()) {\nss >> s;\ncout << \"[\" << s << \"]\";\n}\ncout << endl;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stream I/O", "Header 2": "Un/Formatted I/O", "Header 3": "Stream Manipulators", "path": "../pages/digitalGarden/cs/cpp/streamIO.mdx"}, "page_content": "Manipulators are helper functions to control formatted input/output streams to make life easier then before with the multiple flags etc.  \n```cpp\n// before\ncout.setf(ios::hex, ios::basefield);\ncout.setf(ios::uppercase);\ncout << i << endl;\n// after\ncout << hex << uppercase << i << endl;\n```  \nThese are all pointers to functions. For example, the implementation of `endl` looks something like this:  \n```cpp\nostream& endl(ostream& os){\nos.put('\\n');\nos.flush();\nreturn os;\n}\n```  \nSome of the important manipulators are:  \n- `setw(int i)` is used to set the field width in output operations.\n- `setfill(char c)` is used to fill the character ‘c’ on output stream.\n- `setprecision(int i)` sets val as the new value for the precision of floating-point values.\n- `setbase(int i)` is used to set the numeric base value for numeric values.\n- `showpos` forces to show a positive sign on positive numbers.\n- `noshowpos` forces not to write a positive sign on positive numbers.\n- `showbase` indicates the numeric base of numeric values.\n- `uppercase` forces uppercase letters for numeric values.\n- `nouppercase` forces lowercase letters for numeric values.\n- `fixed` uses decimal notation for floating-point values.\n- `scientific` uses scientific floating-point notation.\n- `hex` read and write hexadecimal values for integers and it works same as the setbase(16).\n- `dec` read and write decimal values for integers i.e. setbase(10).\n- `oct` read and write octal values for integers i.e. setbase(10).\n- `left` adjusts output to the left.\n- `right` adjusts output to the right.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stream I/O", "Header 2": "C++ Stream Classes", "path": "../pages/digitalGarden/cs/cpp/streamIO.mdx"}, "page_content": "Just like in Java there are lots of different streams all with there pros and cons etc.  \n<Image\nsrc=\"/cs/cppStreamHierarchie.png\"\ncaption=\"Hierarchy of all the streaming classes in C++.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stream I/O", "Header 2": "Stream States", "path": "../pages/digitalGarden/cs/cpp/streamIO.mdx"}, "page_content": "You can read the state (a number, `iostate`) of a stream with `rdstate()`. 0 means everything is good, all other numbers mean something different and depending on which of the 3 bits are set something different. `eofbit` means the end of the file has been reached, `failbit` a logical error has happend like an int should be read but \"hello\" was input (EOF also sets this bit). `badbit` means something went wrong when reading or writing for example hardware or OS issue.  \n<Image\nsrc=\"/cs/cppIOState.png\"\ncaption=\"Explanation of the different states of a stream in C++ and how they are set.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stream I/O", "Header 2": "File I/O", "Header 3": "Text Files", "path": "../pages/digitalGarden/cs/cpp/streamIO.mdx"}, "page_content": "#### Unformatted  \nVery C-like.  \n- `peek()` returns the next character in the stream or EOF.\n- `get()` returns a character from the stream.\n- `read(char* s, streamsize n)` reads n characters from the stream.\n- `getting(istream& is, string& str)` reads characters from stream and stores them into str until a newline.\n- `ignore()` reads and discards a character.\n- `gcount()` returns the number of characters extracted by the last unformatted input operation performed on the object.\n- `unget()` Attempts to make the last character extracted from the stream once again available.\n- `put()` writes a character.\n- `write()` writes n characters.  \n```cpp\n#include <iostream>\n#include <fstream>\n#include <string>\nusing namespace std;\nvoid main(){\nifstream inData;\nstring fileName;\ncout << \"Enter the file name: \";\ncin >> fileName;\ninData.open(fileName, ios::in); // open file\nif( !inData ) {\ncerr << \"Couldn't open file!\" << endl;\n} else {\nwhile(!inData.eof()) {\ncout << static_cast<char>(inData.get()); // read char for char.\n}\ninData.close(); // close file\n}\n}\n```  \n#### Formatted  \n```cpp\n#include<fstream>\n#include<iostream>\n\nusing namespace std;\nint main() {\n// Create file\nfstream inOutFile(\"fstream.txt\", ios::out | ios::trunc);\ninOutFile.close();\n\n// open for read and write\ninOutFile.open(\"fstream.txt\", ios::in | ios::out);\n\n// write\nfor(int j = 1; j <= 20; ++j)\ninOutFile<< j << ' ';\ninOutFile<< endl;\n// jump to start\ninOutFile.seekg(0);\n// read\nint i;\nwhile(inOutFile >> i) {\ncout << i << ' ';\n}\ninOutFile.clear(); // remove EOF status\ninOutFile.seekg(25); // jump to position 25\n\nwhile(inOutFile.get() != ' '); // go to start again\n\nwhile(inOutFile>> i) {\ncout << i << ' ';\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stream I/O", "Header 2": "File I/O", "Header 3": "Binary Files", "path": "../pages/digitalGarden/cs/cpp/streamIO.mdx"}, "page_content": "```cpp\n#include<iostream>\n#include<iomanip>\n#include<fstream>\n#include<sstream>\n#include<string>\n\nusing namespace std;\n\nstruct S {\nint value;\nstring text;\n};\n\nint main() {\nconstexpr int size= 10;\nS array[size];\nofstream ofs;\n// Array init\nfor(int i = 0; i < size; i++) {\nstringstream strs;\narray[i].value= i;\nstrs << setw(2) << setfill('0') << (i+1) << \"._Array-Element\";\nstrs >> array[i].text;\n}\n// open file\nofs.open(\"ouput.dat\", ios::out | ios::binary);\nif(!ofs) {\ncerr << \"Couldn't open file!\" << endl;\nreturn 1;\n}\n// write array to file, tricky\nofs.write((char*) array, size * sizeof(S));\nofs.close();\n// open input\nifstream ifs;\nifs.open(\"ouput.dat\", ios::in | ios::binary);\nif(! ifs) {\ncerr << \"Couldn't open file!\" << endl;\nreturn 1;\n}\n// read element by element\nfor(S& s: array) {\nifs.read((char*)&s, sizeof(S));\ncout<< s.text<< \" = \" << s.value<< endl;\n}\nifs.close();\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "Works very similar to other programming languages like Java where you have super/parent classes and sub/child classes and the child inherits the attributes and functions of the parent.  \nIn C++ the above relationship would be implemented like this:  \n```cpp\nclass Person {\nstring m_name;\nint m_age;\npublic:\nPerson(const string& name, int age) : m_name(name), m_age(age) {}\nvirtual string getName() const { return m_name; } // virtual could be overwritten\nvoid setName(const string& name) { m_name = name; }\nint getAge() { return m_age; }\nvoid setAge(int age) { m_age = age; }\nvoid print() const {\ncout << \"Name: \" << m_name << endl;\ncout << \"Alter: \" << m_age << endl;\n}\n};\n\nclass Student : public Person {\nint m_number;\npublic:\nStudent(const string& name, int age, int nr) : Person(name, age), m_number(nr) {} // call super constructor\nint getNumber() { return m_number; }\nvoid setNumber(int nr) { m_number = nr; }\nvoid printNumber() const {\ncout << \"Matrikelnummer: \" << m_number << endl;\n}\n};\n```  \n```cpp\nvoid main() {\nPerson pers(\"Peter\", 20);\npers.setAge(21);\npers.print();\n\nStudent student(\"Anna\", 21, 50101);\nstudent.setName(\"Anne\");\nstudent.setNumber(56123);\nstudent.print();\nstudent.printNumber();\n\nPerson pers2 = student; // projects/copies student onto person\npers2.print();\n// pers2.printNumber(); // not defined\n}\n```  \nAs one would expect you need to call the parent constructor in the initializer list of the child (if this is not done the default constructor will be called). Then you can further delegate to other constructors in the child and make sure all attributes are initialized.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Inheriting Constructors", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "There is a special way to use the `using` keyword to inherit and be able to refer to the constructors of a parent. If in the example below `using` is not used then the default constructor would be called and m_val would not be initialized.  \n```c\nclass A\n{\nint m_val;\npublic:\nA(int x) : m_val(x) {}\n};\n\nclass B: public A\n{\nusing A::A;\n};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Destructors", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "The destructor of a child calls the destructor of its parent after completion. This is so that dynamically allocated attributes can be removed first. Destructors should also in most cases only be implemented if there are dynamically allocated attributes that need to be cleaned up.  \nImportant is that the destructor will also be called at the end of the block in which a static object was used.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Overload Resolution with Inheritance", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "A common scenario is that a child overloads a parent's function. When doing so we can however run into problems for example if he have the following functions:  \n```cpp\nvoid Person::foo(char c);\nvoid Student::foo(int i);\n```  \nand then want to do the following:  \n```cpp\nStudent s;\ns.foo(10); // Student::foo(int i) is called\ns.foo('A'); // Student::foo(int i) is called\n```  \nWe cant see that in both cases the Student implementation gets used. This is because the compiler looks first in the child if there is a function that matches the call, and there is because the char can get implicitly casted to an int (only after it has checked all functions in the child will it scan the parent). But what if we want the char implementation of the parent to be used. There are two ways:  \n- The child offers the parents function by writing `using Person::foo;`.\n- The parent function is explicitly called `s.Person::foo('A');`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Casting and RTTI", "Header 3": "Converting Pointers", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "The type of a reference or pointer variable does not have to be the same type as the object to which the variable points.  \n```cpp\n// Till now\nStudent stud(\"Anna\", 21, 50101);\nPerson pers = stud;\n// But also\nStudent* stud2 = new Student(\"Bob\", 20, 50111);\nPerson* pers2 = new Student(\"Anna\", 21, 50101);\npers2->print();\nPerson* pers3 = stud2; // implicit up-cast\npers3->print();\nStudent* stud3 = static_cast<Student*>(pers2);\nstud3->printNumber();\n// Student* stud4 = dynamic_cast<Student*>(pers2); does not work\n```  \nImportant here to see is that when up-casting and the down-casting back to the original type the data is not lost. Just the type of the reference/pointer variable changes.  \n#### Converting Smart Pointers  \nYou can also convert smart pointers which also follow the same rules as normal pointers.  \n```cpp\nshared_ptr<Person> spPers = make_shared<Person>(\"Bob\", 21);\nshared_ptr<Person> spStud = make_shared<Student>(\"Anna\", 20, 50010);\nauto sp1 = static_pointer_cast<Student>(spStud);\nauto sp2 = dynamic_pointer_cast<Student>(spStud);\nauto sp3 = dynamic_pointer_cast<Student>(spPers); // sp3==nullptr\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Casting and RTTI", "Header 3": "RTTI - Runtime Type Information", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "The problem however is that when you illegally downcast like below then you get a runtime exception (or even nothing) which you want to avoid.  \n```cpp\nPerson* pers = new Person(\"Anna\", 21);\nStudent* stud = (Student*)pers;\nStudent* stud2 = static_cast<Student*>(pers);\nstud->printNumber();\nstud2->printNumber();\n```  \nTo prevent this there is the RTTI system that stores the exact type of each instance. The system can be turned on and off as you wish. The dynamic_cast is based on this and works as expected on a valid down-cast. But if the down-cast is illegal it returns a nullptr instead of causing a runtime error.  \n```cpp\nPerson* pers = new Person(\"Anna\", 21);\nStudent* stud = dynamic_cast<Student*>(pers);\ncout << boolalpha << (stud == nullptr) << endl; // true\n```  \nYou can also read the type information that the RTTI system stores by using the `typeid` operator which is very similar to `instanceof` in Java.  \n```cpp\nPerson* pers = new Person(\"Bob\", 21);\nPerson* stud = new Student(\"Anna\", 20, 50010);\nconst type_info& typePers = typeid(*pers);\nconst type_info& typeStud = typeid(*stud);\ncout << boolalpha << (typePers == typeStud) << endl; // false\ncout << \"Is Person a parent of Student: \"<< boolalpha << typePers.before(typeStud) << endl; // true\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Overriding and Shadowing", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "Children can have functions with the same signature as a function in the parent, this in turn overrides the function in the parent with the version in the child (overriding).\nYou can also define attributes in a child that have the same name as attributes in the parent. This is called shadowing and should be avoided at all costs. If this does happen then you must explicitly define when you want to use the shadowed attribute in the parent `Person::x` or `static_cast<Person*>(this)->x`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Overriding and Shadowing", "Header 3": "Binding", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "Binding is the act of assigning a function body to a function call. This can be done in two ways:  \n- Static (early) binding happens at compile time and allows the compiler to replace the function calls with the function bodies. This is the default behavior.\n- Dynamic (late) binding happens at runtime. For this to work the `virtual` keyword must be used.  \nSo to override a parent's function in a child you must explicitly define a function as virtual meaning it can be overridden. Then when overriding you must explicitly say that you want to override a function. If a function is virtual then all of the overridden functions are also implicitly virtual.  \n```cpp\nclass Person {\nvirtual void print() const {\ncout << \"Some printing\" << endl;\n}\n};\nclass Student : public Person {\nvoid print() const override {\nPerson::print(); // Call parent print()\ncout << \"Some more printing\" << endl;\n}\n};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Overriding and Shadowing", "Header 3": "Blocking Overriding or Inheritance", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "You can block methods from being overridden, this is just simply done by adding the `final` modifier. This can also be used for class definitions to stop classes from being inherited. The final modifier can however only be added to virtual functions. This is done because non-virtual functions are automatically final.  \n```cpp\nclass A { virtual void foo(int i); };\nclass B final : A { void foo(int i) final override {}; };\nclass C : B { void foo(int i) override {}; }; // does not work\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Access specifiers", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "<Image\nsrc=\"/cs/cppAccessModifiersInheritance.png\"\ncaption=\"Effect of access specifiers on inheritance in C++.\"\nwidth={700}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Automatically Created Functions", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "By default when creating a class the system provides a default constructor, destructor and assignment operator which are all non-virtual. For this reason, destructors and assignment operators should be defined as virtual so that they can be overridden in the children.  \n```cpp\nvirtual ~Vehicle() = default;\nvirtual Vehicle& operator=(const Vehicle& v) = default;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Interfaces", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "In C++ there is no such thing as the `interface` type. There also isn't the `abstract` modifier. Instead, an abstract class without any implementations is the same as an interface. An abstract can't be instantiated and for a class to be abstract there must be at least one abstract function. An abstract function is a virtual function with no implementation.  \n```cpp\nstruct IVehicle{\nvirtual ~IVehicle() = default;\nvirtual void drive() = 0; // abstract (pure virtual)\n};\nclass Bicycle: public IVehicle {\npublic:\nvoid drive() override { cout << \"broom\" << endl; }// Implements the abstract function\n};\nint main() {\nIVehicle* v = new Bicycle();\ndelete v;// calls Bicycle destructor because virtual\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Multiple Inheritance", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "In C++ a class can inherit from more than one class. The constructors of inherited classes are called in the same order in which they are inherited. The destructors are called in reverse order of constructors. So in the program below B’s constructor is called before A’s constructor.  \n```cpp\nclass A {\npublic:\nA()  { cout << \"A's constructor called\" << endl; }\n};\n\nclass B {\npublic:\nB()  { cout << \"B's constructor called\" << endl; }\n};\n\nclass C: public B, public A {\npublic:\nC()  { cout << \"C's constructor called\" << endl; }\n};\n```  \nAs you might imagine this can cause a few problems. Suppose two parent classes have the same function which is not overridden in the child class. If you then try to call the function using from the child class the compiler will show an error because it doesn't know which function to call.  \n```cpp\nclass A {\npublic:\nvoid someFunction() { cout << \"A\" << endl; }\n};\nclass B {\nvoid someFunction() { cout << \"B\" << endl; }\n};\nclass C : public A, public B {};\n\nint main() {\nC obj;\nobj.someFunction(); // error\n// to solve it:\nobj.A::someFunction();\nobj.B::someFunction();\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Multiple Inheritance", "Header 3": "Diamond Problem", "path": "../pages/digitalGarden/cs/cpp/inheritance.mdx"}, "page_content": "There is however also the so-called diamond problem which is when we have a commonly seen structure as below:  \n<Image\nsrc=\"/cs/cppDiamondProblem.png\"\ncaption=\"Visual representation of the diamond problem in C++.\"\nwidth={500}\n/>  \nWe then run into problems if we want to do something like this:  \n```cpp\nRechteck r(0, 0, 20, 50);\nRechteckMitText br(10, 5, 60, 60, \"Text\");\nr.zeichnen(); // Rechteck::zeichnen()\nbr.zeichnen()// RechteckMitText::zeichnen()\nPosition rPos = r.getPos(); // works fine\nPosition brPos = br.getPos(); // Error\nGraphObj*pObj = &br; // Error\n```  \nBecause the compiler does not know which getPos to call and the partial class `GraphObj` is inherited twice. To resolve this we can use virtual inheritance which ensures that only one copy of a parent's member variable is inherited.  \n```cpp\nclass Rechteck : virtual public GraphObj {...};\nclass ObjMitText: virtual public GraphObj {...};\nclass RechteckMitText: public ObjMitText, public Rechteck {\npublic:\nRechteckMitText(int x, int y, int w, int h, string text)\n: ObjMitText(-2, -2, text), Rechteck(-1, -1, w, h), GraphObj(x, y) {}\n};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Unions", "path": "../pages/digitalGarden/cs/cpp/unions.mdx"}, "page_content": "In a union all members share the same memory location. This means that at any given time, a union can contain no more than one object from its list of members. It also means that no matter how many members a union has, it always uses only enough memory to store the largest member.  \n```cpp\n#include <iostream>\n#include <string>\nusing namespace std;\n\nunion Record // will only take up a double amount in memory\n{\nchar   ch;\nint    i;\nlong   l;\nfloat  f;\ndouble d; // biggest attribute\nint* int_ptr;\n};\n\nint main() {\nRecord t;\nt.i = 5; // t holds an int\nt.f = 7.25; // t now holds a float, int is gone\n\ncout << t.f << endl;\ncout << t.i << endl;\n}\n```  \n```bash title=\"Output\"\n7.25\n1088946176\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "New and delete", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "Just as in C, C++ supports dynamic allocation and deallocation of objects using the new and delete operators. These operators allocate memory for objects from a pool called the heap which make them equivalent to malloc and free system calls in C.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "RAII - Resource Acquisition Is Initialization", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "RAII is a programming idiom with its main goal being to ensure that resource acquisition occurs at the same time that the object is initialized, so that all resources for the object are created and made ready in one line of code. In practical terms, the main principle of RAII is to give ownership of any heap-allocated resource—for example, dynamically-allocated memory or system object handles—to a stack-allocated object whose destructor contains the code to delete or free the resource and also any associated cleanup code.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Raw pointers", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "Raw pointers are pointers just like in C. In modern C++, raw pointers are only used in small code blocks of limited scope, loops, or helper functions where performance is critical and there is no chance of confusion about ownership otherwise you should pass the pointer to a smart pointer immediately. Just as in C you also have in C++ void pointers and go use the const keyword with pointers.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "References", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "A reference variable is an alias, that is, another name for an already existing variable. Once a reference is initialized with a variable, either the variable name or the reference name may be used to refer to the variable.  \nReferences are often confused with pointers but three major differences between references and pointers are −  \n- You cannot have NULL references. You must always be able to assume that a reference is connected to a legitimate piece of storage.\n- Once a reference is initialized to an object, it cannot be changed to refer to another object. Pointers can be pointed to another object at any time.\n- A reference must be initialized when it is created. Pointers can be initialized at any time.  \n```cpp\n#include <iostream>\nusing namespace std;\nint main () {\nint i;\nint& r = i;\n\ni = 5;\ncout << \"Value of i : \" << i << endl; // 5\ncout << \"Value of i reference : \" << r  << endl; // 5\n//change reference\nr = 3;\ncout << \"Value of i : \" << i << endl; // 3\ncout << \"Value of i reference : \" << r  << endl; //3\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Passing parameters", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "There are multiple ways of passing parameters to a function and depending on how you do it you will have a different result.  \n- By value, for example `void foo(int x)`. All values including pointers are copied.\n- By pointer, for example `void foo(Point* p)`. Good practice is to only use these for output parameters as it is clearly visible.\n- By reference, for example `void foo(Person& p)` or `void foo(const Person& p)`. The referenced value does not get copied so is optimal.  \nGood practice is to just use by value if the data takes as much memory as two pointers.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Returning values", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "The same applies for returning values.  \n- By value, for example `double sqrt(double x)` or `Point move(const Point& p, int dx, int dy)` where a Point is immutable. Here the data is copied back using a temporary object which can be less efficient so if objects are large you should use out parameters or work on the references. Depending in the function and the return type the compiler might do copy elision which means it tries to optimize your code and not make copy the temporary object but actually move it.  \n- By pointer or reference, for example `Point* factory(int x, int y)` or `Point& factory(int x, int y)`. Important here is that the pointer or reference has a longer scope then the passed arguments.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Smart pointers", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "[Microsoft wrote I pretty good explanation](https://docs.microsoft.com/en-us/cpp/cpp/smart-pointers-modern-cpp?view=msvc-170) on how these work which I also used for this summary.  \nYou declare a smart pointer on the stack, and initialize it by using a raw pointer that points to a heap-allocated object. Now the smart pointer is responsible for deleting the memory associated with it and because the smart pointer is declared on the stack this is done automatically.To access the encapsulated raw pointer you can still just use -> and * because the smart pointer class overloaded these operators.  \nSmart pointers are designed to be just as performant as raw pointers which is why they also have the same size as the encapsulated pointer, either four bytes or eight bytes. Smart pointers also have functions. For example the reset function releases the pointer which can be useful when you want to free the memory owned by the smart pointer before the smart pointer goes out of scope.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Smart pointers", "Header 3": "Unique smart pointer", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "A unique_ptr does not share its pointer and can therefore not be copied to another unique_ptr or passed by value to a function. A unique_ptr can only be moved. This means that the ownership of the memory resource is transferred to another unique_ptr and the original unique_ptr no longer owns it. All the make_unique function does is create and return a unique_ptr.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nclass Point {\ndouble m_x, m_y;\npublic:\nPoint(double x = 0, double y = 0)\n: m_x(x), m_y(y)\n{}\n};\nunique_ptr<Point> PointFactory(const double x, const double y)\n{\nreturn make_unique<Point>(x, y);\n}\n\nint main()\n{\n// Create a new unique_ptr with a new object.\nPoint* p = new Point(1, 2);\nunique_ptr<Point> up1(p);\n\n// Move raw pointer from one unique_ptr to another.\nunique_ptr<Point> up2 = move(up1);\n\n// Obtain unique_ptr from function that returns by value.\nauto up3 = PointFactory(1,2);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Smart pointers", "Header 3": "Shared smart pointer", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "A shared pointer is a reference-counted smart pointer meaning one raw pointer can have multiple owners, for example, when you return a copy of a pointer from a container but want to keep the original. After you initialize a shared_ptr you can copy it, pass it by value in function arguments, and assign it to other shared_ptr instances. All the instances point to the same object, and share access to one \"control block\" that increments and decrements the reference count whenever a new shared_ptr is added, goes out of scope, or is reset. When the reference count reaches zero, the control block deletes the memory resource and itself. You can check the amount of references by using the use_count function  \n<Image\nsrc=\"/cs/cppSharedPointer.png\"\ncaption=\"Visual representation of how shared pointers work internally in C++.\"\nwidth={700}\n/>  \n```cpp\n#include <iostream>\nusing namespace std;\nstruct Some {\nint x;\n};\n\nvoid nothing(shared_ptr<Some> p) {\n//Change the underlying object\np->x = 20;\n}\n\nint main() {\n//Create/initialize shared_ptr<Some>\nauto one = std::shared_ptr<Some>(new Some());\n//Another shared_ptr<Some> pointing nowhere\nstd::shared_ptr<Some> two;\n//Change the underlying object\none->x = 10;\n//Read through shared_ptr\ncout << \"x: \" << one->x << endl; //x: 10\n//Pass to a function by value. This increases the ref count.\nnothing(one);\n//Underlying object is changed\ncout << \"x: \" << one->x << \" RefCount: \" << one.use_count() << endl; //x: 20\n//Assign to another shared_ptr\ntwo = one;\n//'one' and 'two' are pointing to the same object\ncout << std::boolalpha << (one.get() == two.get()) << endl;\ncout << \"RefCount: \" << one.use_count() << endl; //x: 20\n// On return 'one' and 'two' are destroyed, Ref count reaches zero, Some is destroyed\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Smart pointers", "Header 3": "Weak smart pointer", "path": "../pages/digitalGarden/cs/cpp/pointers.mdx"}, "page_content": "The weak smart pointer is used in conjunction with shared_ptr for special cases. A weak_ptr provides access to an object that is owned by one or more shared_ptr instances, but does not participate in reference counting. Use when you want to observe an object, but do not require it to remain alive. Required in some cases to break circular references between shared_ptr instances.  \n```cpp\n#include <memory>\n#include <vector>\n\nstruct TreeNode {\nstd::weak_ptr<TreeNode> parent;\nstd::vector< std::shared_ptr<TreeNode> > children;\n};\n\nint main() {\n// Create a TreeNode to serve as the root/parent.\nstd::shared_ptr<TreeNode> root(new TreeNode);\n\n// Give the parent 100 child nodes.\nfor (size_t i = 0; i < 100; ++i) {\nstd::shared_ptr<TreeNode> child(new TreeNode);\nroot->children.push_back(child);\nchild->parent = root;\n}\n\n// Reset the root shared pointer, destroying the root object, and\n// subsequently its child nodes.\nroot.reset();\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Limits", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "The numeric_limits class template provides a standardized way to query various properties of arithmetic types.  \n```cpp\ncout << boolalpha;\ncout << \"Minimum value for int: \" << numeric_limits<int>::min() << '\\n';\ncout << \"Maximum value for int: \" << numeric_limits<int>::max() << '\\n';\ncout << \"int is signed: \" << numeric_limits<int>::is_signed << '\\n';\ncout << \"Non-sign bits in int: \" << numeric_limits<int>::digits << '\\n';\ncout << \"int has infinity: \" << numeric_limits<int>::has_infinity << '\\n';\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Chrono", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "The chrono library is a flexible collection of types that track time with varying degrees of precision (system_clock, steady_clock, high_resolution_clock)  \n```cpp\nusing Clock = chrono::system_clock;\nClock::time_pointstart = Clock::now();\nClock::durationd = Clock::now() - start;\nint64_t ns = chrono::nanoseconds(d).count();\nusing ms_t = chrono::duration<double, milli>; // new durationtype\ndouble ms = chrono::duration_cast<ms_t>(d).count();\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Pair and Tuple", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "Pairs hold pairs of values as you would expect and tuples are like the mathematical tuples that are generalized pairs for any amount of values.  \n```cpp\npair<int, double> id;\nint i = id.first;\ndouble d = id.second;\nid = make_pair(3, 5.5);\n\ntuple<int, double, string> tup(1, 2.2, \"drei\");\nauto val= get<0>(tup); // read\nget<1>(tup) = 1.5; // update\nsize_ts = tuple_size<decltype(tup)>::value; // number of elements\ntup = make_tuple(2, 3.3, \"vier\");\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Optional, Any and Variant", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "The class template optional manages an optional contained value, i.e. a value that may or may not be present which is a common use case when returning a value of a function that may fail.  \n```cpp\noptional<string> create(bool b) {\nif (b)\nreturn \"Godzilla\";\nreturn {}; // gleich wie nullopt;\n}\nauto opt = std::make_optional<std::vector<char>>({'a','b','c'});\nif (opt) // or opt.has_value()\ncout << \"value set to \" << opt.value().value_or(\"nothing\") << '\\n'; // can also use function with or_else()\n```  \nThe class any describes a container for any single value.  \n```cpp\nany a = 1;\nauto a0 = make_any<std::string>(\"Hello, std::any!\\n\");\ncout << a.type().name() << \": \" << any_cast<int>(a) << '\\n';\n```  \nthe class template variant represents a type-safe union.  \n```cpp\nvariant<int, float> v, w;\nv = 12;\nint i = get<int>(v);\nw = get<int>(v);\nw = std::get<0>(v);\nw = v;\n// get<double>(v); error\n// get<3>(v); error\ntry{\nget<float>(w);\n}\ncatch (bad_variant_access&) {}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Container", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "A Container is an object used to store other objects and taking care of the management of the memory used by the objects it contains.  \nAttributes a `ContainerX<T>` needs to have:  \n- `value_type` container-element, T.\n- `reference` reference of container-element.\n- `const_reference` same but read-only.\n- `iterator` the iterator for the container.\n- `const_iterator` same but read-only.\n- `size_type`  \nFunctions a Container should have:  \n- Standard-, copy and moveconstruktor, destructor\n- begin() and end() iterators, cbegin() and  cend() for read only\n- max_size(), size(), empty()  \nBitsets are a fixed-size sequence of N bits.  \n```cpp\nconstexpr bitset<4> b1;\nconstexpr bitset<4> b2{0xA}; // == 0B1010\nbitset<4> b3{\"0011\"}; // can't be constexpr yet\nbitset<8> b4{\"ABBA\", /*length*/4, /*0:*/'A', /*1:*/'B'}; // == 0B0000'0110\n```  \nHalf dynamic structures like `vector`  \n```cpp\nvector<int> vec = {1,2,3};\ncout << vec.size() << endl;\nvec.push_back(5);\ncout << vec[3] << endl;\nvec[1] = 10;\ncout << vec.front() << endl;\ncout << vec.capacity() << endl;\nvec.pop_back();\ncout << vec.size() << endl;\n```  \nList structures like normal linked lists `forward_list`, doubly linked lists `list` and double ended queues `deque`, also known as head-tail linked list.  \n```cpp\nstd::deque<int> d = {7, 5, 16, 8};\nd.push_front(13);\nd.pop_back(25);\nfor(int n : d) {\nstd::cout << n << ' ';\n}\n```  \nA `set` in C++ is a container that contains a sorted set of unique objects of type Key. Sorting is done using the key comparison function. Search, removal, and insertion operations have logarithmic complexity as they are usually implemented as red-black trees. `multiset` is the same except multiple keys with equivalent values are allowed.  \n```cpp\nset<string> mySet;\nmySet.insert(\"first\");\nmySet.insert(\"second\");\nmySet.insert(\"third\");\nmySet.insert(\"first\");\ncout << \"Set Size = \" << mySet.size() << endl; // 3\nmySet.erase(\"third\");\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Container", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "```cpp\nset<string> mySet;\nmySet.insert(\"first\");\nmySet.insert(\"second\");\nmySet.insert(\"third\");\nmySet.insert(\"first\");\ncout << \"Set Size = \" << mySet.size() << endl; // 3\nmySet.erase(\"third\");\n```  \nA map is a sorted associative container that contains key-value pairs with unique keys. Multimap is the same except multiple keys with equivalent values are allowed.  \n```cpp\nmap<string, int> m { {\"CPU\", 10}, {\"GPU\", 15}, {\"RAM\", 20}, };\nfor (const auto& [key, value] : m) {\ncout << '[' << key << \"] = \" << value << \"; \";\n}\nm.erase(\"GPU\");\nm.insert(\"HDD\", 50);\n```  \nThere are also unordered versions of sets and maps where the keys are not sorted: `unordered_set`, `unordered_multiset`, `unordered_map`, `unordered_multimap`", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Container", "Header 3": "Iterators", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "Iterators in C++ do very similiar things to Iterators in Java. In C++ all they are are pointers. `begin()` / `cbegin()` return a pointer pointing to the first element and `end()` / `cend()` return a pointer that points to a fictional element after the last element.  \n```cpp\ntemplate<class Iter> void print(Iter it, Iter end) {\nwhile(it!= end) {\ncout << *it++<< ' ';\n}\ncout << endl;\n}\n```  \n#### Move Iterator  \nA move iterator works exactly the same as a normal iterator except that dereferencing converts the value returned by the underlying iterator into an rvalue so it can be moved.  \n```cpp\nvector<string> source = { \"Move\", \"iterators\", \"in\", \"C++\" };\nvector<string> destination(make_move_iterator(begin(source)), make_move_iterator(end(source)));\n```  \n#### Insert Iterators  \nA `front_insert_iterator` prepends elements to a container for which it was constructed. The container's push_front() member function is called whenever the iterator (whether dereferenced or not) is assigned to. Similarly, there is the `back_insert_iterator` that appends to a container for which it was constructed.  \n```cpp\nvector<int> v{1,2,3,4,5};\ndeque<int> d;\ncopy(v.begin(), v.end(), front_insert_iterator<deque<int>>(d)); // or front_inserter(d)\nfor(int n : d)\ncout << n << ' ';\ncout << '\\n';\n```  \n#### Reverse Iterator  \n`reverse_iterator` is an iterator adaptor that reverses the direction of a given iterator.  \n```cpp\nvector<int> v{1, 2, 3, 4, 5};\nfor (vector<int>::reverse_iterator it = v.rbegin(); it != v.rend(); ++it)\n{\ncout << *it; // prints 54321\n}\n```  \n#### Stream Iterator  \n```cpp\ncopy(to_vector.begin(), to_vector.end(), std::ostream_iterator<int>(cout, \" \"));\ncopy_if(to_vector.begin(), to_vector.end(), ostream_iterator<int>(cout, \" \"), [](int x) { return x % 2 != 0; });\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Algorithm", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "The algorithms in the `algorithm` header can be used on any container, no matter the implementation since they mainly use iterators. However, if there is a function with the same name on a container as in the algorithm header then that special version for the container should be used for better efficiency.  \n- `find<T>(begin(), end(), const T& value), find_if(begin(), end(), [](int i){ return i%2 == 0; }), find_if_not` finds the first element satisfying specific criteria\n- `nth_element(RandomIt first, RandomIt nth, RandomIt last)` is a partial sorting algorithm that rearranges elements so that the element pointed at by nth is changed to whatever element would occur in that position if [first, last) were sorted.\nAll of the elements before this new nth element are less than or equal to the elements after the new nth element. Can get median with: `nth_element(v.begin(), v.begin() + v.size()/2, v.end());`\n- `ForwardIt1 search(ForwardIt1 first, ForwardIt1 last, ForwardIt2 s_first, ForwardIt2 s_last, BinaryPredicate p )` searches for the first occurrence of the sequence of elements [s_first, s_last) in the range [first, last). Elements are either compared using == or the optional binary predicate p. Returns an iterator to the beginning of the first occurrence of the sequence.\n- `ForwardIt search_n( ForwardIt first, ForwardIt last, Size count, const T& value );` searches the range [first, last) for the first sequence of count identical elements, each equal to the given value. for example 5 consecutive zeros etc.\n- `count, count_if` returns the number of elements in the range [first, last) satisfying specific criteria.\n- `T& min(const T& a, const T& b), max` returns the smaller/bigger of the given values.\n- `ForwardIt min_element( ForwardIt first, ForwardIt last ), max_element` Finds the smallest/biggest element in the range [first, last).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Algorithm", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "- `T& min(const T& a, const T& b), max` returns the smaller/bigger of the given values.\n- `ForwardIt min_element( ForwardIt first, ForwardIt last ), max_element` Finds the smallest/biggest element in the range [first, last).\n- `bool lexicographical_compare( InputIt1 first1, InputIt1 last1, InputIt2 first2, InputIt2 last2 )` checks if the first range [first1, last1) is lexicographically less than the second range [first2, last2).\n- `mismatch, equal` mismatch finds the first position where two ranges differ, determines if two sets of elements are the same\n- `OutputIt copy( InputIt first, InputIt last, OutputIt d_first ), copy_if` copies the elements in the range, defined by [first, last), to another range beginning at d_first.\n- `void swap( T& a, T& b )` exchanges the given values\n- `void iter_swap( ForwardIt1 a, ForwardIt2 b )` swaps the values of the elements the given iterators are pointing to. Can implement a selection sort with it: `void selection_sort(ForwardIt begin, ForwardIt end) {for (ForwardIt i = begin; i != end; ++i)iter_swap(i, std::min_element(i, end));}`\n- `void fill( ForwardIt first, ForwardIt last, const T& value ), fill_n( OutputIt first, Size count, const T& value )` assigns the given value to the elements in the range [first, last) (to first n elements).\n- `void generate( ForwardIt first, ForwardIt last, Generator g ), generate_n` assigns each element in range [first, last) a value generated by the given function object g.\n- `void replace( ForwardIt first, ForwardIt last, const T& old_value, const T& new_value ), replace_if` Replaces all elements that are equal to old_value.\n- `replace_copy( InputIt first, InputIt last, OutputIt d_first,const T& old_value, const T& new_value ), replace_copy_if` copies the elements from the range [first, last) to another range beginning at d_first replacing all elements satisfying specific criteria with new_value.\n- `remove, remove_if, remove_copy, remove_copy_if` also works just like replace.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Algorithm", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "- `remove, remove_if, remove_copy, remove_copy_if` also works just like replace.\n- `transform( InputIt first1,InputIt last1, OutputIt d_first, UnaryOperation unary_op )` applies the given function to a range and stores the result in another range, keeping the original elements order and beginning at d_first.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard Template Library - STL", "Header 2": "Exceptions", "path": "../pages/digitalGarden/cs/cpp/stl.mdx"}, "page_content": "Just like in Java some exceptions can be thrown and caught. It is best practice to only catch constant references of the exceptions. Unlike in Java, you don't explicitly say which exceptions a function will throw instead you define when a function does not throw any exceptions so that it can be better optimized.  \n```cpp\ntry{\nthrow runtime_error(\"example\");\n} catch(const runtime_error& e) {\ncout<< \"std::runtime_error: \" << e.what() << endl;\n} catch(...) {\ncout<< \"unknownexception\" << endl;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functional Programming with C++", "Header 2": "Functors", "path": "../pages/digitalGarden/cs/cpp/functionalCpp.mdx"}, "page_content": "Functors are objects that can be treated like functions or function pointers. For this to work you need to override the `()` operator.  \n```cpp\nenum Mode { Rad, Deg, Gon };\nclass Sinus {\nMode m_mode;\npublic:\nSinus(Mode mode = Rad) : m_mode(mode) {}\ndouble operator()(double arg) {\nswitch(m_mode) {\ncase Rad: return sin(arg);\ncase Deg: return sin(arg / 180.0 * M_PI);\ncase Gon: return sin(arg / 200.0 * M_PI);\n}\n}\n};\n```  \nYou can then use the functor like this:  \n```cpp\nint main() {\nSinus sinrad;\nSinus sindeg(Deg); // constructor\ncout << sinrad(M_PI/4); // functor\ncout << sindeg(45.0); // functor\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functional Programming with C++", "Header 2": "Lambdas", "path": "../pages/digitalGarden/cs/cpp/functionalCpp.mdx"}, "page_content": "Lambdas are in C++ anonymous functors and can be defined like this:  \n```cpp\n[bias] (int x, int y) -> int { return bias + x + y; }\n```  \nNothing too special apart from the square brackets at the beginning containing bias. In the square brackets, you can define what the lambda has access to, in this case, the variable bias. This way you can access variables in the same blocks as where the lambda is defined or higher. The lambda always has access to static and global variables.  \n- `[bias]` by-value access to the variable bias.\n- `[&bias]` by-reference access to the variable bias.\n- `[=]` by-value access to all variables in the environment.\n- `[&]` by-reference access to all variables in the environment.\n- `[this]` by-pointer access to all the members of the passed instance.\n- `[=, &bias]` only bias is by-reference everything else by-value.\n- `[factor, &bias]` factor by-value and bias by-reference access.  \nBehind the scenes lambdas are as mentioned anonymous functors meaning they do something like this:  \n```cpp\nvoid fLambda() {\nint notUsed= 3, byval= 4, byref= 5;\nauto op = [byval, &byref](int i) { ++byref; return i + byval + byref; };\ncout<< op(10) << endl;\n}\n// becomes something like this\nvoid fLambda() {\nint notUsed= 3, byval= 4, byref= 5;\nstruct Op{\nconst int m_val;\nint& m_ref;\nOp(int val, int& ref) : m_val{val}, m_ref{ref} {}\nint operator()(int i) const {++m_ref; return i + m_val + m_ref; }\n} op(byval, byref);\ncout << op(10) << endl;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functional Programming with C++", "Header 2": "Lambdas", "Header 3": "Closure", "path": "../pages/digitalGarden/cs/cpp/functionalCpp.mdx"}, "page_content": "Because lambdas are anonymous functors they like in many other languages have the issue of closure. However in C++ this is solved with the `mutable keyword`  \n```cpp\nint main() {\nint f = 2;\n// auto l0 = [f](int x) { return x * f++; }; // not possible\nauto l1 = [&f](int x) { return x * f++; }; // f by-reference\nauto l2 = [f](int x) mutable { return x * f++; }; // f is mutable gets stored here, not lazy\ncout << \"value= \" << l1(3); cout << \", f = \" << f << endl; // value = 6, f = 3\ncout << \"value= \" << l1(3); cout << \", f = \" << f << endl; // value = 9, f = 4\ncout << \"value= \" << l2(3); cout << \", f = \" << f << endl; // value = 6, f = 4 changed internal copy\ncout << \"value= \" << l2(3); cout << \", f = \" << f << endl; // value = 9, f = 4 changed internal copy\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functional Programming with C++", "Header 2": "Functional objects", "path": "../pages/digitalGarden/cs/cpp/functionalCpp.mdx"}, "page_content": "Functors and lambdas are both instances of functional from the `<functional>` header. You can make use of this by doing things like this:  \n```cpp\n#include<iostream>\n#include<functional>\nusing namespace std;\n\nstruct Funktor {\nfloat m_div;\nFunktor(float f) : m_div(f) {}\nfloat operator()(float a, int x) const {\nreturn a + x / m_div;\n}\n};\n\nstruct Binding {\nfloat m_div;\nBinding(float f) : m_div(f) {}\nfloat meth(float a, int x) const {\nreturn a + x / m_div;\n}\n};\n\nfloat foo(float a, int x) { return a + x / 2.0f; }\n\nint main() {\nfunction<float(float a, int x)> func;\nfunc = Funktor(2.0f);\nBinding binding(2.0f);\nfunc = bind(&Binding::meth, &binding, 1.0, 2); // last arguments are parameters\nfunc = &foo;\nfunc = [](float a, int x) {return a + x / 2.0f; };\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "path": "../pages/digitalGarden/cs/cpp/variablesDataTypes.mdx"}, "page_content": "In C++ you pretty much have the same data types as in C and they work the same as C++ in an extension of C. If you want specific sized data types you can just as in C include from the standard library `cstdint.h`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Custom types", "path": "../pages/digitalGarden/cs/cpp/variablesDataTypes.mdx"}, "page_content": "In C++ you can create your own types using typedef just as in C but you can also use the `using` keyword. Later you can also use structures and classes.  \n```cpp\ntypedef int int32_t;// in <cstdint>\ntypedef unsigned long long uint64_t;// in <cstdint>\nusing INT32 = int;\nusing UINT64 = unsigned long long;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Placeholder type specifiers", "path": "../pages/digitalGarden/cs/cpp/variablesDataTypes.mdx"}, "page_content": "The `auto` keyword specifies that the type of the variable that is being declared will be automatically deducted from its initializer. In the case of functions, if their return type is auto then that will be evaluated by return type expression at runtime.  \nThe decltype function/operator lets you extract the type of the passed expression.  \n```cpp\n#include <iostream>\n#include <string>\nusing namespace std;\nauto foo() { return \"foo\"; }\nint main()\n{\nauto x = 7;\nauto f = foo();\ndecltype(69) y = x;\n// decltype(69) z = f; doesn't work\ncout << x << endl;\ncout << y << endl;\ncout << f << endl;\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Constant values", "path": "../pages/digitalGarden/cs/cpp/variablesDataTypes.mdx"}, "page_content": "The `const` keyword can be used just like in C and makes a variable read only same goes for the `define` preprocessor directive. New in C++ however is the `constexpr` keyword. The constant expression allows us to create expressions and functions that are evaluated at compile time to speed things up.  \n```cpp\n#include <iostream>\nusing namespace std;\nconstexpr int product(int x, int y){return (x * y);}\nconstexpr size_t length = 100;\n\nint main()\n{\nconstexpr int x = product(10, 20);\ncout << x << endl << length << endl;\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Type conversions", "path": "../pages/digitalGarden/cs/cpp/variablesDataTypes.mdx"}, "page_content": "In C++ just like in many other language you can convert data of one type to that of another. It has implicit and explicit conversion. Just as with many other languages when casting data can be lost.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nint main() {\nint num_int1 = 9;\ndouble num_double1;\n// implicit conversion int to double\nnum_double1 = num_int1;\ncout << \"num_int1 = \" << num_int1 << endl;\ncout << \"num_double1 = \" << num_double1 << endl;\n\nint num_int2;\ndouble num_double2 = 3.14;\n// implicit conversion double to int\nnum_int2 = num_double2;\ncout << \"num_int2 = \" << num_int2 << endl;\ncout << \"num_double2 = \" << num_double2 << endl;\n\nreturn 0;\n}\n```  \n```bash title=\"Output\"\nnum_int1 = 9\nnum_double1 = 9\nnum_int2 = 3\nnum_double2 = 3.14\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Type conversions", "Header 3": "C-style type casting", "path": "../pages/digitalGarden/cs/cpp/variablesDataTypes.mdx"}, "page_content": "As the name suggests, this type of casting is the same as in the C programming language and is also commonly referred to as the cast notation.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nint main() {\nint num_int = 9;\ndouble num_double;\n// converting from int to double\nnum_double = (double) num_int;\ncout << \"num_int = \" << num_int << endl;\ncout << \"num_double = \" << num_double << endl;\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Type conversions", "Header 3": "Function-style casting", "path": "../pages/digitalGarden/cs/cpp/variablesDataTypes.mdx"}, "page_content": "This is the old way of doing it in C++ before type conversion operators were introduced.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nint main() {\nint num_int = 9;\ndouble num_double;\n// converting from int to double\nnum_double = double(num_int);\ncout << \"num_int = \" << num_int << endl;\ncout << \"num_double = \" << num_double << endl;\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Type conversions", "Header 3": "Type conversion operators", "path": "../pages/digitalGarden/cs/cpp/variablesDataTypes.mdx"}, "page_content": "This is the way how it is done in modern C++.  \n#### Static cast  \nIn general, you use a static cast just like any other cast so far when you are certain of the data types involved in the conversion. This takes the pointer and tries to safely cast it. This cast is done at compile time. It will only perform the cast if the types are related. If the types are not related, you will get a compiler error.  \n```cpp\nclass B {};\nclass D : public B {};\nclass X {};\n\nint main()\n{\nD* d = new D;\nB* b = static_cast<B*>(d); // this works\n// X* x = static_cast<X*>(d); ERROR - Won't compile\nreturn 0;\n}\n```  \n#### Dynamic cast  \nA dynamic cast is executed at runtime, not compile time. Because this is a run-time cast, it is useful, especially when combined with polymorphic classes. In fact, in certain cases, the classes must be polymorphic for the cast to be legal.  \n#### Constant cast  \nIt is used to change the constant value of any object or we can say it is used to remove the constant nature of any object.  \n```cpp\n#include <iostream>\nusing namespace std;\n\nint main()\n{\nint x = 50;\nconst int* y = &x;\ncout << \"old value is \" << *y << \"\\n\";\nint* z = const_cast<int*>(y);\n*z = 100;\ncout << \"new value is \" << *y;\n}\n```  \n#### Reinterpret cast  \n???? very confussing", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "path": "../pages/digitalGarden/cs/cpp/structures.mdx"}, "page_content": "Just like in C you have structs which are also commonly called open classes. Structs can only hold public attributes and no functions.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "Header 2": "Struct packing", "path": "../pages/digitalGarden/cs/cpp/structures.mdx"}, "page_content": "When working with structs it is however important to know how the memory is used. A struct declaration allocates a contiguous memory for the collection. It uses a multiple of the largest attribute in memory which is needed for all attributes + any padding needed. It uses the largest attribute as alignment for easier access. which can lead to the order of your attributes having an influence on how much the object takes up in memory.  \n```cpp\nstruct top { // total of 16 bytes\nchar* p; // 8 bytes on 64 bit, 4 on 32 bit system\nint i; // 4 bytes\nshort s; // 2 bytes\nchar c; // 1 byte\n// 1 byte padding\n}\n\nstruct bottom { // total of 24 bytes\nchar c; // 1 byte\n// 7 bytes padding\nchar* p; // 8 bytes\nshort s; // 2 bytes\nint i; // 4 bytes\n// 2 bytes padding\n}\n```  \nYou can check the alignment of a struct with `alignof(top)`. You can also change the default behaviour with either `#pragma pack(1)` to use 1 byte alignment and then after defining the struct restet it back to default with `#pragma pack()`. Or you can use `struct alignas(4) S{};`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "Header 2": "Bit fields", "path": "../pages/digitalGarden/cs/cpp/structures.mdx"}, "page_content": "Bitfields can only be used inside structured data types, i.e. struct, class, and union types and with integer types. The purpose is to allow you to pack multiple members inside a single byte.  \n```cpp\nstruct halfbyte_t {\nunsigned int half1: 4;\nunsigned int half2: 4;\n} halfbyte;\n```  \nThis declares a variable named halfbyte that contains two 4-bit members, which will be packed into a single 8-bit byte, rather than having to use 2 bytes if you just declared them unsigned char. 1-bit fields are especially useful if you have lots of boolean flags in a structure, since you don't have to have a separate byte for each flag.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Containers", "Header 2": "C arrays", "path": "../pages/digitalGarden/cs/cpp/containers.mdx"}, "page_content": "You can work with arrays in C++ just like in C. However often you will create the array on the heap as it is better for large arrays to not fill the stack and you can do dynamically allocate the length of the array instead of having to define it at runtime.  \n```cpp\nvoid create(int height) {\nint** triangle = new int*[height]; // array of int pointers on heap\nfor(int i = 0; i < height; i++) {\ntriangle[i] = new int[i+1];  // in ith location array of int pointers on heap\n// fill pascals triangle\ntriangle[i][0] = 1;\nfor(int j = 1; j < i; j++) {\ntriangle[i][j] = triangle[i-1][j-1] + triangle[i-1][j];\n}\ntriangle[i][i] = 1;\n}\n// who releases array???\n}\n```  \nWe can see here that it would work however we would never release the memory which is why we should use smart pointers here.  \n```cpp\nvoid create(int height) {\nauto triangle = make_unique<unique_ptr<int[]>[]>(height);; // array of int pointers on heap\nfor(int i = 0; i < height; i++) {\ntriangle[i] = make_unique<int[]>(i + 1);  // in ith location array of int pointers on heap\n// fill pascals triangle\ntriangle[i][0] = 1;\nfor(int j = 1; j < i; j++) {\ntriangle[i][j] = triangle[i-1][j-1] + triangle[i-1][j];\n}\ntriangle[i][i] = 1;\n}\n// now it will be automatically released when out of scope\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Containers", "Header 2": "Pointer arithmetic", "path": "../pages/digitalGarden/cs/cpp/containers.mdx"}, "page_content": "Just like in C you can also do pointer arithmetic in C++.  \n```cpp\nint iArray[20];// not initialized\nconst int size= sizeof(iArray)/sizeof(*iArray);\nint* pArr= iArray;\nfor(int i = 0; i < size; i++) {\n*pArr= i + 1; // iArray[i] = i + 1;\ncout << *pArr++ << ' ';\n}\ncout << endl;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Containers", "Header 2": "Cpp arrays", "Header 3": "Array class", "path": "../pages/digitalGarden/cs/cpp/containers.mdx"}, "page_content": "In C++ there is a class array which is just a wrapper around a C array of fixed length and offers some useful functions on the array.  \n```cpp\n#include <iostream>\n#include <array>\n#include <string>\nusing namespace std;\nint main() {\nconstexpr size_t size = 4;\narray<string, size> names = { \"adam\", \"berta\", \"carlo\", \"doris\" };\nint i = 0;\nfor (const auto& s : names) { // has an iterator\ncout << i++ << \": \" << s << endl;\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Containers", "Header 2": "Cpp arrays", "Header 3": "Vector class", "path": "../pages/digitalGarden/cs/cpp/containers.mdx"}, "page_content": "The vector class in C++ is just like the ArrayList in Java and offers a half dynamic array that is stored on the heap.  \n```cpp\n#include <iostream>\n#include <vector>\n#include <string>\nusing namespace std;\nint main() {\nvector<string> vnames = { \"adam\", \"berta\", \"carlo\", \"doris\" };\nvector<shared_ptr<string>> vsp;\nvsp.reserve(vnames.size());// allocates enough memory on heap\nfor (const auto& s : vnames) {\nvsp.push_back(make_shared<string>(s + ':' + to_string(s.length())));\n}\nfor (size_t i = 0; i < vnames.size(); i++) {\ncout << vnames[i] << endl;\ncout << *vsp[i] << endl;\n}\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Containers", "Header 2": "Binding of values", "path": "../pages/digitalGarden/cs/cpp/containers.mdx"}, "page_content": "??? not sure it is called this jsut splitting no?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "path": "../pages/digitalGarden/cs/cpp/strings.mdx"}, "page_content": "Along with the C way of using strings as character arrays there are many other new things in C++.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "Header 2": "String class", "path": "../pages/digitalGarden/cs/cpp/strings.mdx"}, "page_content": "The standard C++ library provides a string class type which allows you to work with string just like in Java. By adding the s suffix teh string literal becomes a C++ string.  \n```cpp\n#include <iostream>\n#include <string>\nusing namespace std;\nint main() {\nauto name = \"George\"s;\ncout << \"size: \" << name.size() << endl;\ncout << \"length: \" << name.length() << endl;\ncout << \"name[2]: \" << name[2] << endl;\ncout << \"c_str: \" << name.c_str() << endl;// conver to null terminated C string\ncout << \"substr: \" << name.substr(1, 3) << endl;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "Header 2": "String_View class", "path": "../pages/digitalGarden/cs/cpp/strings.mdx"}, "page_content": "The `string_view` class is a good way to pass C strings between methods because all it is, is a wrapper for the size and the char array.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "Header 2": "String types", "path": "../pages/digitalGarden/cs/cpp/strings.mdx"}, "page_content": "In C++ there are lots of ways of creating strings from special representations.  \n```cpp\n#include <iostream>\nusing namespace std;\nint main() {\ncout << \"1 byte: \" << \"abcd\" << endl;\ncout << \"2 byte wide: \" << L\"abcd\" << endl;\ncout << \"UTF-8: \" << u8\"abcd\" << endl;\ncout << \"UTF-16: \" << u\"abcd\" << endl;\ncout << \"UTF-32: \" << U\"abcd\" << endl;\n}\n```  \n```bash title=\"Output\"\n1 byte: abcd\n2 byte wide: 00007FF776D1AC50\nUTF-8: abcd\nUTF-16: 00007FF776D1AC50\nUTF-32: 00007FF776D1AC90\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "Header 2": "Raw string literals", "path": "../pages/digitalGarden/cs/cpp/strings.mdx"}, "page_content": "Often times you want to be able to use backslashes and double quotes in a string litral which can get very annoying with escaping it. In C++ you can create raw litrals which accepts everything between the marker and round brackets.  \n```cpp\n#include <iostream>\n#include <string>\nusing namespace std;\nint main() {\nstring text = R\"---(This is very \"funny\"\nI can write whatever I want \\\\\\\\\\\\)---\";\n\ncout << text << endl;\n}\n```  \n```bash title=\"Output\"\nThis is very \"funny\"\nI can write whatever I want \\\\\\\\\\\\\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "CMake", "path": "../pages/digitalGarden/cs/make/cMake.mdx"}, "page_content": "<Callout type=\"todo\">\n- [Official CMake Tutorial](https://cmake.org/cmake/help/latest/guide/tutorial/index.html)\n- [Jetbrains CMake for CLion](https://www.jetbrains.com/help/clion/quick-cmake-tutorial.html#targets-configs)\n- [Youtube Playlist for CMake](https://www.youtube.com/playlist?list=PLalVdRk2RC6o5GHu618ARWh0VO0bFlif4)\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Makefiles", "path": "../pages/digitalGarden/cs/make/makefiles.mdx"}, "page_content": "<Callout type=\"info\">\nFor further reading I strongly recommend [this website by Theicfire](https://makefiletutorial.com/).\n</Callout>  \nSimply put Makefiles are used to help decide which parts of a large program need to be recompiled. Makefiles are most\ncommonly used with big C/C++ programs as other programming languages have their own build tools that are very similar to\nMakefiles (Java has Gradle or Maven). Makefiles don't necessarily need to be used for compiling programs as they just\ndefine a series of instructions to run depending on what files have changed. The focus of this page will however be on\nhow Makefiles are used to compile C/C++ programs.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Makefiles", "Header 2": "Running a Makefile", "path": "../pages/digitalGarden/cs/make/makefiles.mdx"}, "page_content": "To use Makefiles you need to have `make` installed. To check whether you have installed it use the\ncommand `make --help`.  \nThe series of instructions for a Makefile or written in a file called `Makefile` (no file ending) hence the name\nMakefiles. On Unix based systems this is easily created with the `touch Makefile` command.  \nJust like all programming languages, there is a hello world example for Makefiles as well.  \n```makefile filename=\"Makefile\"\nhello:\necho \"Hello, World\"\n```  \nTo run the example you just need to use the `make` command in the same folder as the Makefile.  \n```bash\nfoo@bar:~$ make\necho \"Hello, World\"\nHello, World\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Makefiles", "Header 2": "Syntax", "path": "../pages/digitalGarden/cs/make/makefiles.mdx"}, "page_content": "Now that we have seen a simple example let's go through how Makefiles are structured and what they exactly do.  \nA Makefile contains a set of **rules**. And a rule looks like this:  \n```makefile filename=\"Makefile\"\ntargets: dependencies\ncommand\ncommand\ncommand\n```  \n- The **targets** are filenames, typically there is only one per rule but there can be multiple separated by spaces.\n- The **commands** are a series of steps typically used to make the target(s). These need to start with a tab character,\nnot spaces (be careful how your editor handles tabs).\n- The **prerequisites** or also more commonly called **dependencies** are also filenames that are separated by spaces.\nThese files need to exist before the commands for the target are run.  \nNow that we know a bit more let's go back to the hello world example:  \n```makefile filename=\"Makefile\"\nhello:\necho \"Hello, World\"\n```  \nWe have the target `hello`, that in turn has no dependencies. This means that in theory, the goal of this rule would be\nto create a file called `hello` and to do this the `echo \"Hello, World\"`. Because it is the first and only rule in the\nMakefile we can just run `make`, but we can also specify which target we want to build by passing it to the make\ncommand `make hello` would in this case be equivalent. Because this Makefile never actually creates the target we can\nrun the command over and over again.  \nAs mentioned before Makefiles are typically used for C/C++ programs so let's look at such an example:  \n```c filename=\"main.c\"\n#include \"stdio.h\"", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Makefiles", "Header 2": "Syntax", "path": "../pages/digitalGarden/cs/make/makefiles.mdx"}, "page_content": "int main(void){\nprintf(\"I am a C program\");\nreturn 0;\n}\n```  \n```makefile filename=\"Makefile\"\nmain:\ngcc main.c -o main\n```  \nNow after executing the `make` command we are actually creating the target as intended. But if we try and run the `make`\ncommand again it will say that the target is already up to date. This is because the main file has already been created.  \n```bash\nfoo@bar:~$ make\ngcc main.c -o main\nfoo@bar:~$ make\nmake: `main' is up to date.\n```  \nSome people that are spoilt from their IDEs might now expect that if we change `main.c` and run the Makefile again that\nit will get recompiled, this however is not the case. This is what the dependencies are for.  \n```makefile filename=\"Makefile\"\nmain: main.c\ngcc main.c -o main\n```  \nIf you add `main.c` as a dependency and execute the `make` command again, it will check if any of the dependencies have\nchanged since last building the target. More technically explained it will check all the modification timestamps of the\ndependency files and compare them to the timestamp of the target. If the target is older than one of the modification\ntimestamps it will rebuild the target.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Kolomogorov Complexity", "path": "../pages/digitalGarden/cs/theoretical/kolomogorovComplexity.mdx"}, "page_content": "Does a few things, says how much information is in a string by the length of the shortest program that can generate it.\nCan also say somehting about randomness, if a string is random it has high kolomogorov complexity because it is hard to compress.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Nondeterministic Finite Automaton", "path": "../pages/digitalGarden/cs/theoretical/nfa.mdx"}, "page_content": "non-deterministic - multiple paths to follow.\nword is accepted if there is at least one path that leads to an accepting state.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Nondeterministic Finite Automaton", "Header 2": "Transforming NFA to DFA", "path": "../pages/digitalGarden/cs/theoretical/nfa.mdx"}, "page_content": "top down approach\nif an input leads to multiple states then write them as a set.\nThe set then becomes the new state. And the transitions are the union of the transitions of the states in the set.\nThis is the power set construction??? continue till done.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Nondeterministic Finite Automaton", "Header 2": "With Empty Word Transitions", "path": "../pages/digitalGarden/cs/theoretical/nfa.mdx"}, "page_content": "epsilon transitions\nthese are transitions that can be taken without consuming any input, like a jump or joker card.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Nondeterministic Finite Automaton", "Header 2": "With Empty Word Transitions", "Header 3": "Epsilon Closure", "path": "../pages/digitalGarden/cs/theoretical/nfa.mdx"}, "page_content": "Epsilon hülle - epsilon closure, the set of states that can be reached from a state with epsilon transitions.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Nondeterministic Finite Automaton", "Header 2": "With Empty Word Transitions", "Header 3": "Transformation to NFA", "path": "../pages/digitalGarden/cs/theoretical/nfa.mdx"}, "page_content": "Does this even work?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Nondeterministic Finite Automaton", "Header 2": "With Empty Word Transitions", "Header 3": "Transformation to DFA", "path": "../pages/digitalGarden/cs/theoretical/nfa.mdx"}, "page_content": "this works  \nbecause of epislon transitions the start state is not always the start state of the NFA. Is actually the epsilon closure of the start state.\nAll becomes a bit more complicated but still works just need to keep track of the epsilon transitions.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Regular Expressions & Languages", "Header 2": "Regular Expressions", "path": "../pages/digitalGarden/cs/theoretical/regularExpressionsLanguages.mdx"}, "page_content": "use cases  \ncan also easily create epsilon-NFAs therefore also DFAs and regular languages\nFollow certain building rules/components such as how to handle multiplication of two regular expressions in the states, star and alternative operations.  \ncan also go the other way around and create regular expressions from DFAs, NFAs and epislon-NFAs.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Regular Expressions & Languages", "Header 2": "Regular Expressions", "Header 3": "Syntax", "path": "../pages/digitalGarden/cs/theoretical/regularExpressionsLanguages.mdx"}, "page_content": "Reguläre Ausdrücke sind “Wörter” über dem Alphabet alphabet union mit den Operatoren `+`, `*`, `.` und `()` etc  \n#### Order of Precedence  \n#### UNIX Regular Expressions  \nslightly different and expanded for easier use", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Regular Expressions & Languages", "Header 2": "Regular Expressions", "Header 3": "Semantik", "path": "../pages/digitalGarden/cs/theoretical/regularExpressionsLanguages.mdx"}, "page_content": "Regular expressions seen as languages", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Regular Expressions & Languages", "Header 2": "Regular Languages", "path": "../pages/digitalGarden/cs/theoretical/regularExpressionsLanguages.mdx"}, "page_content": "Operations on regular languages (union, concatenation, Kleene star)\nThey equal a regular language  \nA DFA is a regular language therefore a regular expression same things", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Regular Expressions & Languages", "Header 2": "Closure Properties", "path": "../pages/digitalGarden/cs/theoretical/regularExpressionsLanguages.mdx"}, "page_content": "Performing certain operations on regular languages will result in a regular language.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Regular Expressions & Languages", "Header 2": "Pumping Lemma", "path": "../pages/digitalGarden/cs/theoretical/regularExpressionsLanguages.mdx"}, "page_content": "Prove that a language is not regular.  \nThe idea is that if a language is regular it can be pumped, i.e. a substring can be repeated an arbitrary number of times and the resulting string will still be in the language.  \nExample for intution.  \nAll strings in a regular language can be pumped if they are longer then a certain length, the pumping length, $p$. for example can set $p$ to be the number of states in the DFA.  \nUsing pigeonhole principle can show that a state must be visited more then once, therefore a loop must exist. The substring that creates the loop can then be pumped and the resulting string will still be in the language.  \nSo it can be split into three parts, $x$, $y$ and $z$ where $y$ is the substring that can be pumped. $x$ and $z$ are the substrings before and after $y$. Formal definition of the pumping lemma.  \n$$\n\\begin{align*}\n\\forall i \\geq 0: xy^iz \\in L \\\\\n\\mid y \\mid > 0 \\\\\n\\mid xy \\mid \\leq p \\\\\n\\end{align*}\n$$  \nThe first condition defines that if a string is in the language then all pumped strings must also be in the language. The second condition defines that the substring $y$ must be longer then 0, i.e an empty string can not be pumped, $x$ and $z$ can be empty.\nThe third condition defines that the prefix $x$ and $y$ must be shorter then the pumping length $p$ because of the pigeonhole principle, dont quiet get this condition.  \nHow do you find the pumping length is it the number of states in the DFA, but what if there are multiple DFAs that can represent the language, need to find the smallest DFA that can represent the language.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Regular Expressions & Languages", "Header 2": "Converting among Representations", "path": "../pages/digitalGarden/cs/theoretical/regularExpressionsLanguages.mdx"}, "page_content": "Between all the different representations of regular languages what are the time complexities of converting between them.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Regular Expressions & Languages", "Header 2": "Myhill-Nerode Theorem", "path": "../pages/digitalGarden/cs/theoretical/regularExpressionsLanguages.mdx"}, "page_content": "Minimimum size of a DFA for a language", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Context-Free Grammars & Languages", "path": "../pages/digitalGarden/cs/theoretical/grammars.mdx"}, "page_content": "defintions etc", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Context-Free Grammars & Languages", "Header 2": "Parse Trees", "path": "../pages/digitalGarden/cs/theoretical/grammars.mdx"}, "page_content": "yield, how to construct etc", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computational Problems", "path": "../pages/digitalGarden/cs/theoretical/problems.mdx"}, "page_content": "homomorphism is a special kind of function that preserves the structure of the input. An algorithm can be seen as a homomorphism from one set of all words in an alphabet to another set of all words in an alphabet.\nan algorithm solves a descision problem if it can decide for any input word if it is in the language or not i.e the output is 1 or 0. We then say that the algorithm a recognizes the language L for the problem.\nIf a language is recognized by an algorithm we say that the language is recursive. Not the same as recursive function in programming???", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computational Problems", "Header 2": "Algorithms and Programs", "path": "../pages/digitalGarden/cs/theoretical/problems.mdx"}, "page_content": "what are they? a homomorphism from one set of all words in an alphabet to another set of all words in an alphabet.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computational Problems", "Header 2": "Algorithms and Programs", "Header 3": "Äquivalent Algorithms", "path": "../pages/digitalGarden/cs/theoretical/problems.mdx"}, "page_content": "if they have the same input alphabet and all outputs are the same.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computational Problems", "Header 2": "Algorithms and Programs", "Header 3": "Enumeration Algorithms", "path": "../pages/digitalGarden/cs/theoretical/problems.mdx"}, "page_content": "for all intergers n it outputs the sequence of the alphabet from 1 to n.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computational Problems", "Header 2": "Decision Problems", "path": "../pages/digitalGarden/cs/theoretical/problems.mdx"}, "page_content": "Is a word in a language. For example, is a number prime?  \nif the word is in the language, the answer is yes, otherwise no.  \nIf there is such an algorithm we call it recursive???  \nlots of examples such as primtest, hamiltonian cycle, etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computational Problems", "Header 2": "Function Problems", "path": "../pages/digitalGarden/cs/theoretical/problems.mdx"}, "page_content": "In german relationsproblem?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Computational Problems", "Header 2": "Optimization Problems", "path": "../pages/digitalGarden/cs/theoretical/problems.mdx"}, "page_content": "takes 6 parameters, minimize or maximize  \nlike TSP, max-sat, max-clique, ILP etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Alphabet", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "An alphabet is a finite non-empty set of symbols. Sometimes these symbols are called letters and the alphabet is called a vocabulary.\nAn alphabet is usually denoted by the symbol $\\Sigma$ and the symbols in the alphabet are in lower case.  \n<Callout type=\"example\">\n- $\\Sigma = \\{a, b, c\\}$ is an alphabet with three symbols.\n- $\\Sigma_{\\text{bool}} = \\{0, 1\\}$ is an alphabet with two symbols representing boolean or binary values.\n- $\\Sigma_{\\text{lat}} = \\{a, b, c, \\ldots, z\\}$ is an alphabet with all the lowercase letters of the Latin alphabet.\n- $\\Sigma = \\{0, 1, 2, \\ldots, 9\\}$ is an alphabet with the digits 0 to 9.\n- $\\Bbb{N}$ is not an alphabet because it is not finite.\n- $\\Sigma = \\{\\}$ is not an alphabet because it is empty.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Words", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "A word is a finite sequence of symbols from an alphabet also commonly called a string. Sequence elements are commonly seperated by commas.\nHowever in the context of formal languages, we often omit the commas and write the sequence of symbols without any seperators as long as no ambiguity arises.  \nThe length of a word is calculated just like the length of a sequence, by counting the number of elements in the sequence, i.e. the number of symbols in the word.  \n<Callout type=\"example\">\n- $w = a, b, c$ is a word with three symbols. $|w| = 3$.\n- $w = abc$ is the same word as above but written without commas.\n- $w = word$ is a word over the alphabet $\\Sigma_{\\text{lat}}$.\n- $w = 101$ is a word over the alphabet $\\Sigma_{\\text{bool}}$.\n- $w = 101010$ is not a word over the alphabet $\\Sigma_{\\text{lat}}$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Words", "Header 3": "Empty Word", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "The empty word is a word with no symbols. It is often denoted by $\\varepsilon$ or $\\lambda$.\nJust like the empty set is a subset of every set, the empty word is a word over every alphabet.  \nBecause the empty word has no symbols, its length is zero, i.e. $|\\varepsilon| = 0$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Words", "Header 3": "Number of Occurrences", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "The number of occurrences of a symbol in a word is the number of times that symbol appears in the word.\nThis is often denoted by $|w|_a$ where $a$ is the symbol and $w$ is the word.  \n<Callout type=\"example\">\n- $w = abacaba$, $|w|_a = 4$.\n- $w = 101010$, $|w|_0 = 3$.\n</Callout>  \nUsing this we can also define the length of a word as the sum of the number of occurrences of each symbol in the word:  \n$$\n|w| = \\sum_{a \\in \\Sigma} |w|_a\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Words", "Header 3": "Concatenating Words", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "Concatenating two words is the operation of joining the two words together to form a new word. This is often denoted by the symbol $\\cdot$.\nSo we define the concatenation of two words $u$ and $v$ as follows:  \n$$\nu \\cdot v = uv\n$$  \nThis is just like in normal arithmetic where we can omit the multiplication sign when multiplying variables. When concatonating words the length of the new word is the sum of the lengths of the two words.  \n$$\n|u \\cdot v| = |u| + |v|\n$$  \nThis then also gives a logical definition of the concatenation of the empty word with any word. The concatenation of the empty word with any word is just the word itself.\nThis is because the empty word has no symbols and no length, so adding it to any word does not change the word or its length.  \n<Callout type=\"info\">\nHowever unlike normal multiplication, the order of the words matters in concatenation, i.e. the concatenation operation is not commutative.  \n$$\nu \\cdot v \\neq v \\cdot u\n$$  \nHowever, the concatenation operation is associative:  \n$$\n(u \\cdot v) \\cdot w = u \\cdot (v \\cdot w)\n$$\n</Callout>  \n<Callout type=\"example\">\n- $u = ab$, $v = cd$, $u \\cdot v = abcd$.\n- $u = 101$, $v = 010$, $u \\cdot v = 101010$.\n- $u = \\varepsilon$, $v = abc$, $u \\cdot v = abc$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Words", "Header 3": "Powers of Words", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "The power of a word is the concatenation of the word with itself a certain number of times. This is denoted by a superscript and also often called iteration or repetition of a word.\nThe power $n$ of a word has to be a non-negative integer, $n \\in \\Bbb{N}_0$. Just like with powers of numbers where $1$ is the identity element for the power of $0$, the empty word is the identity element for the power of $0$ for words.  \n$$\nw^n = \\prod_{i=1}^{n} w\n$$  \nUsing the definition of concatenation, we can also write this as:  \n$$\nw^n = \\underbrace{w \\cdot w \\cdot \\ldots \\cdot w}_{n \\text{ times}}\n$$  \nWhich also leads to a recursive definition of the power of a word:  \n$$\nw^n = \\begin{cases}\n\\varepsilon & \\text{if } n = 0 \\\\\nw \\cdot w^{n-1} & \\text{if } n > 0\n\\end{cases}\n$$  \n<Callout type=\"example\">\n- $w = 101$, $w^3 = 101101101$.\n- $w = abc$, $w^2 = abcabc$.\n- $w = 101$, $w^0 = \\varepsilon$.\n- $w = \\varepsilon$, $w^2 = \\varepsilon$.  \nUsing the power of words we can write long sequences of the same symbol in a more compact way, this can however lead to ambigous words:  \n$$\nw = bbababababbaaaabab = b^2(ab)^4ba^4bab = b(ba)^4b^2a^3(ab)^2\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Words", "Header 3": "Reverse of a Word", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "The reverse of a word is the word with the symbols in the opposite order. This is denoted by the symbol $w^R$.  \n$$\n\\begin{align*}\nw & = a_1a_2 \\ldots a_n \\\\\nw^R & = a_na_{n-1} \\ldots a_2a_1\n\\end{align*}\n$$  \n<Callout type=\"example\">\n- $w = 101$, $w^R = 101$.\n- $w = abc$, $w^R = cba$.\n- $w = 101010$, $w^R = 010101$.\n- $w = \\varepsilon$, $w^R = \\varepsilon$.\n</Callout>  \nWe can also define the reverse of the concatenation of two words as the concatenation of the reverse of the two words in reverse order:  \n$$\n(u \\cdot v)^R = v^R \\cdot u^R\n$$  \n<Callout type=\"example\">\nThis is best shown by example:  \n$$\n\\begin{align*}\nu & = ab \\\\\nv & = cd \\\\\nu \\cdot v & = abcd \\\\\n(u \\cdot v)^R & = dcba \\\\\nv^R \\cdot u^R & = dcba\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Sets of Words", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "If we have an alphabet $\\Sigma$, we can define some special sets of words over that alphabet. Specifically we can define the set of all words of a certain length by using the power of the alphabet.  \n$$\n\\Sigma^n = \\text {set of all words of length } n \\text{ over the alphabet } \\Sigma\n$$  \nThis set is finite because the alphabet is finite and the length of the words is fixed.  \n<Callout type=\"example\">\n- $\\Sigma = \\{a, b\\}$, $\\Sigma^2 = \\{aa, ab, ba, bb\\}$.\n- $\\Sigma = \\{0, 1\\}$, $\\Sigma^3 = \\{000, 001, 010, 011, 100, 101, 110, 111\\}$.\n- $\\Sigma = \\{a, b, c\\}$, $\\Sigma^0 = \\{\\varepsilon\\}$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Sets of Words", "Header 3": "Kleene Star", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "The Kleene star is a special set of words that includes all words of any length that can be made from the alphabet. It is the set of all words over the alphabet.  \n$$\n\\Sigma^* = \\bigcup_{i \\geq 0} \\Sigma^i = \\Sigma^0 \\cup \\Sigma^1 \\cup \\Sigma^2 \\cup \\ldots\n$$  \nor more formally:  \n$$\n\\Sigma^* = \\{\\varepsilon\\} \\cup \\{x_1x_2 \\ldots x_n \\mid i,n \\in \\Bbb{N}^+, x_i \\in \\Sigma\\}\n$$  \n<Callout type=\"example\">\n- $\\Sigma = \\{a, b\\}$, $\\Sigma^* = \\{\\varepsilon, a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, bab, bba, bbb, \\ldots\\}$.\n- $\\Sigma = \\{0, 1\\}$, $\\Sigma^* = \\{\\varepsilon, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 101, 110, 111, \\ldots\\}$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Sets of Words", "Header 3": "Kleene Plus", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "The Kleene plus is a variation of the Kleene star that does not include the empty word. It is the set of all words of length greater than zero that can be made from the alphabet.  \n$$\n\\Sigma^+ = \\Sigma^* - \\{\\varepsilon\\} = \\bigcup_{i \\geq 1} \\Sigma^i = \\Sigma^1 \\cup \\Sigma^2 \\cup \\Sigma^3 \\cup \\ldots\n$$  \n<Callout type=\"example\">\n- $\\Sigma = \\{a, b\\}$, $\\Sigma^+ = \\{a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, bab, bba, bbb, \\ldots\\}$.\n- $\\Sigma = \\{0, 1\\}$, $\\Sigma^+ = \\{0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 101, 110, 111, \\ldots\\}$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Subwords", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "A subword of a word is a word that can be obtained by deleting zero or more symbols from the original word. Subwords are also called substrings or infixes.\nThe word $v$ is a subword of the word $w$ if there are words $x$ and $y$ in the same alphabet such that:  \n$$\nw = xvy\n$$  \nor more formally:  \n$$\nv \\text{ is a subword of } w \\iff \\exists x, y \\in \\Sigma^* : w = xvy\n$$  \nBecause $x$ and $y$ can be the empty word, every word is a subword of itself. This also leads to the empty word being a subword of every word as well because\nwe can always choose $x$ to be the empty word and $y$ to be the word itself, or vice versa, or some split in between.  \n<Image\nsrc=\"/maths/languagesSubword.png\"\ncaption=\"Visual representation of subwords of a word.\"\nwidth={400}\n/>  \nA proper subword is a subword that is not the same as the original word.  \n$$\n\\begin{align*}\nw & = xvy \\\\\nv & \\neq w \\\\\n|v| & < |w| \\\\\nv \\text{ is a proper subword of } w & \\iff \\exists x, y \\in \\Sigma^*- \\{\\varepsilon\\} : w = xvy\n\\end{align*}\n$$  \nThis also means that either $x$ or $y$ or both are not the empty word and that $|v| < |w|$.  \n<Callout type=\"example\">\n- $w = abacaba$, $v = abacaba$ is a subword of $w$.\n- $w = abacaba$, $v = bac$ is a proper subword of $w$.\n- $w = 101010$, $v = 101$ is a proper subword of $w$.\n- $w = 101010$, $v = \\varepsilon$ is a proper subword of $w$.\n- $w = 101010$, $v = abc$ is not a subword of $w$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Subwords", "Header 3": "Prefix", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "A prefix of a word is a subword that starts at the beginning of the word. This means that the prefix $v$ of a word $w$ is a subword of $w$ such that there is a word $y$ in the same alphabet such that:  \n$$\nw = vy\n$$  \nBecause the prefix is a special case of a subword it follows all the same rules as a subword and also has the differenation between normal and proper prefixes.  \n<Image\nsrc=\"/maths/languagesPrefix.png\"\ncaption=\"Visual representation of prefixes of a word.\"\nwidth={400}\n/>  \n<Callout type=\"example\">\n- $w = abacaba$, $v = a$ is a proper prefix of $w$.\n- $w = abacaba$, $v = abacaba$ is a prefix of $w$.\n- $w = 101010$, $v = 101$ is a proper prefix of $w$.\n- $w = 101010$, $v = \\varepsilon$ is a proper prefix of $w$.\n- $w = 101010$, $v = 010$ is not a prefix of $w$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Subwords", "Header 3": "Suffix", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "A suffix of a word is a subword that is at the end of the word. This means that the suffix $v$ of a word $w$ is a subword of $w$ such that there is a word $x$ in the same alphabet such that:  \n$$\nw = xv\n$$  \nor more formally:  \n$$\nv \\text{ is a suffix of } w \\iff \\exists x \\in \\Sigma^* : w = xv\n$$  \nBecause the suffix is a special case of a subword it follows all the same rules as a subword and also has the differenation between normal and proper suffixes.  \n<Image\nsrc=\"/maths/languagesSuffix.png\"\ncaption=\"Visual representation of suffixes of a word.\"\nwidth={400}\n/>  \n<Callout type=\"example\">\n- $w = abacaba$, $v = a$ is a suffix of $w$.\n- $w = abacaba$, $v = abacaba$ is a suffix of $w$.\n- $w = 101010$, $v = 010$ is a proper suffix of $w$.\n- $w = 101010$, $v = \\varepsilon$ is a proper suffix of $w$.\n- $w = 101010$, $v = 101$ is not a suffix of $w$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Languages", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "A language is a specific set of words over an alphabet commonly denoted by the symbol $L$. More precisly a language is a subset of the set of all words over an alphabet.  \n$$\nL \\subseteq \\Sigma^*\n$$  \n<Callout type=\"example\">\nWe can define the english language as a language over the alphabet including all the letters of the English alphabet and some punctuation marks.\nThe language $L_\\text{eng}$ would then include all the words that are valid in the English language.  \n$$\nL_\\text{eng} = \\{\\text{hello}, \\text{world}, \\text{cat}, \\text{dog}, \\ldots\\}\n$$  \nProgramming languages (such as C) are languages based on the ASCII character set, where the programs must have a correct syntax.  \n$$\nL_\\text{C} = \\{\\text{int}, \\text{main}, \\text{printf}, \\text{scanf}, \\ldots\\}\n$$  \nThe language of all binary numbers with the same number of 0s and 1s is a language over the alphabet $\\{0, 1\\}$.  \n$$\nL = \\{00, 11, 0011, 0101, 0110, 1001, 1010, 1100, \\ldots\\}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Languages", "Header 3": "Sub and Supersets of Languages", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "If $L$ is a language over an alphabet $\\Sigma_1$ and $\\Sigma_2$ is a superset of $\\Sigma_1$, then $L$ is obviously also a language over $\\Sigma_2$:  \n$$\nL \\subseteq \\Sigma_1^* \\subseteq \\Sigma_2^*\n$$  \nThe empty set is a language over every alphabet because it is a subset of every set of words. It is often denoted by the symbol $L_\\emptyset$ or just $\\emptyset$.  \n$$\nL_\\emptyset \\subseteq \\Sigma^* \\text{ for all } \\Sigma\n$$  \nThe language that contains only the empty word is also a language over every alphabet. It is often denoted by the symbol $L_\\varepsilon$ or just $\\{\\varepsilon\\}$.  \n$$\nL_{\\varepsilon} \\subseteq \\Sigma^* \\text{ for all } \\Sigma\n$$  \nHowever, this does not mean that the empty set and the set containing only the empty word are the same.  \n$$\nL_\\emptyset \\neq L_{\\varepsilon}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Languages", "Header 3": "Operations on Languages", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "Because languages are just sets we can apply the common set operations to them. This includes union, intersection, difference, and complement. These are all self explanatory and work the way as expected.\nHowever, when the operation is binary, i.e. it takes two languages as input, the two languages must be over the same alphabet.  \n<Callout type=\"info\">\n- Union: $L_1 \\cup L_2 = \\{w \\mid w \\in L_1 \\text{ or } w \\in L_2\\}$\n- Intersection: $L_1 \\cap L_2 = \\{w \\mid w \\in L_1 \\text{ and } w \\in L_2\\}$\n- Difference: $L_1 - L_2 = \\{w \\mid w \\in L_1 \\text{ and } w \\notin L_2\\}$\n- Complement: $L^C = \\{w \\mid w \\notin L\\}$ or $L_1^C = \\Sigma^* - L_1$ i.e the set of all words over the alphabet that are not in the language.\n</Callout>  \nIn addition to these set operations, we can also apply the operations of concatenation and the Kleene star to languages. These operations are defined in the same way as for words.  \n#### Concatenation  \nWhen concatenating two languages, we concatenate every word in the first language with every word in the second language.  \n$$\nL_1 \\cdot L_2 = \\{w_1 \\cdot w_2 \\mid w_1 \\in L_1, w_2 \\in L_2\\}\n$$  \nThis is also called the product of two languages. The concatenation of two languages is very similar to the cartesian product of two sets. This is clearly visible in the example below.  \n<Callout type=\"example\">\n- $L_1 = \\{a, b\\}$, $L_2 = \\{c, d\\}$, $L_1 \\cdot L_2 = \\{ac, ad, bc, bd\\}$.\n- $L_1 = \\{101, 010\\}$, $L_2 = \\{11, 00\\}$, $L_1 \\cdot L_2 = \\{10111, 10100, 01011, 01000\\}$.\n</Callout>  \n#### Kleene Star  \nUsing the concatenation operation we can also define the Kleene star of a language. The Kleene star of a language is the set of all words that can be made by concatenating any number of words from the original languages.\nFor this we define the following:  \n$$\n\\begin{align*}\nL^0 & = \\{\\varepsilon\\} \\\\\nL^1 & = L \\\\\nL^2 & = L \\cdot L \\\\\nL^{n+1} & = L \\cdot L^n\n\\end{align*}\n$$  \nWe can then define the Kleene star of a language as:  \n$$\nL^* = \\bigcup_{i \\geq 0} L^i = L^0 \\cup L^1 \\cup L^2 \\cup \\ldots", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Formal Languages", "Header 2": "Languages", "Header 3": "Operations on Languages", "path": "../pages/digitalGarden/cs/theoretical/formalLanguages.mdx"}, "page_content": "$$\n\\begin{align*}\nL^0 & = \\{\\varepsilon\\} \\\\\nL^1 & = L \\\\\nL^2 & = L \\cdot L \\\\\nL^{n+1} & = L \\cdot L^n\n\\end{align*}\n$$  \nWe can then define the Kleene star of a language as:  \n$$\nL^* = \\bigcup_{i \\geq 0} L^i = L^0 \\cup L^1 \\cup L^2 \\cup \\ldots\n$$  \nWe can also define the Kleene plus, i.e the set of all words that can be made by concatenating any number of words from the original language except the empty word:  \n$$\nL^+ = L^* - \\{\\varepsilon\\} = \\bigcup_{i \\geq 1} L^i = L^1 \\cup L^2 \\cup L^3 \\cup \\ldots\n$$  \n<Callout type=\"example\">\n- $L = \\{a, b\\}$, $L^* = \\{\\varepsilon, a, b, aa, ab, ba, bb, aaa, aab, aba, abb, baa, bab, bba, bbb, \\ldots\\}$.\n- $L = \\{101, 010\\}$, $L^* = \\{\\varepsilon, 101, 010, 101101, 101010, 010101, 010010, 101101101, 101101010, \\ldots\\}$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "A deterministic finite automaton, short DFA, is a finite-state machine that can be in one of a finite number of states at any given time. It can only move from one state to another by following a set of rules called transitions.\nThe deterministic property means that the DFA will always make the same transition to the same state, given the same input, there is no stochastic element to the machine, i.e. there is only one path to follow.  \nDFAs are used to solve decision problems, such as deciding whether a given string is a member of a language or not.  \nImportantly DFAs work in real time and without any memory or variables. They read the input string character by character and make decisions based on the current state and the read character.\nOnce the last character of the input string has been read, the DFA will either accept or reject the input string based on the current state.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "Header 2": "Notation", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "States, transitions, and final states also called configurations and impulses? accepted or rejected states.  \nDifference of configuration and state?  \nAs a graph", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "Header 2": "Formal Definition", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "Formal definition of a DFA  \nas a table", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "Header 2": "Configurations", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "Current state of the DFA and the remaining input  \nStart and end configurations.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "Header 2": "Transition Functions", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "transition notation looks funny, can also somehow denote that there is a transition from one state to another without\na direct connection.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "Header 2": "Language of a DFA", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "Language of a DFA  \nCan define regular languages with DFAs", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "Header 2": "Classes", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "States can be grouped somehow? this can then also be used in the design.  \nUsing this and its properties can also define accepting regular language?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "Header 2": "Designing a DFA", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "in the book he uses the classes?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Deterministic Finite Automaton", "Header 2": "Modular Design", "path": "../pages/digitalGarden/cs/theoretical/dfa.mdx"}, "page_content": "From the slides?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Analysis of Algorithms", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/analysisOfAlgorithms.mdx"}, "page_content": "Asymptotic Complexity / Analysis of Algorithms  \nThe master method and how to calculate it and stuff, go back to algd1, MIT 6.006 and Algorithms Illuminated will help.  \nTelescoping? How to get to recurrance relation and then asymptotic complexity.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "In an ideal world, we would want to be able to access data with $O(1)$ using the data's unique identifier (key).  \n<Image\nsrc=\"/cs/algdHashTable.png\"\ncaption=\"A rough sketch of how a hash table works.\"\nwidth={750}\n/>  \nFor this, to work we need to be able to generate a unique hash code from the key. From this hash code (a number) we then want to get an index in a hash table by using a hash function. For this approach to work, two conditions must be met. Firstly we need to be able to know if two objects are the same (the `equals` function) secondly we need to be able to generate a hash code from the unique identifier which can consist of a combination of attributes or just one.  \nImportantly the following must be true:  \n$$\n(a.equals(b)) \\Rightarrow (a.hashCode() == b.hashCode())\n$$  \nSo if two objects are the same then their hash Code must be the same as well. However, if two hash codes are the same it does not necessarily mean that the objects are the same, this is a so-called collision.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Hashing Function", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "We want to be able to calculate the index as fast as possible. From the above requirements, we also want the same keys to produce the same indices. We also want the hash codes and therefore the indices to be evenly distributed to minimize collisions.  \nFor starters we could use the following hashing function:  \n$$\nindex = hash\\,code \\mod table.length()\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Hash Code", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "We want the generated hash code to be randomly and if possible evenly spread across the entire range of possible numbers.  \nIf the unique identifier is a 32-bit data type, like boolean, byte, short, int, char and float we can just take its value straight as an int.  \nIf the unique identifier is a 64-bit data type, like long or double we can use an exclusive or (XOR, only true if they are different) between the two 32-bit parts.  \n```java\npublic int hashCode() {\n// XOR of two 32-bit parts\nreturn (int)(value ^ (value >>> 32));\n}\n```  \nFor strings, it gets a bit harder. You might think it would be a good idea to add the characters represented as integers together. However, this is a very bad idea because for example AUS and USA would then have the same hash code. Instead, we create a polynomial using the character values as coefficients.  \n```java\npublic final class String {\nprivate final char value[];\n/** Cache the hash code for the string, to avoid recalculation */\nprivate int hash; // Default to 0\n...\npublic int hashCode() {\nint h = hash;\nif (h == 0 && value.length > 0) {\nchar val[] = value;\nfor (int i = 0; i < value.length; i++) {\nh = 31 * h + val[i];\n}\nhash = h;\n}\nreturn h;\n}\n...\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "HashMap", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "In Java, a HashMap always has a size equal to a power of 2. This leads to the map reserving in the worst case twice as much memory as it needs. However, the advantage of this implementation is that it is very easy to calculate powers of 2 with bit shifts. It also allows us to change the hash function `(hashCode() & 0x7FFFFFFF) % length` to `hashCode() & (length -1)`. The bitmask with `0x7FFFFFFF` ensures that the hash code is positive.  \n```java\npublic HashMap(int initialCapacity) {\nint capacity = 1;\nwhile (capacity < initialCapacity)\ncapacity <<= 1;\ntable = new Entry[capacity];\n}\n\nprivate int indexFor(int h) {\nreturn h & (table.length - 1);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Collision Resolution", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "As mentioned before collisions are when different objects have the same hash code and therefore the same index. This can happen and can't be avoided. This is why they need to be handled.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Collision Resolution", "Header 3": "Separate Chaining", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "With this strategy when there is a collision, the colliding elements are chained together just like with a linked list. The advantage of this strategy is that it is very simple and the table never becomes full. The problem however is that it needs additional memory and the memory needs to be dynamic.  \n<Image\nsrc=\"/cs/algdSeperateChaining.png\"\ncaption=\"How separate chaining resolves collisions.\"\nwidth={750}\n/>  \nThe class for a HashMap would then look something like this:  \n```java\npublic class HashMap < K, V > implements Map < K, V > {\nNode <K, V> [] table;\n...\nstatic class Node <K, V> implements Map.Entry <K, V> {\nfinal K key;\nV value;\nNode <K, V> next;\n...\n}\n}\n```  \nIf the table has the size $m$ and we insert $n$ elements we can calculate the probability of a collision using the following formula:  \n$$\n\\prod_{i=0}^{n-1}{\\frac{m-i}{m}}\n$$  \nFrom this we can then also calcualte the probability of there being at least 1 collision:  \n$$\n1 - \\prod_{i=0}^{n-1}{\\frac{m-i}{m}}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Collision Resolution", "Header 3": "Open Addressing", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "With this strategy when there is a collision, we look for a free space in the hash table. The advantage of this strategy is that it does not need any additional space however the table can become full. The process of finding a free space is called probing.  \n#### Linear Probing  \nWhen using linear probing we try the next highest index until we find a free space. If we reach the end of the table we restart the search at index 0 until we are back to the initial area of collision which means the table is full.  \nSo if the hash code is $x$ and the table has the size $m$ the index after $k$ collisions is:  \n$$\nindex= (x \\mod m + k) \\mod m\n$$  \n```java\npublic void add(T elem) {\nint i = (elem.hashCode() & 0x7FFFFFFF) % size;\nwhile (array[i] != null)\ni = (i + 1) % size;\narray[i] = elem;\n}\n```  \nThe above code however doesn't check if the hash table should only hold unique values (set semantic) or if the table is already full. However, with this strategy clusters of values can form. When adding a value you then just make the cluster even bigger and therefore also the probability of hitting a cluster.  \nWhen inserting into a table of size $n$ with a cluster of size $k$ we can calculate the probability of hitting the cluster and therefore also increasing the size of the cluster:  \n$$\n\\frac{k+2}{n}\n$$  \nWe can also calculate the probability of needing at least 3 probe steps when adding which is:  \n$$\n\\frac{k-2}{n}\n$$  \n##### Double Hashing  \nThe idea here is that we don't look at the next highest free space, which is equivalent to a step size of 1 but that each element calculates a step size for itself. This is done to avoid creating clusters. This strategy is called double hashing as you have a hash function for the index and the step size.  \nSo if the hash code is $x$ and the table has the size $m$ the index after $k$ collisions is:  \n$$\nindex= (x \\mod m + k \\times step) \\mod m\n$$  \n```java\npublic void add(T elem) {\nint i = (elem.hashCode() & 0x7FFFFFFF) % size;\nint step = ...?\nwhile (array[i] != null) {", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Collision Resolution", "Header 3": "Open Addressing", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "$$\nindex= (x \\mod m + k \\times step) \\mod m\n$$  \n```java\npublic void add(T elem) {\nint i = (elem.hashCode() & 0x7FFFFFFF) % size;\nint step = ...?\nwhile (array[i] != null) {\ni = (i + step) % size;\n}\narray[i] = elem;\n}\n```  \nHowever, we need to be very careful when choosing the step size otherwise the problem of clusters becomes even worse. Some obvious bad examples would be a step size of 0 or the size of the table. To avoid this we can restrict the step size with the following condition:  \n$$\nggt(step, m)= 1 \\text{ (coprime/teilerfremd) } \\land 0 < step < m\n$$  \nSome common choices are:  \n- The size of the table $m$ is a power of 2 and a step is an odd number $\\in [1, m-1]$.  \n```java\n1 + 2 * ((elem.hashCode() & 0x7FFFFFFF) % (m / 2))\n```  \n- The size of the table $m$ is a prime number and a step is $\\in [1, m-1]$.  \n```java\n1 + (elem.hashCode() & 0x7FFFFFFF) % (m – 2)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Removing Elements", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "When removing an element it can't just be set to `null` because otherwise when looking for an element after the deletion we could hit a null reference and crash before we find the element we are looking for (depending on language and implementation). Instead of setting it to `null` it is common practice to set it to a sentinel object. If we are then looking for an element and we hit a sentinel we can just carry on our search. This then also means that when we add an element and we come across a sentinel we can add the element in place of the sentinel.  \n```java\npublic class HashTable < T > {\nprivate final Object[] arr;\nprivate static final Object sentinel = new Object();\n...\npublic void remove(Object o) {\nassert o != null;\nint i = (o.hashCode() & 0x7FFFFFFF) % arr.length;\nint cnt = 0;\nwhile (arr[i] != null && !o.equals(arr[i]) && cnt != arr.length) {\ni = (i + 1) % arr.length;\ncnt++;\n}\nif (o.equals(arr[i])) arr[i] = sentinel;\n}\npublic boolean contains(Object o) {\nassert o != null;\nint i = (o.hashCode() & 0x7FFFFFFF) % arr.length;\nint cnt = 0;\nwhile (arr[i] != null && !o.equals(arr[i]) && cnt != arr.length) {\ni = (i + 1) % arr.length;\ncnt++;\n}\nreturn cnt != arr.length && arr[i] != null;\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Performance Improvements", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "Using modulo in the probe loop causes is not optimal because of the multiple divisions that need to be calculated.  \nSo instead of `i = (i + step) % size;` we can use one of the following:  \n- If the table size $m$ is a power of 2 we can use a bitmask which is very fast.  \n```java\ni = (i + step) & (size - 1);\n```  \n- Instead of using modulo, we could also manually detect an overflow.  \n```java\ni = i + step; if (i >= size) i -= size;\n```  \n- Because a comparison with 0 is faster than with a given number we could also probe backward and check for an underflow.  \n```java\ni = i - step; if (i < 0) i += size;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Load Factor", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "The number of collisions increases with the number of elements in the table. To be able to make statements on the status of the table there is the so-called load factor which is defined as followed:  \n$$\n\\lambda = \\frac{\\text{Number of element in table}}{\\text{table size}}\n$$  \nIf we know the number of elements to be added we can then also calculate an optimal size for the table depending on the desired load factor.  \nWe can also create a new table and copy all the elements to the new table if a certain threshold load factor has been reached. However, it is important to recalculate the indices when doing this. This process is called **rehashing**.  \nWhen searching for an element in a hash table that is using the separate chained strategy we expect to find the element after half the load factor so $O(1+\\frac{\\lambda}{2})$. If a search is unsuccessful then the waste is $O(1+\\lambda)$ because the entire list was searched.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Load Factor", "Header 3": "Separate Chaining", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "There is no upper limit for the load factor as the chains can be of any length. The average length is equivalent to the load factor. For the table to be efficient the load factor should be $\\lambda < 1$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hash Tables", "Header 2": "Load Factor", "Header 3": "Open Addressing", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/hashTables.mdx"}, "page_content": "The load factor is limited to $\\lambda \\leq 1$. As long as $\\lambda < 1$ there is still space in the table. For optimal performance, it is recommended to have a load factor of $\\lambda < 0.75$ for linear probing and double hashing $\\lambda < 0.9$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stacks", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/stacks.mdx"}, "page_content": "A stack is as the name says like a stack of paper. Meaning it follows the LIFO policy (last in first out). The most common operations on queues are:  \n- `push(E e)`: Puts the element onto the top of the stack.\n- `E pop()`: Takes the element from the top of the stack.\n- `E peek()`: Returns the element at the top of the stack, which corresponds to the element to next be popped.  \n<Image\nsrc=\"/cs/algdStack.png\"\ncaption=\"Visualization of a stack.\"\nwidth={750}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stacks", "Header 2": "Implementing a Stack", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/stacks.mdx"}, "page_content": "<Callout type=\"todo\">\n```java filename=\"MyStack.java\"\n// TODO\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stacks", "Header 2": "Implementing a Stack", "Header 3": "Stack Using two Queues", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/stacks.mdx"}, "page_content": "Although the most common way of implementing a stack is with a [linked list](./linkedLists) it is also possible to implement a stack by using two queues. Just like when [implementing a queue with two stacks](./queues#queue-using-two-stacks) you need to decide if adding or removing an element will be expensive.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sets", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sets.mdx"}, "page_content": "A set is a data structure that can hold unique elements. It represents a mathematical set which in german is called a \"Menge\". This means that an element is either in the set or it isn't. Just like with a bag you have the common operations of adding elements, removing elements and searching for a specific element.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sets", "Header 2": "Implementing a Set", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sets.mdx"}, "page_content": "<Callout type=\"todo\">\n```java filename=\"UnsortedSet.java\"\n// TODO\n```\n</Callout>  \nJust like when [implementing the bag](./bags#array-implementations) we can use `java.util.Arrays.binarysearch(a, from, to, key)` which returns the index of the key, if it is contained and otherwise $(-(insertion point) - 1)$ with insertion point being the point where the key would be inserted, i.e the index of the first element greater than the key.  \n<Callout type=\"todo\">\n```java filename=\"SortedSet.java\"\n// TODO\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sets", "Header 2": "Implementing a Set", "Header 3": "Time Complexities", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sets.mdx"}, "page_content": "When implementing a set and bag there is also the question of whether the data should be sorted or not. Depending on the answer the time complexities will be different and the implementation changes.  \n| Operation        | UnsortedSet                                     | SortedSet                                                                     |\n| ---------------- | ----------------------------------------------- | ----------------------------------------------------------------------------- |\n| add(E e)         | $O(n)$ <br/> check (search) + add $O(n) + O(1)$ | $O(n)$ <br/> search insertion point (check) + shift right $O(\\log{n}) + O(n)$ |\n| search(Object o) | $O(n)$ <br/> linear search                      | $O(\\log{n})$ <br/> binary search                                              |\n| remove(Object o) | $O(n)$ <br/> search + remove $O(n) + O(1)$      | $O(n)$ <br/> search insertion point (check) + shift left $O(\\log{n}) + O(n)$  |\n| Ideal use case   | When set is needed but don't search a lot       | When set is needed and a lot of searching                                     |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Linked Lists vs Arrays", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "When implementing collections with arrays we can encounter a few issues. An array uses a fixed given size which leads us to implementing algorithms that only work for that fixed amount. To solve this issue when adding elements we could also make an array that is one size larger, copy everything over and then add the new element. Another approach is when the array gets full we increase its size by either a fixed amount that could also change depending on how many times we have already increased the size. Meaning the array is either always full or we use to much space.  \nYou can imagine a linked to be like a chain. It consists of nodes that have a value and a reference of the next node. The linked list then just needs to know the first node and can then make its way through the list. With this method the size of the collection is dynamic and we can add as many elements as we want (limited by memory).  \n<Image\nsrc=\"/cs/algdLinkedListVsArrays.png\"\ncaption=\"Comparison of how arrays and linked lists work and are stored in memory.\"\nwidth={750}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Variations", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "There are various variations of linked lists which all have there use cases.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Variations", "Header 3": "Singly Linked List", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "This is the common implementation when talking about linked lists. A node has a value and a reference to the next element.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Variations", "Header 3": "Doubly Linked List", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "Here unlike the singly linked list a node has a value, a reference to the next element and additionally also a reference to the previous element. This makes removal of node much easier.  \n<Image\nsrc=\"/cs/algdLinkedListDoubly.png\"\ncaption=\"A doubly linked list with references to the next and previous element.\"\nwidth={750}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Variations", "Header 3": "Circular Linked List", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "In a circular linked list the last element does not have a reference to null as the next element but instead the head which allows the linked list to be visualized as a circle.  \n<Image\nsrc=\"/cs/algdLinkedListCircular.png\"\ncaption=\"A circular linked list where the last element references the head.\"\nwidth={750}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Implementing a Linked List", "Header 3": "Adding", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "When implementing the `add(E e)` function there are a few options:  \n- You can iterate your way through the linked list to the end and then add the new element onto the end. This however has a complexity of $O(n)$ which is not ideal for a simple operation.\n- To solve the above issue we can keep a private reference in the list of not only the head but also the tail (last element) of the linked list.\n- There is no rule saying you have to add an element at the end. You can also just add it to the front of the list, so it becomes the new head and its reference to the next node is the old head.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Implementing a Linked List", "Header 3": "Removing", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "When implementing the `remove(Object o)` function there is only really one way of doing it and that is to find the node that holds the value to be removed `curr` whilst also remembering the previous node `prev` and then setting the reference of the `prev.next` to `curr.next`. This can be made easier as mentioned above by storing in each node a reference to the previous element to make it a doubly linked list.  \n<Image\nsrc=\"/cs/algdLinkedListRemove.png\"\ncaption=\"A sketch of how the remove operation works in a linked list.\"\nwidth={750}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Implementing a Linked List", "Header 3": "Containing", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "When implementing the `boolean contains(Object o)` you have to iterate over the entire linked list to see if you find the element or reach the end.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linked Lists", "Header 2": "Implementing a Linked List", "Header 3": "Example", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/linkedLists.mdx"}, "page_content": "<Callout type=\"todo\">\n```java filename=\"MySingleLinkedList.java\"\n// TODO\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Recursion", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/recursion.mdx"}, "page_content": "SRTBOT from MIT to design a recursive function merge sort as example:\n- Subproblem identification and definition\n- Relate subproblem solution to the original problem with a recurrence relation\n- Topological order of subproblems (order in which subproblems are solved) to avoid circular dependencies, i.e.\nwe want it to be a DAG\n- Base case(s) to terminate recursion\n- Original problem solution via subproblem solutions\n- Time and space complexity analysis  \nsome blabla about recursion. Can every recursive function be written as an iterative function? What about the other way around?  \nWhy would you use recursion? What are the advantages and disadvantages?  \nTailed recursion and the impact on the stack.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is NP-Hard?", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/np.mdx"}, "page_content": "Lots of Euler diagrams and examples needed. Clear formulations seem to be hard to find.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is NP-Hard?", "Header 2": "Deterministic vs Non-Deterministic Algorithms", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/np.mdx"}, "page_content": "example of deterministic and non-deterministic algorithms  \nleaving a blank part", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is NP-Hard?", "Header 2": "P and NP", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/np.mdx"}, "page_content": "What is the stuff with the verification in polynomial time? Is mentioned but unsure how exactly need and example", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is NP-Hard?", "Header 2": "NP-Complete and NP-Hard", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/np.mdx"}, "page_content": "NP Complete is NP-Hard but als has an algo in NP?  \nBut solving one NP-Complete problem in polynomial time means all NP-Complete problems can be solved in polynomial time???\nSame goes for if one NP-Hard problem can be solved in polynomial time then all NP problems can be solved in polynomial time?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is NP-Hard?", "Header 2": "NP-Complete and NP-Hard", "Header 3": "Reduction", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/np.mdx"}, "page_content": "The conversion of one problem to another has to be in polynomial time???", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is NP-Hard?", "Header 2": "NP-Complete and NP-Hard", "Header 3": "Boolean Satisfiability Problem (SAT)", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/np.mdx"}, "page_content": "CNF (Conjunctive Normal Form) ??? and then reduce to 0/1 Knapsack Problem", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is NP-Hard?", "Header 2": "NP-Complete and NP-Hard", "Header 3": "Cook-Levin Theorem", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/np.mdx"}, "page_content": "Got prize for proving what it means if P = NP???", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is NP-Hard?", "Header 2": "BQP", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/np.mdx"}, "page_content": "Quantum stuff", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/collections.mdx"}, "page_content": "Collections are containers/data structures that can hold elements of the same type. Most programming languages have some basic implementations as part of their standard library. Depending on the problem to be solved certain data structures are better options than others. In Java, there is the `java.util.Collections` package which contains some of the most common collections.  \n<Image\nsrc=\"/cs/algdJavaCollections.png\"\ncaption=\"The Java Collections Framework.\"\nwidth={750}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Bags", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/bags.mdx"}, "page_content": "A bag is a data structure that can contain the same element multiple times which is why it is often also called a multiset. The order of adding elements is not necessarily given, this depends on the implementation. Common operations on a bag are adding elements, removing elements and searching for a specific element.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Bags", "Header 2": "Implementing a Bag", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/bags.mdx"}, "page_content": "One of the simplest ways of implementing data structures is by using arrays. When implementing a data structure the time complexities can be different on whether the data is always in a sorted state or not.  \n<Callout type=\"todo\">\n```java filename=\"UnsortedBag.java\"\n// TODO\n```\n</Callout>  \nWhen implementing a sorted collection in Java you can either implement your own binary search or you can use `java.util.Arrays.binarysearch(a, from, to, key)` which returns the index of the key, if it is contained and otherwise $(-(insertion point) - 1)$ with insertion point being the point where the key would be inserted, i.e the index of the first element greater than the key.  \n<Callout type=\"todo\">\n```java filename=\"SortedBag.java\"\n// TODO\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Bags", "Header 2": "Implementing a Bag", "Header 3": "Time Complexities", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/bags.mdx"}, "page_content": "| Operation        | UnsortedBag                                | SortedBag                                             |\n| ---------------- | ------------------------------------------ | ----------------------------------------------------- |\n| add(E e)         | $O(1)$ <br/> no search, or shift           | $O(n)$ <br/> search + shift right $O(\\log{n}) + O(n)$ |\n| search(Object o) | $O(n)$ <br/> linear search                 | $O(\\log{n})$ <br/> binary search                      |\n| remove(Object o) | $O(n)$ <br/> search + remove $O(n) + O(1)$ | $O(n)$ <br/> search + shift left $O(\\log{n}) + O(n)$  |\n| Ideal use case   | When adding a lot                          | When searching a lot                                  |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Bags", "Header 2": "Bag of Words", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/bags.mdx"}, "page_content": "<Callout type=\"todo\">  \nWhat is a bag of words? How is it used in NLP?\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queues", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/queues.mdx"}, "page_content": "A queue is as the name says like a queue of people. Meaning it follows the FIFO policy (first in first out). The most common operations on queues are:  \n- `enqueue(E e)`: Adds an element to the rear of the queue.\n- `E dequeue()`: Takes the element from the front of the queue.\n- `E peek()`: Returns the element at the front of the queue, which corresponds to the element to next be dequeued.  \n<Image\nsrc=\"/cs/algdQueue.png\"\ncaption=\"Visualization of a queue.\"\nwidth={750}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queues", "Header 2": "Implementing a Queue", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/queues.mdx"}, "page_content": "<Callout type=\"todo\">\n```java filename=\"MyQueue.java\"\n// TODO\n```\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queues", "Header 2": "Implementing a Queue", "Header 3": "Queue Using two Stacks", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/queues.mdx"}, "page_content": "Although the most common way of implementing a queue is with a [linked list](./linkedLists) it is also possible to implement a queue by using two stacks. Just like when [implementing a stack with two queues](./stacks#stack-using-two-queues) you need to decide if adding or removing an element will be expensive.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Search Trees", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binarySearchTrees.mdx"}, "page_content": "A binary search tree is a [binary tree](./binaryTrees) where each node has a key. *key(v)=Key of the node v*. The important thing here is that in the left subtree of a node there are only nodes with keys that are smaller than the key of the node. In the right subtree accordingly only nodes with a key that are the same or larger.  \nInterestingly when traversing the tree in-order we can see that the keys ascend.  \n<Image\nsrc=\"/cs/treesBinarySearchInOrder.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Search Trees", "Header 3": "Operations", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binarySearchTrees.mdx"}, "page_content": "#### Insert  \nWhen inserting a node you need to find the ideal place for insertion. This is started by comparing it with the root and seeing whether it belongs in the left or right half. This is then repeated recursively repeated until the insertion point is found.  \n#### Search  \nWhen searching for a specific key we follow a similar process as with when inserting. By comparing the key with the roots key and then carrying on down the tree. If we don't just want the first node but all nodes with this key, once the first node is found we carry on down the right subtree until it is empty.  \n#### Remove  \nWhen removing a node we distinguish between 3 different cases.  \n##### Leaf  \nWe search for the node and then simply remove it, nothing complicated.  \n<Image\nsrc=\"/cs/treesBinarySearchRemoveLeaf.png\"\ncaption=\"A binary tree.\"\n/>  \n##### Node with 1 child  \nWe search for the node to be removed and remove it. The child of the removed node then takes its place. Also nothing complicated.  \n<Image\nsrc=\"/cs/treesBinarySearchRemove1Child.png\"\ncaption=\"A binary tree.\"\n/>  \n##### Node with 2 children  \nWe search for the node to be removed. We then look for the symmetrical (inorder) successor, which is the node in the right subtree that is the furthest left. We then replace the to be removed node with the symmetrical successor. Lastly we delete the symmetrical successor at its original position which is either case 1 or 2.  \n<Image\nsrc=\"/cs/treesBinarySearchRemove2Children.png\"\ncaption=\"A binary tree.\"\n/>  \n```java\nprivate Node<K, E> remove(Node<K,E> node, K key){\nif (node == null) {\nreturn null;\n}\nelse {\nint c = key.compareTo(node.key);\nif (c < 0) {\nnode.left = remove(node.left, key);\n}\nelse if (c > 0) {\nnode.right = remove(node.right, key);\n}\nelse {\nif (node.left == null) {\nnode = node.right;\nnodeCount--;\n}\nelse if (node.right == null) {\nnode = node.left;\nnodeCount--;\n}\nelse {\nNode<K, E> succ = symSucc(node.right);\nsucc.right = remove(node.right, succ.key);\nsucc.left = node.left;\nnode = succ;\n}\n}\nreturn node;\n}\n}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Search Trees", "Header 3": "Operations", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binarySearchTrees.mdx"}, "page_content": "private Node<K, E> symSucc(Node<K,E> node){\nNode<K, E> succ = node;\nwhile (succ.left != null) {\nsucc = succ.left;\n}\nreturn succ;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Search Trees", "Header 3": "Time complexities", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binarySearchTrees.mdx"}, "page_content": "The time complexities of the operations depend on the height of the tree. In the worst case all operations take O(n), when the tree has become like a list. In the best case all operations O(log n), this is when the tree is complete(excluding the last level). From this we can see it is important to keep the height as small as possible to have the best time complexities.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Heaps", "Header 2": "Priority queue", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/heaps.mdx"}, "page_content": "In a lot of cases we want a queue that is however influenced by a priority. The priority is the key of an element and must be comparable. The elements in the queue are then sorted by this priority resulting in HIFO, highest priority in first out. There are then 3 operations that can be performed on this priority queue. Elements can be added, we can look at the element with the highest priority and remove it. Depending there are then 2 ways in which we can define the key with the highest priority, either it is the largest or smallest key, which then lead to the following definitions:  \nA Minimum Priority Queue where peek()=min() and remove()=removeMin() or a Maximum Priority Queue where peek()=max() annd remove()=removeMax().", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Heaps", "Header 2": "Priority queue", "Header 3": "Priorities", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/heaps.mdx"}, "page_content": "A priority can contain any element that has a comparable key.  \nElement can either have a natural or predefined key and order. For example if the element is a number then the number can be used as its key. However if the element is a clubmember then we might use the amount of years he has been a member as the key. It gets tricky with role hierarchies for example in the military. So we can define the Interface like the following  \n```java\npublic interface MinPriorityQueue<K extends Comparable<? super K>> {\nboolean add(K element);\nK min();\nK removeMin();\nint size();\n}\n```  \nimportantly here is the extends Comparable  \nWe might not have predefined priorities but priorities we assign when adding.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Heaps", "Header 2": "Min-Heap", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/heaps.mdx"}, "page_content": "A Min-Heap is a complete binary tree with the exception of the last level. On the last level it is filled from left to right. Importantly each nodes key is smaller or equal to that of its children.  \n<Image\nsrc=\"/cs/treesMinHeap.png\"\ncaption=\"A binary tree.\"\n/>  \nJust with this definition we can already easily implement 2 of the methods in the interface we defined both with O(1). The min() method is just the root and the size() method is done just like in all other collections with a counter.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Heaps", "Header 2": "Min-Heap", "Header 3": "Add", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/heaps.mdx"}, "page_content": "We start by adding the new element to the furthest left free space on the lowest level or if it is already full left on a new level. To then correct the order the element slowly wanders up the tree by swapping with its parents until it is larger then its parent or is in the root. This process of wandering up the tree we call **sift up**. O(log n) = O(1)+O(log n)  \n<Image\nsrc=\"/cs/treesMinHeapAdd.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Heaps", "Header 2": "Min-Heap", "Header 3": "RemoveMin", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/heaps.mdx"}, "page_content": "We already know that the min element is the root so we can remove it. We then replace it with the furthest right element on the last level and let it sink down the tree, meaning we swap it with its smaller child until it is smaller then both of its children or is a leaf. This process of sinking down the tree we call **sift down**.\nO(log n) = O(1)+O(log n)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Heaps", "Header 2": "Min-Heap", "Header 3": "Array Representation", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/heaps.mdx"}, "page_content": "We can also represent a Min-Heap as an array.  \n<Image\nsrc=\"/cs/treesMinHeapArray.png\"\ncaption=\"A binary tree.\"\n/>  \nWe can then see the following relationships for a node with the index $i$. These are all int operations so we ignore decimal points when deviding.  \n|                       | Root at index 1    | Root at index 0    |\n| --------------------- | ------------------ | ------------------ |\n| Parent Node of i      | i/2                | (i-1)/2            |\n| Left child of i       | 2i                 | 2i+1               |\n| Right child of i      | 2i + 1             | 2i+2               |\n| Indexes of all leaves | size/2+1 till size | size/2till size -1 |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Heaps", "Header 2": "Min-Heap", "Header 3": "Building a heap from a filled array", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/heaps.mdx"}, "page_content": "The first idea is we want to add one element after another from front to back, so in the top down. We however notice that we save a bit of space but it takes O(n log n). So we need a second idea.  \n#### Floyd's heap construction  \nHere instead of a lot of elements having to be sifted up we let the elements sift down which then leads to an algorithm that is O(n).  \n```java\nHeap(HeapNode<K>[] elems) {\nthis.heap = elems;\nthis.size = elems.length;\nfor(int i=size/2; i>= 0; i--){\nsiftDown(i);\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Heaps", "Header 2": "Min-Heap", "Header 3": "Sorting using a heap (Heapsort)", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/heaps.mdx"}, "page_content": "This is the so called heapsort. We take an array and by using floyds heap construction, construct a heap. We can then just for the size of the array removeMin which leads to a time complexity of O(n log n) since the removeMin is O(log n). However we do need to have O(n) additional space. However this can be improved to O(1) if we construct the heap directly in the input array and add the removeMin and add it to the back of the input array so we at the end of the array we have sorted array.  \n<Image\nsrc=\"/cs/treesMinHeapSort.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "B-Trees", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/bTrees.mdx"}, "page_content": "The goal of a B-tree is to not have to load an entire tree into memory. Only a bit by bit can be loaded in for processing. The order of a B-tree means something slightly different then with a normal tree.  \nIf a B-tree has the order of $n$ then it has to meet the following conditions:  \n1. Each node has a maximum of $2n$ elements.\n2. Each node, expect for the root, has at least n elements.\n3. Each node, that is not a leaf, has $m+1$ successors, where $m$ is the number of elements the node has.\n4. All leaf nodes are on the same level.\n5. All nodes have $m$ keys + reference to its data that are sorted in ascending orders of there keys and $m+1$ references to its successors.  \nBelow we can see a B-tree with the order of 2.  \n<Image\nsrc=\"/cs/treesBOrder2.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "B-Trees", "Header 2": "Operations", "Header 3": "Search", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/bTrees.mdx"}, "page_content": "To find a key we start of in a node and look at its elements. We then increase our counter i until one of the elements either is the key or is larger then the key. If the element is the key we get the corresponding data. If not and the node is a leaf we could not find the key. Otherwise we continue recursively by getting the node that is on the right side of the element.  \n```java\nE find(Node<K> node, K key) {\nint i = 0;\nwhile (i < node.m && key.compareTo(node.keys[i]) > 0) {\ni++;\n}\nif (i < node.m && key.equals(node.keys[i])) {\nreturn dataBlock(node.data[i]);\n}\nif (node.isLeaf()) {\nreturn null;\n}\nNode<K> child = diskRead(node.successor[i]);\nreturn find(child, key);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "B-Trees", "Header 2": "Operations", "Header 3": "Insert", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/bTrees.mdx"}, "page_content": "When inserting we always want to insert in a leaf node. Here 3 scenarios can happen.  \n#### Case 1  \nThe leaf isn't full and we can just simply add it.  \n<Image\nsrc=\"/cs/treesBInsertCase1.png\"\ncaption=\"A binary tree.\"\n/>  \n#### Case 2a  \nThe leaf is full so we have to split up the leaf. We split the leaf by taking the middle element and put it into the parent and create a new node with the elements that were to the right of the middle element.  \n<Image\nsrc=\"/cs/treesBInsertCase2a1.png\"\ncaption=\"A binary tree.\"\n/>  \n<Image\nsrc=\"/cs/treesBInsertCase2a2.png\"\ncaption=\"A binary tree.\"\n/>  \n#### Case 2b  \nIt can happen that when splitting the node and putting the middle element in the parent the parent is already full so the you have to perform another split. This process can repeat all the way to the root until a new root is created.  \n<Image\nsrc=\"/cs/treesBInsertCase2b.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "B-Trees", "Header 2": "Operations", "Header 3": "Remove", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/bTrees.mdx"}, "page_content": "When removing an element we define 2 scenarios. Either the element we want to delete is in a leaf or in a inner node.  \n#### From leaf  \nIf after the removal of the element the amount of elements in the node is still larger or equal to n the order of the B-tree, so $m\\geq n$ nothing needs to be done.  \n<Image\nsrc=\"/cs/treesBRemoveLeaf1.png\"\ncaption=\"A binary tree.\"\n/>  \nHowever if $m < n$ then the tree no longer meets the conditions we defined at the beginning. To restore these conditions we have to options. Either to borrow or combine.  \n<Image\nsrc=\"/cs/treesBRemoveLeaf2.png\"\ncaption=\"A binary tree.\"\n/>  \n##### Borrow  \nWe can use this operation when for example the right node doesn't have enough elements and the left node has more then $n$ elements or the other way around then we can simply do almost like a left or right rotation.  \n<Image\nsrc=\"/cs/treesBRemoveBorrow.png\"\ncaption=\"A binary tree.\"\n/>  \n##### Borrow Variant  \nSince we have anyway loaded the neighbouring node into the RAM we might as well use this situation to balance out the 2 nodes so that they have equal amounts of elements. This can be done by doing multiple rotations in a row.  \n<Image\nsrc=\"/cs/treesBRemoveBorrowVariant.png\"\ncaption=\"A binary tree.\"\n/>  \n##### Combine  \nWe can use this operation when we can't borrow from a neighbouring node. So when no neighbours have more then $n$ elements. In this case we combine the 2 nodes together.  \n<Image\nsrc=\"/cs/treesBRemoveCombine.png\"\ncaption=\"A binary tree.\"\n/>  \n##### Combine Variant  \nHere we combine not 2 but 3 nodes together to make 2 nodes so that the resulting nodes are more then half full. The advantage of doing this is that the next insert or remove can be done very easily.  \n<Image\nsrc=\"/cs/treesBRemoveCombineVariant.png\"\ncaption=\"A binary tree.\"\n/>  \n#### From inner node  \nHere just like in the binary tree we replace it with its symmetric successor which then always leads to a remove in a leaf.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "B-Trees", "Header 2": "Time Complexities", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/bTrees.mdx"}, "page_content": "If n is the order and N the total number of elements then we have the following time complexities  \n- Search: worst case from root to leaf so O(log(n) * N)\n- Insert: Find the place O(log(n) *N), insertion is in most cases constant but can be O(log(n)* N).\n- Remove: Find the place O(log(n) *N), removal is in most cases constant but can be O(log(n)* N).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Trees", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binaryTrees.mdx"}, "page_content": "A binary tree is a tree with the order of 2. Meaning that a node is either a leaf or has left and/or right child.  \n<Image\nsrc=\"/cs/treesBinary.png\"\ncaption=\"A binary tree.\"\n/>  \nBy adding empty leaves we can make sure the binary tree is always filled which can make certain operations and algorithms easier. We add the empty leaves by first adding 2 empty leaves to all leaves which make the leaves to inner nodes. Then all inner nodes that only have one child receive an empty leaf.  \n<Image\nsrc=\"/cs/treesBinaryAlternative.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Trees", "Header 2": "Traversal orders", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binaryTrees.mdx"}, "page_content": "There are multiple ways to traverse a tree each one giving a different result.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Trees", "Header 2": "Traversal orders", "Header 3": "Pre-order, NLR", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binaryTrees.mdx"}, "page_content": "In this order a node visited before its left and right subtree is traversed.  \n1. Visit the current node.\n2. Recursively traverse the current node's left subtree.\n3. Recursively traverse the current node's right subtree.  \n<Image\nsrc=\"/cs/treesPreOrder.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Trees", "Header 2": "Traversal orders", "Header 3": "Post-order, LRN", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binaryTrees.mdx"}, "page_content": "In this order a node is visited after its left and right subtree has been traversed.  \n1. Recursively traverse the current node's left subtree.\n2. Recursively traverse the current node's right subtree.\n3. Visit the current node.  \n<Image\nsrc=\"/cs/treesPostOrder.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binary Trees", "Header 2": "Traversal orders", "Header 3": "In-order, LNR", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/binaryTrees.mdx"}, "page_content": "In this order a node is visited in between the traversal of its left and right subtree.  \n1. Recursively traverse the current node's left subtree.\n2. Visit the current node.\n3. Recursively traverse the current node's right subtree.  \n<Image\nsrc=\"/cs/treesInOrder.png\"\ncaption=\"A binary tree.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "Trees have **nodes** that hold the data and **edges** which connect the nodes. An **empty tree** obviously has no nodes and therefore no data.  \n<Callout type=\"todo\">\nLink this up with graphs, rooted trees and forests somehow. Monte Carlo would also be cool.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Node Relationships", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "In trees there are a few relationships between nodes that are important to know:  \n- A Tree like in our real world has a root. The **root** is the highest node in the tree. Each node is also a root of its own **subtree**.\n- A **child** node is a node that is connected to a node above it, the so called **parent**. A **Sibling** is a node that shares the same parent.\n- A **leaf** node has no children as it is hanging alone at the bottom of a subtree. An **inner node** however has at least 1 child.  \n<Image\nsrc=\"/cs/treesSubtree.png\"\ncaption=\"The knapsack problem.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Order", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "The **Order of a tree** is the max amount of children a tree is aloud to have. In the above picture we don't know the order of the tree but we can say that it is $\\geq 3$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Degree", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "The **degree of a node** is the amount of children a specific node has. Often this is denoted as *deg(v)=Number of children of node*. So in the above tree *deg(Q)=2*.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Path", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "A **path** is a combination of edges between 2 Nodes. The length of the path is the amount of nodes visited whilst traversing from the start node to the end node. The amount includes the start and the end node. So in the above tree the length of the path from R to H is 4.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Height", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "The **height of a tree** is the length of  the longest path from the root to a leaf. In the above tree the height is 5.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Depth", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "The **depth of a node** is the amount of nodes on the path to the root. Often this is denoted as *depth(v)=Number of nodes on path to root*. So in the above tree *depth(L)=4*.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Level", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "A **Level** is a grouping of all nodes with the same depth.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Full and Complete Trees", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/trees/generalDefinition.mdx"}, "page_content": "A **Full tree** is a tree where all inner nodes have the maximum amount of Nodes according to the order. In the image below both trees are full with an order of 2.  \nA **Complete tree** is a tree where each level has the maximum amount of nodes according to the order. In the image below only the left tree is complete with an order of 2.  \n<Image\nsrc=\"/cs/treesFullComplete.png\"\ncaption=\"On the left a full and complete tree with an order of 2. On the right a full tree with an order of 2.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Diffusion", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/diffusion.mdx"}, "page_content": "In networks, we can model the spread of information, disease, or other phenomena as a diffusion process. The diffusion\nprocess usually starts with an initial node or a set of initial nodes. The goal is then to model how the information\nspreads through the network. You can imagine why this would be important for modeling the spread of a disease or an\nadvertising campaign on social media where the goal is to reach as many people as possible.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Diffusion", "Header 2": "Innovation Diffusion", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/diffusion.mdx"}, "page_content": "Already in 1962, Everett Rogers published a book called \"Diffusion of Innovations\" where he describes the spread of a\nnew idea or technology through a population. He split the adoption of a new idea into five stages:  \n- **Knowledge/Awareness**: The individual is exposed to the innovation and gains knowledge of the innovation.\n- **Persuasion**: The individual is interested in the innovation and actively seeks information about the\ninnovation.\n- **Decision**: The individual makes a decision to adopt or reject the innovation.\n- **Implementation**: The individual implements the innovation and uses it as a trial.\n- **Confirmation**: The individual finalizes his/her decision to continue using the innovation.  \nWhen analyzing the spread of a new innovation, Rogers found that the adoption of a new innovation follows a normal\ndistribution.  \n- **Innovators 2.5%**: Innovators are the first individuals to adopt an innovation. Innovators are most often young\nand willing to take risks and have a high social status.\n- **Early Adopters 13.5%**: This is the second-fastest category of individuals who adopt an innovation. These individuals\nhave the highest degree of opinion leadership among the other adopter categories. Early adopters take more time to\nadopt an innovation than innovators due to more careful deliberation.\n- **Early Majority 34%**: Individuals in this category adopt an innovation after a varying degree of time. Most often,\nthe early majority waits to adopt an innovation until they see that the innovation has proven useful for others and are\nin contact with the early adopters.\n- **Late Majority 34%**: Individuals in this category will adopt an innovation after the average member of the society.\nThese individuals approach an innovation with a high degree of skepticism.\n- **Laggards 16%**: Individuals in this category are the last to adopt an innovation. Most often bound by traditions.  \n<Image\nsrc=\"/cs/graphsInnovationDiffusion.png\"\ncaption=\"The normal distribution of the adoption of a new innovation.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Diffusion", "Header 2": "Innovation Diffusion", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/diffusion.mdx"}, "page_content": "- **Laggards 16%**: Individuals in this category are the last to adopt an innovation. Most often bound by traditions.  \n<Image\nsrc=\"/cs/graphsInnovationDiffusion.png\"\ncaption=\"The normal distribution of the adoption of a new innovation.\"\n/>  \n<Callout type=\"example\">\nWe can easily give some examples for the above distribution for when the first iPhone was released:  \n- **Innovators (2.5%)**: These were the tech enthusiasts who camped outside Apple stores. They were excited and were\nwilling to embrace the new technology despite its high price and limited features compared to today's standards.\n- **Early Adopters (13.5%)**: The early adopters included individuals who closely followed tech trends and were\nquick to purchase the iPhone once they saw the positive reviews and early adopter experiences. They recognized the\niPhone's potential to change the way people communicate and access information.\n- **Early Majority (34%)**: As the iPhone gained popularity and started to prove its utility, the early majority\njoined in. These individuals might have been initially hesitant but were swayed by the success stories of the early\nadopters.\n- **Late Majority (34%)**: The late majority were more cautious and waited until the iPhone became a mainstream\nproduct. They wanted to ensure that any initial bugs or issues were resolved and that the price had become more\naffordable. Their decision to adopt the iPhone was influenced by its widespread acceptance and integration into daily\nlife.\n- **Laggards (16%)**: Laggards were the last to adopt the iPhone, often sticking with their traditional cell phones\nor resisting smartphones altogether. They were skeptical of the technology's benefits and preferred to maintain\ntheir existing routines and devices.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Diffusion", "Header 2": "ICM - Independent Cascade Model", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/diffusion.mdx"}, "page_content": "The Independent Cascade Model (ICM) is a probabilistic diffusion model that is based on the idea that the spread of information\ntravels through neighbors in a network and therefore has a cascading effect. The model is based on the following assumptions:  \n- A node can only effect its neighbors.\n- A node can only be in one of two states: active or inactive. For example, a node can be infected or not infected.\n- A node only has one chance to activate its neighbors.\n- A node can only go from inactive to active.  \nThe initial setup of the model is as follows:  \n- Each edge has an attribute $p \\in [0,1]$, which is the probability that the node will take over the state of its neighbor.\nHow this probability is calculated depends on the application. For example, in the case of a disease, the probability\ncould be based on a persons age and immune system. In the case of an advertising campaign, the probability could be\nbased on the number of friends that have already seen the ad. You could also just use random probabilities.\n- A set of nodes $S$ is selected as the initial set of active nodes. All other nodes are inactive.  \nThe model then proceeds in discrete time steps. In each time step, the following happens:  \n1. For each node $v \\in S$, the node tries to activate each of its neighbors $u$. The activation is successful with\nprobability $p_{vu}$ so if we generate a random value $r \\in [0,1]$ and it is smaller or equal to $p$. If the activation\nis successful, $u$ is added to the set $S_{new}$.\n2. If $S_{new}$ is empty then the process terminates. Otherwise, $S$ is updated to $S_{new}$ and the process repeats\nfrom step 1.  \n<Image\nsrc=\"/cs/graphsIcmDiffusion.png\"\ncaption=\"An example of the ICM process.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Diffusion", "Header 2": "ICM - Independent Cascade Model", "Header 3": "Spread Maximization", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/diffusion.mdx"}, "page_content": "When working with the ICM model, we are often interested in finding the set of nodes $S$ that maximizes the spread, for\nexample in an advertising campaign. This is a [NP-Hard](../np) problem to solve, but we can use a greedy algorithm to find a good\nbut not necessarily optimal solution. (How is this an NP-Hard problem?)  \nWe can denote the spread after the ICM model as $f(S)$ where $S$ is the set of initial nodes. The output of the function\nis the number of nodes that are active after the ICM model has finished. Using this we can then implement a greedy\nalgorithm that wants to maximize the spread, i.e. find the set of nodes $S$ that maximizes $f(S)$.  \nHowever, we first need to change a few things about the ICM model to make it easier to work with because the model is\nnon-deterministic. Instead of using a random probability $p$ for each edge and then using a random number generator to\ndetermine if the edge is activated. We can instead use a fixed $p$ and fixed $r$ for each edge. Another possible approach\ncould be to define an \"activation function\" that takes the two nodes as input and defines if the edge is activated or not.  \nFor example, we could define the activation function as follows:  \n$$\na(u,v) = |u - v| \\leq 2\n$$  \nMost often, when wanting to maximize the spread, for example of an advertising campaign, we are also on a budget. This\nmeans that we can only select a limited number of nodes $k$ as the initial set of active nodes, i.e. $|S| \\leq k$.  \nThe greedy algorithm then works as follows:  \n<Steps>\n<Step id=\"1\">\nInitialize $S = \\emptyset$.\n</Step>\n<Step id=\"2\">\nFor each vertex $v \\in V \\land v \\notin S$ compute $f(S \\cup \\{v\\})$.\n</Step>\n<Step id=\"3\">\nSelect the vertex $v$ where $f(S \\cup \\{v\\})$ is the highest and add it to $S$. If there are multiple vertices\nwith the same value, select one of them randomly.\n</Step>\n<Step id=\"4\" >\nIf $|S| = k$ then terminate, otherwise repeat from step <StepCircle id=\"2\" scale={0.8}/>.\n</Step>\n</Steps>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Diffusion", "Header 2": "Linear Threshold Model", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/diffusion.mdx"}, "page_content": "The threshold model is a diffusion model that is based on the idea that a node can only be activated if a certain\nproportion of its neighbors are already activated. The model is based on the same assumptions as the ICM model:  \n- A node can only effect its neighbors.\n- A node can only be in one of two states: active or inactive. For example, a node can be infected or not infected.\n- A node only has one chance to activate its neighbors.\n- A node can only go from inactive to active.  \nIn the model we define a threshold $t_v$ for each node $v$. The threshold is a value between $0$ and $1$ and defines\nthe proportion of neighbors that need to be active for the node to be activated. For example, if $t_v = 0.5$ then at\nleast half of the neighbors of $v$ need to be active for $v$ to be activated.  \nFor the algorithm we then define an initial set of active nodes $S$ and then in each time step we do the following:  \n<Steps>\n<Step id=\"1\">\nFor each node $v \\in V \\land v \\notin S$ we compute the proportion of active neighbors $p_v$.\n</Step>\n<Step id=\"2\">\nIf $p_v \\geq t_v$ then we add $v$ to the set $S_{new}$.\n</Step>\n<Step id=\"3\" >\nIf $S_{new}$ is empty then the process terminates. Otherwise, $S$ is merged with $S_{new}$, i.e. $S = S \\cup S_{new}$,\nand the process repeats back to the initial step <StepCircle id=\"1\" scale={0.8}/>.\n</Step>\n</Steps>  \n<Image\nsrc=\"/cs/graphsLinearThresholdModel.png\"\ncaption=\"An example of the linear threshold model, where the threshold is $0.5$ for all nodes.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Diffusion", "Header 2": "Voter Model", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/diffusion.mdx"}, "page_content": "The voter model is a simple probabilistic diffusion model. To start the model, each node is assigned a random state\nwhich is either $0$ or $1$. In each time step, a node is selected at random and then one of its neighbors is also\nselected at random. The node then adopts the state of the selected neighbor. The process repeats until all nodes have\nthe same state.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Traversal", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/graphTraversal.mdx"}, "page_content": "The goal of graph traversal is to visit each vertex in a graph. This can be done a multitude of ways.  \nThe general algorithm is as followed. We have a root vertex $s$, a set of all visited vertices $B$, a subset of $B$ which still has unvisited outgoing edges called $R$ and $O$ which holds the order in which the vertices were visited.  \n```c\nadd $s$ to $R$ and set s.visited = true\n\nwhile R is not empty\ntake any vertex v in R\nif\nv has no unvisited outgoing edges remove v from R\nelse\nfollow a unvisited edge from v to w\nif !w.visited add w to R and set w.visited = true\n```  \nWith an adjacent list this takes $O(n+m)$ because each edge is followed once $O(m)$ and each vertex is added and removed from R $O(n)$.  \nWith an adjacent matrix this takes $O(n^2)$ because the entire matrix has to be checked for edges.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Traversal", "Header 2": "DFS - Depth First Search", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/graphTraversal.mdx"}, "page_content": "A depth first search (DFS) visits the child vertices of a chose vertex before visiting the sibling vertices. In other words it traverses the depth of any particular path before exploring its breadth. A stack (often the program's call stack via recursion) is generally used when implementing this algorithm.  \nThe algorithm begins with a root vertex it then transitions to an adjacent, unvisited vertex, until it can no longer find an unexplored vertex to transition to from its current location. The algorithm then backtracks until it finds a vertex connected to yet more unexplored vertices. This process carries on until the the algorithm has backtracked past the original \"root\" vertex from the very first step.  \nSo if in the general algorithm we replace \"add w to R\" with \"call recursively dfs(w)\". We then get something like this  \n```java\nvoid dfs(Vertex v) {\nprint(v); v.visited=true;\nfor (Vertex w : v.adjList) {\nif (!w.visited) {\ndfs(w);\n}\n}\n}\nvoid dfs_variante(Vertex v) {\nif (!v.visited) {\nprint(v); v.visited = true;\nfor (Vertex w : v.adjList) {\ndfs_variante(w);\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Graph Traversal", "Header 2": "BFS - Breadth First Search", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/graphTraversal.mdx"}, "page_content": "Instead of searching down a single path until we can go no longer, we search all paths at a uniform depth, which is one unit, from the source before moving onto deeper paths. We will be adding vertices to the back of a queue to be searched from in the future. Thus, we start with our source vertex in the queue and then whenever we dequeue an item, we enqueue all of its \"new\" neighbours who are one unit away, so the queue stores all items of distance 1 from the source before all items who are distance 2 from the source, and so forth.  \n```java\nvoid BFS(Vertex s) {\nQueue<Vertex<K>> R = new LinkedList<Vertex<K>>();\nprint(v); s.visited = true;\nR.add(s);\nwhile(!R.isempty()) {\nVertex v = R.remove();\nfor(Vertex w : v.adjList) {\nif(!w.visited) {\nprint(w); w.visited = true;\nR.add(w);\n}\n}\n}\n}\n```  \n<Image\nsrc=\"/cs/graphsDFSandBFS.png\"\ncaption=\"Comparison of DFS and BFS.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Connectivity", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/connectivity.mdx"}, "page_content": "Graph connectivity is also known as graph resilience and is a measure of how well a graph can maintain its connectivity\nwhen vertices or edges are removed, i.e. how many vertices or edges can be removed before the graph becomes disconnected\n(from one connected component to multiple connected components) or has a higher number of connected components.  \nWith this analysis technique we can find out how robust a graph is, i.e. how well it can handle failures which can be\nvery useful in real world applications such as communication, transportation, etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Connectivity", "Header 2": "Bridges", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/connectivity.mdx"}, "page_content": "A Bridge is an edge that if removed would increase the number of connected components in the graph. In the graph below\nyou can quiet clearly see that the edge between vertices $3$ and $4$ marked in red is a bridge.  \nexport const bridgeGraph = {\nnodes: [\n{id: 1, label: \"1\", x: 0, y: 0},\n{id: 2, label: \"2\", x: 0, y: 200},\n{id: 3, label: \"3\", x: 200, y: 100},\n{id: 4, label: \"4\", x: 400, y: 100},\n{id: 5, label: \"5\", x: 600, y: 0},\n{id: 6, label: \"6\", x: 600, y: 200}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 3},\n{from: 3, to: 4, color: \"red\", width: 5},\n{from: 4, to: 5},\n{from: 4, to: 6},\n{from: 5, to: 6}\n]\n};  \n<Graph\ngraph={bridgeGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Connectivity", "Header 2": "Cut Vertices", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/connectivity.mdx"}, "page_content": "The same idea as a bridge also applies to vertices. A vertex is a cut vertex if removing it would increase the number\nof connected components in the graph. In the graph below you can quiet clearly see that the vertices $3$ and $4$ are cut\nvertices. These cut vertices are very important vertices as they are brokers between different parts of the graph.  \nexport const cutVerticesGraph = {\nnodes: [\n{id: 1, label: \"1\", x: 0, y: 0},\n{id: 2, label: \"2\", x: 0, y: 200},\n{id: 3, label: \"3\", value: 5, x: 200, y: 100, color: \"red\"},\n{id: 4, label: \"4\", value: 5, x: 400, y: 100, color: \"red\"},\n{id: 5, label: \"5\", x: 600, y: 0},\n{id: 6, label: \"6\", x: 600, y: 200}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 3},\n{from: 3, to: 4},\n{from: 4, to: 5},\n{from: 4, to: 6},\n{from: 5, to: 6}\n]\n};  \n<Graph\ngraph={cutVerticesGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Connectivity", "Header 2": "k-Connected Graphs", "Header 3": "k-Vertex-Connected Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/connectivity.mdx"}, "page_content": "A graph is $k$-vertex-connected if it has at least $k+1$ vertices and at least $k$ vertices have to be removed to disconnect\nthe graph.  \nThe vertex connectivity of a graph $G$ is the largest $k$ such that $G$ is $k$-vertex-connected. So for example the graph\nbelow has a vertex connectivity of 2, because it is 2-vertex-connected. If we remove the vertices $4$ and $2$ the graph\nbecomes disconnected but if we only remove one vertex the graph stays connected.  \nexport const vertexConnectedGraph = {\nnodes: [\n{id: 1, label: \"1\", x: 0, y: 100},\n{id: 2, label: \"2\", value: 5, x: 200, y: 0, color: \"red\"},\n{id: 3, label: \"3\", x: 200, y: 200},\n{id: 4, label: \"4\", value: 5, x: 400, y: 100, color: \"red\"},\n{id: 5, label: \"5\", x: 600, y: 100},\n{id: 6, label: \"6\", x: 800, y: 200},\n{id: 7, label: \"7\", x: 800, y: 0},\n{id: 8, label: \"8\", x: 1000, y: 100}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 4},\n{from: 3, to: 4},\n{from: 4, to: 5},\n{from: 5, to: 6},\n{from: 5, to: 7},\n{from: 6, to: 8},\n{from: 7, to: 8},\n{from: 2, to: 7},\n]\n};  \n<Graph\ngraph={vertexConnectedGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Connectivity", "Header 2": "k-Connected Graphs", "Header 3": "k-Edge-Connected Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/connectivity.mdx"}, "page_content": "The same idea as for vertex connectivity also applies to edge connectivity. A graph is $k$-edge-connected if it has at\nleast $k+1$ vertices and at least $k$ edges have to be removed to disconnect the graph. So the graph below is 2-edge-connected\nand also has an edge connectivity of 2. If we remove the edges $(2,5)$ and $(4,5)$ the graph becomes disconnected.  \nexport const edgeConnectedGraph = {\nnodes: [\n{id: 1, label: \"1\", x: 0, y: 100},\n{id: 2, label: \"2\", x: 200, y: 0},\n{id: 3, label: \"3\", x: 200, y: 200},\n{id: 4, label: \"4\",  x: 400, y: 100},\n{id: 5, label: \"5\", x: 600, y: 100},\n{id: 6, label: \"6\", x: 800, y: 200},\n{id: 7, label: \"7\", x: 800, y: 0},\n{id: 8, label: \"8\", x: 1000, y: 100}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 4},\n{from: 3, to: 4},\n{from: 4, to: 5, color: \"red\", width: 5},\n{from: 5, to: 6},\n{from: 5, to: 7},\n{from: 6, to: 8},\n{from: 7, to: 8},\n{from: 2, to: 5, color: \"red\", width: 5},\n]\n};  \n<Graph\ngraph={edgeConnectedGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Eulerian Path", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/eulerianPath.mdx"}, "page_content": "<Callout type=\"todo\">\nSeven Bridges of Königsberg and the Eulerian Path\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "The idea of link prediction is as the name suggests to predict which link, i.e. edge will most likely be formed in the\nfuture for a given graph $G$. This is a very important problem in social network analysis and has many applications in\nthe real world such as in recommender systems to recommend friends, products, etc. to users.  \n<Callout type=\"todo\">\nAdd visual examples\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "Header 2": "Neighbourhood-based methods", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "There are many methods to predict links in a graph. One of the most basic methods is to use the neighbourhood of a node\nin some way to predict the links.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "Header 2": "Neighbourhood-based methods", "Header 3": "Common Neighbours", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "This method is based of the common neighbours metric of two nodes. The idea behind it is that if two nodes have many\ncommon neighbours, then they are more likely to be connected in the future. You can imagine this as a friend of a friend\nis more likely to be your friend than a complete stranger.  \nThe algorithm is very simple. For each pair of nodes $(u, v)$, we calculate the number of common neighbours $c$ and\nour prediction is the link/edge that corresponds to the pair with the highest $c$.  \nTo calculate the number of common neighbours, we can use the following formula:  \n$$\n\\text{commonNeighbours}(u, v) = |N[u] \\cap N[v]|\n$$  \nwhere $N[u]$ is the set of neighbours of node $u$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "Header 2": "Neighbourhood-based methods", "Header 3": "Jaccard Index", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "The jaccard index or also commonly known as the jaccard similarity coefficient is a measure of similarity between two\nsets and is used in many different applications. In computer vision for example, it is used to compare the similarity\nof two images but is there more commonly known as the intersection over union (IoU) metric, because that is what it is.  \n$$\n\\text{jaccardIndex}(u, v) = \\frac{|N[u] \\cap N[v]|}{|N[u] \\cup N[v]|}\n$$  \nWhen using the jaccard index for link prediction the idea is the same as with common neighbours. However, it takes into\naccount the size of the neighbourhoods of the two nodes. For example if two nodes have 100 neighbours and 5 of them are\ncommon, then they are less likely to be connected than two nodes with 10 neighbours and 5 of them are common.  \n$$\n\\frac{5}{100} < \\frac{5}{10}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "Header 2": "Neighbourhood-based methods", "Header 3": "Soundarajan-Hopcroft", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "If the community structure of a graph is known, i.e. which nodes belong to which community, then we can use this to\nimprove the common neighbours method for our link prediction. We do this by just adding the number of common neighbours\nthat are also in the same community to the common neighbours metric. So if two nodes have many common neighbours that\nare also in the same community, and another pair of nodes have many common neighbours but with less in the same\ncommunity, then the first pair is more likely to be connected in the future.  \n$$\n\\text{soundarajanHopcroft}(u, v) = |N[u] \\cap N[v]| + \\sum_{w \\in N[u] \\cap N[v]} f(w)\n$$  \nwhere $f(w)$ is a function that returns 1 if $u$ and $v$ are in the same community and 0 otherwise.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "Header 2": "Neighbourhood-based methods", "Header 3": "Resource Allocation Index", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "Imagine we have a resource for example a cake and there are three nodes, $x,y$ and $z$. $x$ and $y$ have the common\nneighbour $z$. If we want the cake to be shared between $x$ and $y$, then we can give it to $z$ and $z$ will then\nequally share it with its neighbours. So if $z$ has 10 neighbours, then $x$ and $y$ will each get $\\frac{1}{10}$ of\nthe cake. If $z$ has 100 neighbours, then $x$ and $y$ will each get $\\frac{1}{100}$ of the cake.  \nThe resource allocation index is based on this idea and is defined as follows:  \n$$\n\\text{resourceAllocationIndex}(u, v) = \\sum_{w \\in N[u] \\cap N[v]} \\frac{1}{deg(w)} = \\sum_{w \\in N[u] \\cap N[v]} \\frac{1}{|N[w]|}\n$$  \nThe resource allocation index can then be interpreted as a form of closeness between two nodes. We would then expect\nthat two nodes that are close to each other are more likely to be connected in the future.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "Header 2": "Neighbourhood-based methods", "Header 3": "Adamic-Adar Index", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "The Adamic-Adar index is very similar to the resource allocation index. The only difference is that Adamic-Adar index\nweakens the denominator by taking the natural logarithm of the number of neighbours of a common neighbour.  \n<Callout type=\"info\">\nWhy is this done? I have no idea. I have not found any explanation for this. If you know why, please let me know.\nIt just seems to make the metric more complicated for no reason and the results larger.\n</Callout>  \n$$\n\\text{adamicAdarIndex}(u, v) = \\sum_{w \\in N[u] \\cap N[v]} \\frac{1}{\\ln(deg(w))} = \\sum_{w \\in N[u] \\cap N[v]} \\frac{1}{\\ln(|N[w]|)}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "Header 2": "Neighbourhood-based methods", "Header 3": "Preferential Attachment", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "The preferential attachment method is based on the idea that nodes with a high degree are more likely to be effected\nby the addition of a new link than nodes with a low degree. The preferential attachment is defined as follows:  \n$$\n\\text{preferentialAttachment}(u, v) = deg(u) \\cdot deg(v) = |N[u]| \\cdot |N[v]|\n$$  \nSo if we have two nodes with a high degree, then the preferential attachment will be high and if we have two nodes with\na low degree, then the preferential attachment and therefore the likelihood of them being connected in the future will\nbe low.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Link Prediction", "Header 2": "The Link Prediction Problem", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/linkPrediction.mdx"}, "page_content": "In the research paper [The Link Prediction Problem for Social Networks](https://www.cs.cornell.edu/home/kleinber/link-pred.pdf)\nfrom 2004 an experiment was conducted to test the performance of the different link prediction methods. The experiment\nused a network containing publications and authors from different research fields.  \nThey had training networks from 1994 - 1996 and test networks from 1997 - 1999.  \nThe goal was to predict which authors would publish together in the future. For this they extract the core, which\ncontained the authors that published at least 3 papers in the timeframe of the training networks and 3 papers in the\ntimeframe of the test networks.  \nThey then used the different link prediction methods to predict which authors would publish together in the future but\nonly kept the highest predictions that connected 2 authors within the core. They then compared the predictions to the\nactual publications in the test networks core where the common neighbours method was the baseline.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Shortest Path", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/shortestPath.mdx"}, "page_content": "Before we see how to find the shortest path from vertex $a$ to vertex $b$ we need to define a few things.  \n$D(a,b)=$the length of the shortest path from vertex $a$ to vertex $b$. If no such path exists, the length is $\\infty$.  \nIf the graph is unweighted the length of a path is the number of edges it takes to get from $a$ to $b$. If however the graph is weighted the length is the sum of the edge weights.  \nSingle source shortest path (SSSP) are all the shortest paths from vertex $s$ to all other vertices.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Shortest Path", "Header 2": "For Unweighted Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/shortestPath.mdx"}, "page_content": "In an unweighted graph we can just use a BFS as this starts with all vertices with distance 1 then 2 etc.  \n```java\nvoid BFS(Vertex s) {\nQueue<Vertex<K>> R = new LinkedList<Vertex<K>>();\ns.dist = 0;\nR.add(s);\nwhile(!R.isempty()) {\nVertex v = R.remove();\nfor(Vertex w : v.adjList) {\nif (w.dist == Integer.MAX_VALUE) {\nw.dist = v.dist + 1;\nR.add(w);\n}\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Shortest Path", "Header 2": "For weighted graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/shortestPath.mdx"}, "page_content": "For a weighted graph this is slightly trickier as the shortest path isn't necessarily the path with the least edges.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Shortest Path", "Header 2": "For weighted graphs", "Header 3": "Dijkstra's algorithm", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/shortestPath.mdx"}, "page_content": "1. Assign all the vertices the attributes \"finished\", \"Distance and „Via/Predecessor“. Initialize the distance of the root vertex as 0 and all others as $\\infty$.\n2. While there are unvisited nodes. So finished=false and distance $\\leq \\infty$.\n1. Choose the vertex $v$ with the smallest distance.\n2. Set $v.finished = true$\n3. For all vertices $w$ that have and edge between $v$ and $w$\n1. Set int d = v.dist + edge weight between $v$ and $w$.\n2. if(d < w.dist) w.dist = d; w.via = v;  \nThe time complexity of this algorithm is as followed. For 1 While loop step 1 takes $O(n)$, step 2 takes $O(1)$, step 3 takes $O(outdeg(v))$  \nFor n While loops this the becomes $O(n^2 + m)$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Shortest Path", "Header 2": "For weighted graphs", "Header 3": "Improvements", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/shortestPath.mdx"}, "page_content": "We could save the vertices that are not finished in a Set so we don't have to look through the entire table. This however doesn't have an effect on the time complexity.  \nWe could save the vertices that are not finished in a Min-Heap. Init=$=(n)$ step 1 then becomes deleteMin() which is $O(log n)$ and step 3 becomes decreaseKey outdeg times O(log n). With these improvements our time complexity is O((n+m)log n).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "Vertex centrality measures can be used to determine the importance of a vertex in a graph. There are many different\nvertex centrality measures, each with their own advantages and disadvantages. In a communication network a vertex with\nhigh centrality is an actor that is important for the communication in the network, hence they are also often called\nactor centrality measures. An actor with high centrality can control the flow of information in the network for good or\nbad. They can also be used to determine key actors in a network, for example in a power grid it is important to know\nwhich vertices are key actors, because if they fail, the whole network fails.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Degree Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "The degree centrality is the simplest centrality measure. It is simply the number of edges connected to a vertex. The\ndegree centrality is a local measure, because it only takes into account the direct neighbors of a vertex. It can be\ncalculated using the $\\text{deg()}$ function. Or alternatively using the $\\text{indeg()}$ and $\\text{outdeg()}$\ndepending on whether the graph is directed or not and the use-case.  \nexport const vertexDegreeGraph = {\nnodes: [\n{id: 1, label: \"2\", x: 0, y: 0},\n{id: 2, label: \"2\", x: 0, y: 200},\n{id: 3, label: \"3\", x: 200, y: 100, color: \"red\"},\n{id: 4, label: \"2\", x: 400, y: 100},\n{id: 5, label: \"3\", x: 600, y: 100, color: \"red\"},\n{id: 6, label: \"2\", x: 800, y: 0},\n{id: 7, label: \"2\", x: 800, y: 200}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 3},\n{from: 3, to: 4},\n{from: 4, to: 5},\n{from: 5, to: 6},\n{from: 5, to: 7},\n{from: 6, to: 7}\n]\n};  \n<Graph\ngraph={vertexDegreeGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>  \nThe degree centrality can be normalized by dividing it by the maximum possible degree in the graph. This is rarely done\nin practice, because a lot of values will be small, and we are most often interested in the actual degree of a vertex.  \nThe interpretation of the degree centrality is pretty self-explanatory. And is closely related to the\n[prestige](#prestige) of a vertex.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Closeness Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "Unlike the degree centrality, the closeness centrality is a global measure, because it takes into account the whole\ngraph, however the consequence of this is that it is more expensive to calculate.  \n> The idea of the closeness centrality is that a vertex is important if it is close to the center of the graph. So a vertex\nis important if it is **close** to all other vertices in the graph, i.e. it is close to the center of the graph.  \nThis also means that a vertex can be important even if it only has one edge. As seen by the green vertex in the following graph.  \nexport const vertexDegreeProblemGraph = {\nnodes: [\n{id: 1, label: \"2\", x: 0, y: 0},\n{id: 2, label: \"2\", x: 0, y: 200},\n{id: 3, label: \"3\", x: 200, y: 100, color: \"red\"},\n{id: 4, label: \"2\", x: 400, y: 100},\n{id: 5, label: \"3\", x: 600, y: 100, color: \"red\"},\n{id: 6, label: \"2\", x: 800, y: 0},\n{id: 7, label: \"2\", x: 800, y: 200},\n{id: 8, label: \"1\", x: 400, y: 0, color: \"green\"}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 3},\n{from: 3, to: 4},\n{from: 4, to: 5},\n{from: 5, to: 6},\n{from: 5, to: 7},\n{from: 6, to: 7},\n{from: 4, to: 8},\n]\n};  \n<Graph\ngraph={vertexDegreeProblemGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>  \nThe closeness centrality for a vertex $v$ is calculated by taking the inverse distance of all shortest paths from the\nvertex $v$ to all other vertices in the graph. This can be interpreted as how efficiently can all the other vertices\nbe reached from $v$. The formula for the closeness centrality is as follows:  \n$$\n\\text{closenessCentrality}(v) = \\sum_{u \\in V \\setminus \\{v\\}}{d(v,u)^{-1}} = \\sum_{u \\in V \\setminus \\{v\\}}{\\frac{1}{d(v,u)}}\n$$  \nWhere $d(v,u)$ is the length of the shortest path from $v$ to $u$. Let us calculate the closeness centrality for the\ngreen vertex in the graph above.  \n$$\n\\begin{align*}\n1 + \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} &= \\frac{10}{3} \\\\\n\\frac{10}{3} \\cdot \\frac{1}{8-1} &= \\frac{10}{21} \\approx 0.476\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Closeness Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "green vertex in the graph above.  \n$$\n\\begin{align*}\n1 + \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} &= \\frac{10}{3} \\\\\n\\frac{10}{3} \\cdot \\frac{1}{8-1} &= \\frac{10}{21} \\approx 0.476\n\\end{align*}\n$$  \nTo normalize the closeness centrality, it can be divided by $|V| - 1$.  \nexport const vertexClosenessGraph = {\nnodes: [\n{id: 1, label: \"0.524\", x: 0, y: 0},\n{id: 2, label: \"0.524\", x: 0, y: 200},\n{id: 3, label: \"0.596\", x: 200, y: 100},\n{id: 4, label: \"0.714\", x: 400, y: 100, color: \"red\"},\n{id: 5, label: \"0.596\", x: 600, y: 100},\n{id: 6, label: \"0.524\", x: 800, y: 0},\n{id: 7, label: \"0.524\", x: 800, y: 200},\n{id: 8, label: \"0.476\", x: 400, y: 0, color: \"green\"}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 3},\n{from: 3, to: 4},\n{from: 4, to: 5},\n{from: 5, to: 6},\n{from: 5, to: 7},\n{from: 6, to: 7},\n{from: 4, to: 8},\n]\n};  \n<Graph\ngraph={vertexClosenessGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>  \n<Callout type=\"info\">\nThis gives different values to the formula from wikipedia and networkx. They use the following formula:  \n$$\n\\text{closenessCentrality}(v) = \\frac{1}{\\sum_{u \\in V \\setminus \\{v\\}}{d(v,u)}}\n$$  \nand for the normalized closeness centrality:  \n$$\n\\text{closenessCentrality}(v) = \\frac{|V| - 1}{\\sum_{u \\in V \\setminus \\{v\\}}{d(v,u)}}\n$$  \nwhere $d(v,u)$ is the length of the shortest path from $v$ to $u$.  \nThe issue with the above formula is that if no path exists between $v$ and $u$ then the distance is $\\infty$ which\nwould lead to the closeness centrality being $0$. This could be solved by just using 0 instead of $\\infty$ which would\nlead to the same result as the formula above because 1 divided by $\\infty$ is $0$, i.e. 0 is added to the sum.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Betweenness Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "In the above example using the degree centrality we saw that the green ones are the most important. However,\nwe can clearly visually see that the vertex inbetween them is the most important one as it connects the two communities.\nBecause of this we could say that that vertex is in Brokerage position or is a Broker/Gatekeeper of information.  \n<Image\nsrc=\"/cs/graphsDegreeCentralityIssue.png\"\nwidth={600}\n/>  \nThe betweenness centrality is a global measure that takes into account the whole graph and tries to solve the above\nissue.  \n> The idea of the betweenness centrality is that a vertex is important if a lot of shortest paths go through it, i.e. it is\n> **between** a lot of vertices.  \nTo calculate the betweenness centrality we need to calculate the number of shortest paths that go through a vertex $v$.\nSo for every pair of vertices $u$ and $w$ we need to calculate the shortest paths and then count how many of them go\nthrough $v$. The formula for the betweenness centrality is as follows:  \n$$\n\\text{betweennessCentrality}(v) = \\sum_{u \\neq v \\neq w}{\\frac{\\sigma_{uw}(v)}{\\sigma_{uw}}}\n$$  \nWhere $\\sigma_{uw}$ is the number of shortest paths from $u$ to $w$ and $\\sigma_{uw}(v)$ is the number of shortest paths\nfrom $u$ to $w$ that go through $v$.  \n<Callout type=\"info\">\nThe fraction in the formula leads to the weight being split if there are multiple shortest paths between $u$ and $w$.\n</Callout>  \nBecause the calculations for the betweenness centrality are quite complex and take a while to calculate, we will use a\nsmaller graph to calculate the betweenness centrality.  \n<Callout type=\"todo\">\nmake this more algorithmic and use the pictures from the script.\n</Callout>  \n<Steps>\n<Step id={1}>\nWe start with all betweenness centralities being $0$. We start with the first vertex on the left and mark it green.\n</Step>\n<Step id={2}>\nWe then calculate the shortest path to the next one in a BFS manner. The vertex to the right is the next one so we", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Betweenness Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "We start with all betweenness centralities being $0$. We start with the first vertex on the left and mark it green.\n</Step>\n<Step id={2}>\nWe then calculate the shortest path to the next one in a BFS manner. The vertex to the right is the next one so we\nmark it green as the target vertex. Because it is directly connected to the other green one nothing changes. Now\nthat we have visited it we mark it gray.\n</Step>\n<Step id={3}>\nWe take the next vertex, the one above and mark it green. We then calculate the shortest path between the two green\nvertices. There is only one shortest path going over the previously visited gray vertex. So we add $1$ to that gray\nvertexes betweenness centrality.\n</Step>\n<Step id={4}>\nWe continue this process until we have visited all vertices once. We then mark the initial vertex on the left as red.\nAll shortest paths that start at this vertex have been calculated. We then pick a new start vertex in a BFS manner.\nRepeat the process until all shortest paths have been calculated.\n</Step>\n</Steps>  \nexport const vertexBetweennessGraph = {\nnodes: [\n{id: 1, label: \"0\", x: 0, y: 200},\n{id: 2, label: \"3\", x: 200, y: 200},\n{id: 3, label: \"1\", x: 400, y: 0},\n{id: 4, label: \"1\", x: 400, y: 400},\n{id: 5, label: \"0\", x: 600, y: 200},\n],\nedges: [\n{from: 1, to: 2},\n{from: 2, to: 3},\n{from: 2, to: 4},\n{from: 3, to: 4},\n{from: 3, to: 5},\n{from: 4, to: 5},\n]\n};  \n<Graph\ngraph={vertexBetweennessGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>  \nTo normalize the betweenness centrality, you devide by the centrality by following:  \n- For an undirected graph: $\\frac{(n-1)(n-2)}{2}$\n- For a directed graph: $(n-1)(n-2)$  \nThe Image below summarizes all the centrality measures we have seen so far and compares the most central vertices.  \n<Image\nsrc=\"/cs/graphsBetweennessCentrality.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Eigenvector Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "Before I start explaining the eigenvector centrality, I describe what an eigenvector is. An eigenvector is a\nvector that does not change its direction when multiplied by a square matrix, only its magnitude changes, i.e. it is only\nscaled. Because a matrix can have multiple eigenvectors, the solution is to allow for only eigenvectors with a magnitude of\n1, i.e. $||\\boldsymbol{v}||_2 = 1$, i.e. the normalized eigenvector. The scaling factor is then called the eigenvalue,\ndenoted by $\\lambda$. The formula for the eigenvector is as follows:  \n$$\n\\boldsymbol{Av}=\\lambda \\boldsymbol{v}\n$$  \nThe eigenvector centrality is the eigenvector corresponding to the largest eigenvalue of the adjacency matrix of the\ngraph. The eigenvector corresponding to the largest eigenvalue is also commonly called the dominant eigenvalue/vector.\nThis can just be calculated but is most often calculated using the power iteration method.  \nThe eigenvector centrality is an interesting centrality measure.  \n> The idea is that a node is important if its neighbors are important.  \nWhat makes a vertex important could be any attribute of the vertex, for example if we have\na network of people, their salary. However, the simplest and most commonly used approach is to use the degree\ncentrality as the importance measure. In an undirected graph most commonly the in-degree centrality.  \nTo show the idea that the eigenvector centrality is based on the importance of the neighbors, I will use the following\ngraph and calculate the eigenvector centrality using the degree centrality as the importance measure with the power\niteration method.  \n<Image\nsrc=\"/cs/graphsEigenCentralityMatrix.png\"\ncaption=\"The adjacency matrix of the graph, mirrored because it is undirected.\"\nwidth={400}\n/>  \n<Image\nsrc=\"/cs/graphsEigenCentralityVector.png\"\ncaption=\"The initial importance measure, the degree of each vertex.\"\nwidth={400}\n/>  \n#### Power Iteration Method", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Eigenvector Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "width={400}\n/>  \n<Image\nsrc=\"/cs/graphsEigenCentralityVector.png\"\ncaption=\"The initial importance measure, the degree of each vertex.\"\nwidth={400}\n/>  \n#### Power Iteration Method  \nThe power iteration method is a simple iterative method to calculate the eigenvector corresponding to the largest eigenvalue.  \nThe idea is to start with an initial vector $\\boldsymbol{b_0}$ and then multiply it with the adjacency matrix $\\boldsymbol{A}$.\nThen we normalize the resulting vector $\\boldsymbol{b_1}$ and repeat the process until the vector converges. Most often to\ncheck for convergence we calculate the difference between the two vectors and check if it is smaller than a threshold.  \n$$\n\\boldsymbol{b_{i+1}} = \\frac{\\boldsymbol{Ab_i}}{||\\boldsymbol{Ab_i}||_2}\n$$  \n<Callout type=\"info\">\nThe initial vector $b_0$ in the power iteration method is the importance measure, in this case the degree centrality. However,\nthe initial vector can be any non-zero vector and the method will still converge to the same eigenvector. You could interpret\nthis as the eigenvector centrality being the \"true underlying importance\" of the vertices.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "PageRank", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "<Callout type=\"todo\">\nDo this\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Prestige", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "In a directed Graph it is possible to analyze the prestige of a vertex, i.e the stature or reputation associated with\na vertex. The vertices relationships however need to resemble this. For example, if a person has a lot of followers\nbut doesn't follow a lot of people, then that person has a high prestige and stature, for example a celebrity.  \n#### Popularity  \nThe simplest way to measure prestige is to count the number of incoming edges, i.e using the $\\text{indeg()}$ function.\nThis is called popularity.  \nexport const localGraph = {\nnodes: [\n{id: 1, label: \"Bob, 1\"},\n{id: 2, label: \"Alice, 2\"},\n{id: 3, label: \"Michael, 4\", color: \"red\"},\n{id: 4, label: \"Urs, 2\"},\n{id: 5, label: \"Karen, 3\"},\n{id: 6, label: \"John, 2\"},\n{id: 7, label: \"Peter, 2\"},\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 1, to: 4},\n{from: 1, to: 5},\n{from: 2, to: 5},\n{from: 2, to: 6},\n{from: 2, to: 3},\n{from: 3, to: 4},\n{from: 3, to: 5},\n{from: 3, to: 6},\n{from: 3, to: 7},\n{from: 5, to: 1},\n{from: 5, to: 2},\n{from: 6, to: 3},\n{from: 6, to: 7},\n{from: 7, to: 3},\n],\n};  \n<Graph\ngraph={localGraph}\ndirected={true}\n/>  \n#### Proximity Prestige  \nThe proximity prestige measure does not just account for the number of directly incoming edges, but also the number of\nindirectly incoming edges, i.e. the number of paths that lead to the vertex. However, the longer the path, the lower\nprestige from that path is weighted.  \nSimply put the proximity prestige is the sum of all paths that lead to the vertex weighted by the length of the path.  \nThe formula for the proximity prestige can be summarized pretty simply:  \n> The proximity prestige of a vertex is the number of vertices that have a path to the vertex divided by the average\nshortest path length leading to the vertex.  \nMore formally:  \n$$\n\\text{proximityPrestige}(v) = \\frac{\\frac{|I|}{n-1}}{\\frac{\\sum_{i \\in I}{d(i,v)}}{|I|}}\n$$  \nWhere $I$ is the set of all vertices that have a path to $v$ and $d(u,v)$ is the length of the shortest path from $u$ to\n$v$.  \n<Callout type=\"example\">\n<Image", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Vertex Centrality", "Header 3": "Prestige", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "\\text{proximityPrestige}(v) = \\frac{\\frac{|I|}{n-1}}{\\frac{\\sum_{i \\in I}{d(i,v)}}{|I|}}\n$$  \nWhere $I$ is the set of all vertices that have a path to $v$ and $d(u,v)$ is the length of the shortest path from $u$ to\n$v$.  \n<Callout type=\"example\">\n<Image\nsrc=\"/cs/graphsProximityPrestigeInput.png\"\ncaption=\"The input graph for the proximity prestige.\"\nwidth={500}\n/>\n<Image\nsrc=\"/cs/graphsProximityPrestigeOutput.png\"\ncaption=\"The resulting proximity prestige for each vertex.\"\nwidth={500}\n/>  \n$$\n\\begin{align*}\n\\text{proximityPrestige}(2) &= \\frac{\\frac{1}{(8-1)}}{\\frac{1}{1}} = 0.14 \\\\\n\\text{proximityPrestige}(4) &= \\frac{\\frac{2}{(8-1)}}{\\frac{2}{2}} = 0.29 \\\\\n\\text{proximityPrestige}(6) &= \\frac{\\frac{7}{(8-1)}}{\\frac{10}{7}} = 0.7 \\\\\n\\end{align*}\n$$  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Group Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "The goal of group centrality measures is to determine the importance of a group of vertices in a graph. These measures\nare based on the vertex centrality measures, but they are more complex and expensive to calculate.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Group Centrality", "Header 3": "Degree Group Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "The degree group centrality is the simplest group centrality measure. It is simply the fraction of the number of\nvertices outside the group that are directly connected to the group. So in the following graph with the group $G$ being\ndefined as $G={v_6,v_7,v_8}$ the degree group centrality would be $\\frac{3}{10}$ so $0.3$.  \n<Image\nsrc=\"/cs/graphsDegreeGroupCentrality.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Group Centrality", "Header 3": "Closeness Group Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "The closeness group centrality measures how close the group is to the other vertices in the graph. It is calculted by\nadding up all inverse distances from the vertices outside the group to the closest vertex in the group. So in the\nsame graph and group $G={v_6,v_7,v_8}$ as above the closeness group centrality would be:  \n$$\n1+1+1+\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{3} = 6.333\n$$  \nIt can be simply normalized by dividing it by the number of vertices outside the group, which would lead to $0.6333.$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Group Centrality", "Header 3": "Betweenness Group Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "The betweenness group centrality measures how many shortest paths go through the group. It is calculated by counting\nhow many shortest paths between all the vertices outside the group go through the group.  \n<Callout type=\"example\">\nIf we define our group to contain the vertices $C,E$ from the graph below we can calculate the betweenness group\ncentrality simply by calculating all the shortest paths between the vertices outside the group and counting how many\nof them go through the group.  \n<Image\nsrc=\"/cs/graphsBetweenessGroupCentrality.png\"\ncaption=\"Our graph where the group we are inspecting contains the vertices C and E.\"\nwidth={400}\n/>  \nWe have the following shortest paths between the vertices outside the group:  \n- $A \\rightarrow B$\n- $A \\rightarrow C \\rightarrow D$ goes through the group via $C$.\n- $A \\rightarrow C \\rightarrow D \\rightarrow E \\rightarrow G$ goes through the group via $C$ and $E$.\n- $A \\rightarrow C \\rightarrow D \\rightarrow E \\rightarrow F$ goes through the group via $C$ and $E$.\n- $B \\rightarrow C \\rightarrow D$, goes through the group via $C$.\n- $B \\rightarrow C \\rightarrow D \\rightarrow E \\rightarrow F$ goes through the group via $C$ and $E$.\n- $B \\rightarrow C \\rightarrow D \\rightarrow E \\rightarrow G$ goes through the group via $C$ and $E$.\n- $D \\rightarrow E \\rightarrow G$ goes through the group via $E$.\n- $D \\rightarrow E \\rightarrow F$ goes through the group via $E$.\n- $F \\rightarrow G$  \nTherefore 8 of the 10 shortest paths go through the group, so the betweenness group centrality is $\\frac{8}{10} = 0.8$.  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Network Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "The idea of network centrality is to measure the centrality of the entire network, i.e. to compare the difference in\ncentrality between the vertices in the network. The goal is then to show how different the key vertices are from the\nrest of the network.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Network Centrality", "Header 3": "General Network Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "To calculate the network centrality the vertex centrality measures are used. For this Linton Freeman defined a general\nformula that returns a value between $0$ and $1$ with the following meanings:  \n- $0$ means that all vertices have the same centrality, i.e. the network is a ring network.\n- $1$ means that one vertex has all the centrality, i.e. the network is a star network.  \nexport const starGraph = {\nnodes: [\n{id: 1, label: \"1\"},\n{id: 2, label: \"2\"},\n{id: 3, label: \"3\"},\n{id: 4, label: \"4\"},\n{id: 5, label: \"5\"},\n{id: 6, label: \"6\"},\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 1, to: 4},\n{from: 1, to: 5},\n{from: 1, to: 6},\n],\n};  \nexport const ringGraph = {\nnodes: [\n{id: 1, label: \"1\"},\n{id: 2, label: \"2\"},\n{id: 3, label: \"3\"},\n{id: 4, label: \"4\"},\n{id: 5, label: \"5\"},\n{id: 6, label: \"6\"},\n],\nedges: [\n{from: 1, to: 2},\n{from: 2, to: 3},\n{from: 3, to: 4},\n{from: 4, to: 5},\n{from: 5, to: 6},\n{from: 6, to: 1},\n],\n};  \n<SideBySideBlock>\n<Block>\n<Caption caption=\"Star Network\">\n<Graph\ngraph={starGraph}\n/>\n</Caption>\n</Block>\n<Block>\n<Caption caption=\"Ring Network\">\n<Graph\ngraph={ringGraph}\n/>\n</Caption>\n</Block>\n</SideBySideBlock>  \nThe formula is as follows:  \n$$\n\\text{networkCentrality}(G) = \\frac{\\sum_{v \\in V}{C_{max} - C(v)}}{Star_n}\n$$  \nWhere:\n- $C(v)$ is the centrality function for a vertex $v$.\n- $C_{max}$ is the maximum centrality of all vertices in the graph, i.e $ C_{max}= \\argmax_{v \\in V}{C(v)}$.\n- The denominator $Star_n$ is the maximal sum of differences between\nthe centrality of a vertex and the maximum centrality of all vertices in the graph, i.e. if the graph was a star graph\nwith the same amount of vertices as the graph $G$, so $n=|V|$ (Is this always the case, no matter the centrality measure?).  \nWith the definition above it is now logical why the value is $1$ when the graph is a star graph because the numerator and\ndenominator are the same. Whereas if the graph is a ring graph, i.e. all vertices have the same centrality, then the", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Network Centrality", "Header 3": "General Network Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "With the definition above it is now logical why the value is $1$ when the graph is a star graph because the numerator and\ndenominator are the same. Whereas if the graph is a ring graph, i.e. all vertices have the same centrality, then the\nsum of differences in the numerator is $0$ and the denominator is the maximum sum of differences, which leads to the\nvalue being $0$.  \n<Callout type=\"warning\">\nDepending on the definition of the general formula the Sum in the nominator skips the vertex with the maximum\ncentrality since the difference would be $0$. I find the definition above more intuitive, but it is important to\nknow that there are different definitions.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Network Centrality", "Header 3": "Degree Network Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "For the degree network centrality the denominator is pretty simple, because for a star graph the key vertex will have a\ndegree of $n-1$ and the other vertices will have a degree of $1$. So the denominator is simply $(n-1)(n-2)$ for an\nundirected Graph, if it is a directed Graph then the nominator can just be doubled.  \nIf you are working with the normalized degree centrality, then the denominator can be even further simplified to just\n$n-2$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Network Centrality", "Header 3": "Closeness Network Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "When using the normalized closeness centrality, the denominator is simply $\\frac{n-2}{2}$. I will save you the details\njust trust me bro.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Centrality", "Header 2": "Network Centrality", "Header 3": "Betweenness Network Centrality", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/centrality.mdx"}, "page_content": "When using the normalized betweenness centrality, the denominator is simply $n-1$, just like with the degree centrality.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Network Reduction", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/networkReduction.mdx"}, "page_content": "Often times when working with big networks such as social networks there is too much data to be able to visualize it all\nat once or analyze. To make it easier to work with the data, we can either create a specific view of the data like when\nworking with database tables, or we can use sampling to take samples of the data.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Network Reduction", "Header 2": "Views", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/networkReduction.mdx"}, "page_content": "To visualize the different views we will use the following network as an example, where the different colors represent\ndifferent departments in a company:  \n- Blue: IT department\n- Red: HR department\n- Green: Management department  \nexport const departmentGraph = {\nnodes: [\n{id: 1, label: \"Bob\", color: \"blue\"},\n{id: 2, label: \"Alice\", color: \"blue\"},\n{id: 3, label: \"Michael\", color: \"blue\"},\n{id: 4, label: \"Urs\", color: \"blue\"},\n{id: 5, label: \"Karen\", color: \"blue\"},\n{id: 6, label: \"David\", color: \"green\"},\n{id: 7, label: \"Emily\", color: \"green\"},\n{id: 8, label: \"Linda\", color: \"red\"},\n{id: 9, label: \"John\", color: \"red\"},\n],\nedges: [\n// Edges within the IT department (blue)\n{from: 1, to: 2},\n{from: 1, to: 4},\n{from: 1, to: 5},\n{from: 2, to: 5},\n{from: 5, to: 1},\n{from: 1, to: 3},\n{from: 3, to: 4},\n{from: 3, to: 5},\n// Edges within the HR department (green)\n{from: 6, to: 7},\n{from: 7, to: 6},\n// Edges within the Management department (red)\n{from: 8, to: 9},\n// Edges connecting different departments\n{from: 2, to: 6},\n{from: 5, to: 9},\n{from: 4, to: 8},\n],\n};  \n<Graph\ngraph={departmentGraph}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Network Reduction", "Header 2": "Views", "Header 3": "Local View", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/networkReduction.mdx"}, "page_content": "The local view focuses on a specific group of nodes and their connections. So the local view is a subset of the entire\nnetwork that has been selected based on some criteria. For example if we have a network of people in a company, we can\ncreate a local view of the network that only contains people that are in the same department to analyze how they\ncommunicate with each other.  \nBelow you can see the local view of the IT department:  \nexport const localGraph = {\nnodes: [\n{id: 1, label: \"Bob\", color: \"blue\"},\n{id: 2, label: \"Alice\", color: \"blue\"},\n{id: 3, label: \"Michael\", color: \"blue\"},\n{id: 4, label: \"Urs\", color: \"blue\"},\n{id: 5, label: \"Karen\", color: \"blue\"},\n],\nedges: [\n// Edges within the IT department (blue)\n{from: 1, to: 2},\n{from: 1, to: 4},\n{from: 1, to: 5},\n{from: 2, to: 5},\n{from: 5, to: 1},\n{from: 1, to: 3},\n{from: 3, to: 4},\n{from: 3, to: 5},\n],\n};  \n<Graph\ngraph={localGraph}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Network Reduction", "Header 2": "Views", "Header 3": "Global View", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/networkReduction.mdx"}, "page_content": "The global view allows for a general view of the entire network. Here we summarize nodes to a single node based on some\ncriteria. For example if we have a network of people in a company, we can create a global view of the network that\nsummarizes all the people in the same department to a single node. This allows us to see how the different departments\ncommunicate with each other.  \nBelow you can see the global view of the network (whether it is a good thing that HR an Management don't communicate\ndirectly is up for debate).  \nexport const globalGraph = {\nnodes: [\n{id: 1, label: \"IT\", color: \"blue\"},\n{id: 2, label: \"HR\", color: \"red\"},\n{id: 3, label: \"Management\", color: \"green\"},\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n],\n};  \n<Graph\ngraph={globalGraph}\noptions={\n{\nnodes: {\nshape: \"dot\",\nfont: {\ncolor: \"#FFFFFF\",\n}\n},\n}\n}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Network Reduction", "Header 2": "Views", "Header 3": "Context View", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/networkReduction.mdx"}, "page_content": "The context view is a combination of the local and global view. First we summarize the network to a global view. Then\nwe pick a node in the global view and expand it again. If we have our previous example of a network of people in a\ncompany, we can create a context view of the network that summarizes all the people in the same department to a single\nnode. Then we pick a department and expand it again to see how the people in that department communicate with the other\ndepartments.  \nFrom the graph below we could assume that Alice is the team lead of the IT department, since she is the one that talks\nto management.  \nexport const contextGraph = {\nnodes: [\n{id: 1, label: \"Bob\", color: \"blue\"},\n{id: 2, label: \"Alice\", color: \"blue\"},\n{id: 3, label: \"Michael\", color: \"blue\"},\n{id: 4, label: \"Urs\", color: \"blue\"},\n{id: 5, label: \"Karen\", color: \"blue\"},\n{id: 6, label: \"HR\", color: \"red\"},\n{id: 7, label: \"Management\", color: \"green\"},\n],\nedges: [\n// Edges within the IT department (blue)\n{from: 1, to: 2},\n{from: 1, to: 4},\n{from: 1, to: 5},\n{from: 2, to: 5},\n{from: 5, to: 1},\n{from: 1, to: 3},\n{from: 3, to: 4},\n{from: 3, to: 5},\n// Edges connecting different departments\n{from: 2, to: 7},\n{from: 5, to: 6},\n{from: 4, to: 6},\n],\n};  \n<Graph\ngraph={contextGraph}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Network Reduction", "Header 2": "Views", "Header 3": "Ego View", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/networkReduction.mdx"}, "page_content": "Ego/node/focus view is a view of the network that is centered around a specific node. In this view, the selected node\nis the \"ego,\" and its immediate connections, i.e. its neighbours are analyzed.  \nexport const egoGraph = {\nnodes: [\n{id: 1, label: \"Bob\", color: \"blue\"},\n{id: 2, label: \"Alice\", color: \"blue\"},\n{id: 3, label: \"Michael\", color: \"blue\"},\n{id: 5, label: \"Karen\", color: \"blue\"},\n{id: 9, label: \"John\", color: \"red\"},\n],\nedges: [\n{from: 1, to: 5},\n{from: 2, to: 5},\n{from: 5, to: 1},\n{from: 3, to: 5},\n{from: 5, to: 9},\n],\n};  \n<Graph\ngraph={egoGraph}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Network Reduction", "Header 2": "Views", "Header 3": "Filtering Edges", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/networkReduction.mdx"}, "page_content": "Another common method is remove edges from the network based on some criteria. For example if we have a network of\npeople in a company, we can remove all edges that are not between people in the same department to analyze how each\ndepartment communicates within.  \nOr if we had a network with weights on the edges, we could remove all edges that have a weight below a certain threshold.  \nexport const filteredGraph = {\nnodes: [\n{id: 1, label: \"Bob\", color: \"blue\"},\n{id: 2, label: \"Alice\", color: \"blue\"},\n{id: 3, label: \"Michael\", color: \"blue\"},\n{id: 4, label: \"Urs\", color: \"blue\"},\n{id: 5, label: \"Karen\", color: \"blue\"},\n{id: 6, label: \"David\", color: \"green\"},\n{id: 7, label: \"Emily\", color: \"green\"},\n{id: 8, label: \"Linda\", color: \"red\"},\n{id: 9, label: \"John\", color: \"red\"},\n],\nedges: [\n// Edges within the IT department (blue)\n{from: 1, to: 2},\n{from: 1, to: 4},\n{from: 1, to: 5},\n{from: 2, to: 5},\n{from: 5, to: 1},\n{from: 1, to: 3},\n{from: 3, to: 4},\n{from: 3, to: 5},\n// Edges within the HR department (green)\n{from: 6, to: 7},\n{from: 7, to: 6},\n// Edges within the Management department (red)\n{from: 8, to: 9},\n],\n};  \n<Graph\ngraph={filteredGraph}\n/>  \n#### Inter and Intra-Edges  \nA form of filtering edges is to reduce a network down to its inter or intra-edges.  \nInter-edges can be defined as the edges that connect vertices between two different groups or communities and intra-edges\nconnect vertices within a group.  \nSo if for example we have a graph containing people in a company, we can group them by their gender. Then we can for\nexample only look at the edges between same gendered people (intra-edges) or between different gendered people\n(inter-edges).  \n<Callout type=\"todo\">\nAdd example\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Network Reduction", "Header 2": "Sampling", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/networkReduction.mdx"}, "page_content": "Sampling is the process of taking a subset of the data and working with that instead of the entire network.  \n<Callout type=\"todo\">\nThis is probably more general and doesn't need to be in this section\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "Communities are subgraphs (subsets or groups of vertices of the original graph), that are better connected to each\nother than to the rest of the graph. Communities are very important when analyzing social networks and networks in\ngeneral as they often form around a context or a topic such as family, friends, work, hobbies, etc.  \nThese communities can then be further analyzed such as to find out who are the most important people in a community,\nwhat is there impact on the community, and how do they relate to other communities.  \n<Image\nsrc=\"/cs/graphsCommunities.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Neighborhoods", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "The neighborhood of a vertex $v$ is the set of all vertices that are connected to $v$ by an edge, and is denoted by\n$N(v)$ or $N_G(v)$ if the graph is not ambiguous. The neighborhood of a vertex is also sometimes referred to as the open\nneighborhood when it does not include the vertex itself $v$, and the closed neighborhood when it does include the vertex\nitself. The default is the open neighborhood, whereas the closed neighborhood is denoted by $N[v]$ or $N_G[v]$.  \nexport const neighborhoodGraph = {\nnodes: [\n{id: 1, label: \"a\", x: 0, y: 0, color: \"green\"},\n{id: 2, label: \"b\", x: 0, y: 200, color: \"green\"},\n{id: 3, label: \"c\", x: 200, y: 100, color: \"red\"},\n{id: 4, label: \"d\", x: 400, y: 100, color: \"green\"},\n{id: 5, label: \"e\", x: 600, y: 100},\n{id: 6, label: \"f\", x: 800, y: 0},\n{id: 7, label: \"g\", x: 800, y: 200}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 3},\n{from: 3, to: 4},\n{from: 4, to: 5},\n{from: 5, to: 6},\n{from: 5, to: 7},\n{from: 6, to: 7}\n]\n};  \n<MdxCaption>\n<Graph\ngraph={neighborhoodGraph}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>\nFor the given Graph $G$ and the vertex $c$, the neighborhood $N[c]$ is the set of vertices $\\{a, b, d\\}$.\n</MdxCaption>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Connected Components", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "Simply put a connected component is a subgraph of the original graph where all vertices are connected to each other. So\nThere are no disconnected vertices in a connected component. These can quiet easily be seen by eye but the definition\ncan become more complex when we look at directed graphs.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Connected Components", "Header 3": "Undirected Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "In an undirected graph, a connected component is a subset of vertices such that there is a path between every pair of\nvertices in the subset. In other words, a connected component is a subgraph of the original graph where all vertices\nare connected to each other.  \nThis could be useful for example to find out if a graph is fully connected or not. If the graph has only one connected\ncomponent, then it is fully connected. If it has more than one connected component, then it is not fully connected.  \nIf we think of a communication network, then a connected component would be a group of people that can communicate with\neach other. If there are multiple connected components, then there are groups of people that cannot communicate with\neach other.  \n<Image\nsrc=\"/cs/graphsConnectedComponents.png\"\nwidth={600}\n/>  \nTo find the connected components of a graph, we can simply use either a breadth-first search or a depth-first search\nover all vertices. The algorithm would then look something like this:  \n<Image\nsrc=\"/cs/graphsConnectedComponentsDFS.png\"\nwidth={600}\ncaption=\"Finding the number of connected components using a depth-first search.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Connected Components", "Header 3": "Directed Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "In a directed graph the directions of the edges matter. This gives us two types of connected components, weakly\nconnected components and strongly connected components.  \n#### Weakly Connected Components  \nWeakly connected components are the same as connected components in an undirected graphs. So you just ignore the\ndirections of the edges.  \n<Image\nsrc=\"/cs/graphsWeaklyConnectedComponents.png\"\n/>  \n#### Strongly Connected Components  \nStrongly connected components are a bit more complex. In a directed graph, a strongly connected component is a subset of\nvertices such that there is a path between every pair of vertices in the subset, but the path must follow the direction\nof the edges.  \n<Image\nsrc=\"/cs/graphsStronglyConnectedComponents.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Connected Components", "Header 3": "Giant Components", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "If a connected component includes a large portion of the graph, then it is commonly referred to as a\n**\"giant component\"**. There is no strict definition of what a giant component is, but it is commonly used to refer to\nconnected components that include more than 50% of the vertices in the graph.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Cliques", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "Cliques focus on undirected graphs. A clique is a complete subgraph of the original graph, i.e a subgraph where all\nvertices are connected to each other. Cliques are very important in social networks as they represent groups of people\nthat all know each other, however in a communication network they would represent a group with redundant connections.  \nBecause cliques are complete subgraphs, they are very easy to see but also happen to be very rare and hard to find\nalgorithmically. In the graph below the two cliques have been highlighted in red and blue.  \n<Image\nsrc=\"/cs/graphsCliques.png\"\n/>  \n<Callout type=\"todo\">\nAlogrithms seem to be seperated in finding a maximal clique or finding cliques of a certain size.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Cliques", "Header 3": "Clustering Coefficient", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "The clustering coefficient is a metric that measures how close a graph is to being a clique (don't ask me why it isn't\ncalled Clique Coefficient). There are two different versions of the clustering coefficient, the global clustering\ncoefficient and the local clustering coefficient, where the global clustering coefficient is just the average of the\nlocal clustering coefficients.  \nThe idea behind the local clustering coefficient is to check how many of the neighbors of a vertex are connected to each\nother. If all neighbors are connected to each other, then the local clustering coefficient for that vertex is $1$. More\nformally, the local clustering coefficient for a vertex $v$ is defined as:  \n$$\n\\text{localClusterCoeff}(v) = \\frac{2 \\cdot \\text{numEdgesBetweenNeighbors}(v)}{|N(v)| \\cdot (|N(v)| - 1)}\n$$  \nwhere $N(v)$ denotes the set of neighbors of $v$.  \nexport const clusterCoeff1 = {\nnodes: [\n{id: 1, label: \"a\", x: 0, y: 100, color: \"red\"},\n{id: 2, label: \"b\", x: 100, y: 200, color: \"green\"},\n{id: 3, label: \"c\", x: 200, y: 100, color: \"green\"},\n{id: 4, label: \"d\", x: 100, y: 0, color: \"green\"},\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 1, to: 4},\n{from: 2, to: 3},\n{from: 2, to: 4},\n{from: 3, to: 4},\n]\n};  \nexport const clusterCoeff0 = {\nnodes: [\n{id: 1, label: \"a\", x: 0, y: 100, color: \"red\"},\n{id: 2, label: \"b\", x: 100, y: 200, color: \"green\"},\n{id: 3, label: \"c\", x: 200, y: 100, color: \"green\"},\n{id: 4, label: \"d\", x: 100, y: 0, color: \"green\"},\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 1, to: 4},\n]\n};  \n<SideBySideBlock>\n<Block>\n<MdxCaption>\n<Graph\ngraph={clusterCoeff1}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>\nFor the given Graph $G$ and the vertex $a$, the cluster coefficient is $1$ because all neighbors are connected.\n</MdxCaption>\n</Block>\n<Block>\n<MdxCaption>\n<Graph\ngraph={clusterCoeff0}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>\nFor the given Graph $G$ and the vertex $a$, the cluster coefficient is $0$ because none of the neighbors are\nconnected.\n</MdxCaption>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Cliques", "Header 3": "Clustering Coefficient", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "</MdxCaption>\n</Block>\n<Block>\n<MdxCaption>\n<Graph\ngraph={clusterCoeff0}\noptions={\n{\nphysics: {\nenabled: false\n}\n}\n}\n/>\nFor the given Graph $G$ and the vertex $a$, the cluster coefficient is $0$ because none of the neighbors are\nconnected.\n</MdxCaption>\n</Block>\n</SideBySideBlock>  \nThe global clustering coefficient is then just the average of the local clustering coefficients of all vertices in the\ngraph.  \n$$\n\\text{globalClusterCoeff}(G) = \\frac{1}{|V|} \\sum_{v \\in V} \\text{localClusterCoeff}(v)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Cliques", "Header 3": "k-Core", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "For a k-Core the rules of a clique are slightly relaxed. A k-Core is a subgraph where all vertices are at least connected\nto $k$ other vertices in the subgraph.  \n<Image\nsrc=\"/cs/graphsKCore.png\"\ncaption=\"Example of differen k-Cores for the same graph.\"\n/>  \nAlthough this is a relaxation of the rules, it is still a very strict rule and\ncan lead to vertices that don't fulfill the $k$ connections but are only connected to other vertices in a core to not\nbe included in the core.  \n<Callout type=\"todo\">\nDegeneracy of a graph and k-degenerate graphs is exactly this\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Cliques", "Header 3": "p-Clique", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "The idea of a p-clique is also to relax the rules of a clique whilst also solving the above-mentioned issue of the\nk-core. In a p-clique, the p stands for a percentage in decimal i.e. a ratio. So in a p-clique at least the given\npercentage of edges of a vertices must be connected to other vertices in the subgraph.  \nSo if we have a 0.5-clique, then at least 50% of the edges of a vertex must be connected to other vertices in the subgraph. This\nthen allows for the vertices that don't fulfill the rule to be included in the subgraph for a k-core but are only\nconnected to other vertices in the subgraph to be included in the subgraph.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Cliques", "Header 3": "n-Clique", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "<Callout type=\"warning\">\nSometimes cliques are named after the number of vertices they contain. For example a clique with 3 vertices is\ncalled a 3-clique, a clique with 4 vertices is called a 4-clique, etc. this can be generalized to a k-clique. Not an\nn-clique though, that is something else, but when it just says 4-clique it can be ambiguous.\n</Callout>  \nThe idea of an n-clique is that we want a maximal subgraph, i.e. with the most vertices, where each pair of vertices can\nbe connected by a path of length at most n. So a 1-clique is just a normal clique, a 2-clique is a clique where each\npair of vertices can be connected by a path of length at most 2, etc.  \n<Callout type=\"warning\">\nThe path doesn't have to be the shortest path, just a path of length at most n. And the path can go over any vertex,\nnot just vertices that are part of the clique.\n</Callout>  \nThis can lead to two interesting scenarios:  \n1. The diameter of the subgraph can actually be longer then n. This is due to the path being able to go over any vertex,\nnot just vertices that are part of the clique. So in the example below, the diameter of the subgraph is 3 even though it\nis a 2-clique.  \n<Image\nsrc=\"/cs/graphsNCliqueDiameter.png\"\nwidth={300}\n/>  \n2. The subgraph can be disconnected. In the example below you can see two possible 2-cliques of many for the given graph.\nInterestingly, they are both disconnected, because if one of the vertices inbetween is included, then a different vertex\ncan no longer be included.  \n<Image\nsrc=\"/cs/graphsNCliqueDisconnected.png\"\nwidth={300}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Clustering", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "In general clustering is the process of grouping similar objects together. In graph theory, the clustering process can\nbe seen as a way to group vertices together i.e. to find communities that aren't based on specific rules like cliques\nor connected components.  \nThere are two main approaches to clustering graphs:  \n- bottom-up: start with each vertex in its own cluster and then merge clusters together\n- top-down: start with all vertices in one cluster and then split the cluster into smaller clusters", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Clustering", "Header 3": "Girvan-Newman Clustering", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "The Girvan-Newman clustering algorithm is a bottom-up approach to clustering which is based on edge betweenness, hence\nit is also called edge betweenness clustering. The idea is to iteratively calculate the edge-betweenness of each edge in\nthe graph and then remove the edge with the highest edge-betweenness.  \nThe thought process behind this is that the edges with the highest edge-betweenness are the edges that have the highest\ninformation flow. So by removing these edges, we are removing the edges that connect two groups/clusters/communities\ntogether. Eventually this will lead to two components, which are then the clusters.  \n<SideBySideBlock>\n<Block>\n<Image\nsrc=\"/cs/graphsGirvanNewman1.png\"\ncaption=\"Initial state of the graph with its edge-betweenness.\"\n/>\n</Block>\n<Block>\n<Image\nsrc=\"/cs/graphsGirvanNewman2.png\"\ncaption=\"Graph after removing the edge with the highest edge-betweenness.\"\n/>\n</Block>\n</SideBySideBlock>  \nThe issue with this approach is that it is very computationally expensive. The edge-betweenness of each edge has to be\ncalculated, which is $O(|V||E|)$ and then that has to be done iteratively multiple times so the overall complexity can\nbe summarized to $O(n^3)$ which is not ideal for large graphs.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Clustering", "Header 3": "LPA - Label Propagation Algorithm", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "The LPA is a more general algorithm which doesn't have to just be used for clustering graphs it can also be used to\ncluster data in general. However, I will explain it in the context of graph clustering.  \n<Callout type=\"todo\">\nMaybe one day this can be done in the context of semi-supervised labeling\n</Callout>  \nLPA consists of 2 parts, the preparation and the actual algorithm. In the preparation we do the following:  \n1. We assign each vertex a unique label from $0$ to $|V| - 1$. The labels in the end will be the clusters, which makes\nthis a bottom-up approach.  \n2. We perform graph coloring. I will not go into detail about graph coloring here, but the idea is to color the graph\nsuch that no two connected/neighboring vertices have the same color whilst using the least amount of colors possible.  \n<Callout type=\"todo\">\nMaybe add a link to the graph coloring chapter if it ever gets written.\n</Callout>  \nOnce the preparation is done, we can start the actual algorithm. The algorithm is very simple:  \nFor each color (always in the same order) we go through each vertex (also always in the same order) and check the\nlabels of its neighbors and count how many times each one occurs. If there is a label that occurs more often than the\nothers, then we assign that label to the vertex. If there are multiple labels that occur the same amount of times, then\nthere are two options:  \n- If the vertexes label is one of the labels that occur the most, then we keep the label.\n- If the vertexes label is not one of the labels that occur the most, then we assign it the label with the highest value.\nLowest would also work, as long as it is consistent.  \nThis is repeated until the labels don't change anymore. The labels in the end then represent the clusters. The algorithm\nis very simple and fast making it a good choice for large graphs. However, it is not\ndeterministic, i.e. it can lead to different results depending on the order of the colors and vertices. This can be", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Clustering", "Header 3": "LPA - Label Propagation Algorithm", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "is very simple and fast making it a good choice for large graphs. However, it is not\ndeterministic, i.e. it can lead to different results depending on the order of the colors and vertices. This can be\nmitigated by running the algorithm multiple times and then taking the most common result.  \n<Callout type=\"example\">\nAfter the initial setup, we get the graph below:  \n<Image\nsrc=\"/cs/graphsLPA1.png\"\ncaption=\"Initial state of the graph.\"\nwidth={300}\n/>  \nWe will work through the graph in the following order:  \n- Blue: $B, F$\n- Green: $D, A, H, C$\n- Brown: $E, G$  \n<Steps>\n<Step id={1}>\nWe start with vertex $B$ which has the neighbors $A,C,D,E$ with the labels $0,2,3,4$. The vertex $B$ has the\nlabel $1$. Because all the neighboring labels occur once and the vertexes label is not one of the labels we\npick the one with the height value, which is $4$. So we assign the label $4$ to $B$.\n</Step>\n<Step id={2}>\nWe have a similar situation for the next vertex $F$ which gets assigned the label $7$.\n<Image\nsrc=\"/cs/graphsLPA2.png\"\ncaption=\"The state of the graph after the initial iteration of the blue vertices.\"\nwidth={300}\n/>\n</Step>\n<Step id={3}>\nNow we do the same with the green vertices.\n<Image\nsrc=\"/cs/graphsLPA3.png\"\ncaption=\"The state of the graph after assigning the new labels for the green vertices.\"\nwidth={300}\n/>\n</Step>\n<Step id={4}>\nLastly, we process the brown vertices in the given order.\n<Image\nsrc=\"/cs/graphsLPA4.png\"\ncaption=\"The final state of the graph after assigning the new labels for the brown vertices.\"\nwidth={300}\n/>\nLuckily with this graph, we already have our clusters after the first iteration. We have two clusters, the\nvertices with the label 4 and the vertices with the label 7.\n</Step>\n</Steps>  \n</Callout>  \n<Callout type=\"todo\">\nMake my own images where the graph is processed alphabetically. And what if we want more then 2 clusters?\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Clustering", "Header 3": "Louvain Clustering", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "The Louvain clustering algorithm is a bottom-up greedy approach to clustering which is based on modularity. So we first\nneed to understand what modularity is.  \n#### Modularity  \nModularity is a metric that measures the quality of a clustering. The idea is to compare the number of edges within a\ncluster with the number of edges between clusters. A good clustering would then have a lot of edges within a cluster\nand not many edges between clusters.  \nModularity is defined as the fraction of edges of a graph within a cluster minus the expected fraction of edges within\na cluster if the edges were distributed randomly. The value of modularity is between $\\frac{-1}{2}$ and $1$, where\nany value above 0 means that the number of edges within a cluster is higher than the expected number of edges within a\ncluster if the edges were distributed randomly. The higher the value, the better the clustering, if the value is above\n0.3 then the clustering is considered to be good.  \n$$\n\\text{modularity}(G) = \\frac{1}{2m} \\sum_{i,j \\in V} \\left( A_{ij} - \\frac{deg(i) deg(j)}{2m} \\right) \\delta(c_i, c_j)\n$$  \nwith the following definitions:  \n- $A_{ij}$ is the weight of the edge between vertices $i$ and $j$\n- $m$ is the sum of all edge weights so for an unweighted graph $m = |E|$ and for a weighted graph $m = \\sum_{i,j \\in V} A_{ij}$.\n- $\\delta(c_i, c_j)$ is the Kronecker delta function (1 if $c_i = c_j$ and 0 otherwise), which is used to check if two\nvertices are in the same cluster.  \n#### The Louvain Algorithm  \nThe Louvain algorithm then tries to maximize the modularity of a graph in an iterative process until the modularity\ncannot be increased anymore, hence it is a greedy approach.  \nInitially each vertex is in its own cluster. We then iteratively perform the following steps:  \n- **Modularity Optimization:** For each vertex we check how the modularity would change if we would\nmove it to a neighboring cluster. If the modularity would increase, then we move the vertex to the neighboring cluster", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Communities", "Header 2": "Clustering", "Header 3": "Louvain Clustering", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/communities.mdx"}, "page_content": "- **Modularity Optimization:** For each vertex we check how the modularity would change if we would\nmove it to a neighboring cluster. If the modularity would increase, then we move the vertex to the neighboring cluster\nwhich would increase the modularity the most. If the modularity would not increase, then we leave the vertex in its\ncurrent cluster. Once we have gone through all vertices, we move on to the next step.\n- **Cluster Aggregation:** We then aggregate all vertices in the same cluster into a single vertex. This vertex has a\nself-looping edge with a weight equal to the sum of all the edges of the vertices in the cluster. The vertices resembling\nthe clusters are then connected to each other with edges of weight equal to the sum of all the edges between the\nclusters before the aggregation. We then go back to the first step and repeat the process until the modularity cannot be\nincreased anymore.  \n<Image\nsrc=\"/cs/graphsLouvainClustering.png\"\ncaption=\"Example of the Louvain algorithm.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Topological Sort/Ordering", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/topologicalSortOrdering.mdx"}, "page_content": "The goal of a topological sort is given a list of items with dependencies, (ie. item 5 must be completed before item 3, etc.) to produce an ordering of the items that satisfies the given constraints. In order for the problem to be solvable, there can not be a cyclic set of constraints. (We can't have that item 5 must be completed before item 3, item 3 must be completed before item 7, and item 7 must be completed before item 5, since that would be an impossible set of constraints to satisfy.) Meaning we can model this problem with a directed unweighted acyclic graph. When all the vertices are topologically ordered in a row all the edges go from left to right.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Topological Sort/Ordering", "Header 2": "Kahn's Algorithm", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/topologicalSortOrdering.mdx"}, "page_content": "Kahn's algorithm describes a way in which we can find a topological order. In pseudocode the algorithm goes like this  \n```c\nL ← Empty list that will contain the sorted elements\nS ← Set of all nodes with no incoming edge so indeg(v)=0\n\nwhile S is not empty do\nremove a node n from S\nadd n to L\nfor each node m with an edge e from n to m do\nremove edge e from the graph\nif m has no other incoming edges then\ninsert m into S\n!!!!!OR!!!!!\nfor each node m with an edge e from n to m do\nreduce indeg(m) by one\nif indeg(m)==0\ninsert m into S", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Topological Sort/Ordering", "Header 2": "Kahn's Algorithm", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/topologicalSortOrdering.mdx"}, "page_content": "if graph still has edges then\nreturn error (graph has at least one cycle)\nelse\nreturn L (a topologically sorted order)\n```  \nDepending on the order that the nodes n are removed from set S, a different solution is created. A possible solution with an Adjacent matrix could look something like this.  \n```java\npublic int[] topsort(){\nint[] indeg = new int[n]; // calculate indegree\nfor (int i=0; i<n;i++){\nfor (int j=0; j<n; j++){\nindeg[i] += adjMatrix[j][i] ? 1:0;\n}\n}\nQueue S = new LinkedList(); // topsort init queue\nfor (int i=0; i<n; i++){\nif(indeg[i]==0) S.add(i);\n}\nint[] result = new int[n]; // topsort step\nfor(int i=0; i<n; i++){\nif(!S.isEmpty()){\nresult[i] = (int) S.remove();\nfor(int j=0; j<n; j++){\nif(adjMatrix[result[i]][j]) {\nindeg[j]--;\nif(indeg[j] == 0) S.add(j);\n}\n}\n}\nreturn null;\n}\nreturn result;\n}\n```  \nOr with an Adjacent list  \n```java\npublic void sort() {\nStringBuffer sb = new StringBuffer();\nif (isDirected()) {\nLinkedList<Vertex<K>> queue = new LinkedList<Vertex<K>>();\nint counter = 0;\nfor (Vertex<K> v : vertices.values()) {\nv.deg = v.indegree; // set indegree of each vertex\nif (v.deg == 0) queue.addFirst(v); // start set\n}\nwhile (!queue.isEmpty()) {\nVertex<K> v = queue.removeLast();\nsb.append(v.data + \"  \");\ncounter++; // count processed vertices\nfor (Vertex<K> w : v.adjList)\nif (--w.deg == 0) // decrease indegree of adjecent\nqueue.addFirst(w); // Add to S\n}\nif (counter != vertices.size()) {\nsb.replace(0, sb.length(), \"Cycle found\");\n}\n} else {\nsb.append(\"Graph is not directed, TopSort not possible.\");\n}\nSystem.out.println(sb);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Topological Sort/Ordering", "Header 2": "Kahn's Algorithm", "Header 3": "Time Complexity", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/topologicalSortOrdering.mdx"}, "page_content": "If we have $n$ vertices with $m$ edges and have stored that graph in an Adjacency list.  \n- To calculate all vertices indeg takes $O(n+m)$.\n- Adding all vertices with indeg 0 to S takes worst case $O(n)$.\n- Each edge is followed once to reduce the indeg of a vertex $O(m)$.\n- Each node is added and removed once from S so $O(n)$  \nThis then leads to a time complexity of $O(n+m)$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Node Embeddings", "Header 2": "DeepWalk", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/nodeEmbeddings.mdx"}, "page_content": "think of random walks being like sentences, and nodes being like words. We can then use the SkipGram model to learn embeddings for nodes. In other words\ngiven a random walk missing a node, we want to predict the missing node.  \nThe goal is to have nodes that are close in the graph be close in the embedding space.  \nIs it really skipgram not cbow?  \nthe probabilities formulas dont make a lot of sense to me.  \nuse random walks to get the context of a node without having to look at the entire graph. The random walk is unbiased,\ni.e. it is not biased towards any particular node, it chooes the next node uniformly at random.  \nsimiliar context => similiar nodes\nsimiliar sentences => similiar words  \nWhy use dot product instead of cosine similarity?\nhttps://developers.google.com/machine-learning/clustering/similarity/measuring-similarity  \nencoder node to embeding\ndecoder embedding to similarity ??? whyyy wtf, i.e just simple dot product?  \nEncoder can be just a lookup table, i.e. the embedding matrix. For every node we have a row or columnnn? in the embedding matrix. We then learn the embedding matrix.\nnot very scalable tho if we have a lot of nodes because matrix is V by D where V is the number of nodes and D is the embedding dimension.  \nhow to defined the node similiarites? Are linked, shared neighbors, have similiar surroundings i.e context => random walks  \n=> only learning the graph structure, not the node features. But unsupervised so we dont have labels which is good. Hence, the embeddings are also task independent.  \nP(v|z_u) = probability of visiting node v given we start random walk at node u. This is our model precition.???  \n=> softmax turns a vector of k real numbers into a vector of k real numbers that sum to 1, i.e a probability distribution. Soft version of a max function.  \ndont forget at the random walk you can also go back to where you came from.  \nUsing the random walks is much more effiecient?? than using the entire graph. But isnt the lookup table still V by D? So how is it effiecient?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Node Embeddings", "Header 2": "DeepWalk", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/nodeEmbeddings.mdx"}, "page_content": "dont forget at the random walk you can also go back to where you came from.  \nUsing the random walks is much more effiecient?? than using the entire graph. But isnt the lookup table still V by D? So how is it effiecient?  \nNR_u is the neighborhood of node u using the strategy R. So if we use a random walk of length 10, then NR_u is the set of nodes that are within 10 hops of node u. i.e multiset  \nmaximize sum of log-likelihoods of the random walks?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Node Embeddings", "Header 2": "Node2Vec", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/nodeEmbeddings.mdx"}, "page_content": "same idea but instead of using a random walk, we use a biased random walk.  \ndevelop biased 2nd order random walk can swap between local and global search, i.e breadth first search and depth first search.  \nhave two parameters p and q. p is the return parameter, i.e. how likely are you to go back to where you came from. q is the in-out parameter,\ni.e. how likely are you to go to a node that is close to you or far away from you, i.e ratio between breadth first search and depth first search. Depending on q\nwe decide to go further away, DFS from where we came from or stay at the same level, BFS.  \n2nd order because it remembers where it came from. 1st order is just a random walk.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Node Embeddings", "Header 2": "Embedding entire graph", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/nodeEmbeddings.mdx"}, "page_content": "First idea just sum up or average the embeddings of the nodes. was used in 2016 for molecule classification.  \nother idea: introduce virtual node to represnet graph or subgraph. virtual node is connected to all nodes that i want to embed.  \n3rd idea is using anonymous walks. Instead of using the node labels, we use the node ids. So we dont know what the node is, we enumerate them. this way\nwe can use the same random walk for different walks. A -> B -> A and A -> C -> A are the same walk, 1 -> 2 -> 1? agnostic to the node labels.  \ncan calcualte the number of anonymous walks for a given length/number of nodes.  \nCould have a bag of walks, i.e. count the number of times a walk appears in the graph. with length 3 there are 5 possible anonymous walks so get a dimension of 5.\nCan be seen as a probability distribution over the walks.  \nTo know  how many walks we can use a formula with epsilon and delta. epsilon. Can also learn an embedding of the anonymous walks.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "A multirelational graph is a graph where there are multiple types of edges. For example, in a social network, there\ncould be edges for friendships, likes, follows, etc. This then also most often leads to a multigraph, which is a graph\nwhere there can be multiple edges between two vertices.  \nAn example of a multigraph could be a road network where each edge represents a road and the vertices represent cities.  \nexport const multiGraph = {\nnodes: [\n{id: 1, label: \"London\", x: 150, y: 0},\n{id: 2, label: \"Paris\", x: 0, y: 200},\n{id: 3, label: \"Berlin\", x: 300, y: 200}\n],\nedges: [\n{from: 1, to: 2, label: \"Road 1\", smooth: {type: \"curvedCCW\", roundness: 0.5}},\n{from: 1, to: 2, label: \"Road 2\", smooth: {type: \"curvedCW\", roundness: 0.5}},\n{from: 1, to: 2, label: \"Road 3\", smooth: {type: \"continuous\", roundness: 0.2}},\n{from: 1, to: 3, label: \"Road 4\", smooth: {type: \"curvedCCW\", roundness: 0.3}},\n{from: 1, to: 3, label: \"Road 5\", smooth: {type: \"curvedCW\", roundness: 0.3}},\n{from: 2, to: 3, label: \"Road 6\", smooth: {type: \"continuous\", roundness: 0.2}}\n]\n}  \n<Graph\ngraph={multiGraph}\noptions={\n{\nedges: {\nwidth: 2,\nsmooth: true,\ncolor: \"#7300E6\",\nfont: {\ncolor: \"#FFFFFF\",\nsize: 12,\nstrokeWidth: 0,\n}\n},\nphysics: {\nenabled: false\n}\n}\n}\n/>  \nAs an example of a multirelational graph, we can expand on the graph from above and add edges for other types of\ntransport like trains.  \nexport const multirelationalGraph = {\nnodes: [\n{id: 1, label: \"London\", x: 150, y: 0},\n{id: 2, label: \"Paris\", x: 0, y: 200},\n{id: 3, label: \"Berlin\", x: 300, y: 200}\n],\nedges: [\n{from: 1, to: 2, label: \"Road 1\", smooth: {type: \"curvedCCW\", roundness: 0.5}},\n{from: 1, to: 2, label: \"Road 2\", smooth: {type: \"curvedCW\", roundness: 0.5}},\n{from: 1, to: 2, label: \"Plane 1\", smooth: {type: \"continuous\", roundness: 0.2}, color: \"pink\"},\n{from: 1, to: 3, label: \"Road 4\", smooth: {type: \"curvedCCW\", roundness: 0.3}},\n{from: 1, to: 3, label: \"Plane 2\", smooth: {type: \"curvedCW\", roundness: 0.3}, color: \"pink\"},", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "{from: 1, to: 3, label: \"Road 4\", smooth: {type: \"curvedCCW\", roundness: 0.3}},\n{from: 1, to: 3, label: \"Plane 2\", smooth: {type: \"curvedCW\", roundness: 0.3}, color: \"pink\"},\n{from: 2, to: 3, label: \"Road 6\", smooth: {type: \"continuous\", roundness: 0.2}}\n]\n}  \n<Graph\ngraph={multirelationalGraph}\noptions={\n{\nedges: {\nwidth: 2,\nsmooth: true,\ncolor: \"#7300E6\",\nfont: {\ncolor: \"#FFFFFF\",\nsize: 12,\nstrokeWidth: 0,\n}\n},\nphysics: {\nenabled: false\n}\n}\n}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "Multiplex Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "A multireletional graph can also be split into layers of a multiplex graph. A multiplex graph is a graph where there\nare multiple layers of different edges but the same vertices.  \n<Image\nsrc=\"/cs/graphsMultiplex.png\"\ncaption=\"A multirelational graph split into a multiplex graph.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "Signed Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "A signed graph is a graph in which each edge has a positive or negative sign. They can be used to represent a\nrelationship between two vertices. Because there are two possible edges between two vertices, a positive and a negative\none, it means that a signed graph is a [multirelational graph](#multirelational-graphs).  \nFor example, a positive sign could mean that two people are allies and a negative\nsign could mean that they are enemies.  \nexport const signedGraph = {\nnodes: [\n{id: 1, label: \"Bob\"},\n{id: 2, label: \"Alice\"},\n{id: 3, label: \"Michael\"},\n{id: 4, label: \"Urs\"},\n{id: 5, label: \"Karen\"}\n],\nedges: [\n{from: 1, to: 2, label: \"-\", color: \"red\"},\n{from: 1, to: 3, label: \"+\", color: \"green\"},\n{from: 2, to: 3, label: \"-\", color: \"red\"},\n{from: 2, to: 4, label: \"+\", color: \"green\"},\n{from: 2, to: 5, label: \"-\", color: \"red\"},\n{from: 3, to: 5, label: \"+\", color: \"green\"},\n]\n}  \n<Graph\ngraph={signedGraph}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "Signed Graphs", "Header 3": "Triads and Balance Theory", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "Triads are a set of three vertices in a signed graph where each pair of vertices is connected by an edge, i.e forming a\ntriangle. Triads are important in social network analysis as they can be used to determine the stability of a social\nnetwork.  \nThe balance theory states that a social network is balanced if all triads within that network are balanced. For a triad\nto be balanced, the number of negative edges must be even (0 being even). This leads to the following four possible\ntriads:  \n![balancedTriads](/compSci/balancedTriads.png)\n<Image\nsrc=\"/cs/graphsBalancedTriads.png\"\n/>  \nThe first and last one are the simplest as they are either all positive or all negative. The idea of the second one\nis that it is balanced because \"the enemy of my enemy is my friend\". The third one is a common scenario that leads to\nissues in social networks. For example, if Alice and Eve are allies and Bob and Eve are alies but Alice and\nBob are enemies, then this leads to issues in the social network if Eve wants to introduce Alice and Bob to each other.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "Signed Graphs", "Header 3": "Balanced Signed Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "From the above, we can see that a signed graph is balanced if all triads are balanced. Where a triade could also be\ndefined as a cycle of length 3, $C_3$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "N-Mode Networks", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "An n-mode network is a graph where there are multiple types of vertices and the edges can only connect vertices of\ndifferent types. So in other words the graphs vertices can be split into $n$ disjoint sets and the edges can only\nconnect vertices from different sets. A normal graph is a 1-mode network as there is only one type of vertex.\nA 2-mode network is a graph where there are two types of vertices and the edges can only connect vertices of\ndifferent types.  \nFor example, if we have a graph containing people and movies and the edges represent whether a person has watched a\nmovie or not.  \nexport const movieReviewGraph = {\nnodes: [\n{id: 1, label: \"Bob\"},\n{id: 2, label: \"Alice\"},\n{id: 3, label: \"Michael\"},\n{id: 4, label: \"Urs\"},\n{id: 5, label: \"Karen\"},\n{id: 6, label: \"Inception\", color: \"pink\"},\n{id: 7, label: \"Titanic\", color: \"pink\"},\n{id: 8, label: \"The Godfather\", color: \"pink\"},\n{id: 9, label: \"Pulp Fiction\", color: \"pink\"},\n{id: 10, label: \"The Dark Knight\", color: \"pink\"},\n],\nedges: [\n{from: 1, to: 6, label: \"4\"},\n{from: 1, to: 7, label: \"3\"},\n{from: 2, to: 8, label: \"5\"},\n{from: 2, to: 9, label: \"4\"},\n{from: 5, to: 8, label: \"2\"},\n{from: 3, to: 6, label: \"3\"},\n{from: 3, to: 7, label: \"5\"},\n{from: 4, to: 9, label: \"3\"},\n{from: 5, to: 10, label: \"4\"},\n{from: 1, to: 10, label: \"5\"}\n]\n}  \n<Graph\ngraph={movieReviewGraph}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "N-Mode Networks", "Header 3": "Bipartite Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "A graph $G$ is bipartite if its vertices can be split into two disjoint sets $V_1$ and $V_2$ such that every edge in\n$G$ connects a vertex in $V_1$ to a vertex in $V_2$. So there are no edges between vertices in the same set!  \nThis makes a 2-mode network a bipartite graph.  \n<Image\nsrc=\"/cs/graphsBipartite.png\"\ncaption=\"Some examples of bipartite graphs.\"\nwidth={400}\n/>  \n#### Bipartite Network Projections  \nThe idea of a bipartite network projection is to project a bipartite graph to a \"normal\" graph i.e. 1-mode network. This\ncan be done in a few ways.  \n##### Simple Projection  \nIn the simple bipartite network projection, we project the bipartite graph to a 1-mode network by connecting two vertices\nif they have a common neighbor of the type to be removed in the bipartite graph.  \nFor example, if we have a bipartite graph with people and events and the edges represent whether a person has attended\nan event or not. We can then project this bipartite graph to a 1-mode network where the vertices are people, and they\nare connected if they have attended the same event.  \nBy doing this we can quickly find people that have been to the same events and might have similar interests. So if we\nhave the below graph:  \nexport const eventGraph = {\nnodes: [\n{id: 1, label: \"Concert\", color: \"pink\"},\n{id: 2, label: \"University Open Day\", color: \"pink\"},\n{id: 3, label: \"Birthday Party\", color: \"pink\"},\n{id: 4, label: \"Bob\"},\n{id: 5, label: \"Alice\"},\n{id: 6, label: \"Michael\"},\n{id: 7, label: \"Urs\"},\n{id: 8, label: \"Karen\"},\n{id: 9, label: \"John\"},\n{id: 10, label: \"Emma\"},\n{id: 11, label: \"David\"},\n{id: 12, label: \"Sophia\"},\n],\nedges: [\n{from: 4, to: 1},\n{from: 6, to: 1},\n{from: 7, to: 1},\n{from: 8, to: 1},\n{from: 10, to: 1},\n{from: 11, to: 1},\n{from: 12, to: 1},\n{from: 6, to: 2},\n{from: 7, to: 2},\n{from: 8, to: 2},\n{from: 9, to: 2},\n{from: 4, to: 3},\n{from: 5, to: 3},\n]\n}  \n<Graph\ngraph={eventGraph}\n/>  \nand perform a simple projection we get the following graph:  \nexport const eventSimpleProjGraph = {\nnodes: [", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "N-Mode Networks", "Header 3": "Bipartite Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "{from: 6, to: 2},\n{from: 7, to: 2},\n{from: 8, to: 2},\n{from: 9, to: 2},\n{from: 4, to: 3},\n{from: 5, to: 3},\n]\n}  \n<Graph\ngraph={eventGraph}\n/>  \nand perform a simple projection we get the following graph:  \nexport const eventSimpleProjGraph = {\nnodes: [\n{id: 4, label: \"Bob\"},\n{id: 5, label: \"Alice\"},\n{id: 6, label: \"Michael\"},\n{id: 7, label: \"Urs\"},\n{id: 8, label: \"Karen\"},\n{id: 9, label: \"John\"},\n{id: 10, label: \"Emma\"},\n{id: 11, label: \"David\"},\n{id: 12, label: \"Sophia\"},\n],\nedges: [\n// Bob knows everyone except John\n{from: 4, to: 5},\n{from: 4, to: 6},\n{from: 4, to: 7},\n{from: 4, to: 8},\n{from: 4, to: 10},\n{from: 4, to: 11},\n{from: 4, to: 12},\n// Alice only knows Bob, covered above\n// John only knows Karen, Michael and Urs\n{from: 9, to: 6},\n{from: 9, to: 7},\n{from: 9, to: 8},\n// the rest all know each other\n{from: 6, to: 7},\n{from: 6, to: 8},\n{from: 6, to: 10},\n{from: 6, to: 11},\n{from: 6, to: 12},\n{from: 7, to: 8},\n{from: 7, to: 10},\n{from: 7, to: 11},\n{from: 7, to: 12},\n{from: 8, to: 10},\n{from: 8, to: 11},\n{from: 8, to: 12},\n{from: 10, to: 11},\n{from: 10, to: 12},\n{from: 11, to: 12},\n]\n}  \n<Graph\ngraph={eventSimpleProjGraph}\n/>  \n##### Weighted Projection  \nThe problem with the simple projection is that it does not take into account how many edges two vertices have in common.\nFor example, if two people have been to the same event 10 times, they will be connected the same way as two people that\nhave only been to the same event once. To solve this, we can use a weighted projection.  \nIn a weighted projection, we connect two vertices if they have a common neighbor of the second type (the one to be removed),\njust like in the simple projection, but the weight of the edge is the number of common neighbors. More formally:  \n$$\nw_{ab} = \\sum_{k \\in V_2} d_k^a d_k^b\n$$  \n- Where $V_2$ is the set that contains the vertices of the second type, i.e. the one that will be projected away (in the\nexample above, the events).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "N-Mode Networks", "Header 3": "Bipartite Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "$$\nw_{ab} = \\sum_{k \\in V_2} d_k^a d_k^b\n$$  \n- Where $V_2$ is the set that contains the vertices of the second type, i.e. the one that will be projected away (in the\nexample above, the events).\n- And where $d_k^a$ is 1 if $a$ and $k$ are connected and 0 otherwise and $d_k^b$ is 1 if $b$ and $k$ are connected.  \nThe weighted projection could then also be normalized by dividing each weight by the maximum weight of the graph. This\nwould then give us a value between 0 and 1.  \nexport const eventWeightedProjGraph = {\nnodes: [\n{id: 4, label: \"Bob\"},\n{id: 5, label: \"Alice\"},\n{id: 6, label: \"Michael\"},\n{id: 7, label: \"Urs\"},\n{id: 8, label: \"Karen\"},\n{id: 9, label: \"John\"},\n{id: 10, label: \"Emma\"},\n{id: 11, label: \"David\"},\n{id: 12, label: \"Sophia\"},\n],\nedges: [\n// Bob knows everyone except John\n{from: 4, to: 5, label: \"1\", length: 200},\n{from: 4, to: 6, label: \"1\", length: 200},\n{from: 4, to: 7, label: \"1\", length: 200},\n{from: 4, to: 8, label: \"1\", length: 200},\n{from: 4, to: 10, label: \"1\", length: 200},\n{from: 4, to: 11, label: \"1\", length: 200},\n{from: 4, to: 12, label: \"1\", length: 200},\n// Alice only knows Bob, covered above\n// John only knows Karen, Michael and Urs\n{from: 9, to: 6, label: \"1\", length: 200},\n{from: 9, to: 7, label: \"1\", length: 200},\n{from: 9, to: 8, label: \"1\", length: 200},\n// the rest all know each other\n{from: 6, to: 7, label: \"2\", length: 200, color: \"red\"},\n{from: 6, to: 8, label: \"2\", length: 200, color: \"red\"},\n{from: 6, to: 10, label: \"1\", length: 200},\n{from: 6, to: 11, label: \"1\", length: 200},\n{from: 6, to: 12, label: \"1\", length: 200},\n{from: 7, to: 8, label: \"2\", length: 200, color: \"red\"},\n{from: 7, to: 10, label: \"1\", length: 200},\n{from: 7, to: 11, label: \"1\", length: 200},\n{from: 7, to: 12, label: \"1\", length: 200},\n{from: 8, to: 10, label: \"1\", length: 200},\n{from: 8, to: 11, label: \"1\", length: 200},\n{from: 8, to: 12, label: \"1\", length: 200},\n{from: 10, to: 11, label: \"1\", length: 200},\n{from: 10, to: 12, label: \"1\", length: 200},\n{from: 11, to: 12, label: \"1\", length: 200},", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "N-Mode Networks", "Header 3": "Bipartite Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "{from: 8, to: 11, label: \"1\", length: 200},\n{from: 8, to: 12, label: \"1\", length: 200},\n{from: 10, to: 11, label: \"1\", length: 200},\n{from: 10, to: 12, label: \"1\", length: 200},\n{from: 11, to: 12, label: \"1\", length: 200},\n]\n}  \n<Graph\ngraph={eventWeightedProjGraph}\n/>  \nWe can clearly see that only Michael, Urs and Karen are the only ones that have been to the same event more than once\n(highlighted in red).  \n##### Newmann-weighted Projection  \nThis projection is also sometimes called \"collaboration weighted projection\" (no idea why).  \nThe idea of this projection is to further build up on the weighted projection by also taking into account the degree of\nthe common neighbor, i.e. the number of edges connected to the common neighbor. To take this into account we can use the\nfollowing formula to calculate the weight of the edge between two vertices $a$ and $b$:  \n$$\nw_{ab} = \\sum_{k \\in V_2} \\frac{d_k^a d_k^b}{\\text{deg}(k) - 1}\n$$  \n- Where $V_2$ is the set that contains the vertices of the second type, i.e. the one that will be projected away (in the\nexample above, the events).\n- And where $d_k^a$ is 1 if $a$ and $k$ are connected and 0 otherwise and $d_k^b$ is 1 if $b$ and $k$ are connected.  \nThis projection can be valuable if we imagine the following scenario:  \nWe have a graph of people and events. We have an\nevent like a concert where 5000 people attended, a birthday party where 15 people attended and an\nOpen day at a university where 100 people attended. We can assume that if two people the party and the open day,\nthey are more likely to have similar interests than if they both attended the concert, or we could simply state that it\nis more likely that they came in contact with each other at the party or open day than at the concert.  \nSo if we project the same graph as above, we get the following graph:  \nexport const eventNewmannProjGraph = {\nnodes: [\n{id: 4, label: \"Bob\"},\n{id: 5, label: \"Alice\"},\n{id: 6, label: \"Michael\"},\n{id: 7, label: \"Urs\"},\n{id: 8, label: \"Karen\"},\n{id: 9, label: \"John\"},", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "N-Mode Networks", "Header 3": "Bipartite Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "export const eventNewmannProjGraph = {\nnodes: [\n{id: 4, label: \"Bob\"},\n{id: 5, label: \"Alice\"},\n{id: 6, label: \"Michael\"},\n{id: 7, label: \"Urs\"},\n{id: 8, label: \"Karen\"},\n{id: 9, label: \"John\"},\n{id: 10, label: \"Emma\"},\n{id: 11, label: \"David\"},\n{id: 12, label: \"Sophia\"},\n],\nedges: [\n// Bob knows everyone except John\n{from: 4, to: 5, label: \"1\", length: 200},\n{from: 4, to: 6, label: \"0.17\", length: 200},\n{from: 4, to: 7, label: \"0.17\", length: 200},\n{from: 4, to: 8, label: \"0.17\", length: 200},\n{from: 4, to: 10, label: \"0.17\", length: 200},\n{from: 4, to: 11, label: \"0.17\", length: 200},\n{from: 4, to: 12, label: \"0.17\", length: 200},\n// Alice only knows Bob, covered above\n// John only knows Karen, Michael and Urs\n{from: 9, to: 6, label: \"0.33\", length: 200},\n{from: 9, to: 7, label: \"0.33\", length: 200},\n{from: 9, to: 8, label: \"0.33\", length: 200},\n// the rest all know each other\n{from: 6, to: 7, label: \"0.5\", length: 200},\n{from: 6, to: 8, label: \"0.5\", length: 200},\n{from: 6, to: 10, label: \"0.17\", length: 200},\n{from: 6, to: 11, label: \"0.17\", length: 200},\n{from: 6, to: 12, label: \"0.17\", length: 200},\n{from: 7, to: 8, label: \"0.5\", length: 200},\n{from: 7, to: 10, label: \"0.17\", length: 200},\n{from: 7, to: 11, label: \"0.17\", length: 200},\n{from: 7, to: 12, label: \"0.17\", length: 200},\n{from: 8, to: 10, label: \"0.17\", length: 200},\n{from: 8, to: 11, label: \"0.17\", length: 200},\n{from: 8, to: 12, label: \"0.17\", length: 200},\n{from: 10, to: 11, label: \"0.17\", length: 200},\n{from: 10, to: 12, label: \"0.17\", length: 200},\n{from: 11, to: 12, label: \"0.17\", length: 200},\n]\n}  \n<Graph\ngraph={eventNewmannProjGraph}\n/>  \nThe calculations are a bit more complicated than for the weighted projection, but nothing to complex:  \n$$\n\\begin{align*}\n\\text{Alice to Bob} &= \\frac{1 \\cdot 1}{2-1} + \\frac{0 \\cdot 1}{7-1} + \\frac{0 \\cdot 0}{4-1} = 1 \\\\\n\\text{Karen to John} &= \\frac{0 \\cdot 0}{2-1} + \\frac{1 \\cdot 0}{7-1} + \\frac{1 \\cdot 1}{4-1} = 0.33 \\\\\n\\text{etc...}\n\\end{align*}\n$$  \n##### Overlap Weighted Projection", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Multirelational Graph", "Header 2": "N-Mode Networks", "Header 3": "Bipartite Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/multirelational.mdx"}, "page_content": "\\text{Karen to John} &= \\frac{0 \\cdot 0}{2-1} + \\frac{1 \\cdot 0}{7-1} + \\frac{1 \\cdot 1}{4-1} = 0.33 \\\\\n\\text{etc...}\n\\end{align*}\n$$  \n##### Overlap Weighted Projection  \nThe overlap weighted projection is similar to the weighted projection, but instead of using the number of common\nneighbors, it uses jaccard similarity. So the weight of the edge between two vertices $a$ and $b$ is:  \n$$\nw_{ab} = \\frac{N(a) \\cap N(b)}{N(a) \\cup N(b)}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Storing Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/storingGraphs.mdx"}, "page_content": "Because I am a computer scientist I don't just care about using graphs but also about how to store them. There are\nmultiple ways of storing graphs. Depending on the type of graph and the requirements a certain storage method might\nbe preferred as it might be more efficient in terms of memory or time complexity.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Storing Graphs", "Header 2": "Adjacency Matrix", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/storingGraphs.mdx"}, "page_content": "An adjacency matrix is the most common and the most simple way of storing graphs. If we have a graph with $n$ vertices,\nwe create an adjacency matrix with dimensions of $n \\times n$. As the name suggests this matrix stores the adjacency of\nvertices i.e. the relationship between the vertices i.e. the edges.  \nBelow you can see an example of a very simple adjacency matrix of an undirected unweighted graph.  \n<Image\nsrc=\"/cs/graphsAdjacencyMatrix.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Storing Graphs", "Header 2": "Adjacency Matrix", "Header 3": "Weighted and Unweighted Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/storingGraphs.mdx"}, "page_content": "If we have a weighted graph where the weights are integer values we can create the following matrix:  \n```java\nint[][] G = new int[n][n]\n```  \nThe weight of the edge from vertex $x$ to vertex $y$ would then be stored at `G[y][x]`.  \nIf there is no edge between 2 vertices then there are multiple ways to indicate this. The simplest way would be to set\nthe value to 0 (which it already is at initialization) another common approach is to set the weight to\n`Integer.MAX_VALUE`.  \nThe last possibility but the worst in space complexity is to use a `null` value. This is only possible if we use an\nobject array instead of a primitive array. This would mean that instead of using `int[][]` we use `Integer[][]`.\nThis would however mean that we would end up using a lot more memory as you can read more about\n[here](https://stackoverflow.com/a/65568047/10994912).  \nIf the graph is unweighted we can use the same int array and just store all edge weights as 1 or 0. We could however\nalso use a 2D boolean array which interestingly does use less space in Java than an int array. In Java, a normal boolean\nvariable uses 32 bits like an int. However, in an array each boolean value only takes up 8 bits because that is what the\nCPU likes to use internally. This means that a boolean array uses 4 times less space than an int array.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Storing Graphs", "Header 2": "Adjacency Matrix", "Header 3": "Directed and Undirected Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/storingGraphs.mdx"}, "page_content": "If the graph is undirected then we commonly set the value at `G[y][x]` and `G[x][y]` to the same value. This means that\nmathematically $G = G^T$ i.e. the matrix is symmetric along the diagonal, and that we could get away with only storing\nthe upper or lower triangle of the matrix, which would half the memory usage. However, this would make the code more\ncomplicated.  \nIf the graph is directed then we only set the value at `G[y][x]` and not at `G[x][y]`. So $G \\eq G^T$ is no longer\nguaranteed.  \nThe biggest problem with storing graphs with an adjacency matrix is that its space complexity is $O(n^2)$. What makes\nthis worse is that a lot of the space is wasted as in most cases there are only a few edges between vertices making it\na sparse matrix. Very rarely do we need to store a [complete graph](generalDefinition#complete-graphs).  \n<Image\nsrc=\"/cs/graphsAdjacencyMatrixWeighted.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Storing Graphs", "Header 2": "Adjacency Matrix", "Header 3": "Implementation", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/storingGraphs.mdx"}, "page_content": "As mentioned although the adjacency matrix isn't the most efficient way of storing graphs it is the most common and\nvery simple to implement. Below you can see a simple implementation for an undirected unweighted graph.  \n```java\npublic class UndirectedUnweightedGraph {\nfinal boolean[][] adjMatrix;\nfinal int n;\n\npublic GraphI(int numNodes) {\nif (numNodes < 1) throw new IllegalArgumentException();\nthis.adjMatrix = new boolean[numNodes][numNodes];\nthis.n = numNodes;\n}\n\npublic boolean addEdge(int x, int y) {\nif (0 <= x && x < n && 0 <= y && y < n) {\nif (adjMatrix[y][x]) return false; // already set\nadjMatrix[y][x] = adjMatrix[x][y] = true;\nreturn true;\n}\nthrow new IndexOutOfBoundsException();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Storing Graphs", "Header 2": "Edge List", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/storingGraphs.mdx"}, "page_content": "Another simple but less common way of storing graphs is to just store a list of the edges. The edges could have the\nfollowing structure:  \n```java\nclass Edge {\nint from, to, weight;\n}\n```  \nIf it is an unweighted graph the weight attribute could just be omitted and if it is an undirected graph you can either\nhave two entries for each edge or handle from and to the same way when processing.  \nThe advantage of this solution is that it only uses $O(m)$ memory with $m$ being the number of edges. The disadvantage\nof this storage solution is that you can not quickly find out how many vertices are in the graph and what they are. This\ncould however be solved by just adding another list containing all the vertices. This solution would then be very\nsimilar to the formal definition of a graph $G=(V, E)$. We would then have a memory usage of $O(n+m)$ with $n$ being the\nnumber of vertices and $m$ the number of edges.  \n<Callout type=\"todo\">\nImplement this\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Storing Graphs", "Header 2": "Adjacency Lists", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/storingGraphs.mdx"}, "page_content": "An adjacency list stores for each vertex has a list of its edges. An adjacency list can just be a simple array but the\nlist storing the edges is most commonly a linked list due to the storage and length being dynamic. If the graph is\nundirected you can again either handle it by just storing an edge in one of the list, for example always in the source\nvertex, or you can also store it additionally in the destination vertexes list. This structure uses just like the edge\ntable $O(n+m)$ memory with $n$ being the number of vertices and $m$ the number of edges.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Storing Graphs", "Header 2": "Adjacency Lists", "Header 3": "Implementation", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/storingGraphs.mdx"}, "page_content": "<Callout type=\"todo\">\nIs this really correct and they way I want it?\n</Callout>  \n```java\npublic class UnweightedGraph<K> {\nprivate static class Vertex<K> {\nK data;\nint indegree, deg = 0;\nboolean visited;\nList<Vertex<K>> adjList = new LinkedList<Vertex<K>>();\n\nVertex(K value) {\ndata = value;\n}\n\nboolean addEdgeTo(Vertex <K> to) {\nreturn (adjList.contains(to)) ? false : adjList.add(to);\n}\n}\n\nprivate Map<K, Vertex<K>> vertices;\nprivate int nOfEdges = 0;\n\npublic UnweightedGraph() {\nthis(false);\n}\n\npublic UnweightedGraph(boolean directed) {\nsuper(directed);\nvertices = new HashMap<K, Vertex<K>>();\n}\n\npublic UnweightedGraph(UnweightedGraph<K> orig) { // copy constructor\nthis(orig.isDirected());\nfor (K k: orig.vertices.keySet()) {\naddVertex(k);\n}\nfor (Vertex<K> v: orig.vertices.values()) {\nfor (Vertex<K> w: v.adjList) {\naddEdge(v.data, w.data);\n}\n}\n}\n\npublic boolean addVertex(K vertex) {\nif (vertex != null && !vertices.containsKey(vertex)) {\nvertices.put(vertex, new Vertex<K>(vertex));\nreturn true;\n} else {\nreturn false;\n}\n}\n\npublic boolean addEdge(K from, K to) {\nVertex<K> vf = vertices.get(from);\nVertex<K> vt = vertices.get(to);\nif (vf != null && vt != null && vf.addEdgeTo(vt)) {\nvt.indegree++;\nif (!isDirected()) {\nvt.addEdgeTo(vf);\nvf.indegree++;\n}\nnOfEdges++;\nreturn true;\n} else {\nreturn false;\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "A Graph is one of the most fundamental but also diverse data structures in computer science. A Graph consists of a set\nof vertices $V$ and a set of edges $E$ where each edge is an unordered pair. Hence, $G=(V,E)$. They are used to\nrepresent relationships between various entities or elements (the vertices) by connecting them with edges.  \nFor example, a graph can be used to represent a social network where the vertices are people and the edges represent\nwhether they are friends with each other or not, no edge signifying that they are not friends. In the below graph\n$G=(V,E)$ where:  \n- $V=\\{\\text{Bob, Alice, Michael, Urs, Karen}\\}$ and\n- $E=\\{(1,2),(1,3),(2,4),(2,5)\\}$  \nexport const friendsGraph = {\nnodes: [\n{id: 1, label: \"Bob\"},\n{id: 2, label: \"Alice\"},\n{id: 3, label: \"Michael\"},\n{id: 4, label: \"Urs\"},\n{id: 5, label: \"Karen\"}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 4},\n{from: 2, to: 5}\n]\n};  \n<Graph\ngraph={friendsGraph}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Metrics", "Header 3": "Degrees", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "If we do some quick analysis of this graph using the degree function which returns the number of edges connected to a\nvertex, we can see that $\\text{deg(Alice)}=3$ and therefore Alice has the most friends in this social network.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Metrics", "Header 3": "Order", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "The order of a graph is the number of vertices in the graph. So in the above example, the order of the graph is 5. So it\ncould also be called an order-5 graph.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Metrics", "Header 3": "Diameter", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "The diameter of a graph is the longest shortest path between two vertices in the graph. So in the above example, the\ndiameter of the graph is 3 as the longest shortest path is between Michael and Karen.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Metrics", "Header 3": "Density", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "The density of a graph is the ratio of the number of edges to the number of possible edges. So in other words how\ndensely connected the graph is. In a directed graph there are $|V|(|V|-1)$ possible edges. Which means the density of\na directed graph is:  \n$$\nD = \\frac{|E|}{|V|(|V|-1)}\n$$  \nIn an undirected graph, there are $\\frac{|V|(|V|-1)}{2}$ possible edges. Which means the density of an undirected graph\nis:  \n$$\nD = \\frac{|E|}{\\frac{|V|(|V|-1)}{2}} = \\frac{2|E|}{|V|(|V|-1)}\n$$  \nSo in the above example, the density of the graph is $\\frac{8}{20} = 0.4$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Graphs of Functions", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "You might be more familiar with Graphs when talking about mathematical functions. In mathematics, a Graph of a Function\nis a visual representation of the relationship between the input values (domain) and their corresponding output values\n(range) under a specific function.  \nFormally, a Graph of a Function can be defined as follows:  \nLet $f$ be a function defined on a set of input values, called the domain $D$, and taking values in a set of output\nvalues, called the range $R$. The Graph of the Function $f$, denoted as $G(f)$, is a mathematical representation\nconsisting of a set of ordered pairs $(x, y)$, where $x \\in D$ and $y = f(x)$. Each ordered pair represents a point on\nthe graph, with $x$ as the independent variable (input) and $y$ as the dependent variable (output).  \nIn other words, the Graph of a Function is a visual representation of how the elements in the domain are mapped to\nthe corresponding elements in the range through the function $f$.  \nFor example, consider the following function:  \n$$\nf(x) = 2x + 1\n$$  \nIts domain could be the set of all real numbers $\\Bbb{R}$, and its range could also be $\\Bbb{R}$. To represent this\nfunction graphically, we plot points on the Cartesian plane where the $x$-coordinate corresponds to the input value,\nand the $y$-coordinate is the output value obtained by evaluating $f(x)$.  \n<div className=\"flex justify-center mt-5\">\n<iframe src=\"https://www.desmos.com/calculator/jmj44mpzfe?embed\" width=\"500\" height=\"500\"/>\n</div>  \nGraphs of Functions play a crucial role in analyzing and understanding the behavior of functions and studying their\noverall patterns and trends.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Types of Graphs", "Header 3": "Complete Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "A complete graph is a graph where each vertex is connected to every other vertex, very simple. In other words, a\ncomplete graph contains all possible edges. A complete graph with $n$ vertices is denoted as $K_n$.  \nFor example, the below graph is a complete graph with 5 vertices, $K_5$.  \nexport const completeGraph = {\nnodes: [\n{id: 1, label: \"1\"},\n{id: 2, label: \"2\"},\n{id: 3, label: \"3\"},\n{id: 4, label: \"4\"},\n{id: 5, label: \"5\"}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 1, to: 4},\n{from: 1, to: 5},\n{from: 2, to: 3},\n{from: 2, to: 4},\n{from: 2, to: 5},\n{from: 3, to: 4},\n{from: 3, to: 5},\n{from: 4, to: 5}\n]\n};  \n<Graph\ngraph={completeGraph}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Types of Graphs", "Header 3": "Directed Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "A Directed Graph is a graph where each edge is directed from one vertex to another. In other words, the edges have a\ndirection. The previous graph was an example of an undirected graph as we would hope that if person A thinks of person B\nas a friend then Person B feels the same way. However, in a directed graph, this is not necessarily the case.  \nSo we can define a directed graph as $G=(V,A)$ where:  \n- V is again the set of vertices\n- A is a set of ordered pairs of vertices, called arcs, arrows or directed edges (sometimes simply edges with the\ncorresponding set named E instead of A). Ordered pairs are used here because (unlike for undirected graphs) the\ndirection of an edge $(u,v)$ is important: $(u,v)$ is not the same edge as $(v,u)$. The first vertex is called the\ntail or initial vertex and the second vertex is called the head or terminal vertex.  \nFor example, let us imagine a directed graph where the vertices are the same as the previous example but the edges\nsignify if a person has liked another person's post on social media. We then get the below graph $G=(V,A)$ where:  \n- $V=\\{\\text{Bob, Alice, Michael, Urs, Karen}\\}$ and\n- $A=\\{(1,2),(1,3),(2,4),(2,5),(5,2)\\}$  \nexport const postLikedGraph = {\nnodes: [\n{id: 1, label: \"Bob\"},\n{id: 2, label: \"Alice\"},\n{id: 3, label: \"Michael\"},\n{id: 4, label: \"Urs\"},\n{id: 5, label: \"Karen\"}\n],\nedges: [\n{from: 1, to: 2},\n{from: 1, to: 3},\n{from: 2, to: 4},\n{from: 2, to: 5},\n{from: 5, to: 2}\n]\n};  \n<Graph\ngraph={postLikedGraph}\ndirected={true}\n/>  \nWhen talking about degrees in a directed graph, we more often distinguish between the in-degree and out-degree of a vertex.\nThe in-degree of a vertex is the number of edges that are pointing to that vertex and the out-degree is the number of\nedges that are pointing away from that vertex. So in the above example, the in-degree of Alice is 2 and the out-degree\nis also 2.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Types of Graphs", "Header 3": "Weighted Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "A weighted graph is a graph where each edge has a weight (a number associated with it). It can be as a triple\n$G = (E, V, w)$ where $w$ is a function that maps edges or directed edges to their weights. So, $w: E \\rightarrow \\Bbb{R}$\ncould be a function for a graph with real numbers as weights.  \nFor example, in a graph where each city is a vertex and each edge is a road between two cities, the weight could\nbe the distance in km between them.  \nexport const cityGraph = {\nnodes: [\n{id: 1, label: \"London\"},\n{id: 2, label: \"Paris\"},\n{id: 3, label: \"Berlin\"}\n],\nedges: [\n{from: 1, to: 2, value: 343, label: \"343\", length: 200},\n{from: 1, to: 3, value: 933, label: \"933\", length: 400},\n{from: 2, to: 3, value: 878, label: \"878\", length: 300},\n]\n}  \n<Graph\ngraph={cityGraph}\noptions={\n{\nlayout: {\nrandomSeed: 1\n},\n}\n}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Types of Graphs", "Header 3": "Networks vs Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "The only real difference between a network and a graph is the terminology. A network is a graph with a real-world context.\nFor example, a social network is a graph with a real-world context. A graph is a mathematical structure that represents\nrelationships between objects. When talking about a network we also tend to talk about nodes and links instead of\nvertices and edges.  \n| Graphs | Networks |\n|--------|----------|\n| Vertices | Nodes |\n| Edges | Links |  \n[Ressource](https://bence.ferdinandy.com/2018/05/27/whats-the-difference-between-a-graph-and-a-network/)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Types of Graphs", "Header 3": "Trees", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "A tree is a special type of graph where there is only one path between any two vertices. This means that there are no\ncycles in a tree. Trees are used in many different algorithms and data structures such as binary search trees. You can\nread more about trees in the [Trees](../trees/generalDefinition) section.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Types of Graphs", "Header 3": "Cycle/Circular Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "A cycle or circular graph is a graph with exactly one cycle. Where a cycle is non-empty path in which only the first and\nlast vertices are equal, i.e a path that starts and ends at the same vertex. You can think of it as a closed chain.  \nA path/trail/walk in a graph is defined as being a sequence of vertices, where consecutive vertices in the sequence are\nconnected by an edge.  \nCommonly a cycle with length $n$ is called an $n$-cycle and is denoted as $C_n$.  \nFor example, the below graph is a circular graph as it has only one cycle with the following path:  \n$$\nP = (a,b,c,a)\n$$  \nexport const cycleGraph = {\nnodes: [\n{id: 1, label: \"a\"},\n{id: 2, label: \"b\"},\n{id: 3, label: \"c\"}\n],\nedges: [\n{from: 1, to: 2},\n{from: 2, to: 3},\n{from: 3, to: 1},\n]\n}  \n<Graph\ngraph={cycleGraph}\n/>  \nTo read more about cycle graphs, check out [this article](https://mathworld.wolfram.com/CycleGraph.html)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "General Definition", "Header 2": "Types of Graphs", "Header 3": "Acyclic Graphs", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/generalDefinition.mdx"}, "page_content": "An acyclic graph is a graph that is almost the opposite of a cycle graph. It is a graph that has no cycles. This means\nthat there is no path that starts and ends at the same vertex. A popular example of an acyclic graph is a tree.  \nexport const acyclicGraph = {\nnodes: [\n{id: 5, label: \"5\", level: 0},\n{id: 2, label: \"2\", level: 1},\n{id: 6, label: \"6\", level: 1},\n{id: 1, label: \"1\", level: 2},\n{id: 4, label: \"4\", level: 2},\n{id: 8, label: \"8\", level: 2},\n{id: 3, label: \"3\", level: 3},\n{id: 7, label: \"7\", level: 3},\n{id: 9, label: \"9\", level: 3},\n],\nedges: [\n{from: 5, to: 2},\n{from: 5, to: 6},\n{from: 2, to: 1},\n{from: 2, to: 4},\n{from: 6, to: 8},\n{from: 4, to: 3},\n{from: 8, to: 7},\n{from: 8, to: 9},\n]\n}  \nexport const acyclicOptions = {\nlayout: {\nhierarchical: {\nenabled: true,\ndirection: \"UD\"\n},\n},\n}  \n<Graph\ngraph={acyclicGraph}\noptions={acyclicOptions}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Statistics with Graphs", "Header 2": "Hypothesis Testing", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/statistics.mdx"}, "page_content": "The goal of a hypothesis test is to determine if there is a statistical relationship between two variables. To test\nif there is a relationship between two variables we check if the two variables have a correlation.  \nWhen building a hypothesis test we have to define the null hypothesis often denoted as $H_0$ which is a test that\nchecks if there is no relationship between the two variables. We then also define an alternative hypothesis $H_1$\nwhich is a test that checks if there is a relationship between the two variables.  \nThe null hypothesis is the hypothesis that we want to reject. If we reject the null hypothesis then we accept the\nalternative hypothesis. If we do not reject the null hypothesis then we do not accept the alternative hypothesis.  \nHypothesis tests also have a significance level which is the probability of rejecting the null hypothesis when it is\ntrue (so a false positive). The significance level is usually denoted as $\\alpha$ and is usually set to 0.05 (5%).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Statistics with Graphs", "Header 2": "Permutation Tests", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/statistics.mdx"}, "page_content": "When working with networks we use permutation tests to test our hypothesis because other tests assume that the data is\nindependent, which is not the case in networks for obvious reasons.  \nIn a permutation test we start with the initial state of the network and calculate the statistic of interest, i.e. the\ncorrelation coefficient. We then randomly permute the network and calculate the statistic of interest again and keep\ntrack of how many times the statistic of interest is greater than the initial statistic of interest. We then repeat this\nprocess many times and compare the statistic of interest to the distribution of the permuted statistics.  \nThe relative frequency of the statistic of interest being greater than the initial statistic of interest is the p-value.\nThe p-value is the probability of observing a statistic as extreme as the one observed given the null hypothesis is true.  \n<Callout type=\"example\">  \nThe initial Pearson Correlation between the degree centrality and the \"happiness\" attribute of the vertices is 0.4.  \nWe then permute the network 10000 times and calculate the Pearson Correlation between the degree centrality and the\n\"happiness\" attribute of the vertices for each permutation. We then count the number of times the Pearson Correlation\nwas greater or equal to 0.4 and divide it by the number of permutations.  \nSay we counted 200 times that the Pearson Correlation was greater or equal to 0.4 then the p-value would be\n$200/10000 = 0.02$. This means that there is a 2% chance of observing a Pearson Correlation as extreme as 0.4\ngiven the null hypothesis is true, i.e. we say there isn't a correlation, but there is a 2% chance of observing a\ncorrelation as extreme as 0.4, so a 2% chance of a false positive.  \nIf the p-value is less than the significance level, i.e. $p < \\alpha$, then we reject the null hypothesis and accept the\nalternative hypothesis, i.e. we say there is a correlation. If the p-value is greater than the significance level, i.e.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Statistics with Graphs", "Header 2": "Permutation Tests", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/statistics.mdx"}, "page_content": "If the p-value is less than the significance level, i.e. $p < \\alpha$, then we reject the null hypothesis and accept the\nalternative hypothesis, i.e. we say there is a correlation. If the p-value is greater than the significance level, i.e.\n$p > \\alpha$, then we accept the null hypothesis and do not accept the alternative hypothesis, i.e. we say there\nis no correlation.  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Statistics with Graphs", "Header 2": "Permutation Tests", "Header 3": "Monadic Tests", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/statistics.mdx"}, "page_content": "A monadic test is a test for a relationship between vertex attributes, for example we hypothesize that rich people\nare well connected.  \n<Callout type=\"example\">  \nAs always we set the significance level to 5%, $\\alpha = 0.05$. We then calculate the initial Pearson Correlation\nbetween the two variables.  \n$$\n\\begin{align*}\n\\text{degree} &= [0.1,0.2,0.3,0.5,0.6] \\\\\n\\text{wealth} &= [100, 400, 300, 900, 300]\n\\end{align*}\n$$  \nWe get the initial value $r=0.522$, so we would be inclined to say that there is a moderate correlation.  \nWe then permute the network by shuffling both the degree and wealth attributes and calculate the Pearson Correlation again.  \n$$\n\\begin{align*}\n\\text{degree} &= [0.5,0.3,0.1,0.6,0.2] \\\\\n\\text{wealth} &= [300, 300, 100, 400, 900]\n\\end{align*}\n$$  \nand this time get $r=-0.04$ which is a weak correlation. We repeat this process 10000 times and count the number of\ntimes $r \\leq 0.522$ and divide it by the number of permutations. Say we counted 400 times that $r \\leq 0.522$ then\nthe p-value would be $400/10000 = 0.0.4$. This means because the 4% is lower than the significance level of 5% we\nreject the null hypothesis and accept the alternative hypothesis, i.e. we say there is a correlation.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Statistics with Graphs", "Header 2": "Permutation Tests", "Header 3": "Dyadic Tests", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/statistics.mdx"}, "page_content": "Dyadic tests are tests between relationships, i.e. the edges of the network unlike the monadic tests which are tests\nbetween vertex attributes. To be able to do a dyadic test we need to have a multirelational network, i.e. a network\nwith multiple types of edges. This is then easily split into a multiplex graph, i.e. we have separate graph for each\ntype of relationship.  \n<Callout type=\"example\">\nFor example we want to test if students that study together also drink together. Then we have a network showing who\nstudies with who and a network showing who drinks with who. To then calculate the Pearson Correlation between the two\nnetworks we take their adjacency matrix, remove the diagonal elements because we are interested in relationships\nbetween different people, and then calculate the Pearson Correlation between the two flattend matrices.  \n$$\n\\begin{align*}\n\\text{study} &= \\begin{bmatrix}\n0 & 10 & 4 & 3 \\\\\n10 & 0 & 2 & 0 \\\\\n4 & 2 & 0 & 2 \\\\\n3 & 0 & 2 & 0 \\\\\n\\end{bmatrix} &= [10,4,3,10,2,0,4,2,2,3,0,2] \\\\\n\\text{drink} &= \\begin{bmatrix}\n0 & 3 & 2 & 0 \\\\\n3 & 0 & 1 & 0 \\\\\n2 & 1 & 0 & 1 \\\\\n0 & 0 & 1 & 0 \\\\\n\\end{bmatrix}  &= [3,2,0,3,1,0,2,1,1,0,0,1]\n\\end{align*}\n$$  \nBased on these vectors we can then do our hypothesis tests.\n</Callout>  \n#### QAP - Quadratic Assignment Procedure  \nWhen doing dyadic tests we can't do our normal permutation tests and just randomly shuffle the vectors because we\nwould lose the structure of the network. To solve this we use the Quadratic Assignment Procedure (QAP) which is a\npermutation test for dyadic tests.  \nWhen using QAP instead of randomly shuffling we swap an entire row and column with another.  \n<Callout type=\"example\">  \nWe start with the initial adjacency matrix of the study network and then swap the first row and column with the\nsecond row and column.  \n$$\n\\begin{align*}\n\\text{study} &= \\begin{bmatrix}\n0 & 10 & 4 & 3 \\\\\n10 & 0 & 2 & 0 \\\\\n4 & 2 & 0 & 2 \\\\\n3 & 0 & 2 & 0 \\\\\n\\end{bmatrix} \\Rightarrow \\begin{bmatrix}\n0 & 10 & 2 & 0 \\\\\n10 & 0 & 4 & 3 \\\\\n2 & 4 & 0 & 2 \\\\", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Statistics with Graphs", "Header 2": "Permutation Tests", "Header 3": "Dyadic Tests", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/statistics.mdx"}, "page_content": "second row and column.  \n$$\n\\begin{align*}\n\\text{study} &= \\begin{bmatrix}\n0 & 10 & 4 & 3 \\\\\n10 & 0 & 2 & 0 \\\\\n4 & 2 & 0 & 2 \\\\\n3 & 0 & 2 & 0 \\\\\n\\end{bmatrix} \\Rightarrow \\begin{bmatrix}\n0 & 10 & 2 & 0 \\\\\n10 & 0 & 4 & 3 \\\\\n2 & 4 & 0 & 2 \\\\\n0 & 3 & 2 & 0 \\\\\n\\end{bmatrix}\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Statistics with Graphs", "Header 2": "Permutation Tests", "Header 3": "Mixed Tests", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/graphsNetworks/statistics.mdx"}, "page_content": "But what if we want to test for a correlation between a vertex attribute and its edges? For example, we want to\ntest if people of the same gender communicate more often with each other then with of the opposite gender.  \nTo do this we call it a mixed test but in the end it is just a dyadic test. We create a new network where the edges\nsomehow signify the vertex attributes. For example, we can create a network where the people are only connected if\nthey have the same gender.  \nWe then just simply do our dyadic test.  \nAnother possible test could be to see if the age of a person is correlated with the way they communicate. To do this\nWe can create a fully connected network where the weight of the edges is the age difference between the two people.\nAnd just like that we can do a dyadic test.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Bubble Sort", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sorting/bubbleSort.mdx"}, "page_content": "When starting to study algorithms and data structures, the first algorithm that is usually taught is the Bubble Sort.\nIt is a simple sorting algorithm that is easy to understand and implement. However, it is not very efficient and is not\nused in practice.  \nThe algorithm works by repeatedly swapping neighboring elements if they are in the wrong order, i.e. if we are sorting\nan array of numbers in ascending order we check if the current number is greater than the next number, if so we swap\nthem, if not we go to the next.  \nThe algorithm is called Bubble Sort because with each iteration the largest element in the array \"bubbles up\" to the end\nof the array due to this swapping.  \n<Image\nsrc=\"/cs/algdBubbleSort.gif\"\ncaption=\"Visualization of the bubble sort algorithm.\"\nwidth={400}\n/>  \n```python\ndef bubble_sort(arr):\nfor i in range(len(arr)):\nfor j in range(len(arr) - 1):\nif arr[j] > arr[j + 1]:\narr[j], arr[j + 1] = arr[j + 1], arr[j]\n```  \nFrom the code above we can see that the algorithm has a time complexity of $O(n^2)$ and a space complexity of $O(1)$ as\nit works in place.  \nThe above implementation can be slightly by considering the fact that after each iteration the largest element at the\nend of the array is already in the correct position, so we can reduce the number of iterations by 1 each time.  \n```python\ndef bubble_sort(arr):\nfor i in range(len(arr)):\nfor j in range(len(arr) - i - 1):\nif arr[j] > arr[j + 1]:\narr[j], arr[j + 1] = arr[j + 1], arr[j]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Insertion Sort", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sorting/insertionSort.mdx"}, "page_content": "The insertion sort algorithm is a bit more complex than the bubble sort or selection sort but still relatively simple.\nYou can think of it in the following way:  \nImagine you have a deck of cards, and you want to sort them in ascending order.\nYou assume that the first card is already sorted. Then, you take the second card and compare it to the first card, if\nit is smaller, you place it in front of it. Now the first two cards are sorted. Then, you take the third card and\ncompare it to the ones before it, one after another until you find the right place to insert it, hence the name\ninsertion sort as you repeatedly insert cards into the sorted part of the deck. This way the sorted part of the array slowly grows from left to right.  \n<Image\nsrc=\"/cs/algdInsertionSort.gif\"\ncaption=\"The input array.\"\n/>  \n```python\ndef insertion_sort(arr):\nfor i in range(1, len(arr)):\nkey = arr[i]\nj = i - 1\n# Compare key with each element to the left of it until a smaller one is found\nwhile j >= 0 and key < arr[j]:\narr[j + 1] = arr[j]\nj -= 1\narr[j + 1] = key\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Stable Sorting Algorithms", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sorting/stableSorting.mdx"}, "page_content": "Whether a sorting algorithm is stable or not is quite simple to determine. If the algorithm preserves the relative order\nof equal elements, it is stable. If it does not, it is not stable.  \n<Image\nsrc=\"/cs/algdStableSorting.png\"\ncaption=\"Stable vs Unstable Sorting Algorithms.\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Counting Sort", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sorting/countingSort.mdx"}, "page_content": "The counting sort can be defined in a dumb way or a smart way that can be implemented very efficiently.  \nThe counting algorithm is mainly intended as a sub-routine for the radix sort and unlucky many other sorting algorithms,\nit does not use comparisons to sort the elements. The counting sort is generally defined to sort a list of integers\nin a known range, most commonly 0 to k, i.e. $0 \\leq A[i] \\leq k$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Counting Sort", "Header 2": "The Naive Approach", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sorting/countingSort.mdx"}, "page_content": "The naive way to define the counting sort as follows:  \n<Callout type=\"todo\">\nThe native nextra steps is shit.\n</Callout>  \n<Steps>  \n<Step id=\"1\">\n**Create the Counting Array**  \nWe create a so-called counting array, an array of size k+1 and then iterate over the input array and increment the value\nat the index of the input array in the counting array.  \n<Image\nsrc=\"/cs/algdCountingSort1.png\"\ncaption=\"The input array.\"\n/>  \n<Image\nsrc=\"/cs/algdCountingSort2.png\"\ncaption=\"The resulting counting array.\"\n/>\n</Step>\n<Step id=\"2\">\n**Generate the Sorted Array**  \nThanks to the counting array we then know how many times each value occurs in the input array and can therefore just\ngenerate the sorted array according to the counting array. Technically this means the dumb counting sort is an in-place\nsorting algorithm, because we don't actually need to allocate a new array to store the sorted array, we can just\noverwrite the input array.  \nThis is a very dumb way to do it, but it works and actually runs in $O(n+k)$ time, where $n$ is the\nlength of the input array and $k$ is the range of the input array.\n</Step>\n</Steps>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Counting Sort", "Header 2": "The Smart Approach", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sorting/countingSort.mdx"}, "page_content": "The smart way to define the counting sort is to use the counting array as a way to store the index of a value in\nthe sorted array. This implementation of the counting sort is a bit more complicated but still runs in $O(n+k)$ time.  \nHowever, the advantage of this implementation is that it is stable, i.e. it preserves the relative order of equal\nelements. The other advantage is that it is actually faster than the dumb implementation in practice because it\ncan be parallelized using the scan and scatter pattern.  \n<Steps>\n<Step id=\"1\">\n**Create the Counting Array**  \nThe first step is the same as in the dumb implementation, we create a counting array.  \n</Step>\n<Step id=\"2\">\n**Compute the Cumulative Sum**  \nWe then iterate over the counting array and compute the cumulative sum of the counts. The cumulative sum is\nalso often called the prefix or scan sum. This part that can be parallelized using the scan pattern.  \n<Image\nsrc=\"/cs/algdCountingSort3.png\"\ncaption=\"The cumalaive sum of the counting array.\"\n/>  \n</Step>\n<Step id=\"3\">\n**Scatter the Input Array**  \nThe magical thing about the counting sort is that the values in the counting array are actually the indexes of the\nvalues in the sorted array. So we can just iterate over the input array and use the value at the index of the input\narray as the index in the sorted array and then increment the value at the index of the input array in the counting\narray. This part can be parallelized using the scatter pattern (not sure how you can handle the incrementing if the same\nvalue occurs multiple times in the input array).  \nBecause of the scatter pattern, the counting sort becomes a stable and in-place sorting algorithm that runs in\n$O(n+k)$.  \n<Image\nsrc=\"/cs/algdCountingSort4.png\"\ncaption=\"The scattering of the input array into the sorted array.\"\n/>  \n</Step>\n</Steps>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Selection Sort", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/sorting/selectionSort.mdx"}, "page_content": "The selection sort algorithm is probably one of the most intuitive sorting algorithms and the one I actually use when\nsorting a deck of cards. The algorithm works by iterating through the list, finding the smallest element, and moving it\nto the front of the array. Then, the algorithm repeats this process for the rest of the array, moving the next smallest\nelement to the second position, and so on until the array is sorted.  \nThe algorithm is called a selection sort because it works by repeatedly selecting the smallest remaining element.  \n<Image\nsrc=\"/cs/algdSelectionSort.gif\"\ncaption=\"Visualization of the selection sort algorithm.\"\n/>  \n```python\ndef selection_sort(arr):\nfor i in range(len(arr)):\nmin_index = i\n# Find the index of the smallest element in the unsorted portion of the array.\nfor j in range(i + 1, len(arr)):\nif arr[j] < arr[min_index]:\nmin_index = j\narr[i], arr[min_index] = arr[min_index], arr[i]\n```  \nJust like the bubble sort, the selection sort is an in-place sorting algorithm and has a time complexity of $O(n^2)$ and\na space complexity of $O(1)$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to DP", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/introduction.mdx"}, "page_content": "Dynamic programming, short DP, is a problem-solving technique or more formally an algorithmic design paradigm just like \"divide\nand conquer\" or a \"greedy algorithm\". It is used to solve problems that can be broken down into sub-problems (just like\ndivide and conquer) which are then solved recursively. For a problem to be solved using dynamic programming, it must\nhave two properties:  \n- **Overlapping Sub-problems**: When the problem is broken down into sub-problems, the same sub-problems are solved\nmultiple times, i.e. there is an overlap.\n- **Optimal Substructure**: When the most optimal solution for the original problem can be constructed using the\noptimal solutions of the sub-problems.  \nWe can illustrate these two properties using the Fibonacci sequence. The Fibonacci sequence is defined as follows:  \n```java\npublic int fib(int n) {\nif (n <= 1)\nreturn n;\nreturn fib(n - 1) + fib(n - 2);\n}\n```  \nWhen we illustrate the recursive calls of the `fib` function as a tree (always a good idea when working with dynamic\nprogramming problems), we can see that the same sub-problems are solved multiple times. For example for `fib(6)` we can\nsee that `fib(3)` is solved three times, so there is an overlap.\nThe other property, optimal substructure, is also satisfied. The optimal solution for `fib(6)` is constructed using the\noptimal solutions of `fib(5)` and `fib(4)`.  \n<Image\nsrc=\"/cs/dpFibTree.png\"\ncaption=\"The recursive calls of the fibonnaci function as a tree.\"\n/>  \nFrom the tree above we can also see that the time complexity of the `fib` function is exponential, i.e. `O(2^n)`. This\nis because the same sub-problems are solved multiple times. As we will see later, dynamic programming can be used to\nimprove the time complexity of the `fib` function to `O(n)`. This is a huge improvement and is most often the reason why\ndynamic programming is used because it can drastically improve the time complexity of a function.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to DP", "Header 2": "Top-Down Approach (Memoization)", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/introduction.mdx"}, "page_content": "The top-down approach is the most common way to solve dynamic programming problems. It is also called **memoization**.\nThe idea is to store the results of the sub-problems so that we do not have to re-compute them when they are needed\nagain later due to the overlapping sub-problems property. This technique is called memoization because we store the\nresults of the sub-problems in a lookup table (memo).  \nIt is called top-down because we still start with the original problem and break it down into sub-problems and solve\nthem recursively.  \nWhen implementing memoization it is important to think about the data structure that will be used to store the results\nas we want quick lookups. This leads to most implementations using either just a simple array where the index is the\ninput to the function or a hash map where the key is the input to the function.  \n```java\npublic int fib(int n) {\nif (n < 0)\nthrow new IllegalArgumentException(\"n must be greater than or equal to 0\");\nif (n <= 1)\nreturn n;\n\nInteger[] memo = new Integer[n + 1]; // This uses more memory than a simple array but is more convenient\n\n// base cases\nmemo[0] = 0;\nmemo[1] = 1;\nreturn fibMemo(n, memo);\n}\n\npublic int fibMemo(int n, int[] memo) {\nif (memo[n] != null)\nreturn memo[n];\nmemo[n] = fibMemo(n - 1, memo) + fibMemo(n - 2, memo);\nreturn memo[n];\n}\n```  \nAfter implementing the memoization technique, we can see in the tree below that the time complexity of the `fib`\nfunction is now `O(n)` as each sub-problem is only solved once.  \n<Image\nsrc=\"/cs/dpFibTreeMemo.png\"\ncaption=\"The recursive calls of the fibonnaci function as a tree using memoization.\"\nwidth={400}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to DP", "Header 2": "Bottom-Up Approach (Tabulation)", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/introduction.mdx"}, "page_content": "The bottom-up approach is the other way to solve dynamic programming problems. It is also called **tabulation**. The\nidea is to solve the sub-problems first, i.e. some of the base cases, and then use the results of those sub-problems to\nsolve the original problem, hence the name bottom-up. This technique is called tabulation because we store the results\nof the sub-problems in a table (depending on the problem, this can be a 1D or 2D array).  \nWhen implementing memoization it helped to visualize the recursive calls as a tree. When implementing tabulation it\nis also additionally helpful to visualize the results as a table or list (depending on the problem) to find a pattern.  \nFor a visualisation of the tabulation technique I can recommend watching [this video](https://youtu.be/oBt53YbR9Kk?t=11513)\nat the 3:11:50 mark. The whole video is great and I can recommend watching it all and also the 4 part [video series by\nMIT on dynamic programming from 2020](https://www.youtube.com/watch?v=r4-cftqTcdI&t=7s).  \nFor the Fibonacci sequence, we can see that the base cases are `fib(0)` and `fib(1)`. We can then use those results to\nthen iteratively solve the rest of the sub-problems until we reach the original problem.  \n```java\npublic int fib(int n) {\nif (n < 0)\nthrow new IllegalArgumentException(\"n must be greater than or equal to 0\");\nif (n <= 1)\nreturn n;\n\nint[] memo = new int[n + 1];\n\n// base cases\nmemo[0] = 0;\nmemo[1] = 1;\n\nfor (int i = 2; i <= n; i++) {\nmemo[i] = memo[i - 1] + memo[i - 2];\n}\nreturn memo[n];\n}\n```  \nThe above code then again results in a time complexity of `O(n)`, much better than the original `O(2^n)`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coins in a Line", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/coinsLine.mdx"}, "page_content": "This game is a tricky little coding problem that works has the following rules:  \n- There are an even number $n$ of coins in a line, with values $v_1, v_2, ..., v_n$, i.e. $v_i$ is the value of the i-th coin.\n- Two players, often called Alice and Bob, take turns to take a coin either from the left or the right end of the line\nuntil there are no more coins left.\n- The player whose coins have the higher total value wins.  \n<Image\nsrc=\"/cs/dpCoinsLine.png\"\ncaption=\"Coins in a Line Game.\"\nwidth={600}\n/>  \nThe goal is to find an algorithm that maximizes the value of the coins that the first player (Alice) gets.  \n<Callout type=\"example\">\nThere are 4 coins with values [1, 2, 3, 4], Alice will get the maximum value of 6 by taking the\nlast coin twice (4 + 2), assuming Bob also plays optimally.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coins in a Line", "Header 2": "Greedy Algorithm", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/coinsLine.mdx"}, "page_content": "This game isn't as simple as it seems, and it's not immediately obvious how to solve it. Most commonly people will\nstart with a greedy algorithm, which is to take the coin with the highest value at each turn. This is a good start,\nand will win in the example above, but it's not optimal. Consider the following example:  \n<Callout type=\"example\">\nThere are again 4 coins but with the values [5, 10, 25, 10].  \n1. Alice takes the right coin with value 10.\n2. Bob takes the right coin with value 25.\n3. Alice takes the right coin with value 10.\n4. Bob takes the last coin with value 5.  \nAlice will have a total value of 20, and Bob will have a total value of 30. Bob wins!\n</Callout>  \nBy tweaking the greedy algorithm, we can get an algorithm that will always win, but not necessarily get the maximum\nvalue. Instead of taking the coin with the highest value, Alice first calculates the total value of coins in the odd\npositions, and then calculates the total value of coins in the even positions (starting at 0). She then takes the coin\nin the positions with the highest total sum.  \n<Callout type=\"example\">\nThere are again 6 coins but with the values [1,3,6,3,1,3]. First Alice calculates the total value of coins in the\neven positions, which is 1 + 6 + 1 = 8. Then she calculates the total value of coins in the odd positions, which\nis 3 + 3 + 3 = 9. So she takes the coins in the odd positions. If Bob uses the greedy approach we get the following:  \n1. Alice takes the right coin with value 3 (original position=5).\n2. Bob takes the left coin with value 1.\n3. Alice takes the left coin with value 3 (original position=1).\n4. Bob takes the left coin with value 6.\n5. Alice takes the left coin with value 3 (original position=3).\n6. Bob takes the last coin with value 1.  \nAlice will have a total value of 9, and Bob will have a total value of 8. Alice wins, but there is a way to get 10!  \nIf Bob uses the same tweaked greedy approach as Alice, we get the following:  \n1. Alice takes the right coin with value 3 (original position=5).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coins in a Line", "Header 2": "Greedy Algorithm", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/coinsLine.mdx"}, "page_content": "If Bob uses the same tweaked greedy approach as Alice, we get the following:  \n1. Alice takes the right coin with value 3 (original position=5).\n2. Bob can't take an odd position coin, so he can take either coin as they are both odd positions and have the same value.\nLet's say he takes the left coin with value 1 because he built his algorithm to scan from left to right.\n3. Alice takes the left coin with value 3 (original position=1).\n4. Bob again can't take an odd position coin, but he takes the left coin with value 6 because it has a higher value\nthan the right coin with value 1.\n5. Alice takes the left coin with value 3 (original position=3)\n6. Bob takes the last coin with value 1.  \nThe result is the same as if Bob used the normal greedy approach, because Alice always take the coins away from Bob\nas she gets to go first.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Coins in a Line", "Header 2": "Dynamic Programming Algorithm", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/coinsLine.mdx"}, "page_content": "We always assume that Bob will play optimally, optimally meaning that he will always take the coin which minimizes the\n**total amount** of coins that Alice can get.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Knapsack Problem", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/knapsack.mdx"}, "page_content": "The knapsack problem is a very popular problem with many different variations. The problem is as follows:  \n> Given a set of items, each with a weight and a value, determine which items you should pick to maximize the value\n> while keeping the overall weight smaller than the limit of your knapsack (backpack).  \n<Image\nsrc=\"/cs/dpKnapsack.png\"\ncaption=\"The knapsack problem.\"\nwidth={300}\n/>  \nSome popular variations of the knapsack problem are:  \n- 0/1 Knapsack: You can either take an item or not take it.\n- Unbounded Knapsack: You can take an item multiple times.\n- Bounded Knapsack: You can take an item a limited number of times.\n- Fractional Knapsack: You can take a fraction of an item.  \nThe [subset sum problem](./subsetSum) is a variation of the knapsack problem where the weight of each item is equal to its value and\nthe goal is not to maximize the value but to get a specific value and weight. In my definition of the subset sum problem I allowed\nan item to be used multiple times, so it is a variation of the unbounded knapsack problem.  \n<Callout type=\"todo\">\nActually implement the knapsack problem with the different variations.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "For the subset sum problem, we are given an array of integers and a target sum, to keep it simple we will assume that\nthe array only contains positive integers and that the target sum is also positive. We will also allow an element in the\narray to be used multiple times.  \nFrom this input we can then ask the following questions:  \n- Is there a subset of the array that sums to the target sum? I will call this the `canSum` problem.\n- How many subsets of the array sum to the target sum? I will call this the `countSum` problem.\n- If there is a subset that sums to the target sum, what is the subset? I will call this the `howSum` problem.\n- If there is a subset that sums to the target sum, what is the minimum number of elements in the subset? I will call\nthis the `bestSum` problem.  \n<Callout type=\"example\">  \nIf we are given the array `[2, 3, 5]` and the target sum `8`, then the answers to the above questions are:  \n- `canSum(8, [2, 3, 5]) = true`\n- `countSum(8, [2, 3, 5]) = 2` (the subsets are `[2, 2, 2, 2]` and `[3, 5]`)\n- `howSum(8, [2, 3, 5]) = [2, 2, 2, 2]`\n- `bestSum(8, [2, 3, 5]) = [3, 5]`  \nAnd for the array `[2, 4]` and the target sum `7` we get:  \n- `canSum(7, [2, 4]) = false`\n- `countSum(7, [2, 4]) = 0`\n- `howSum(7, [2, 4]) = null`\n- `bestSum(7, [2, 4]) = null`  \nAnd for an example that is not so trivial, we can use the array `[1, 2, 5, 25]` and the target sum `100`:  \n- `canSum(100, [1, 2, 5, 25]) = true`\n- `countSum(100, [1, 2, 5, 25]) = 154050750` seems about right\n- `howSum(100, [1, 2, 5, 25]) = [1,1,1,1,1...1]` (100 times) because of the order of the for loop\n- `bestSum(100, [1, 2, 5, 25]) = [25, 25, 25, 25]`  \n</Callout>  \nThe subset sum problem is a very popular problem but also a very hard problem computationally. As will become clearer\nbelow the time complexity of the subset sum problem is `O(n^m)` where `n` is the length of the array and `m` is the\ntarget sum. This is because we have to try all possible combinations of the elements in the array to find a subset that", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "below the time complexity of the subset sum problem is `O(n^m)` where `n` is the length of the array and `m` is the\ntarget sum. This is because we have to try all possible combinations of the elements in the array to find a subset that\nsums to the target sum. This is also why dynamic programming is so useful for this problem because it can drastically\nimprove the time complexity.  \n<Callout type=\"todo\">\nWhat does it mean for a problem to be NP-complete? Is the subset sum problem NP-complete etc.?\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "Header 2": "Can Sum", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "Our first approach to this problem is most lightly a brute force approach. We can use recursion to solve this problem\nby trying to subtract each element in the array from the target sum and then recursively calling the function again with\nthe new target sum. If the target sum is 0 then we have found a subset that sums to the target, and we can return\ntrue. If the target sum is negative then we have not found a subset that sums to the target sum and we can return false.\nThese results are then propagated back up the call stack until we reach the original call (the parent node in the tree\nbecomes true if any of its children are true and otherwise false).  \nWe can construct the following tree to visualize the recursive calls:  \n<Image\nsrc=\"/cs/dpCanSumTree.png\"\ncaption=\"The recursive calls of the canSum function as a tree.\"\n/>  \n```java\npublic boolean canSum(int targetSum, int[] numbers) {\nif (targetSum == 0)\nreturn true;\nif (targetSum < 0)\nreturn false;\n\nfor (int num : numbers) {\nint remainder = targetSum - num;\nif (canSum(remainder, numbers))\nreturn true;\n}\nreturn false;\n}\n```  \nFrom the tree above we can see that the time complexity of the `canSum` function is `O(n^m)` where `n` is the length of\nthe array (the number of children per node) and `m` is the target sum (the depth of the tree, which would be maximal if\nthe array contained a 1). We can improve the time complexity of the `canSum` function to `O(n*m)` by using memoization.  \n```java\npublic boolean canSum(int targetSum, int[] numbers) {\nif (targetSum < 0)\nthrow new IllegalArgumentException(\"targetSum must be greater than or equal to 0\");\n\nboolean[] memo = new boolean[targetSum + 1];\nArrays.fill(memo, false); // not needed but makes it more clear\nmemo[0] = true;\n\nreturn canSumMemo(targetSum, numbers, memo);\n}\n\npublic boolean canSumMemo(int targetSum, int[] numbers, boolean[] memo) {\nif (memo[targetSum])\nreturn true;\nif (targetSum < 0)\nreturn false;", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "Header 2": "Can Sum", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "return canSumMemo(targetSum, numbers, memo);\n}\n\npublic boolean canSumMemo(int targetSum, int[] numbers, boolean[] memo) {\nif (memo[targetSum])\nreturn true;\nif (targetSum < 0)\nreturn false;\n\nfor (int num : numbers) {\nint remainder = targetSum - num;\nif (canSumMemo(remainder, numbers, memo)) {\nmemo[targetSum] = true;\nreturn true;\n}\n}\nmemo[targetSum] = false;\nreturn false;\n}\n```  \nTo use tabulation instead of memoization we would need to construct a table (array) of size `targetSum + 1` and then\nfill it with the base cases and find some sort of pattern. So we would initially fill the list with `false` and then\nset the index 0 to `true` because the target sum 0 can always be constructed using an empty array. Then we need to\ndo something thinking to find the pattern.  \nIf we think of our current position in the array as the target sum, i.e. in the first iteration we are at index 0, then\nwe know that we can construct the target sums where we add each number in the array to the current position. For example\nif we are at index 0 and the array is `[5,4,3]` and we have the target 7 then we know that we can construct the target\nsums 5,4 and 3 by adding the number at index 0 to the current position. So we can set the values at index 5, 4 and 3 to\n`true`. We can then move on and set our current index to 1 and we know that we can't construct the target sum 1 using\nthe array so we can skip it, same goes for index 2. But we can construct the target sum 3, so it gets interesting again.\nWe can then again add each number in the array to the current position and set the values at index 8, 7 and 6 to `true`.\nThis process continues until we reach the end of the array. If we then return the value at the last index we will have\nour result.  \nThis [blog post](https://teepika-r-m.medium.com/dynamic-programming-basics-part-2-758b00e0a4b0) visualizes the process very well.  \n```java\npublic boolean canSum(int targetSum, int[] numbers) {\nif (targetSum < 0)\nthrow new IllegalArgumentException(\"targetSum must be greater than or equal to 0\");", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "Header 2": "Can Sum", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "boolean[] table = new boolean[targetSum + 1];\nArrays.fill(table, false); // not needed but makes it more clear\ntable[0] = true;\n\nfor (int i = 0; i <= targetSum; i++) {\nif (table[i]) {\nfor (int num : numbers) {\nif (i + num < table.length)\ntable[i + num] = true;\n}\n}\n}\nreturn memo[targetSum];\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "Header 2": "Count Sum", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "The `countSum` problem is very similar to the `canSum` problem. The only difference is that when the target sum is 0 we\nreturn 1 instead of true and when the target sum is negative we return 0 instead of false and then in the parent node\nwe sum up the results of the children.  \n<Image\nsrc=\"/cs/dpCountSumTree.png\"\ncaption=\"The recursive calls of the countSum function as a tree.\"\n/>  \nThe brute force approach would look like this with a time complexity of `O(n^m)`:  \n```java\npublic int countSum(int targetSum, int[] numbers) {\nif (targetSum == 0)\nreturn 1;\nif (targetSum < 0)\nreturn 0;\n\nint count = 0;\nfor (int num : numbers) {\nint remainder = targetSum - num;\ncount += countSum(remainder, numbers);\n}\nreturn count;\n}\n```  \nAnd the memoized version would look like this with a time complexity of `O(n*m)`:  \n```java\npublic int countSum(int targetSum, int[] numbers) {\nif (targetSum < 0)\nthrow new IllegalArgumentException(\"targetSum must be greater than or equal to 0\");\n\nint[] memo = new int[targetSum + 1];\nArrays.fill(memo, -1);\nmemo[0] = 1;\n\nreturn countSumMemo(targetSum, numbers, memo);\n}\n\npublic int countSumMemo(int targetSum, int[] numbers, int[] memo) {\nif (targetSum < 0)\nreturn 0;\nif (memo[targetSum] != -1)\nreturn memo[targetSum];\n\nint count = 0;\nfor (int num : numbers) {\nint remainder = targetSum - num;\ncount += countSumMemo(remainder, numbers, memo);\n}\nmemo[targetSum] = count;\nreturn count;\n}\n```  \nOne issue is that it will count the same subset multiple times but with different ordering of the elements, as we can\nsee in the tree above.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "Header 2": "How Sum", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "The `howSum` problem is again a variation of the `canSum` problem. The only difference is that when the target sum is 0\nwe return an empty array instead of true and when the target sum is negative we return null instead of false and then\nin the parent node we return the array with the element that was used to get to the target sum. To solve this problem\nit doesn't matter if the array is the shortest or longest possible array that sums to the target sum it will just be\none of the possible solutions (The furthest left solution in the tree above because of the order of the for loop and\nthe recursive call).  \n```java\npublic int[] howSum(int targetSum, int[] numbers) {\nif (targetSum == 0)\nreturn new int[0];\nif (targetSum < 0)\nreturn null;\n\nfor (int num : numbers) {\nint remainder = targetSum - num;\nint[] result = howSum(remainder, numbers);\nif (result != null) {\nint[] newArray = new int[result.length + 1];\nSystem.arraycopy(result, 0, newArray, 0, result.length); // O(n)\nnewArray[result.length] = num;\nreturn newArray;\n}\n}\nreturn null;\n}\n```  \nWith memoization:  \n```java\npublic int[] howSum(int targetSum, int[] numbers) {\nif (targetSum < 0)\nthrow new IllegalArgumentException(\"targetSum must be greater than or equal to 0\");\n\nint[][] memo = new int[targetSum + 1][]; // will be jagged array\nArrays.fill(memo, null); // not needed but makes it more clear\nmemo[0] = new int[0];\n\nreturn howSumMemo(targetSum, numbers, memo);\n}\n\npublic int[] howSumMemo(int targetSum, int[] numbers, int[][] memo) {\nif (targetSum < 0)\nreturn null;\nif (memo[targetSum] != null)\nreturn memo[targetSum];\n\nfor (int num : numbers) {\nint remainder = targetSum - num;\nint[] result = howSumMemo(remainder, numbers, memo);\nif (result != null) {\nint[] newArray = new int[result.length + 1];\nSystem.arraycopy(result, 0, newArray, 0, result.length); // O(n)\nnewArray[result.length] = num;\nmemo[targetSum] = newArray;\nreturn newArray;\n}\n}\nmemo[targetSum] = null;\nreturn null;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "Header 2": "Best Sum", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "The `bestSum` problem is again a variation of the `howSum` problem. It is very similar to the `howSum` problem but\ninstead of returning the first array that sums to the target sum, we return the shortest array that sums to the target\nsum.  \n```java\npublic int[] bestSum(int targetSum, int[] numbers) {\nif (targetSum == 0)\nreturn new int[0];\nif (targetSum < 0)\nreturn null;\n\nint[] shortestArray = null;\nfor (int num : numbers) {\nint remainder = targetSum - num;\nint[] result = bestSum(remainder, numbers);\nif (result != null) {\nint[] newArray = new int[result.length + 1];\nSystem.arraycopy(result, 0, newArray, 0, result.length); // O(n)\nnewArray[result.length] = num;\nif (shortestArray == null || newArray.length < shortestArray.length)\nshortestArray = newArray;\n}\n}\nreturn shortestArray;\n}\n```  \nWith memoization:  \n```java\npublic int[] bestSum(int targetSum, int[] numbers) {\nif (targetSum < 0)\nthrow new IllegalArgumentException(\"targetSum must be greater than or equal to 0\");\n\nint[][] memo = new int[targetSum + 1][]; // will be jagged array\nArrays.fill(memo, null); // not needed but makes it more clear\nmemo[0] = new int[0];\n\nreturn bestSumMemo(targetSum, numbers, memo);\n}\n\npublic int[] bestSumMemo(int targetSum, int[] numbers, int[][] memo) {\nif (targetSum < 0)\nreturn null;\nif (memo[targetSum] != null)\nreturn memo[targetSum];\n\nint[] shortestArray = null;\nfor (int num : numbers) {\nint remainder = targetSum - num;\nint[] result = bestSumMemo(remainder, numbers, memo);\nif (result != null) {\nint[] newArray = new int[result.length + 1];\nSystem.arraycopy(result, 0, newArray, 0, result.length); // O(n)\nnewArray[result.length] = num;\nif (shortestArray == null || newArray.length < shortestArray.length)\nshortestArray = newArray;\n}\n}\nmemo[targetSum] = shortestArray;\nreturn shortestArray;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Subset Sum Problem", "Header 2": "All Sum", "path": "../pages/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/subsetSum.mdx"}, "page_content": "The `allSum` problem is again a variation of the `canSum` problem, and is almost a combination of the `countSum` and\n`howSum` problems. However, it is a bit more complicated because we need to return a list of arrays instead of just one\nresult.  \n<Callout type=\"todo\">\nCan't be bothered to implement this right now. Maybe later. Same goes for the tabulation versions of the above\nproblems.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "All collections are in the namespace `System.Collections` and are structered like the following  \n<Image\nsrc=\"/cs/csharpCollections.png\"\ncaption=\"Hierarchy of collections in C#\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Arrays", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "Whether an array element type is a value type or a reference type has important performance implications. Value type they all get given the default value, where as reference they all have null references.  \n```csharp\nchar[] vowels = new char[5]; // Declare an array of 5 characters\nchar[] vowels = new char[] {'a','e','i','o','u'};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Arrays", "Header 3": "Rectangular arrays", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "```csharp\nint[,] a = new int[4,6];\nint[,] b = new int[,]\n{\n{0,1,2},\n{3,4,5},\n{6,7,8}\n};\nint[, ,] c = new int[2, 4, 2];\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Arrays", "Header 3": "Jagged arrays", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "Jagged arrays are declared using successive square brackets to represent each\ndimension. Each inner array is implicitly initialized to null. You must manually create each inner array.  \n```csharp\nint[][] a = new int[2][];\na[0] = new int []{ 1, 2, 3, 4}\na[1] = new int []{ 4, 5, 6};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Arrays", "Header 3": "Simplified Array Initialization", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "Can be done even simpler with var, but has shit readability IMO.  \n```csharp\nchar[] vowels = {'a','e','i','o','u'};\nint[,] rectangularMatrix =\n{\n{0,1,2},\n{3,4,5},\n{6,7,8}\n};\nint[][] jaggedMatrix =\n{\nnew int[] {0,1,2},\nnew int[] {3,4,5},\nnew int[] {6,7,8,9}\n};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Indices", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "Indices let you refer to elements relative to the end of an array, with the `^` operator.  \n```csharp\nchar[] vowels = new char[] {'a','e','i','o','u'};\nchar lastElement = vowels [^1]; // 'u'\nchar secondToLast = vowels [^2]; // 'o'\nIndex first = 0;\nIndex last = ^1;\nchar firstElement = vowels[first]; // 'a'\nchar lastElement = vowels[last]; // 'u'\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Ranges", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "Ranges let you “slice” an array by using the `..` operator. The second number in the range is exclusive.  \n```csharp\nchar[] vowels = new char[] {'a','e','i','o','u'};\nchar[] firstTwo = vowels [..2]; // 'a', 'e'\nchar[] lastThree = vowels [2..]; // 'i', 'o', 'u'\nchar[] middleOne = vowels [2..3]; // 'i'\nchar[] lastTwo = vowels [^2..]; // 'o', 'u' can also be combined with indices\n\nRange firstTwoRange = 0..2;\nchar[] firstTwo = vowels [firstTwoRange]; // 'a', 'e'\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Indexers", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "```csharp\nclass Sentence\n{\nstring[] words = \"The quick brown fox\".Split();\npublic string this [int wordNum]\n{\nget { return words [wordNum];}\nset { words [wordNum] = value;}\n}\n}\n\n\nSentence s = new Sentence();\nConsole.WriteLine(s[3]); // fox\ns[3] = \"kangaroo\";\nConsole.WriteLine(s[3]); // kangaroo\nConsole.WriteLine(s); // The quick brown kangaroo\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Indexers", "Header 3": "Using indices and ranges with indexers", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "```csharp\npublic string this [Index index] => words [index];\npublic string[] this [Range range] => words [range];\n\n// Enables us to do this\nSentence s = new Sentence();\nConsole.WriteLine (s [^1]); // fox\nstring[] firstTwoWords = s [..2]; // (The, quick)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Generics", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "Generics express reusability through placeholder types and are similar to generics in java.  \n```csharp\nclass Buffer<TElement, TPriority>\n{\nprivate TElement[] data;\nprivate TPriority[] prio;\npublic void Put(TElementx, TPriorityprio) {...}\npublic void Get(out TElementx, out TPriorityprio) {...}\n}\n\nvar a = new Buffer<int,int>();\na.Put(100, 0);\nint elem, prio;\na.Get(out elem, out prio);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Collections and Generics", "Header 2": "Generics", "Header 3": "Bounding", "path": "../pages/digitalGarden/cs/csharp/collectionsGenerics.mdx"}, "page_content": "This method sorts any array , as long as the element `T` implements `IComparable`  \n```csharp\nstatic void Sort<T>(T[] a) where T : IComparable\n{\nfor(int i = 0; i < a.Length 1; i++)\nfor(int j = i + 1; j < a.Length ; j++)\nif (a[j].CompareTo(a[i]) < 0)\n{\nT x = a[i];\na[i] = a[j];\na[j] = x;\n}\n}\n}\n}\n\nint[] a = {3, 7, 2, 5};\nSort<int>(a);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Reflection", "path": "../pages/digitalGarden/cs/csharp/reflection.mdx"}, "page_content": "Is the ability to inspect and change information about assemblies, types and code at runtime. Is used for plugin development, logging, code analysis etc.\nAssemblies contain modules => Modules contain types => Types contain code  \n```csharp\nvar a = Assembly.LoadFrom(\"MyAssembly.dll\");\nvar b = Assembly.GetExecutingAssembly();\n\n// List all types in assembly\nType[] types = a.GetTypes();\nforeach (var t in types)\nConsole.WriteLine(t.FullName);\nType t = assembly.GetType(\"Namespace.SomeClass\");\nforeach (MethodInfo m in t.GetMethods())\nConsole.WriteLine(m.Name);\n\nobject o = a.CreateInstance(\"MyPlugin.HelloWorld\");\nType hwClass = a.GetType(\"MyPlugin.MethodInfo\");\nMethodInfo mi = hwClass.GetMethod(\"ToString\"); // by default only public\nMethodInfo mi = hwClass.GetMethod(\"SayItPrivate\", BindingFlags.Instance |\nBindingFlags.NonPublic);\nobject retVal = mi.Invoke(o, null);\n// Does class implement interface?\nType type = assembly.GetTypes () // scan all types\n.Where(t => t.IsClass) // it must by a class , that\n.Single(t => t.GetInterface(\"IMyInterface\" != null ); // implements my interface\nIMyInterface myObj = assembly.CreateInstance(type.FullName) as IMyInterface;\n// call method\ndouble res = myObj.DoSomething(\"input\", 20);\n```  \n<Image\nsrc=\"/cs/csharpReflectionApi.png\"\ncaption=\"Reflection API in C#\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Reflection", "Header 2": "System.Type", "path": "../pages/digitalGarden/cs/csharp/reflection.mdx"}, "page_content": "Represents type declarations: class types, interface types, array and value types, enumeration types, type parameters, and more.\nValue, Interface or Class? => IsValueType , IsInterface , IsClass\nPublic, Private or Sealed => IsNotPublic , IsSealed\nAbstract or Implementation? => IsAbstract", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Reflection", "Header 2": "Attributes", "path": "../pages/digitalGarden/cs/csharp/reflection.mdx"}, "page_content": "```csharp\n[Serializable]\nclass C {...} // marks the class as serializable\n\n// will force compiler to produce a message\npublic class ObsoleteAttribute : Attribute\n{\npublic string Message { get {...}\npublic bool IsError { get {...} set {...}\npublic ObsoleteAttribute() {...}\npublic ObsoleteAttribute(string msg) {...}\npublic ObsoleteAttribute(string msg , bool error) {...}\n}\n\n[Obsolete(\"Message Use class C1 instead\", true)]\npublic class C {.. }\n\n// Querying attributes at runtime\nType t = typeof(C);\nobject[] a = t.GetCustomAttributes(typeof(Comment), true);\nComment ca = (Comment)a[0];\nConsole.WriteLine(ca.Text + \", \" + ca.Author);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Reflection", "Header 2": "Attributes", "Header 3": "AttributeUsage", "path": "../pages/digitalGarden/cs/csharp/reflection.mdx"}, "page_content": "```csharp\npublic class AttributeUsageAttribute : Attribute{\npublic AttributeUsageAttribute(AttributeTargets validOn){...}\npublic bool AllowMultiple { get; set; } // default: false\npublic bool Inherited { get; set; } // default: true\npublic AttributeTargets ValidOn { get; set; } // default: All\npublic virtual Object TypeId {get;}\n}\n\n[AttributeUsage(AttributeTargets.Class | AttributeTargets.Interface, AllowMultiple = false)]\npublic class MyAttribute : Attribute {...}\n```  \nvalidOn => to which program elements is the attribute applicable?\nAllowMultiple => can it be applied to the same program element multiple times?\nInherited => is it inherited by subclasses?\nTypeID => when implemented, gets unique identifier for derived attribute classes", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrency", "path": "../pages/digitalGarden/cs/csharp/concurrency.mdx"}, "page_content": "```csharp\npublic partial class MainWindow : Window\n{\npublic MainWindow()\n{\nInitializeComponent();\n}\n\n// TASK IS FROZEN\nprivate void executeSync_Click(object sender, RoutedEventArgs e)\n{\nvar watch = System.Diagnostics.Stopwatch.StartNew();\n\nRunDownloadSync();\n\nwatch.Stop();\nvar elapsedMs = watch.ElapsedMilliseconds;\n\nresultsWindow.Text += $\"Total execution time: { elapsedMs }\";\n}\n\nprivate async void executeAsync_Click(object sender, RoutedEventArgs e)\n{\nvar watch = System.Diagnostics.Stopwatch.StartNew();\n// Without await it would first print out time then all the sites\nawait RunDownloadParallelAsync();\n\nwatch.Stop();\nvar elapsedMs = watch.ElapsedMilliseconds;\n\nresultsWindow.Text += $\"Total execution time: { elapsedMs }\";\n}\n\nprivate List<string> PrepData()\n{\nList<string> output = new List<string>();\n\nresultsWindow.Text = \"\";\n\noutput.Add(\"https://www.yahoo.com\");\noutput.Add(\"https://www.google.com\");\noutput.Add(\"https://www.microsoft.com\");\noutput.Add(\"https://www.cnn.com\");\noutput.Add(\"https://www.codeproject.com\");\noutput.Add(\"https://www.stackoverflow.com\");\n\nreturn output;\n}\n\nprivate async Task RunDownloadAsync()\n{\nList<string> websites = PrepData();\n\nforeach (string site in websites)\n{\nWebsiteDataModel results = await Task.Run(() => DownloadWebsite(site));\nReportWebsiteInfo(results);\n}\n}\n\nprivate async Task RunDownloadParallelAsync()\n{\nList<string> websites = PrepData();\nList<Task<WebsiteDataModel>> tasks = new List<Task<WebsiteDataModel>>();\n\nforeach (string site in websites)\n{\n\n// tasks.Add(DownloadWebsiteAsync(site));\ntasks.Add(Task.Run(()=>DownloadWebsite(site)))\n}\n\nvar results = await Task.WhenAll(tasks);\n\nforeach (var item in results)\n{\nReportWebsiteInfo(item);\n}\n}\n\nprivate void RunDownloadSync()\n{\nList<string> websites = PrepData();\n\nforeach (string site in websites)\n{\nWebsiteDataModel results = DownloadWebsite(site);\nReportWebsiteInfo(results);\n}\n}\n\nprivate WebsiteDataModel DownloadWebsite(string websiteURL)\n{\nWebsiteDataModel output = new WebsiteDataModel();\nWebClient client = new WebClient();", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrency", "path": "../pages/digitalGarden/cs/csharp/concurrency.mdx"}, "page_content": "private WebsiteDataModel DownloadWebsite(string websiteURL)\n{\nWebsiteDataModel output = new WebsiteDataModel();\nWebClient client = new WebClient();\n\noutput.WebsiteUrl = websiteURL;\noutput.WebsiteData = client.DownloadString(websiteURL);\n\nreturn output;\n}\n\nprivate async Task<WebsiteDataModel> DownloadWebsiteAsync(string websiteURL)\n{\nWebsiteDataModel output = new WebsiteDataModel();\nWebClient client = new WebClient();\n\noutput.WebsiteUrl = websiteURL;\noutput.WebsiteData = await client.DownloadStringTaskAsync(websiteURL);\n\nreturn output;\n}\n\nprivate void ReportWebsiteInfo(WebsiteDataModel data)\n{\nresultsWindow.Text += $\"{ data.WebsiteUrl } downloaded: { data.WebsiteData.Length } characters long.{ Environment.NewLine }\";\n}\n}\n```  \n```csharp\nasyncTask<int> Delay1() { awaitTask.Delay(1000); return1; }\nasyncTask<int> Delay2() { awaitTask.Delay(2000); return2; }\nasyncTask<int> Delay3() { awaitTask.Delay(3000); return3; }\n\nvart1 = Delay1();\nvart2 = Delay2();\nvart3 = Delay3();\n\n// Include handling of results ↓↓↓ this takes 3s in total ↓↓↓\nConsole.WriteLine(\"{0} {1} {2}\", awaitt1, awaitt2, awaitt3);\n\n// Or, without handling the results:\nawait Task.WhenAll(t1, t2, t3);\nConsole.WriteLine(\"All done!\");\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Building Blocks of C#", "Header 2": ".NET", "path": "../pages/digitalGarden/cs/csharp/buildingBlocks.mdx"}, "page_content": "The key component is the CLR (Common Language Runtime) which contains Exception Handling, Garbage Collection, and compilers that converts the code to CIL (Common Intermediate Language)+metadata which is then used to compile JIT (Just in time), prior to execution to native machine code which makes it cross-platform. .NET emphasizes language interoperability (using multiple Languages like C++ and C# together) and platform independence.  \nThe .Net Framework also includes a set of standard Libraries.  \nAll the codes that is controlled by the CLR is called managed code, while the parts of the program which are written using the `unsafe` keyword are called unmanaged code. These are beyond the control of the CLR.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Building Blocks of C#", "Header 2": ".NET", "Header 3": "Assemblies", "path": "../pages/digitalGarden/cs/csharp/buildingBlocks.mdx"}, "page_content": "Assemblies form the fundamental units of deployment, version control, reuse. Assemblies take the form of executable (_.exe_) or dynamic link library (_.dll_) files, and are the building blocks of .NET applications. They contain a Manifest(assembly name, version number list of modules/types), metadata(dynamic loading versioning, reflection) and the CIL Code of each Class.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Building Blocks of C#", "Header 2": ".NET", "Header 3": "Namespaces", "path": "../pages/digitalGarden/cs/csharp/buildingBlocks.mdx"}, "page_content": "Namespaces allow for the logical grouping of related types, like packages in Java.\nAnd  `using` is like import in Java. A file can declare multiple namespaces. Namespaces and classes are not mapped to directories and files (but recommended).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "Multithreading = Use of multiple threads\nConcurrency = Order in which multiple tasks execute is not determined\nParallelism = Simultaneous execution (e.g. on multiple cores)  \n```csharp\nclass Printer {\nchar ch;\nint sleepTime\npublic Printer(char c, int t) {\nch = c;\nsleepTime = t;\n}\npublic void Print() {\nfor (var i = 0; i < 100; i++) {\nConsole.Write(ch);\nThread.Sleep(sleepTime)\n}\n}\n}\nclass Test {\nstatic void Main() {\nvar a = new Printer('.', 60);\nvar b = new Printer('*', 70);\nnew Thread(() => a.Print()).Start();\nnew Thread(() => b.Print()).Start();\n}\n}\n```  \n![csharpThreadStatus](/compSci/csharpThreadStatus.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Thread types", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "Foreground thread = Program will not terminate as long as at least one foreground thread is running  \nBackground thread = Background threads do not prevent the program from terminating  \n```csharp\nvar bgThread = new Thread(…);\nbgThread.IsBackground = true;\nbgThread.Start();\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Threadpooling", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "Thread creation requires quite some resources ~1’000’000 clock cycles, about 1MB of memory and also requires kernel interaction.\nThreadPool offers automatic thread management and\nrecycling Number of threads is limited, additional requests are queued Used for short running tasks Don’t change thread priority or thread state Background threads only Control over the thread only inside the method given.  \nNumber of Cores: Environment.ProcessorCount  \nStarting a new thread: var t = new Thread (() => { /*...*/});\nt.Start();  \nWait for another thread to finish: t.Join()  \nUse a thread from the ThreadPool: ThreadPool.QueueUserWorkItem((o) => { /*…*/});  \nMutual exclusion: lock(someObject){}  \nUsing semaphores: var sem = new Semaphore(0,3);\nsem.WaitOne();\nsem.Release();  \nUsing barriers: var b = new Barrier(7); b.SignalAndWait();", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Parallel Library", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "Task Parallel Library (TPL) offers abstractions\nand reuse approach over threads", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Parallel Library", "Header 3": "PLINQ", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "PLINQ allows us to parallelize LINQ statements which is based on the TPL.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Parallel Library", "Header 3": "Partitioning Strategies", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "Data parallelism = The simultaneous execution of the same function across split data of a data set. For example processing 100 elements, two cores work on 500 each.  \nTask parallelism = The simultaneous execution of multiple and different functions across the same or different data sets. For example sharpen and resize 100 pictures. First task sharpens, second task resizes.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Parallel Library", "Header 3": "Parallel.For", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "```csharp\n//sequential execution\nfor(var i = 0; i < 10; i++)\n{\nConsole.WriteLine(i);\n}\n// parallel execution\nParallel.For(0, 10, i =>\n{\nConsole.WriteLine(i); // order is unspecified!\n});\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Parallel Library", "Header 3": "Parallel.ForEach", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "```csharp\nstring[] capitals = {\"London\", \"Paris\", ...}\n//sequential execution\nforeach(var city in capitals)\n{\nConsole.WriteLine(city);\n}\n// parallel execution\nParallel.ForEach(capitals, city =>\n{\nConsole.WriteLine(city); // order is unspecified!\n});\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Parallel Library", "Header 3": "Parallel.Invoke", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "```csharp\nParallel.Invoke(()=>Function1(),\n()=>Functions2()\n);\n```  \nInvoke returns when all actions are finished and order is unspecified. Not necessarly in parallel but will try.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Parallel Library", "Header 3": "Task.Run", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "```csharp\nTask < double > [] tasks = {\nTask.Run(() => DoComputation1()),\nTask.Run(() => DoComputation2())\n};\nvar results = new double[tasks.Length];\nfor (var i = 0; i < tasks.Length; i++)\nresults[i] = await tasks[i];\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Parallelism", "Header 2": "Parallel Library", "Header 3": "Using PLINQ", "path": "../pages/digitalGarden/cs/csharp/parallelism.mdx"}, "page_content": "```csharp\nvar parallelQuery = Enumerable.Range(3, 30)\n.Where(n => SomePredicate(n))\n.Sum(n => n * n);\n// becomes\nvar parallelQuery = Enumerable.Range(3, 30).AsParallel()\n.Where(n => SomePredicate(n))\n.Sum(n => n * n);\n```  \nAsParallel() = Chunk Partitioning: threads grab chunk by chunk itself\nParallelEnumerable.Range(3,30) = Range Partitioning: range of work preassigned  \nLimit the number of threads with .WithDegreeOfParallelism(4)  \nPLINQ only parallelizes work, if it suspects benefits. You can force parallelization with .WithExecutionMode(ParallelExecutionMode.ForceParallelism)  \nOrdering can be forced with .AsOrdered()\nLift ordering requirement with .AsUnordered()  \nA function has “side effects” when it modifies something outside the function make sure this never happens!", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "Lambdas", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "Lambda expressions are just another way to write methods.  \n```csharp\nstatic bool SomePredicate(Point p)\n{\nreturn p.X * p.Y> 100000;\n}\nPredicate<Point> d = SomePredicate;\n\nPredicate<Point> d = delegate(Point p)\n{\nreturn p.X*p.Y> 100000;\n};\n\nPredicate<Point> d = p => p.X* p.Y> 100000;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "Lambdas", "Header 3": "Closures", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "A lambda expression may use variables defined outside its context («outer variables»). here factor  \n```csharp\nvoid doSomething()\n{\nvar factor = 2;\nFunc<int, int> multiplier= n => n * factor;\nConsole.WriteLine(multiplier(3)); //6\n}\n```  \nBe careful!! Writes the current value of i and not the value of i back when the delegate was created.  \n```csharp\nvar actions = newList<Action>();\nfor(var i = 0; i < 10; i++)\nactions.Add(() => Console.WriteLine(i));\nforeach(var action in actions)\naction(); // 101010101010101...\n```  \nThe solution to this is to declare a new “inner variable\" for each iteration to be captured, instead of a single “outer variable\" which is captured only once.  \n```csharp\nfor(var i = 0; i < 10; i++)\n{\nvarj = i; //each iteration gets its own,new variable j\nactions.Add(() => Console.WriteLine(j));\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "LINQ", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "LINQ = Language INtegrted Query are just like streams in Java.\nLINQ features can be used in a C# program by importing the `System.Linq` namespace.  \nLINQ is executed when **results are accessed**, not when the query is created.\nExecution happens when:  \n- iterating over results\n- calling immediate execution methods like toList, Count etc.  \n**_NEVER modify_**state using LINQ!!!  \nForEach is _NOT LINQ_  \n```csharp\npersons.ForEach(p => Console.WriteLine(p.FirstName));\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "LINQ", "Header 3": "Method & Query Syntax", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "Method syntax resembles most other C# method calls, while query syntax resembles SQL.\nQuery must begin with `from` clause, and end with `select` or `group` clause\nBetween first `from` clause and last `select/group` clause, it can contain one or more of the following clauses:Where, Orderby, Join, Let, From, Into  \n```csharp\n// Method syntax\nvar custQuery2 = customers.Where(c => c.City == \"London\");\nvar orderedByLength = names.OrderBy(n => n.Length);\n\n// Query syntax\nvar custQuery =\nFROM c IN customers\nWHERE c.City == \"London\"\nSELECT c;\n// Mix\nvar results = (FROM c IN Comedians SELECT c).Count();\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "LINQ", "Header 3": "Simple queries", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "```csharp\n// **** AGGREGATE ****\nvar min = array.Min(); // All below can also be called straight on array\nvar count = array.Count()\nvar condCount = data.Count(x => x.Condition == true)\nvar sumAges = persons.Sum(p => p.Age);\nvar minAge = persons.Min(p => p.Age);\nvar minAge = persons.Max(p => p.Age);\nvar avgAge = persons.Average(p => p.Age);\nvar simplePersons = persons.Select(p => new { Surname = p.Surname, Firstname = p.Firstname}) // creates an enumarable of anonymous class\n\n// **** CONVERSIONS ****\ndata.ToArray(); // Convert to Array\ndata.ToList(); // Convert to List\ndata.ToDictionary( x=> x.Name ); // Convert to Dictionary keyed on Name\n// **** ELEMENT ****\n\ndata.First() // Returns the first element\ndata.First( x=> x.Type == Types.A ) // Returns the first element passing the condition\ndata.FirstOrDefault() // Returns the first element or default\ndata.FirstOrDefault( x => x.Type == Types.B ) // Returns the first element passing the condition or default\n\ndata.Last() // Returns the last element\ndata.Last( x=> x.Type == Types.A ) // Returns the last element passing the condition\ndata.LastOrDefault( ) // Returns the last element or default*\ndata.LastOrDefault( x => x.Type == Types.B ) // Returns the last element passing the condition or default*\n\ndata.ElementAt(0) // Returns the element at position 0\n\n// **** FILTERS ****\n\nvar even array.Where(x => x%2==0) // Returns all elements passing the condition\n\ndata.Where(( x, index) => index <= 4 && x.Type == Types.A) // The elements index can be passed into the delegate\n\n// **** GENERATION ****\nEnumerable.Range(1, 10); // Creates collection of 10 items between 1 and 10\nEnumerable.Repeat(1, 10); // Creates a collection of 10 1s.\n\n// **** ORDERING ****", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "LINQ", "Header 3": "Simple queries", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "// **** GENERATION ****\nEnumerable.Range(1, 10); // Creates collection of 10 items between 1 and 10\nEnumerable.Repeat(1, 10); // Creates a collection of 10 1s.\n\n// **** ORDERING ****\n\ndata.OrderBy(x => x.Name); // Order by Name ASC\ndata.OrderBy(x => x.Name).ThenBy(x => x.Age); // Order by Name ASC the Age ASC\ndata.OrderBy(x => x.Name).ThenByDescending(x => x.Age); // Order by Name ASC then Age DESC\ndata.OrderByDescending (x => x.Name); // Order by Name DESC\ndata.OrderBy(x => x.Name).Reverse(); // Reverse elements\n\n// **** PARTITIONING ****\n\ndata.Take (3); // Take 3 items\ndata.Skip (3); // Skip 3 items\n\ndata.TakeWhile (x=>x.Type ==Types.A); // Take all the items while the condition is met\ndata.SkipWhile (x=>x.Type ==Types.A); // Skip all the items while the condition is met\n\n// **** PROJECTION ****\n\ndata.Select(x => x.Name); // Select collection of a column\n\ndata.Select(x => new { Name = x.Name, Age = x.Age }); // Select a collection of columns through an anonymus type\n\n// **** QUANTIFIERS ****\nif(array.Any(i=> i% 2 == 0))\nif(array.All(i=> i% 2 == 0))\n\n// **** SET ****\n\n\ndata.Intersect(dataTwo); // Returns the union / intersection of data; elements in both collections\n\ndata.Except(dataTwo); // Returns elements in data which are not in dataTwo\n\ndata.Concat(dataTwo); // Concatonates both collections; appends dataTwo to data\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "LINQ", "Header 3": "Grouping", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "```csharp\nvar users = new List<User>()\n{\nnew User { Name = \"John Doe\", Age = 42, HomeCountry = \"USA\" },\nnew User { Name = \"Jane Doe\", Age = 38, HomeCountry = \"USA\" },\nnew User { Name = \"Joe Doe\", Age = 19, HomeCountry = \"Germany\" },\nnew User { Name = \"Jenna Doe\", Age = 19, HomeCountry = \"Germany\" },\nnew User { Name = \"James Doe\", Age = 8, HomeCountry = \"USA\" },\n};\nvar usersGroupedByCountry = users.GroupBy(user => user.HomeCountry);\nforeach(var group in usersGroupedByCountry)\n{\nConsole.WriteLine(\"Users from \" + group.Key + \":\");\nforeach(var user in group)\nConsole.WriteLine(\"* \" + user.Name);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "LINQ", "Header 3": "Distinct", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "```csharp\nstudents.Distinct(); // Returns a collection of distinct elements\nstudents.Distinct (new StudentComparer()). // Distinct with providing an equality provider\n\npublic class StudentComparer : IEqualityComparer<Student>\n{\npublic bool Equals(Student x, Student y)\n{\n//First check if both object reference are equal then return true\nif(object.ReferenceEquals(x, y))\n{\nreturn true;\n}\n//If either one of the object refernce is null, return false\nif (object.ReferenceEquals(x,null) || object.ReferenceEquals(y, null))\n{\nreturn false;\n}\n//Comparing all the properties one by one\nreturn x.ID == y.ID && x.Name == y.Name;\n}\npublic int GetHashCode(Student obj)\n{\n//If obj is null then return 0\nif (obj == null)\n{\nreturn 0;\n}\n//Get the ID hash code value\nint IDHashCode = obj.ID.GetHashCode();\n//Get the string HashCode Value\n//Check for null refernece exception\nint NameHashCode = obj.Name == null ? 0 : obj.Name.GetHashCode();\nreturn IDHashCode ^ NameHashCode;\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "LINQ", "Header 3": "Join", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "```csharp\nIList<Student> studentList = new List<Student>() {\nnew Student() { StudentID = 1, StudentName = \"John\", StandardID =1 },\nnew Student() { StudentID = 2, StudentName = \"Moin\", StandardID =1 },\nnew Student() { StudentID = 3, StudentName = \"Bill\", StandardID =2 },\nnew Student() { StudentID = 4, StudentName = \"Ram\" , StandardID =2 },\nnew Student() { StudentID = 5, StudentName = \"Ron\"  }\n};\n\nIList<Standard> standardList = new List<Standard>() {\nnew Standard(){ StandardID = 1, StandardName=\"Standard 1\"},\nnew Standard(){ StandardID = 2, StandardName=\"Standard 2\"},\nnew Standard(){ StandardID = 3, StandardName=\"Standard 3\"}\n};\n\nvar innerJoin = studentList.Join(// outer sequence\nstandardList,  // inner sequence\nstudent => student.StandardID,    // outerKeySelector\nstandard => standard.StandardID,  // innerKeySelector\n(student, standard) => new  // result selector\n{\nStudentName = student.StudentName,\nStandardName = standard.StandardName\n});\n/*\nJohn - Standard 1\nMoin - Standard 1\nBill - Standard 2\nRam - Standard 2\n*/\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lambdas and LINQ", "Header 2": "LINQ", "Header 3": "SelectMany", "path": "../pages/digitalGarden/cs/csharp/lambdasLinq.mdx"}, "page_content": "SelectMany() flattens the resulting sequences into one sequence, and invokes a result selector function on each element therein.  \n```csharp\nPetOwner[] petOwners =\n{ new PetOwner { Name=\"Higa, Sidney\",\nPets = new List<string>{ \"Scruffy\", \"Sam\" } },\nnew PetOwner { Name=\"Ashkenazi, Ronen\",\nPets = new List<string>{ \"Walker\", \"Sugar\" } },\nnew PetOwner { Name=\"Price, Vernette\",\nPets = new List<string>{ \"Scratches\", \"Diesel\" } } };\n\n// Query using SelectMany().\nIEnumerable<string> query1 = petOwners.SelectMany(petOwner => petOwner.Pets);\n\nConsole.WriteLine(\"Using SelectMany():\");\n\n// Only one foreach loop is required to iterate\n// through the results since it is a\n// one-dimensional collection.\nforeach (string pet in query1)\n{\nConsole.WriteLine(pet);\n}\n\n// This code shows how to use Select()\n// instead of SelectMany().\nIEnumerable<List<String>> query2 =\npetOwners.Select(petOwner => petOwner.Pets);\n\nConsole.WriteLine(\"\\nUsing Select():\");\n\n// Notice that two foreach loops are required to\n// iterate through the results\n// because the query returns a collection of arrays.\nforeach (List<String> petList in query2)\n{\nforeach (string pet in petList)\n{\nConsole.WriteLine(pet);\n}\nConsole.WriteLine();\n}\n/*\nUsing SelectMany():\nScruffy\nSam\nWalker\nSugar\nScratches\nDiesel\n\nUsing Select():\nScruffy\nSam\n\nWalker\nSugar\n\nScratches\nDiesel\n*/\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "path": "../pages/digitalGarden/cs/csharp/inheritance.mdx"}, "page_content": "Inheritance works pretty much the same as in java. Constructors are not inherited. Inherited methods can be overwritten. Classes can only inherit from a single base class, if there is no explicit super class then it inherits from `object`.  \n```csharp\nclass B : A // subclass (inherits from A, extends\n{\nint b;\npublic B() {...}\npublic void G() {...}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Overriding methods", "path": "../pages/digitalGarden/cs/csharp/inheritance.mdx"}, "page_content": "Only virtual methods can be overridden in subclasses. Properties and indexers can also be overridden (virtual, override). Static methods can not be overridden.  \n```csharp\nclass A\n{\npublic void F() {...} // cannot be overridden\npublic virtual void G() {...} // can be overridden in a subclass\n}\n\nclass B : A\n{\npublic void F() {...} // warning: hides inherited F(), should add new before\npublic void G() {...} // warning: hides inherited G(), should add new before\npublic override void G() {...} // ok: overrides inherited G()\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Inheritance", "Header 2": "Hiding methods", "path": "../pages/digitalGarden/cs/csharp/inheritance.mdx"}, "page_content": "In Method Hiding you can hide the implementation of the methods of a base class from the derived class using the new keyword. Or in other words, in method hiding, you can redefine the method of the base class in the derived class by using the new keyword.  \n```csharp\npublic class MyParent {\npublic void Show()\n{\nConsole.WriteLine(\"This is my parent class.\");\n}\n}\n\npublic class MyChild : MyParent {\n// Hide the method of base class using new keyword\npublic new void Show() {\nConsole.WriteLine(\"This is my child class.\");\n}\n}\n\npublic class GFG {\nstatic public void Main()\n{\nMyChild obj = new MyChild();\nobj.Show(); // This is my child class.\n((MyParent)obj).Show(); // This is my parent class.\nMyParent par = obj;\npar.Show(); // This is my parent class.\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Resource Management", "Header 2": "Garbage collection", "path": "../pages/digitalGarden/cs/csharp/resourceManagement.mdx"}, "page_content": "C# programmers don’t have to release allocated memory, the CLR takes care of it with the so called Garbage Collector.  \nGarbage collection has 2 Phases, Mark and sweep/Detection and Reclamation.\nDetection: The garbage collector searches for managed objects that are referenced in managed code.\nReclamation: The garbage collector attempts to finalize\nobjects that are unreachable. The garbage collector frees objects that are\nunmarked and reclaims their memory.  \nThis example is a tracing, compacting ,stop the world , mark & sweep garbage collector.\ntracing = Follow references to decide reachability\ncompacting = Free memory by compacting heap\nstop the world = Stop all threads during GC\nmark & sweep = GC in two phases  \nRuntime performs GC whenever it “feels like it”", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Resource Management", "Header 2": "Finalization/destructors", "path": "../pages/digitalGarden/cs/csharp/resourceManagement.mdx"}, "page_content": "Prior to an object being released, the GC calls its finalizer/destructor. Collection can be forced with GC.Collect().\nRuns on the finalizer thread (concurrent to the rest of the application)\n.NET provides the IDisposable interface then users can call Dispose() Objects also call Dispose on their child objects  \n```csharp\npublic sealed class OSHandle : IDisposable\n{\nprivate bool disposed;\npublic OSHandle(IntPtr h) { handle = h; disposed = false ;}\npublic void Dispose(){\nDispose(true);\nGC.SuppressFinalize(this);\n}\n\n~OSHandle(){\nDispose(false);\n}\n\nprotected void Dispose(bool disposing)\n{\nif(!disposed){\nif(disposing){\n/* safe to access references here */\n}\ndisposed = true;\n// dispose unmanaged resources here\n}\nbase.Dispose(disposing);\n}\n}\n```  \nDispose() = Deterministic, Explicitly called by user, Free resources File handlers, locks, OS resources, ...)  \nFinalizers destructors = Non deterministic, Automatically called by GC, Free memory or as safety net  \nDispose pattern combines best of both worlds", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Commenting", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "```csharp\n// This is a normal comment\n/// This is a documentation comment\n/* This is a multiline comment\nIt can span over multiple lines which makes it a multiline comment */\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Naming conventions", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "In C# almost everything is PascalCase. However, use camelCasing when naming `private` or `internal` fields, and prefix them with `_`. You also use camelCasing for method parameters.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Access Modifiers", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "`public` The code is accessible by any other code in the same assembly or another assembly that references it.\n`private`The code is only accessible within the same class or struct.\n`protected`The code is accessible within the same class, or in a class that inherits the class containing the protected\n`internal`The code is only accessible within its own assembly, not from any other assembly.\n`protected internal` The code can be accessed by any code in the assembly in which it's declared, or from within a derived class in another assembly.\n`readonly` This prevents a field from being modified after construction. A read-only field can be assigned only in its declaration or within the enclosing type’s constructor.\n`const` A constant is evaluated statically at compile time and the compiler literally substitutes its value whenever used. A constant can be any of the built-in numeric types, bool, char, string, or an enum type.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Aliasing", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Namespaces can also be aliased to allow shorter writing  \n```csharp\nusing Dict = System.Collections.Generics.Dictionary<int, int>;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Types", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Each type is defined as either a _value type_ or a _reference type_. Just as in Java `object` ist the mother of all types meaning there is type unification.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Types", "Header 3": "Value and reference types", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "A variable of a value type contains an instance of the type. This differs from a variable of a reference type, which contains a reference to an instance of the type. By default, on assignment, passing an argument to a method, and returning a method result, variable values are copied. In the case of value-type variables, the corresponding type instances are copied. Reference types comprise all class, array, delegate, and interface types. This includes string. Multiple references can point to the same object. `null` means the reference points to no object.  \nStack and the heap are the places where variables reside.  \nThe stack is a block of memory for storing local variables and parameters. The stack grows and shrinks as a method or function is entered and exited.  \nThe heap is the memory in which objects (i.e., reference-type instances) reside. The runtime has a garbage collector that periodically deallocates objects from the heap. An\nunreferenced object is eventually collected by the garbage collector.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Types", "Header 3": "Built-in types", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "C# provides a standard set of built-in types. These represent integers, floating point values, Boolean expressions, text characters, decimal values, and other types of data. There are also built-in string and object types. Most built-in types (all numeric types, char and bool) as well as custom `struct` and `enum` types are value types.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Types", "Header 3": "Custom types", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "You use the struct, class, interface, enum, and record constructs to create your own custom types.  \n<Image\nsrc=\"/cs/csharpTypes.png\"\ncaption=\"Hierarchy of types in C#.\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Types", "Header 3": "Boxing", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Converting a value type into a reference type wraps up the value of intOne from the stack in to a heap object.  \n```csharp\nint intOne = 3;\nobject obj= intOne;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Types", "Header 3": "Unboxing", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "converting a reference type into a value type. Unwraps the value again.  \n```csharp\nint intOne = (int) obj;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Classes", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "```csharp\nclass Hello\n{\nprivate string name; // private field so camelCase\nprivate void Greet() {...}\npublic static void Main(string[] args) {...}\nstatic Hello() { /* static constructor */}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Classes", "Header 3": "Static fields", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Static fields are Initialized before the static constructor is called and are called on the Class not the object.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Classes", "Header 3": "Static constructor", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Executed once per type before any instances of the type are created and any\nother static members are accessed.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Classes", "Header 3": "Properties", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Properties look like fields from the outside, but internally they contain logic, like\nmethods do. If only get is defined then it is a read-only property.  \n```csharp\npublic class Stock\n{\ndecimal currentPrice; // The private \"backing\" field\npublic decimal CurrentPrice // The public property\n{\nget { return currentPrice; } // property accessors\nset { currentPrice = value; }\n}\n}\n\nStock msft = new Stock();\nmsft.CurrentPrice = 30;\nmsft.CurrentPrice -= 3;\nConsole.WriteLine (msft.CurrentPrice);\n```  \n#### Automatic properties  \nCompiler generates a private field internally and automatically does getter and setter like in java.  \n```csharp\nclass Data\n{\npublic string CreateDate{ get; set; } = DateTime.Now; // initial value\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Classes", "Header 3": "Abstract classes", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "You can not create objects of abstract classes. Abstract methods have no implementation and are implicitly virtual meaning the need to be overwritten by the subclass.  \n```csharp\nabstract class Stream\n{\npublic abstract voidWrite(char ch);\npublic void WriteString(strings)\n{\nforeach(char ch in s)\nWrite(s);\n}\n}\n\nclass File: Stream\n{\npublic override voidWrite(charch)\n{\n...\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "String interpolation", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "```csharp\nint x = 4;\nConsole.WriteLine($\"{s.Name} is {s.Width:F2}m wide and is {(s.IsRed ? \"red\":\"not red\")}\");\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Verbatim string literals", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "A verbatim string literal is prefixed with @ and does not support escape sequences and can also span multiple lines.  \n```csharp\nConsole.WriteLine(\"\\\\\\\\server\\\\fileshare\\\\helloworld.cs\");\nConsole.WriteLine(@\"\\\\server\\fileshare\\helloworld.cs\");\nstring escaped = \"First Line\\r\\nSecond Line\";\nstring verbatim = @\"First Line\nSecond Line\";\n// True if your text editor uses CR-LF line separators:\nConsole.WriteLine(escaped == verbatim);\nConsole.WriteLine(@$\"c:\\{pathname}\"); // Can combine with interpolation\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Parameters", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "By default, arguments in C# are passed by value, this means that a copy of the value is created when passed to the method.  \n```csharp\nclass Test\n{\nstatic void Foo (int p)\n{\np = p + 1; // Increment p by 1\nConsole.WriteLine (p); // Write p to screen\n}\nstatic void Main()\n{\nint x = 8;\nFoo (x); // Make a copy of x\nConsole.WriteLine (x); // x will still be 8\n}\n}\n```  \nPassing a reference-type argument by value copies the reference, but not the object.  \n```csharp\nclass Test\n{\nstatic void Foo (StringBuilder fooSB)\n{\nfooSB.Append (\"test\");\nfooSB = null; // copy of reference, doesn’t make sb null.\n}\nstatic void Main()\n{\nStringBuilder sb = new StringBuilder();\nFoo (sb);\nConsole.WriteLine (sb.ToString()); // test\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Parameters", "Header 3": "ref modifier", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "To pass by reference. ref modifier is required both when writing and when calling the method.  \n```csharp\nclass Test\n{\nstatic void Foo (ref int p)\n{\np = p + 1; // Increment p by 1\nConsole.WriteLine (p); // Write p to screen\n}\nstatic void Main()\n{\nint x = 8;\nFoo (ref x); // Ask Foo to deal directly with x\nConsole.WriteLine (x); // x is now 9\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Parameters", "Header 3": "out modifier", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "An out argument is like a ref argument except for the following:  \n1. It need not be assigned before going into the function.\n2. It must be assigned before it comes out of the function.  \nMost commonly used to get multiple return values.\n`out _` tells the compiler a so called discard.  \n```csharp\nclass Test\n{\nstatic void Split (string name, out string firstNames, out string lastName)\n{\nint i = name.LastIndexOf (' ');\nfirstNames = name.Substring (0, i);\nlastName = name.Substring (i + 1);\n}\nstatic void Main()\n{\nstring a, b;\nSplit (\"Stevie Ray Vaughan\", out a, out b);\n// Or Split (\"Stevie Ray Vaughan\", out string a, out string b);\n// Or Split (\"Stevie Ray Vaughan\", out string a, out _);\nConsole.WriteLine (a); // Stevie Ray\nConsole.WriteLine (b); // Vaughan\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Parameters", "Header 3": "in modifier", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "An in parameter is similar to a ref parameter except that value cannot be\nmodified by the method (doing so generates a compile-time error). This\nis most useful when passing a large value type(some big struct) to the method because it allows the compiler to avoid the overhead of copying the argument prior to passing it in while still protecting the original value from modification.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Parameters", "Header 3": "params modifier", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Specify the params parameter modifier on the last parameter so that the method accepts any number of arguments of a particular type.  \n```csharp\nclass Test\n{\nstatic int Sum (params int[] ints)\n{\nint sum = 0;\nfor (int i = 0; i < ints.Length; i++)\nsum += ints[i]; // Increase sum by ints[i]\nreturn sum;\n}\nstatic void Main()\n{\nint total = Sum (1, 2, 3, 4);\nConsole.WriteLine (total); // 10\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Parameters", "Header 3": "Optional parameters", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "```csharp\nvoid Foo (int x = 23) { Console.WriteLine (x); }\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Parameters", "Header 3": "Named arguments", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Go very well with optional parameters.  \n```csharp\nvoid Foo (int x, int y) { Console.WriteLine (x + \", \" + y); }\nvoid Test()\n{\nFoo (x:1, y:2); // 1, 2\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Parameters", "Header 3": "Ref locals and returns", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "A local variable that references an element in an array or field in an object.  \n```csharp\nint[] numbers = { 0, 1, 2, 3, 4 };\nref int numRef = ref numbers [2];\nnumRef *= 10;\nConsole.WriteLine (numRef); // 20\nConsole.WriteLine (numbers [2]); // 20\n```  \nYou can also return a ref local, this is called a ref return. If you omit the ref modifier on the calling side, it reverts to returning an ordinary value.  \n```csharp\nstatic string x = \"Old Value\";\nstatic ref string GetX() => ref x; // This method returns a ref\nstatic void Main()\n{\nref string xRef = ref GetX(); // Assign result to a ref local\nxRef = \"New Value\";\nConsole.WriteLine (x); // New Value\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Expression-bodied methods", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "A method that comprises a single expression, can be written as an expression-bodied method.  \n```csharp\nint Foo (int x) { return x * 2; }\nint Foo (int x) => x * 2;\nvoid Foo (int x) => Console.WriteLine (x); // Can also have void\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Interfaces", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Interface = only signatures, no implementation (apart from default implementations). Interfaces may contain methods, properties, indexers, events (no fields, constants, constructors, destructors, operators or nested types). Interface members are implicitly public abstract (virtual) and can extend other interfaces and be static. Classes and structs may implement multiple interfaces  \n```csharp\npublic interface IList: ICollection, IEnumerable\n{\nint Add(objectvalue); //methods\nbool Contains(objectvalue);\nbool IsReadOnly{ get; } //property\nobject this[intindex] { get; set; } //indexer\nvoid Log(stringt) {Console.WriteLine(Prefix + t);} //default impl.\nstatic string Prefix=\"\"; //static field\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "break", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Same as Java. The break statement ends the execution of the body of an iteration or switch statement.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "continue", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "The continue statement forgoes the remaining statements in a loop and makes an\nearly start on the next iteration.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "goto", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "The goto statement transfers execution to another label within a statement block.  \n```csharp\nint i = 1;\nstartLoop:\nif (i <= 5)\n{\nConsole.Write (i + \" \");\ni++;\ngoto startLoop;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Controls", "Header 3": "if, else if, else", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Same as Java.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Controls", "Header 3": "while and do while", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Same as Java.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Controls", "Header 3": "for", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Same as Java.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Controls", "Header 3": "foreach", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "The foreach statement executes a statement or a block of statements for each element in an instance of the type that implements the `System.Collections.IEnumerable` or `System.Collections.Generic.IEnumerable<T>` interface.  \n```csharp\nforeach (char c in \"beer\")\nConsole.WriteLine(c);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Controls", "Header 3": "switch", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "When more than one value should execute the same code, you can list the common cases sequentially. Unlike in Java there is no fall-through unless the case is empty. If you need fall-through you can use `goto case/default`.  \n```csharp\nswitch (cardNumber)\n{\ncase 13:\ncase 12:\ncase 11:\nConsole.WriteLine (\"Face card\");\nbreak;\ndefault:\nConsole.WriteLine (\"Plain card\");\nbreak;\n}\n```  \n#### Pattern matching  \nSwitching on a type is a special case of switching on a pattern. Each case clause specifies a type upon which to match, and a variable upon which to assign the typed value if the match succeeds (the “pattern” variable). The compiler lets us consume the pattern variables only in the when clauses.  \n```csharp\nobject o = \"ecnf\"\nswitch(o)\n{\ncase byte b:\nConsole .WriteLine($\"I’m a byte with value {b});\nbreak;\ncase string s when s == \"ecnf\"\nConsole .WriteLine(\"I’m THE ecnf string\");\nbreak;\ncase string s:\nConsole .WriteLine(\"I’m a string that contains {0}\", s);\nbreak;\ndefault:\nConsole.WriteLine(\"Don’t know anything\");\nbreak;\n}\n```  \n#### Switch expressions  \nSwitches can also switch on multiple values.  \n```csharp\nstring module = \"ecnf\";\nchar grade = ‘B’;\nstring msg= (module, grade) switch\n{\n(\"ecnf\",’A’) => \"Congrats, regards Yves Senn\",\n(\"ecnf\",’B’) => \"Very good, regards Yves Senn\",\n(\"ecnf\",’C’) => \"Good job, regards Yves Senn\",\n(\"oop1\",’A’) => \"Nice one, regards Dieter Holz\",\n(\"oop1\",’B’) => \"Well done, regards Dieter Holz\",\n(\"oop1\",’C’) => \"Good job, regards Dieter Holz\",\n_ => throw new InvalidArgumentException() // equivalent to default\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Type checks and conversions", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Implicit conversions are allowed when both of the following are true:  \n1. The compiler can guarantee that they will always succeed.\n2. No information is lost in conversion.  \nOtherwise you need to use an explicit conversion  \n```csharp\nint x = 12345; // int is a 32-bit integer\nlong y = x; // Implicit conversion to 64-bit integer\nshort z = (short)x; // Explicit conversion to 16-bit integer\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Type checks and conversions", "Header 3": "is operator", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Checks whether an object is compatible with a given type and returns a Boolean. If the object reference is null it returns false.  \n```csharp\nObject o = new Object();\nBoolean b1 = (o is Object); // b1 is true.\nBoolean b2 = (o is Employee); // b2 is false.\n```  \nThe is operator is typically used as follows. However this causes 2 checks which can have an effect on performance.  \n```csharp\nif (o is Employee) {\nEmployee e = (Employee) o;\n}\n```  \nIt can also be used like this to make life easy:  \n```csharp\nif(b is D a)\n{\na.Foo(\"blBLA\");\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Type checks and conversions", "Header 3": "as operator", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "if o is compatible with the type returns a Employee as non-null reference to the same object. If o is not compatible with the type, returns null.\nWarning `as` operator will never throw an exception!!!!  \n```csharp\nObject o = new Object(); // Creates a new Object object\nEmployee e = o as Employee; // Casts o to an Employee\n// The cast above fails: no exception is thrown, but e is set to null.\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Conditional operator (ternary operator)", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "Has the form `q ? a : b;` thus, if condition q is true, a is evaluated, else b is evaluated:  \n```csharp\nstatic int Max (int a, int b)\n{\nreturn (a > b) ? a : b;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Null operators", "Header 3": "Null-Coalescing Operator", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "The ?? operator is the null-coalescing operator. It says, “If the operand to the left is\nnon-null, give it to me; otherwise, give me another value.”  \n```csharp\nstring s1 = null;\nstring s2 = s1 ?? \"nothing\"; // s2 evaluates to \"nothing\"\n```  \n#### Null-Coalescing Assignment Operator  \nThe ??= operator is the null-coalescing assignment operator. It says, “If the operand\nto the left is null, assign the right operand to the left operand.”  \n```csharp\nstring s1 = null;\ns1 ??= \"something\";\nConsole.WriteLine (s1); // something\ns1 ??= \"everything\";\nConsole.WriteLine (s1); // something\n// instead of\nif (myVariable == null) myVariable = someDefault;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Null operators", "Header 3": "Null-Conditional Operator", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "The ?. operator is the null-conditional or “Elvis” operator. if the operand on the left is null, the expression evaluates to null instead of throwing a NullReferenceException.  \n```csharp\nSystem.Text.StringBuilder sb = null;\nstring s = sb?.ToString(); // No error; s instead evaluates to null\n// same as\nstring s = (sb == null ? null : sb.ToString());\n```  \nYou can also use the null-conditional operator to call a void method:  \n```csharp\nsomeObject?.SomeVoidMethod();\n```  \nIf someObject is null, this becomes a “no-operation” rather than throwing a Null\nReferenceException.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Essentials", "Header 2": "Null operators", "Header 3": "Nullable value types", "path": "../pages/digitalGarden/cs/csharp/essentials.mdx"}, "page_content": "```csharp\nint? length = sb?.ToString().Length; // OK: int? can be null\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Delegates and Events", "path": "../pages/digitalGarden/cs/csharp/delegatesEvents.mdx"}, "page_content": "A delegate is an object that knows how to call a method.\nA delegate type defines the kind of method that delegate instances can call. Specifically, it defines the method’s return type and its parameter types.  \n```csharp\n\ndelegate int Transformer (int x);\n\nclass Test\n{\nstatic void Main()\n{\nTransformer t = Square; // Create delegate instance\nint result = t(3); // Invoke delegate, t.invoke() also works.\nConsole.WriteLine (result); // 9\n}\n\nstatic int Square (int x) => x * x;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Delegates and Events", "Header 2": "Using Delegate as utility functions", "path": "../pages/digitalGarden/cs/csharp/delegatesEvents.mdx"}, "page_content": "In this example, we have a utility method named Transform that applies a transform to each element in an integer array. Our Transform method is a higher-order function because it’s a function that takes a function as an argument.  \n```csharp\npublic delegate int Transformer (int x);\n\nclass Util\n{\npublic static void Transform (int[] values, Transformer t)\n{\nfor (int i = 0; i < values.Length; i++)\nvalues[i] = t (values[i]);\n}\n\n}\n\nclass Test\n{\n\nstatic void Main()\n{\nint[] values = { 1, 2, 3 };\nUtil.Transform (values, Square); // Hook in the Square method\nforeach (int i in values)\nConsole.Write (i + \" \"); // 1 4 9\n}\nstatic int Square (int x) => x * x;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Delegates and Events", "Header 2": "Multicast Delegate", "path": "../pages/digitalGarden/cs/csharp/delegatesEvents.mdx"}, "page_content": "All delegate instances have multicast capability. This means that a delegate instance\ncan reference not just a single target method, but also a list of target methods. The +\nand += operators combine delegate instances. They are invoked in the order in which they are added.  \n```csharp\nSomeDelegate d = SomeMethod1;\nd += SomeMethod2;\nd() // both get invoked\n\nd -= SomeMethod1;\nd() // now only SomeMethod2\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Delegates and Events", "Header 2": "Generic Delegate", "path": "../pages/digitalGarden/cs/csharp/delegatesEvents.mdx"}, "page_content": "A delegate type can contain generic type parameters.  \n```csharp\npublic delegate T Transformer<T> (T arg);\n```  \nWith generic delegates, it becomes possible to write a small set of delegate types that\nare so general they can work for methods of any return type and any (reasonable)\nnumber of arguments.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Delegates and Events", "Header 2": "Generic Delegate", "Header 3": "Action", "path": "../pages/digitalGarden/cs/csharp/delegatesEvents.mdx"}, "page_content": "Generic delegate type for methods with any parameters and no return value.  \n```csharp\ndelegate voidAction();\ndelegate voidAction<inT1> (T1arg);\ndelegate voidAction<inT1, inT2> (T1arg1, T2arg2);\n\nprivate static void ActionDelegateExample()\n{\nAction<string> act = ShowMessage;\nact(\"C# Langauge\")\n}\n\nprivate static void ShowMessage(string message)\n{\nConsole.WriteLine(message);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Delegates and Events", "Header 2": "Generic Delegate", "Header 3": "Func", "path": "../pages/digitalGarden/cs/csharp/delegatesEvents.mdx"}, "page_content": "Generic delegate type for methods with any parameters and a return value.  \n```csharp\ndelegate TResult Func<out TResult> ();\ndelegate TResult Func<in T1, out TResult> (T1 arg);\ndelegate TResult Func<in T1, in T2, out TResult> (T1 arg1, T2 arg2);\n\npublic void FuncDelegateExample()\n{\nFunc<string, string> convertMethod = UppercaseString;\nConsole.WriteLine(convertMethod(\"Dakota\"));\n}\n\nprivate string UppercaseString(string inputString)\n{\nreturn inputString.ToUpper();\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Delegates and Events", "Header 2": "Generic Delegate", "Header 3": "Predicate", "path": "../pages/digitalGarden/cs/csharp/delegatesEvents.mdx"}, "page_content": "Generic delegate type for methods with a single parameter and a return type bool.  \n```csharp\nclass List <T>\n{\nList <T> FindAll (Predicate <T> match);\nT Find(Predicate <T> match);\n}\nbool GreaterThan10(int x) => return x > 10;\nvoid Main() {\nvar listOfNumbers = new int [] {1, 2, 25, 3, 11}.ToList();\nvar firstMatch = listOfNumbers.Find(GreaterThan10);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Delegates and Events", "Header 2": "Events", "path": "../pages/digitalGarden/cs/csharp/delegatesEvents.mdx"}, "page_content": "When using delegates, two emergent roles commonly appear: broadcaster and\nsubscriber. Broadcaster is a type that contains a delegate field and decides when to invoke the delegate. A subscriber decides when to start and stop listening by calling += and -= on the broadcaster’s delegate.  \n```csharp\nusing System;\npublic class PriceChangedEventArgs : EventArgs\n{\npublic readonly decimal LastPrice;\npublic readonly decimal NewPrice;\n\npublic PriceChangedEventArgs (decimal lastPrice, decimal newPrice)\n{\nLastPrice = lastPrice; NewPrice = newPrice;\n}\n}\npublic delegate void PriceChangedEvent(object source, PriceChangedEventArgs args);\npublic class Stock\n{\nstring symbol;\ndecimal price;\npublic Stock(string symbol) => this.symbol = symbol;\n\npublic event PriceChangedEvent PriceChanged;\n\nprotected virtual void OnPriceChanged(PriceChangedEventArgs e)\n{\nPriceChanged?.Invoke (this, e);\n}\n\npublic decimal Price\n{\nget => price;\nset\n{\nif (price == value) return;\ndecimal oldPrice = price;\nprice = value;\nOnPriceChanged(new PriceChangedEventArgs (oldPrice, price));\n}\n}\n}\n\nclass Test\n{\nstatic void Main()\n{\nStock stock = new Stock (\"THPW\");\nstock.Price = 27.10M;\n// Register with the PriceChanged event\nstock.PriceChanged += stock_PriceChanged;\nstock.Price = 31.59M;\n}\nstatic void stock_PriceChanged (object sender, PriceChangedEventArgs e)\n{\nif ((e.NewPrice - e.LastPrice) / e.LastPrice > 0.1M)\nConsole.WriteLine (\"Alert, 10% stock price increase!\");\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Advanced Usage", "Header 2": "Operator Overloading", "path": "../pages/digitalGarden/cs/csharp/advancedUsage.mdx"}, "page_content": "List of overrideable operators  \nIf == and !=get are overloaded then so should Equals(object obj) and int GetHashCode.  \n```csharp\npublic struct Rational\n{\npublic Rational( int n, int d) { … }\npublic int Numerator { get {…} }\npublic int Denominator { get {…} }\npublic override string ToString () { … }\n\n// *= is provided for free, if you implement operator *\npublic static Rational operator* (Rational lhs, Rational rhs)\n{\nreturn new Rational(lhs.Numerator*rhs.Numerator,\nlhs.Denominator*rhs.Denominator);\n}\n\n// lossless conversions so Rational r = 2;\npublic static implicit operator Rational (int i)\n{\nreturn new Rational (i,1);\n}\n\n// lossy conversions/exceptions double d = (double) r;\npublic static explicit operator double (Rational r)\n{\nreturn r.Numerator / (double) r.Denominator\n}\n}\n```  \n```csharp\npublic struct Complex : IEquatable<Complex>\n{\npublic Complex(double re, double im)\n{\nthis.Re = re;\nthis.Im = im;\n}\n\npublic double Re { get; }\npublic double Im { get; }\n\npublic override string ToString() {\nreturn String.Format(\"({0,5:0.0},{1,5:0.0}i)\", Re, Im);\n}\n\npublic static Complex operator +(Complex lh, Complex rh)\n{\nreturn new Complex(lh.Re + rh.Re, lh.Im + rh.Im);\n}\n\npublic static Complex operator -(Complex lh, Complex rh)\n{\nreturn new Complex(lh.Re - rh.Re, lh.Im - rh.Im);\n}\n\npublic static bool operator ==(Complex lh, Complex rh)\n{\nreturn lh.Re.CompareTo(rh.Re) == 0 && lh.Im.CompareTo(rh.Im) == 0;\n}\n\npublic static bool operator !=(Complex lh, Complex rh)\n{\nreturn lh.Re.CompareTo(rh.Re) != 0 || lh.Im.CompareTo(rh.Im) != 0;\n}\n\npublic static Complex operator ++(Complex complex)\n{\nreturn new Complex(complex.Re + 1, complex.Im + 1);\n}\n\npublic bool Equals(Complex other)\n{\nreturn Re.Equals(other.Re) && Im.Equals(other.Im);\n}\n\npublic override bool Equals(object obj)\n{\nreturn obj is Complex other && Equals(other);\n}\n\npublic override int GetHashCode()\n{\nreturn HashCode.Combine(Re, Im);\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Advanced Usage", "Header 2": "Extension Methods", "path": "../pages/digitalGarden/cs/csharp/advancedUsage.mdx"}, "page_content": "Adding methods to existing types is hard and Subclassing is not always sensible which is why in C# you can use extension methods.\nThe old way:  \n```csharp\npublic static class StringExtensions\n{\npublic static string Without(string text, char ch)\n{\nreturn string.Join(\"\", text.Split(ch));\n}\n}\n\nvar text = \"Hxellxo\";\nConsole.WriteLine(StringExtensions.Without(text, 'x'));\n```  \nNow with extension methods:  \n```csharp\npublic static class StringExtensions\n{\n// First param specifies which type you are extending\npublic static String Without(this string text, char ch)\n{\nreturn string.Join((\"\", text.Split(ch));\n}\n}\nvar s = \"Hexllxox\";\nvar result = s.Without('x');\nresult = \"Hexllxox\".Without('x'\n);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Advanced Usage", "Header 2": "Yield", "path": "../pages/digitalGarden/cs/csharp/advancedUsage.mdx"}, "page_content": "Just like generators in python.  \n```csharp\npublic static IEnumerable<int> GenerateNumbers(int num)\n{\nfor(var i = 0; i < num; i++)\nyield return i;\n}\n\nstatic void Main()\n{\nforeach (var a in GenerateNumbers(10))\nConsole.WriteLine(a);\n}\n```  \nInstead of  \n```csharp\npublic static IEnumerable<int> GenerateNumbers(int num)\n{\nList<int> list = newList<int>();\nfor(var i = 0; i < num; i++)\nlist.Add(i);\nreturn list;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Filo I/O", "Header 2": "Working with file streams", "path": "../pages/digitalGarden/cs/csharp/fileIO.mdx"}, "page_content": "Using is the same as in Java try-with resources it automatically closes and disposes of the resource.  \n```csharp\nusing(varstream = new FileStream(\"test.txt\", FileMode.Create))\n{\nConsole.WriteLine(stream.CanRead); // true\nConsole.WriteLine(stream.CanWrite); // true\nConsole.WriteLine(stream.CanSeek); // true\nstream.WriteByte(201);\nstream.WriteByte(210);\nstream.Position= 0;\nConsole.WriteLine(stream.ReadByte());\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Filo I/O", "Header 2": "Reading and writing to .txt", "path": "../pages/digitalGarden/cs/csharp/fileIO.mdx"}, "page_content": "```csharp\nusing(var writer = new StreamWriter(\"text.txt\"))\n{\nwriter.WriteLine(\"First line.\");\nwriter.WriteLine(\"Last line.\");\n}\n\nusing(var reader = new StreamReader(\"text.txt\"))\n{\nConsole.WriteLine(reader.ReadLine());\nString line;\nwhile ((line = streamReader.ReadLine()) != null)\nConsole.WriteLine(line)\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Filo I/O", "Header 2": "Read all lines, line by line", "path": "../pages/digitalGarden/cs/csharp/fileIO.mdx"}, "page_content": "```csharp\nvar lines = File.ReadLines(filename, Encoding.UTF8).ToList();\nforeach (var line in lines)\n{\nvar tokens = line.Split(separator: \"\\t\");\npersonList.Add(new Person(tokens[0], tokens[1]));\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interoperability", "Header 2": "The `dynamic` Type", "Header 3": "Advantages", "path": "../pages/digitalGarden/cs/csharp/interoperability.mdx"}, "page_content": "Interoperability with dynamic languages and frameworks  \n```csharp\ndynamic d = \"Test\";\nConsole.WriteLine(d); // Output: Test\nd = d.ToUpper();\nConsole.WriteLine(d); // Output: TEST\nd = 3; // d changes its runtime type to int\nConsole.WriteLine(d); // Output: 3\nd += 7;\nConsole.WriteLine(d); // Output: 10\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interoperability", "Header 2": "The `dynamic` Type", "Header 3": "Disadvantages", "path": "../pages/digitalGarden/cs/csharp/interoperability.mdx"}, "page_content": "Deactivates type checking and IntelliSense, Everything compiles:  \n```csharp\ndynamic speaker = new NonSenseTalker();\nspeaker.HelloWorld();\nspeaker.Im().Just().Demonstrating().This().Feature();\nspeaker.GoodBye();\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interoperability", "Header 2": "Run python in C #", "path": "../pages/digitalGarden/cs/csharp/interoperability.mdx"}, "page_content": "```csharp\nstatic dynamic Calculate(string expression) {\nvar engine = Python.CreateEngine();\nreturn engine.Execute(expression);\n/*To execute Script\nengine.Execute(\"import pythonDemo\\n pythonDemo.factorial(1000)\");\n*/\n}\n\nstatic void Main(string[] args) {\ndynamic result = Calculate(\"2 * 3\");\nConsole.WriteLine(\"result : {0}\", result);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interoperability", "Header 2": "ExpandoObject", "path": "../pages/digitalGarden/cs/csharp/interoperability.mdx"}, "page_content": "ExpandoObjects implements dynamic properties  \n```csharp\ndynamic o = new ExpandoObject();\no.Test = 123;\no.Foo = \"hallo\";\no.Bar = new object();\no.F = (Action)(() => Console.WriteLine(\"Action done\"));\no.F()\nConsole.WriteLine(o.Foo);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interoperability", "Header 2": "Custom Dynamic Types", "path": "../pages/digitalGarden/cs/csharp/interoperability.mdx"}, "page_content": "```csharp\npublic class World : DynamicObject\n{\nprivate Cities _cities;\n\npublic World(Cities cities)\n{\n_cities = cities;\n}\n\npublic override bool TryGetMember(GetMemberBinder binder, out object result)\n{\nresult = _cities.CityList.FirstOrDefault(city => city.Name == binder.Name)\n?? (object) $\"The city \\\"{binder.Name}\\\" does not exist!\";\n\nreturn true;\n}\n\npublic override bool TrySetMember(SetMemberBinder binder, object? args)\n{\nCity city = null;\ntry\n{\ncity = _cities[binder.Name];\n}\ncatch (Exception e)\n{\nthrow new InvalidOperationException($\"The city \\\"{binder.Name}\\\" does not exist!\");\n}\n\ntry\n{\nvar inputs = (args as string).Split(':');\nswitch (inputs[0])\n{\ncase \"Population\":\ncity.Population = Convert.ToInt32(inputs[1]);\nbreak;\ncase \"Latitude\":\ncity.Location.Latitude = Convert.ToDouble(inputs[1]);\nbreak;\ncase \"Longitude\":\ncity.Location.Longitude = Convert.ToDouble(inputs[1]);\nbreak;\ndefault: throw new ArgumentException(\"Invalid property assigned\");\n}\n}\ncatch (Exception e)\n{\nthrow new ArgumentException(\"Invalid property value passed\");\n}\n\nreturn true;\n}\n}\n```  \npublic virtual bool TryInvokeMember(InvokeBinder binder, Object[] args , out Object result) so you can call dynamic methods like duck.Quack();", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interoperability", "Header 2": "P/Invoke", "path": "../pages/digitalGarden/cs/csharp/interoperability.mdx"}, "page_content": "Allows you to execute unmanaged/native code for example you can call a C++ DLL.  \n1. Locates the DLL containing the function.\n2. Loads the DLL into memory.\n3. Locates the address of the function in memory and pushes its arguments\nonto the stack, marshaling data as required.\n4. Transfers control to the unmanaged function  \n```csharp\nclass Test {\n[DllImport(\"user32.dll)\"]\nstatic extern int MessageBox(uint hWnd, string text, string caption, uint type);\nstatic void Main() {\nMessageBox(0, \"Calling native code isn't that cool?\", \"\", 1,\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interoperability", "Header 2": "COM interoperability", "path": "../pages/digitalGarden/cs/csharp/interoperability.mdx"}, "page_content": "COM stands for \"Component Object Model\". Mostly used to control Microsoft products, windows word etc. So for example you can build plugins.  \n```csharp\nusing Word = Microsoft.Office.Interop.Word\nvar word = new Word.Application();\nword.Documents.Add();\nword.ActiveDocument.SaveAs(\"test.doc\");\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "There are sample questions somewhere\nHands on is free to 12 euro (depending on service different payment schemes but everything pay as you go per use/time)\nAws console is where you actually do stuff\nMake sure account is activated and choose free support plan", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "What is cloud computing", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "Client, server via network with ip addresses. Just like postboxes.\nBuild up of a server: cpur, ram data and strucutred data (databases)\nNetwork our of cables routers switch and servers incl. dns  \nMainting own data center(server cluster) pay rent, power, scaling is limited, people etc. solution: cloud  \nCloud criteria from cloud computing lecture:\n- on demand\n- Broad network access\n- Ressource sharing/pooling multi-tenancy\n- Rapid elasticity and scaliblity\n- Measured service pay as you go  \nTypes of clouds:\n- private (rackspace? Proxmox?)\n- public aws azure, gc\n- Hybrid some local some cloud.  \nWhen using a cloud u Trade capital expense capex for operational expense opex  \nDont need to guess capacity, save on having to maintain a data center, high availability and fault tolerance  \nTypes of cloud computing:  \n- IaaS, provide infrastructure highest level of flexibility (EC2, digital ocean)\n- PaaS, dont need to manage infrastructure just run app, elastic beanstalk\n- SaaS, just work no managment. Face rocgnition on aws or gmail  \nShow different levels the 3 + on premise of what has to be managed.  \nAws has map on infrastructure.aws:\n- regions\n- availability zones\n- data centers\n- edge locations/points of presence  \nRegions have 3-6 zones with each 1 or more datacenters. Each zone has redudant power, network and connectivity. Availability zones are isolated from each other for disasters. And are connected with high bandwidth, low-latency network within region  \nA bit unclear of meaning of edge locations?  \nHow to choose a region:\n- compliance, data governance and legal requirements\n- Proximity, reduce latency\n- Availiability, not all services are available in all regions\n- Pricing  \nSearch at the top is very useful as also has docs and tutorials  \nSome services are global and have the same view no matter which region (can be seen in the top right)  \nCan list services by region (link todo)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "What is cloud computing", "Header 3": "Shared Responsibility Model", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "Shared responsibility diagram shows who is responsible for what (u or aws) how u configure services are ur responsibility if u configure shit security ur fault. Aws is responsible for security on software and hardware level.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "IAM - Identity and access management", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "Directly vs inline? Is it the same?  \nThe first service to look at is IAM, short for Identity and Access Management. This is the service that allows you to manage users and their level of access to the AWS Management Console.\nBecause it is one of the most important services and is used to control access to all other services, it is a **global service**, meaning it is available in all regions.  \nTo remember the acronym, think of yourself saying \"I am\" the person who is going to manage the users and their permissions.  \nWhen you first create an AWS account, you have created a **root account**. This account has complete access to all AWS services and can therfore also rack up a huge bill.\nIt is not recommended to use the root account for everyday tasks, or to share it with others. Instead, you should create use the **least privilege principle** and create an\n**IAM user** for yourself and give it the necessary permissions. This is also a best practice for security reasons.  \nIn AWS permissions are managed using **policies**. A policy is a document that defines permissions. It is written in JSON format and consists of a version, an identifier and a statement.  \nFor example, the following policy `AdminstratorcAccess` allows all actions on all resources:  \n```json\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": \"*\",\n\"Resource\": \"*\"\n}\n]\n}\n```  \nThe `Effect` can be either `Allow` or `Deny`. The `Action` is the action that is allowed or denied on a `Resource`.\nActions are API calls that allow you to interact with AWS services such as read, write delete etc. The `Resource` is the resource that the policy applies to such as an S3 bucket or an EC2 instance.\nThe `*` is a wildcard that matches all actions or resources.  \nWhen creating a new user, you can assign them permissions by attaching policies to them. You can also create **groups** and assign policies to the group and then add users to the group.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "IAM - Identity and access management", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "The `*` is a wildcard that matches all actions or resources.  \nWhen creating a new user, you can assign them permissions by attaching policies to them. You can also create **groups** and assign policies to the group and then add users to the group.\nYou can also create an alias for the sign-in link, which can be useful if you are working with multiple accounts, for example for different projects.  \nIn AWS users, groups, roles etc. are also reffered to as **identities**.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "IAM - Identity and access management", "Header 3": "Types of Policies", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "For a clear overview I suggest you look at the [AWS documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html) on the topic.  \nHowever, in short there are three types of policies:  \n- **Managed Policies**: These are standalone policies that you can attach to multiple identities. They are maintained by AWS and are the recommended way to assign permissions.\nThis is for example the `AdministratorAccess` policy that allows full access to all AWS services or the `IAMFullAccess` policy that allows full access to IAM.\n- **Customer Managed Policies**: These are policies that you create and manage yourself. You can attach them to multiple identities. This is when you define a custom policy\nvia JSON or the visual editor. These policies are stored in your account and are reusable.\n- **Inline Policies**: These are basically customer managed policies that are embedded directly into a single identity and are then also deleted when the identity is deleted.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "IAM - Identity and access management", "Header 3": "Keeping Users Secure", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "Can be found under Account settings and then password policy.  \nWhen creating users for others there are some password policies that you can set to ensure that the passwords are secure:\n- **Minimum password length**\n- **Require specific character types**\n- **Allow users to change their own password**\n- **Require password change on first login**\n- **Password expiration** and **password reuse prevention**  \nThe most secure way to access the AWS Management Console is by using **Multi-Factor Authentication (MFA)**.  \nMFA = something you know (password) and something you have (token on physical device)  \nVirtual MFA devices like Google Authenticator or Authy\nUniversal 2nd Factor (U2F) security key like YubiKey\nHardware Key Fob MFA devices  \nTo activate MFA its at the top right of the console under Security Credentials. You can then choose to activate MFA for the root account.\nFor IAM users you can activate the same MFA or enforce it for all users.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "IAM - Identity and access management", "Header 3": "How to access AWS", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "- **AWS Management Console**: Web-based user interface that you can use to access and manage AWS services.\n- **AWS Command Line Interface (CLI)**: Command line tool that allows you to control multiple AWS services from the command line and automate them through scripts.\n- **AWS Software Development Kits (SDKs)**: Libraries or APIs that allow you to interact with AWS services from your preferred programming language.  \nVia access keys do not share. Can be used for CLI or SDKs. Can be rotated.\nTo configure CLI you can use `aws configure` and enter your access key id , secret access key, region and output format (just enter for default).  \naws iam list-users to list users very useful  \nWhat is cloudshell? is basically CLI in the management console, recommended to use it for CLI commands over local installation.\ncan alos download and upload files to it, seems very useful. Not available in all regions.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "IAM - Identity and access management", "Header 3": "Roles", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "In AWS some services might want to perform actions. For example an EC2 instance might want to access an S3 bucket. To do this you\ncan use **roles** which are similar to users but are assigned to AWS services.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "IAM - Identity and access management", "Header 3": "Security Tools", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "Can use **Credentials Report** to see all users and the status of their credentials. Account-level\nCan use **IAM Access Advisor** shows the services that a user has accessed and the last time they accessed them to align permissions with actual usage (least privilege principle). User-level", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Billing and Cost Management", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "We can view usage and billing information in the **Billing and Cost Management Dashboard**. Here we can also set up budgets and alerts to notify us when we are exceeding our budget. \"charges by service\" is useful to see what is costing the most.\nZero cost budget or montly budget can be set up etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "One of the key compute services in AWS is EC2, short for Elastic Compute Cloud.  \nIaaS  \nMainly consist of:\n- EC2, renting virtual machines\n- EBS, virtual block storage for EC2 (what is block storage?)\n- ELB, load balancer\n- ASG, auto scaling group  \nConfiguration:\n- OS, linux, mac or windows\n- cpu and ram\n- storage, network attached storage with EBS or EFS or hardware storage with instance store\n- network card for speed, security group for firewall, public ip, private ip, dns\n- bootstrap script for configuration, ec2 user data for startup script, only runs once when instance is launched (restart?) installing updates, softeare\n- ec2 user data script runs as root  \nshow example of ec2 instances  \nto ssh into ec2 create a key pair and use ssh -i key.pem ec2-user@ip\nin the security group allow port 22 for ssh and 80 for http as will start a Web server\nebs volumes can be configured like deletion on termination, encryption, snapshotting etc.  \nat the bottom of the advanced section of the ec2 instance creation there is a user data section where you can add a\nbash script that will run on startup. This can be used to install software, configure the instance etc. Why is it called user data?  \nAfter starting and stopping an instance the public ip will change. To have a static ip you can use an elastic ip.  \nThere are different types of instances like general purpose (balanced), compute optimized, memory optimized and storage optimized.  \nm5.2xlarge,\nm = instance class\n5 = generation\n2xlarge = size within the instance class  \nec2instances.info is a useful website to compare instances", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "Header 3": "Security Groups", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "Fundamental firewall rules for your EC2 instances. They control the inbound and outbound traffic to your instances.  \nregualte:\n- access to ports\n- authorise ip ranges, also if ipv4 or ipv6\n- control of inbound network (ingress) and outbound network (egress) traffic  \nBy default all inbound traffic is blocked and all outbound traffic is allowed.  \nmany to many relationship between security groups and instances.  \nIf a timeout occurs when trying to connect to an instance it is likely a security group issue.\nfor example if you are trying to connect to an instance via ssh and the security group does not allow port 22.  \nIf you get connection refused it is likely an issue with the instance itself.  \nCan authorize specific security groups to allow traffic between instances in the same security group without specifying ip addresses.  \nMost important ports:\n- 22 for ssh\n- 21 for ftp, unencrypted\n- 22 SFTP, encrypted ftp over ssh\n- 80 for http unencrypted\n- 443 for https encrypted\n- 3389 for rdp remote desktop protocol, windows instances  \n0.0.0.0/0 means all ip addresses  \nec2 instance connect is a new feature that allows you to connect to an instance without manually using ssh.  \nGo into the instance and click connect, then connect with ec2 instance connect. Will temporaily create a key pair and connect to the instance.\nPort 22 must be open in the security group.  \nNever cofnigure of enter iam credentials into an instance. Use roles instead.  \npurchsing options:\n- on demand, pay as you go\n- reserved instances, 1-3 years, cheaper up to 70%, upfront, partial upfront or no upfront\n- convertible reserved instances, change instance type, same but less discount\n- savings plans, commit to a certain amount of usage 10 per hour for 1 year, cheaper than on demand excess is on-demand\n- spot instances, bid for unused capacity, cheap but can be terminated at any time, less reliable up to 90% cheaper, not suitable for critical jobs", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "Header 3": "Security Groups", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "- spot instances, bid for unused capacity, cheap but can be terminated at any time, less reliable up to 90% cheaper, not suitable for critical jobs\n- dedicated hosts, physical server for you, expensive, complicance requirements or licensing like per core etc.\n- dedicated instances, no other customers on the same hardware, expensive\n- capacity reservations, reserve capacity for specific instance type in a specific availability zone, expensive, are assured capacity, billed on-demand even if don't use  \nthere is a table for between dedicated host vs dedicated instance", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "Header 3": "EBS - Elastic Block Store", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "Block storage vs object storage?  \n\"network usb stick\"  \nstorage options for ec2 instances.\nnetwork drive that can be attached to an ec2 instance. persist data even if instance is terminated.\nNot a physical drive, more latency than local storage.\ncan only be attached to one instance at a time and are bound to a specific availability zone.  \nto move ebs between AZs you can create a snapshot and then create a new volume from the snapshot in the new AZ.\nFixed size but can be increased over time.  \ncan set \"delete on termination\" to false to keep the volume when the instance is terminated.\nby default the root volume is deleted on termination and additional volumes are kept.  \nEBS Snapshots are backups of your EBS volumes. recommend to first detach the volume before creating a snapshot (make it clean).\nusing snapshots you can create new volumes, move volumes between AZs or regions, share snapshots with other accounts etc.\nsnapshots can be copied to other regions for disaster recovery.  \ncan be moved to archive which is cheaper but takes longer to restore 24 hours to 72 hours.  \nyou can setup rules for when snapshots are deleted to go into a recycle bin for 7 days before being permanently deleted. \"retention rules\"", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "Header 3": "AMI - Amazon Machine Image", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "powers ec2 instances. like software configuration, os, application server, applications etc.\nThis allows for faster boot times and consistency across instances as everything preconfigured and packeged.  \na public ami is for example the amazon linux 2 ami. you can also create your own ami from an existing instance. think of docker images.\nCreate own ami or use marketplace ami can potentially save time but also cost and security concerns.  \nAMIs are built from a ec2 instance. Ideally you should stop the instance before creating the AMI to ensure that the file system is in a consistent state.\nThis creation process will also create a snapshot of the EBS volumes attached to the instance.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "Header 3": "EC2 Image Builder", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "automate the creation of VMs or container images.\ni.e automate the creation, maintain, validate and test AMIs.  \nStarts a builder ec2 instance where you can install software, configure etc. and then create an image from that.\nThen an instance is launched from the image and the image is tested with tests that you define. if they pass then published.  \nThis can be setup on a schedule to ensure that the images are always up to date.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "Header 3": "EC2 Instance Store", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "certain instance types come with instance store volumes which are physically attached, so better performance but data is lost when the instance is stopped or terminated (ephemeral storage).\ngood for like caches, buffers, scratch data etc.\nrisk of data loss, so not recommended for critical data.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "Header 3": "EFS - Elastic File System", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "Managed network file system that can be shared across multiple ec2 instances. Think of it as a network usb stick like the AD.\nhighly available, scalable, expensive, pay for what you use no capacity planning.\ninstances can be across different AZs.  \nEFS-IA is infrequent access storage class that is cheaper but has a retrieval fee. for data that is not accessed often.\nwill automatically move files that are not accessed often to the infrequent access storage class based on a lifecycle policy.\nonce you access the file it will be moved back to the standard storage class.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "Ec2 - elastic compute cloud", "Header 3": "FSx", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "use 3rd party high performance file systems like windows file server or lustre for high performance computing.\nnative and supported for windows via smb and integrated with Microsoft Active Directory for user authentication.  \nLustre is Linux and cluster file system for high performance computing. for ML and analytics workloads.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "ELS and ASG", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "ELS is an elastic load balancer that distributes incoming traffic across multiple targets like ec2 instances.\nASG is an auto scaling group that automatically adjusts the number of ec2 instances in response to demand.  \nScalability = ability to handle increased load, either:\n- vertically, for call center instead of junior operator hire senior operator, common for databases\n- horizontally (elasticity), for call center just get more operators, implies a distributed system, common for web servers\n- elasticity = ability to automatically increase or decrease capacity based on demand, auto-scaling pay per use\n- agility = agile development, fast deployment, fast changes, fast feedback  \nhigh availability = ability to stay up and running, run in at least 2 availability zones to survivie a disaster.  \nboth require load balancing and auto scaling. high availability also wants multi AZ mode for both.  \nload balancer forward internet traffic to multiple ec2 instances downstream. load balancer can be internal or external, i.e internet facing or not.\nsingle point of acces, spread load, high availability, fault tolerance, health checks for downstream instances.  \nEBS = managed load balancer, meaning aws manages the load balancer for you makes sure is up and running, scales automatically, no need to worry about maintenance.\nCan setup custom load balancer which is cheaper but more work.  \n4 types of load balancers:\n- Application Load Balancer (ALB), http and https grpc, layer 7, http routing features, static dns/url\n- Network Load Balancer (NLB), TCP, high performance, layer 4, static ip through elastic ip\n- Gateway Load Balancer, layer 3, GENEVE protocol on ip packets, route traffic to firewalls managed by ec2 instances for intrusion detection etc. traffic is checked and insprected via GWLB.\n- Classic Load Balancer, retired 2023, layer 4 and 7  \ntraffic is routed to \"target groups\" which depend on the protocol and port. can have multiple target groups per load balancer. and each target group can have multiple targets.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "ELS and ASG", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "- Classic Load Balancer, retired 2023, layer 4 and 7  \ntraffic is routed to \"target groups\" which depend on the protocol and port. can have multiple target groups per load balancer. and each target group can have multiple targets.  \nASG for auto scaling, like shopping during the day but not at night. can scale out (add instances) or scale in (remove instances) based on demand.\nCan setup min/desired/max, they can also be registerd to a target group. the ASG will also replace unhealthy instances because of health checks.  \nlaunch templates are instructions for the ASG on how to launch instances. can be used to launch instances with specific configurations like ami, instance type, key pair etc.  \nscaling strategies:\n- manual, not recommended, update the desired capacity manually\n- dynamic, either simple or step scaling, based on cloudwatch alarms based on a trigger like cpu usage > 70% for 5 minutes add or under 30% for 5 minutes remove instances??\ntarget tracking scaling, scale out or in to keep a metric at a specific value like cpu usage at 70%.\nlastly scheduled scaling, scale out or in based on a schedule like every saturday scale out for sports betting website.\n- predictive scaling, machine learning to predict future demand based on historical data. will provision instances in advance for easy pattern recognition.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "S3 - Simple Storage Service", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "many use cases like backup and storage, disaster recovery, archiving, hybrid cloud storage, data lakes, static websites etc.  \nstore objects (files) in buckets (folders). buckets must have a globally unique name across all regions and accounts. buckets is a global service\nbut are stored in a region.  \nobjects have a key (full path) prefix + object name. Seems like a folder but doesn't actually exist.\nobject values are the content. max size of an object is 5TB and can store any type of file like images, videos, backups, logs etc.\nWhen uploading more then 5GB must use \"multipart upload\", X/5GB parts.\nobject metadata, list of text key value pairs like content type from system or user.\nobject tags unicode key value pairs by user for organization, cost allocation etc. max 10 tags per object.\nversion id for versioning if versioning is enabled. can be used to restore previous versions of an object.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cloud Practitioner", "Header 2": "S3 - Simple Storage Service", "Header 3": "Security", "path": "../pages/digitalGarden/cs/aws/practitioner.mdx"}, "page_content": "- user based, IAM policies, sets which api calls a user can make\n- resource based, bucket policies, bucket wide rules such as public access, cross account access etc.\n- Object Access Control List (ACL), fine grained control\n- Bucket Access Control List (ACL), less common  \nobjects can be encrypted with encryption keys  \nBucket policies, that allows anyone to read, i.e public read access. principal are the users that are allowed to do the action, therefor * is everyone.  \n```json\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"PublicRead\",\n\"Effect\": \"Allow\",\n\"Principal\": \"*\",\n\"Action\": \"s3:GetObject\",\n\"Resource\": \"arn:aws:s3:::mybucketname/*\"\n}\n]\n}\n```  \nYou can set block \"all public access\" to true to prevent public access to the bucket even if there is a bucket policy that allows it.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "path": "../pages/digitalGarden/cs/concurrentParallel/actorModel.mdx"}, "page_content": "The actor model is a mechanism to deal with concurrent computation. An actor (autonomous concurrent object) is the primitive unit of computation. Actors are executed asynchronously of each other and have the following:  \n- Private state, as actors there should be no shared mutable state between actors!\n- An Inbox that stores the received messages in FIFO order.\n- Behavior that is executed asynchronously when it receives a message from other actors.  \nThe key concept is that actors are completely isolated from each other and they will never share memory. The only way they can share states is by exchanging messages with each other.  \n<Image\nsrc=\"/cs/concurrentActorModel.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Scala Akka", "path": "../pages/digitalGarden/cs/concurrentParallel/actorModel.mdx"}, "page_content": "Actors in Scala are implemented with the akka library. Akka is very popular because it is very simple and self-explanatory but also because it has very high performance. It can send up to 50 million messages per second and 2.5 million actors take up 1 GB on the heap.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Scala Akka", "Header 3": "Creating an Actor", "path": "../pages/digitalGarden/cs/concurrentParallel/actorModel.mdx"}, "page_content": "To work with akka you need to create an actor system. This is the network/container for all the actors. To then create an Actor you need to extend the Actor class and implement the receive method, this is the method that will be called when a message is received. To then create a reference to the actor you need to instantiate it as part of the actor system with the `actorOf` method. The returned reference is immutable. You can only create actors this way if you try to instantiate an actor with new you will get an `akka.actor.ActorInitializationException`. Finally, you can send messages to the actor by using the `!` operator.  \n```scala\nimport scala.language.postfixOps // required for `a ! msg` p\nimport akka.actor.{ActorSystem, Actor, ActorRef, Props}\n\nval as = ActorSystem(\"as\") // Actor infrastructure\nclass PrintActor extends Actor { // Actor definition\nvar nthRequest = 0 // Mutable state\ndef receive = { case msg => // Behavior\nnthRequest += 1\nprintln(s\"$nthRequest:$msg\")\n}\n}\nval printActor: ActorRef = as.actorOf(Props[PrintActor]) // Creates and starts actor\n\nprintActor ! \"Hello\"\nprintActor ! \"Bye\"\n```  \nIn most cases the actor is created with the default constructor then you can use `as.actorOf(Props[PrintActor])`. However, if you want to pass arguments to the constructor then you can do something like this:  \n```scala\nclass HelloActor(myName: String) extends Actor {\ndef receive = {\ncase \"hello\" => println(s\"hello from $myName\")\ncase _       => println(s\"'huh?', said $myName\")\n}\n}\n\nobject Main extends App {\nval as = ActorSystem(\"HelloSystem\")\nval helloActor = as.actorOf(Props(new HelloActor(\"Fred\")), name = \"helloActor\")\n}\n```  \nYou can also create an actor as an anonymous subclass:  \n```scala\nval print: ActorRef = as.actorOf(Props(\nnew Actor {\ndef receive = { case msg => println(msg) }\n}\n))\n```  \nAkka also supports hierarchies between actors for example you can have child actors for specific functionality:  \n```scala\nval as = ActorSystem(\"as\")", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Scala Akka", "Header 3": "Creating an Actor", "path": "../pages/digitalGarden/cs/concurrentParallel/actorModel.mdx"}, "page_content": "class ChildActor() extends Actor {\ndef receive = {\ncase msg => println(\"I'm \" + self + \" : \" + msg) // self is like this in actor\n}\n}\n\nclass ParentActor extends Actor {\nval child = context.actorOf(Props[ChildActor], \"child\")\ndef receive = {\ncase msg: String =>\nchild ! \"Greets from dad\"\nprintln(msg)\n}\n}\n\nval p = as.actorOf(Props[ParentActor], \"parent\")\np ! \"Hi Kid\"\np ! \"Bye Kid\"\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Scala Akka", "Header 3": "Sending Messages", "path": "../pages/digitalGarden/cs/concurrentParallel/actorModel.mdx"}, "page_content": "Messages are sent asynchronously with the tell `!` operator. Messages are stored in the mailbox of the receiver and can be anything (type Any). Messages are sent with the guarantee of at-most-once delivery / no guaranteed delivery (send-and-pray) however if the actor system is local then it is as guaranteed as to when calling a method.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Scala Akka", "Header 3": "Receiving Messages", "path": "../pages/digitalGarden/cs/concurrentParallel/actorModel.mdx"}, "page_content": "The `receive` method specifies the initial behavior of an actor when it receives a message. The function uses pattern matching and is defined as `def receive: PartialFunction[Any,Unit]` meaning it is only defined for certain arguments. You can check if a function is defined for a given argument using the `isDefinedAt` method.  \n```scala\nval pf: PartialFunction[Any,Unit] = {\ncase i: Int if i > 42 => println(\"huge\")\ncase s: String => println(s.reverse)\n}\npf.isDefinedAt(42) // false\npf.isDefinedAt(43) // true\n```  \nIf there is no match then there is `MatchError` and the message is published to the actor system's EventStream, which you can imagine as a dead letterbox. You can however add listeners to this letterbox and react to messages that end up there.  \nTypically case classes (similar to Java records) are used as messages because they describe the vocabulary an actor understands (its API) and they are convenient for match expressions. You can also refine cases with so-called Guards as seen below.  \n```scala\ncase class PrintMsg(msg: String)\ncase class ShoutMsg(msg: String)\nclass PrintActor extends Actor {\ndef receive = {\ncase PrintMsg(m) if m.contains(\"@\") => println(\"mail: \" + m)\ncase PrintMsg(m) => println(\"text: \" + m)\ncase ShoutMsg(m) => println(\"RECEIVED: \" + m.toUpperCase)\n}\n}\n```  \nMessage processing is scheduled on a thread pool this means that not every message of an actor is necessarily processed by the same thread. This also means we need some new guarantees:  \n- The send of a message happens-before the receive of that message\n- Processing of one message happens-before processing the next message by the same actor", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Scala Akka", "Header 3": "Advanced Messaging", "path": "../pages/digitalGarden/cs/concurrentParallel/actorModel.mdx"}, "page_content": "You should use `self` (of type ActorRef) to refer to the current actor to safely pass it around. You can also use `sender` to refer to the actor that sent a message.  \n```scala\nval as = ActorSystem(\"as\")\n\ncase class Msg(msg: String, sender: ActorRef)\n\nclass EchoActor extends Actor {\ndef receive = { case Msg(msg,client) => client ! msg}\n}\nval echoActor = as.actorOf(Props[EchoActor])\n\nclass Sender extends Actor {\nechoActor ! Msg(\"Hello\", self)\ndef receive = { case t => println(t) }\n}\n```  \nThe above example could be simplified to:  \n```scala\nval as = ActorSystem(\"as\")\n\nclass EchoActor extends Actor {\ndef receive = { case msg => sender ! msg }\n}\nval echoActor = as.actorOf(Props[EchoActor])\n\nclass Sender extends Actor {\nechoActor ! \"Hello\"\ndef receive = { case t => println(t) }\n}\n```  \nUsing `context.setReceiveTimeout` you can set a timeout for inactivity (when no messages are sent). When the timeout is excited a ReceiveTimeout message is triggered.  \n```scala\nclass TimeOutActor extends Actor {\ncontext.setReceiveTimeout(3.second)\ndef receive = {\ncase \"Tick\" => println(\"Tick\")\ncase ReceiveTimeout => println(\"TIMEOUT\")\n}\n}\n```  \n#### Ask Pattern  \nAkka also supports Futures. So you can send a message and receive a future containing the answer of the actor. This is done using the ask `?` operator.  \n```scala\nclass EchoActor extends Actor {\ndef receive = { case msg => sender ! msg }\n}\nval as = ActorSystem(\"as\")\nval echoActor = as.actorOf(Props[EchoActor])\nimplicit val timeout = Timeout(3.seconds) // consumed by '?'\nval futResult: Future[Any] = (echoActor ? \"Hello\") // completed with AskTimeoutException in case of timeout\n// OR\nval timeout = Timeout(3 seconds)\nval futResultString: Future[String] = (echoActor ? (\"Hello\")(timeout)).mapTo[String] // cast to specific type\n\nimport as.dispatcher // ExecutionContext required by Future#map, Future#onComplete etc.\nfutResultString.map(s => s.toUpperCase)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Actor Model", "Header 2": "Finite State Machines", "path": "../pages/digitalGarden/cs/concurrentParallel/actorModel.mdx"}, "page_content": "The actor model is ideal for representing finite state machines, as the states are the actors and the events can be represented with messages. For example, we can model a simple light switch:  \n```mermaid\nstateDiagram-v2\nOn --> Off\nOff --> On\n```  \n```scala\ncase object On\ncase object Off\nclass Switch extends Actor {\nvar on = false\ndef receive = {\ncase On if !on => println(\"turned on\"); on = true\ncase Off if on => println(\"turned off\"); on = false\ncase _ => println(\"ignore\")\n}\n}\n```  \nAkka also offers so-called hot switching which enables us to swap the behavior of an actor at runtime.  \n```scala\nclass Switch extends Actor {\nval offBehavior: PartialFunction[Any,Unit] = {\ncase On => println(\"turned on\"); context.become(onBehavior)\ncase _ => println(\"ignore\")\n}\nval onBehavior: PartialFunction[Any,Unit] = {\ncase Off => println(\"turned off\"); context.become(offBehavior)\ncase _ => println(\"ignore\")\n}\ndef receive = offBehavior // initial behavior\n}\n```  \nThere is also the last variation of this example where the actor remembers his previous behavior. So from the initial off behavior, the new on behavior is placed on top and then removed again so it can return to the initial off behavior  \n```scala\nclass Switch extends Actor {\nval offBehavior: PartialFunction[Any,Unit] = {\ncase On => println(\"turned on\"); context.become(onBehavior, false)\ncase _ => println(\"ignore\")\n}\nval onBehavior: PartialFunction[Any,Unit] = {\ncase Off => println(\"turned off\"); context.unbecome()\ncase _ => println(\"ignore\")\n}\ndef receive = offBehavior\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Condition Variables", "path": "../pages/digitalGarden/cs/concurrentParallel/conditionVariables.mdx"}, "page_content": "Condition variables are another mechanism for synchronizing a program. Condition variables allow threads to enter the waiting state (stop running) until they are signaled/notified by another thread that some condition maybe have been fulfilled, and they can take over. The most common example used to illustrate this is a carpark. When the carpark is full you have to wait until a car drives out, and it is no longer full. Once this happens you want to be notified that the carpark is no longer full, so you can enter the carpark.  \n```java\npublic class CarPark {\nprivate int spaces;\npublic CarPark(int spaces) { this.spaces = spaces; }\npublic synchronized void enter() {\nwhile(spaces == 0) {\ntry { this.wait(); } // wait and releases lock\ncatch (InterruptedException e) { }\n}\nspaces--;\n}\npublic synchronized void exit() {\nspaces++;\nthis.notifyAll(); // wakes up all threads for race to get the lock\n}\n}\n```  \nImportant is that the wait and notify/notifyAll functions are called on the lock object. Because every object can be a lock object the functions are implemented in the `Object` class. Another important thing to note is that when the wait function is called the lock is released so that other threads can still do work. When the thread acquires the lock again it continues from where it was waiting.  \n<Callout type=\"warning\">\nMake sure to use a while loop, because of interrupts or spurious wakeups.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Condition Variables", "Header 2": "Notify vs NotifyAll", "path": "../pages/digitalGarden/cs/concurrentParallel/conditionVariables.mdx"}, "page_content": "The `notify()` function wakes up one waiting thread by random selection, which might still have to compete for the lock. If there are no threads waiting then the notify function is just like an empty statement.  \nThe `notifyAll()` function wakes up all the waiting threads which then must compete for the lock.  \nThere are two forms of waiters (waiting threads):  \n- **Uniform waiters**: All waiters are equal (wait for the same condition)\n- **One-in, one-out**: A notification on the condition variable enables at most one thread to proceed  \nWhen you are working with uniform waiters notify() is fine however it is much safer but less efficient to use notifyAll().", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Condition Variables", "Header 2": "BlockingQueue", "path": "../pages/digitalGarden/cs/concurrentParallel/conditionVariables.mdx"}, "page_content": "A blocking queue is a queue that blocks when you try to dequeue from it and the queue is empty, or if you try to enqueue items to it and the queue is already full.  \n<Image\nsrc=\"/cs/concurrentCircularBlockingQueue.png\"\ncaption=\"A circular blocking queue.\"\nwidth={500}\n/>  \nThere are a few ways to implement a thread-safe blocking queue. You can either use three locks, one for each condition and one for the synchronization:  \n```java\npublic class Queue {\nprivate final static int SIZE = 10;\nprivate Object[] buf = new Object[SIZE];\nprivate int tail = 0, head = 0;\n\nprivate Object notEmpty = new Object();\nprivate Object notFull = new Object();\n\npublic synchronized Object dequeue() {\nwhile (tail == head) { // while empty\nsynchronized (notEmpty) {\ntry { notEmpty.wait(); } catch (Exception e) {}\n}\n}\nsynchronized (notFull) { notFull.notify(); }\nObject e = buf[head]; head = (head + 1) % SIZE;\nreturn e;\n}\npublic synchronized void enqueue(Object c) {\nwhile ((tail + 1) % SIZE == head) {\nsynchronized (notFull) {\ntry { notFull.wait(); } catch (Exception e) {}\n}\n}\nsynchronized (notEmpty) { notEmpty.notify(); }\nbuf[tail] = c;\ntail = (tail + 1) % SIZE;\n}\n}\n```  \nOr when working with the Lock interface we can add conditions to the lock:  \n```java\npublic class Queue {\nprivate final static int SIZE = 10;\nprivate final Object[] buf = new Object[SIZE];\nprivate int tail = 0, head = 0;\n\nprivate final Lock lock = new ReentrantLock();\nprivate final Condition notEmpty = lock.newCondition();\nprivate final Condition notFull = lock.newCondition();\n\npublic Object dequeue() {\nlock.lock();\ntry {\nwhile (tail == head) { // while empty\ntry { notEmpty.await(); } catch (Exception e) {}\n}\nObject e = buf[head]; head = (head + 1) % SIZE;\nnotFull.signal(); return e;\n} finally { lock.unlock(); }\n}\npublic void enqueue(Object c) {\nlock.lock();\ntry {\nwhile ((tail + 1) % SIZE == head) {\ntry { notFull.await(); } catch (Exception e) {}\n}\nbuf[tail] = c; tail = (tail + 1) % SIZE;\nnotEmpty.signal();\n} finally {\nlock.unlock();\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads", "Header 2": "Processes vs Threads", "path": "../pages/digitalGarden/cs/concurrentParallel/threads.mdx"}, "page_content": "A process is an executable program that is loaded into memory. A process has its own logical memory address space allocated by the kernel. As seen in C we can also switch between processes but this is a rather expensive operation. Processes can communicate with each other via signals, interprocess communication - IPC, files or sockets.  \nA thread is a single sequential flow that runs in the address space of its process. This also means it shares the same address space with threads of the same process. It does, however, have its personal execution context containing amongst other things the call stack. For comparison threads communicate with each other via shared memory which we will see is a very dangerous but practical thing.  \n<Image\nsrc=\"/cs/concurrentJVMProcess.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads", "Header 2": "Threading models", "path": "../pages/digitalGarden/cs/concurrentParallel/threads.mdx"}, "page_content": "Threading models define how threads are managed.  \n- Kernel-Level (1:1): The kernel controls the threads and processes and threads are scheduled to available CPUs by the kernel. This approach is used by most current JVM implementations.\n- User-level (1:n): Threads are implemented and managed/scheduled by a runtime library, so-called green threads. This allows for efficient context switching and application-specific scheduling as the kernel is not involved. This does however mean that different threads can not be scheduled on different processors.\n- Hybrid (m:n): User-level threads are assigned to some kernel threads.  \n<Image\nsrc=\"/cs/concurrentThreadingModels.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads", "Header 2": "Scheduling", "path": "../pages/digitalGarden/cs/concurrentParallel/threads.mdx"}, "page_content": "Scheduling is done by the kernel and is the act of allocating CPU time to threads. It also has to make sure that each CPU processor only has one thread running at any given time.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads", "Header 2": "Scheduling", "Header 3": "Cooperative", "path": "../pages/digitalGarden/cs/concurrentParallel/threads.mdx"}, "page_content": "With cooperative scheduling, the threads decide when they should give up the processor to other threads. Meaning the processor never interrupts a thread to initiate a context switch from one thread to another. This can lead to threads hogging or even completely locking out the processor.  \n<Image\nsrc=\"/cs/concurrentSchedulingCooperative.png\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads", "Header 2": "Scheduling", "Header 3": "Preemptive", "path": "../pages/digitalGarden/cs/concurrentParallel/threads.mdx"}, "page_content": "With preemptive scheduling, the kernel can interrupt the running thread at any time. This stops threads from unfairly hogging the processor. It is up to the Java implementation but in most implementations, preemptive scheduling is used.  \n<Image\nsrc=\"/cs/concurrentSchedulingPreemptive.png\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads", "Header 2": "Java Threads", "path": "../pages/digitalGarden/cs/concurrentParallel/threads.mdx"}, "page_content": "In Java, a program's entry point is the main function that starts the initial thread, the main thread (non daemon). Java defines the functional interface `Runnable` which should be implemented by any class whose instance is intended to be executed by a thread.  \n```java\ninterface Runnable {\nvoid run();\n}\n```  \nThe `Thread` class represents a thread in Java and takes a runnable whilst also implementing the Runnable interface. The `start()` function creates a new thread and then executes the `thread.run()` which executes the passed `runnable.run()` in a separate thread. The start and run functions return immediately as the rest is executed on a separate thread.  \n```java\nclass Thread implements Runnable{\nThread(Runnable target){...}\nThread(Runnable target, String name){...}\n\nvoid run(){...}\nvoid start(){...}\nvoid join(){...}\nvoid join(long millis){...}\nstatic void sleep(long millis){...}\nstatic void yield(){...}\nvoid setDaemon(boolean b){...}\nvoid setPriority(int newPriority){...}\n}\n```  \nThere are a few ways you can use the Thread class. You can extend the Thread class and implement the run method which is an easy and simple way to make use of threads. However, it is better to implement runnable separately and pass it to the Thread class as it is a better separation of concerns. You can still access the thread methods by using static imports. Because Runnable is a functional interface you can also use lambdas which is in my opinion the way to go for simple examples.  \n```java\nclass ThreadExamples {\n// Extending Thread\nstatic class MyThread extends Thread {\npublic void run() {\nSystem.out.println(\"MyThread running\");\n}\n}\n\n// Anonymous subclass of Thread\nstatic Thread thread = new Thread() {\npublic void run() {\nSystem.out.println(\"Anonymous MyThread running\");\n}\n};\n\n// Implementing Runnable\nstatic class MyRunnable implements Runnable {\npublic void run() {\nSystem.out.println(\"MyRunnable running\");\n}\n}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads", "Header 2": "Java Threads", "path": "../pages/digitalGarden/cs/concurrentParallel/threads.mdx"}, "page_content": "// Implementing Runnable\nstatic class MyRunnable implements Runnable {\npublic void run() {\nSystem.out.println(\"MyRunnable running\");\n}\n}\n\n// Anonymous implementation of Runnable\nRunnable myRunnable = new Runnable() {\npublic void run() {\nSystem.out.println(\"Anonymous MyRunnable running\");\n}\n};\n\n// Lambda runnable\nstatic Runnable lambdaRunnable = () -> System.out.println(\"Lambda Runnable running\");\n\npublic static void main(String[] args) {\nMyThread t1 = new MyThread();\nThread t2 = new Thread(new MyRunnable());\nThread t3 = new Thread(lambdaRunnable);\nThread t4 = new Thread(() -> System.out.println(\"Inline Lambda Runnable running\"));\nt1.start();\nt2.start();\nt3.start();\nt4.start();\n// main waits for all to finish before exiting\n}\n}\n```  \nThe yield function hints to the scheduler that the calling thread is willing to yield its use of the processor, but it can just be ignored by the processor.  \nThe join function blocks the calling thread and waits for the thread on which it was called until it terminates. A number of milliseconds can also be passed to the join function which defines the maximum amount of time to wait for the thread to terminate.  \nWith the setDaemon function, a thread can be marked as either a daemon or user thread. This function must be called before the thread is started because the type of thread can not be changed whilst it is running. If a process only has demon threads left then the process stops and therefore also the threads.  \nThreads can have a priority which is an integer value in the range of 1 to 10 (10 being the highest priority). The JVM is free to implement these priorities which means that they can also be ignored.  \n<Image\nsrc=\"/cs/concurrentJavaThreadStates.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads", "Header 2": "Java Threads", "Header 3": "Exceptions in Java Threads", "path": "../pages/digitalGarden/cs/concurrentParallel/threads.mdx"}, "page_content": "If an exception is thrown in a thread it can be caught and handled inside the thread. However, if the exception is never caught the thread will just terminate. This is why `join()` returns and the main thread can carry on with its work, the exception itself is lost.  \n```java\npublic static void main(String[] args) throws Exception {\nThread t = new Thread(() -> {\nint value = 1 / 0;\n});\nt.start();\nt.join();\nSystem.out.println(\"Main continues\");\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "The Java Memory Model (JMM) specifies guarantees that are given by the Java Virtual Machine (JVM) relating to concurrency:  \n- When writing operations on variables become visible to other threads\n- Which operations are atomic\n- Ordering of operations, meaning under which circumstances can the effects of operations appear out of order to any given thread.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Memory layout", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "Modern CPUs don't just work with the main memory (RAM) they also have multiple layers of caches and registers to perform more efficiently. You can see [on this page](https://gist.github.com/hellerbarde/2843375) why it is worth having these caches and the difference in the time it takes to read depending on how far down the CPU has to reach for the data. However, this means that there can be multiple versions of the same data on different levels which can lead to issues. Additionally, as we know all threads share the main memory however each core and therefore thread has its own cache levels so there can be inconsistency inside a thread but also between threads.  \n<Image\nsrc=\"/cs/concurrentCpuMemoryLayout.png\"\nwidth={300}\n/>  \nTo illustrate this we have the program below. When running the program we expect to see the values (1,0), (1,1) and (0,1) for all 6 possible interleavings.  \n<Image\nsrc=\"/cs/concurrentMemoryLayoutExample1.png\"\nwidth={600}\n/>  \nHowever, when running the program we also get (0,0). This is due to either compiler reordering or caching.  \n<Image\nsrc=\"/cs/concurrentMemoryLayoutExample2.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Happens before rules", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "The JMM defines a relationship called happens-before on actions such as reading/writing to variables, locking/releasing monitors and starting/joining threads. These happens-before relationships guarantee that a thread executing action A can see the results of action B on the same or a different thread. If there is no such relationship then there is no guarantee!", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Happens before rules", "Header 3": "Rule 1", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "Each action in a thread happens-before every action in that thread that comes later in the program order.  \n<Image\nsrc=\"/cs/concurrentHappensBefore1.png\"\nwidth={400}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Happens before rules", "Header 3": "Rule 2", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "Releasing a lock happens-before every subsequent lock on the same lock.  \n<Image\nsrc=\"/cs/concurrentHappensBefore2.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Happens before rules", "Header 3": "Rule 3", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "A write to a volatile field happens-before every subsequent read of the same field.  \n<Image\nsrc=\"/cs/concurrentHappensBefore3.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Happens before rules", "Header 3": "Rule 4", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "A call to start a thread with `start()` happens-before every subsequent action in the started thread.  \n<Image\nsrc=\"/cs/concurrentHappensBefore4.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Happens before rules", "Header 3": "Rule 5", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "Actions in a thread `t1` happens-before another thread detects the termination of thread `t1`.  \n<Image\nsrc=\"/cs/concurrentHappensBefore5.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Happens before rules", "Header 3": "Rule 6", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "The happens-before order is transitive.  \n<Image\nsrc=\"/cs/concurrentHappensBefore6.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Volatile", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "Volatile fields guarantee the visibility of writes (i.e. volatile variables are never cached). Read access to a volatile field implies getting fresh values from memory (slower). Write access to a volatile field forces the thread to flush all pending writes to the memory level. Volatile variables have a cost due to these things having to be done and caching no longer being allowed. Important to note is also that access to a volatile variable inside a loop can be more expensive than synchronizing the entire loop.  \n```java\nclass MyExchanger {\nprivate volatile Pair data = null;\npublic String getPairAsString() {\nreturn data == null ? null : data.toString();\n}\npublic boolean isReady() {\nreturn data != null;\n}\npublic void setPair(Object first, Object second) {\nPair tmp = new Pair();\ntmp.setFirst(first);\ntmp.setSecond(second);\ndata = tmp; // guaranteed to have both\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Volatile", "Header 3": "Fixing Assignment Atomicity", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "Depending on the implementation a long or double assignment `double x = 3;` is not atomic, it will most lightly write 32 bits at a time. To prevent this we can make the double volatile which will guarantee the assignment to be atomic.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "JMM - Java Memory Model", "Header 2": "Double-checked Locking Problem", "path": "../pages/digitalGarden/cs/concurrentParallel/javaMemoryModel.mdx"}, "page_content": "We want a Singleton that has lazy initialization that is also thread-safe. Our first attempt could be something like the code below with the `getInstance()` function being synchronized so that we don't run into problems. And this works fine however it is very expensive because for every getInstance we have the synchronization overhead.  \n```java\npublic class Singleton {\nprivate static Singleton instance;\npublic synchronized static Singleton getInstance() {\nif(instance == null) {\ninstance = new Singleton();\n}\nreturn instance;\n}\nprivate Singleton() { /* initialization */ }\n}\n```  \nTo fix this we need to do so-called double-checking. We also need to make the instance volatile to prevent there being uninitialized objects.  \n```java\npublic class Singleton {\nprivate volatile static Singleton instance;\npublic static Singleton getInstance() {\nif(instance == null) {\nsynchronized(Singleton.class) {\nif(instance == null) {\ninstance = new Singleton();\n}\n}\n}\nreturn instance;\n}\nprivate Singleton() { /* initialization */ }\n// other methods\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Synchronizers", "path": "../pages/digitalGarden/cs/concurrentParallel/synchronizers.mdx"}, "page_content": "A synchronizer is any object that coordinates and synchronizes the control flow of threads based on its state. The simplest form synchronizer we have already used, being locks.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Synchronizers", "Header 2": "Semaphore", "path": "../pages/digitalGarden/cs/concurrentParallel/synchronizers.mdx"}, "page_content": "A semaphore is an integer variable that represents a resource counter which can also be interpreted as a number of permits to access the resource. The main usage for semaphores is to restrict the number of threads than can access some physical or logical resource.  \n```java\npublic class Semaphore {\npublic Semaphore(int permits) {...}\n// acquires a permit, blocking until one is available, or the thread is interrupted.\npublic void acquire() throws InterruptedException {...}\n// acquires a permit, blocking until one is available.\npublic void acquireUninterruptibly() {...}\npublic void release() {...}\n}\n```  \n<Image\nsrc=\"/cs/concurrentSemaphore.png\"\nwidth={600}\n/>  \nIt is for example perfect to implement the CarPark Class as previously seen:  \n```java\nclass SemaphoreCarPark implements CarPark {\nprivate final Semaphore s;\npublic SemaphoreCarPark(int places) {\ns = new Semaphore(places);\n}\npublic void enter() {\ns.acquireUninterruptibly();\nlog(\"enter carpark\");\n}\npublic void exit() {\nlog(\"exit carpark\");\ns.release();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Synchronizers", "Header 2": "Semaphore", "Header 3": "Lock Using a Semaphore", "path": "../pages/digitalGarden/cs/concurrentParallel/synchronizers.mdx"}, "page_content": "A binary semaphore (only holding 1 permit) can be used as a lock. The only problem with this lock is that it isn't reentrant and a different thread can release the lock that was originally acquired by a different thread.  \n```java\nclass SemaphoreLock {\nprivate final Semaphore mutex = new Semaphore(1);\npublic void lock() { mutex.acquireUninterruptibly();}\npublic void unlock() { mutex.release(); }\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Synchronizers", "Header 2": "Read-Write Lock", "path": "../pages/digitalGarden/cs/concurrentParallel/synchronizers.mdx"}, "page_content": "The motivation for a ReadWriteLock is that if we use the same lock for reading and writing then only one thread can read at a time even tho there wouldn't be any problems if multiple threads could read at a time. To solve this a ReadWriteLock maintains a pair of locks, a lock for reading which can be held simultaneously by multiple readers and a write lock that can only be held by one thread. This leads to there being 2 possible states. Either one thread is writing or one or multiple threads are reading.  \n```java\npublic interface ReadWriteLock {\nLock readLock(); // allows for concurrent reads\nLock writeLock(); // writes are exclusive\n}\n```  \n<Image\nsrc=\"/cs/concurrentReadWriteLock.png\"\nwidth={600}\n/>  \n```java\nclass KeyValueStore {\nprivate final Map<String, Object> m = new TreeMap<>();\nprivate final ReadWriteLock rwl = new ReentrantReadWriteLock();\nprivate final Lock r = rwl.readLock();\nprivate final Lock w = rwl.writeLock();\npublic Object get(String key) {\nr.lock(); try { return m.get(key); } finally { r.unlock(); }\n}\npublic Set<String> allKeys() {\nr.lock(); try { return new HashSet<>(m.keySet()); } finally { r.unlock(); }\n}\npublic void put(String key, Object value) {\nw.lock(); try { m.put(key, value); } finally { w.unlock(); }\n}\npublic void clear() {\nw.lock(); try { m.clear(); } finally { w.unlock(); }\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Synchronizers", "Header 2": "Countdown Latch", "path": "../pages/digitalGarden/cs/concurrentParallel/synchronizers.mdx"}, "page_content": "A CountDownLatch delays the progress of threads until the Latch reaches its terminal state. The main usage for a CoundDownLatch is to ensure that an activity does not proceed until another one-time action has been completed.  \n```java\npublic class CountDownLatch {\npublic CountDownLatch(int count) {...}\n// Causes the current thread to wait until the latch has counted down to zero\npublic void await() {...}\n// Decrements the count, releasing all waiting threads if the count reaches zero.\npublic void countDown() {...}\npublic long getCount() {...}\n}\n```  \nHere there are two common scenarios. Either a thread wants to wait until some other actions are done, or a thread is used a sort of starting gun for other threads.  \n<Image\nsrc=\"/cs/concurrentCountDownLatch1.png\"\nwidth={600}\n/>  \n<Image\nsrc=\"/cs/concurrentCountDownLatch2.png\"\nwidth={600}\n/>  \n```java\nclass KeyValueStore {\nprivate final Map<String, Object> m = new TreeMap<>();\nprivate final ReadWriteLock rwl = new ReentrantReadWriteLock();\nprivate final Lock r = rwl.readLock();\nprivate final Lock w = rwl.writeLock();\npublic Object get(String key) {\nr.lock(); try { return m.get(key); } finally { r.unlock(); }\n}\npublic Set<String> allKeys() {\nr.lock(); try { return new HashSet<>(m.keySet()); } finally { r.unlock(); }\n}\npublic void put(String key, Object value) {\nw.lock(); try { m.put(key, value); } finally { w.unlock(); }\n}\npublic void clear() {\nw.lock(); try { m.clear(); } finally { w.unlock(); }\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Synchronizers", "Header 2": "Cyclic Barrier", "path": "../pages/digitalGarden/cs/concurrentParallel/synchronizers.mdx"}, "page_content": "A CyclicBarrier allows a set of threads to all wait for each other to reach a common barrier point.  \n```java\npublic class CyclicBarrier {\npublic CyclicBarrier(int nThreads) {...}\npublic CyclicBarrier(int nThreads, Runnable barrierAction)\npublic void await() {...}\n}\n```  \n<Image\nsrc=\"/cs/concurrentCyclicBarrier.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Synchronizers", "Header 2": "Exchanger", "path": "../pages/digitalGarden/cs/concurrentParallel/synchronizers.mdx"}, "page_content": "An Exchanger allows two threads to wait for each other and exchange an object. This can be especially useful when the object is very big as it can be reused.  \n```java\npublic class Exchanger<T> {\npublic T exchange(T t) {...}\n}\n```  \n<Image\nsrc=\"/cs/concurrentExchanger.png\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Synchronizers", "Header 2": "Blocking Queue", "path": "../pages/digitalGarden/cs/concurrentParallel/synchronizers.mdx"}, "page_content": "A BlockingQueue is a queue that supports operations to wait for the queue to become non-empty when retrieving an element, and wait for space to become available when storing an element. This is especially commonly used in the Product-Consumer pattern.  \n<Image\nsrc=\"/cs/concurrentBlockingQueueProductConsumer.png\"\nwidth={500}\n/>  \n```java\npublic interface BlockingQueue<E> extends Queue<E> {\nE take() throws InterruptedException;\nvoid put(E e) throws InterruptedException;\n...\n}\n```  \n<Image\nsrc=\"/cs/concurrentBlockingQueue.png\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Interleavings", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "Interleaving is a possible way in which a series of statements could be executed. This concept is important because in concurrent programming the interleaving of a program could influence the result. Choosing the interleaving is however not up to us but the scheduler.  \n<Image\nsrc=\"/cs/concurrentInterleaving.png\"\n/>  \nThe picture above shows some possible interleaving of a program split up between two threads.  \n```java\nclass Counter {\nprivate int i = 0;\npublic void inc() { i++; }\npublic int getCount() { return i; }\n}\nclass R implements Runnable {\nprivate Counter c;\npublic R(Counter c) { this.c = c; }\npublic void run() {\nfor (int i = 0; i < 100000; i++) {\nc.inc();\n}\n}\n}\npublic class CounterTest {\npublic static void main(String[] args) {\nCounter c = new Counter();\nRunnable r = new R(c);\nThread t1 = new Thread(r); Thread t2 = new Thread(r);\nThread t3 = new Thread(r); Thread t4 = new Thread(r);\nt1.start(); t2.start(); t3.start(); t4.start();\ntry {\nt1.join(); t2.join(); t3.join(); t4.join();\n} catch (InterruptedException e) {}\nSystem.out.println(c.getCount());\n}\n}\n```  \nIf we execute the above code we could expect the result to be 400000 because there are 4 threads and each thread increases the counter 100000 times and we only output the result once all threads have terminated. However, when executing this program this is not the case we might see something like 108600 and if we execute it another time 118127. These results happen because the scheduler is allowed to switch context between every CPU operation. So we can see that read and write operations are not guaranteed to be atomic meaning it is done as one instruction by the CPU. Even writing to a value of the type double might be done in 2 parts, it might assign the first 32 bits and then the next 32 bits. In the example, the scenario below happend a few times which causes modifications to get lost.  \n<Image\nsrc=\"/cs/concurrentCounterExampleProblem.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Interleavings", "Header 3": "Interleaving Model", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "The interleaving model is used to calculate the number of possible interleavings (size of the set of possible interleavings) depending on the number of threads $n$ and the number of atomic instructions $m$.  \n$$\ninterleavings = \\frac{(n \\cdot m)!}{(m!)^n}\n$$  \nFor example, if there are 2 threads and a program with 3 atomic instructions then there are 20 possible ways the program could be executed across the 2 threads. Just by increasing the number of threads to 4 the number of possible interleavings skyrockets to 369'600.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Race Conditions", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "A race condition can happen when a result depends on the interleaving of the program across two or more threads. Critically race conditions can also happen when two or more threads are accessing shared data and at least one of them is modifying the data. This leads to unpredictable results as thread scheduling is nondeterministic.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Synchronization", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "Synchronization is a technique of managing access to shared mutable data to prevent race conditions.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Locks", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "A lock or mutex (from mutual exclusion) is a mechanism to enforce mutual exclusion i.e limits access to a resource when multiple threads want to access the resource. Mutual exclusion prevents simultaneous access by only allowing one thread at a time to access a shared resource and therefore guarding critical sections against concurrent execution. By locking a certain section you are also forcing atomicity as no other thread can enter that section of code whilst another thread holds it. This can be a double-edged as it makes the program thread-safe but also means that we are not making use of concurrency.  \n<Image\nsrc=\"/cs/concurrentCounterExampleFix.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Locks", "Header 3": "Built-in Locking in Java", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "Java has a built-in locking mechanism, the `synchronized` keyword. Locking consists of two parts: The object that will serve as a lock and a block of code, the critical section, that is guarded by the lock. When a thread reaches the synchronized block and the lock is not in use the thread can acquire the lock to the block. However, if the lock is not available because it has already been taken then the thread enters the waiting list. When a thread exits a synchronized section the lock is released and there is a race to which thread gets to acquire the lock next. Often the lock is just on the current instance (`this`) or class in a static context. This is what Java does by default if you do not specify a certain lock object. Something to be careful of is using String literals as a lock as it can [cause some big issues](https://stackoverflow.com/a/463437) because according to [Section 3.10.5 of the Java Language Specification](https://docs.oracle.com/javase/specs/jls/se18/html/jls-3.html#jls-3.10.5): Literal strings within different classes in different packages likewise represent references to the same String object.  \n<Callout type=\"info\">\nSynchronizing is not free it comes with additional code (monitorenter and monitorexit are added in the byte code) and also means that the compiler can make fewer optimizations.\n</Callout>  \nThe above example could be fixed by doing one of the following:  \n```java\nclass Counter {\nprivate int i = 0;\nprivate final Object lock = new Object();\npublic synchronized void inc() { i++; }\n// OR public void inc() { synchronized(this){ i++; } }\n// OR public void inc() { synchronized(lock){ i++; } }\npublic int getCount() { return i; }\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Locks", "Header 3": "Deadlock", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "A Deadlock is a situation where at least one thread is blocked because it is holding a resource and is waiting for another resource which is already being held by another thread that wants the other resource being held. So in other words the necessary conditions for a deadlock to happen are:  \n- Mutual Exclusion\n- Hold and Wait, threads are requesting additional resources whilst also holding other resources.\n- No Preemption, resources are released exclusively by threads.\n- Circular Wait, two or more threads form a circular chain where each thread waits for a\nresource that the next thread in the chain holds.  \n<Image\nsrc=\"/cs/concurrentDeadlock.png\"\nwidth={500}\n/>  \n#### Global Ordering  \nOne way of avoiding deadlocks is to order the way the locks are obtained so instead of having the following situation:  \n<Image\nsrc=\"/cs/concurrentGlobalOrder.png\"\n/>  \nWe can acquire the locks in lexicographical order.  \n<Image\nsrc=\"/cs/concurrentGlobalOrderFix.png\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Locks", "Header 3": "Reentrancy", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "Synchronized is also reentrant. Meaning that the same lock can be acquired multiple times by the same thread. Java does this by keeping a counter for each lock with the initial value being 0. When a thread then acquires initially acquires the lock it sets the lock-id to the current thread and increments the counter. For each further acquisition of that lock, the counter is just further incremented. Each lock release then decrements the counter and once the counter reaches 0 again the lock is completely released and made available again to the other threads. The following examples do not cause a deadlock.  \n```java\nsynchronized f() { g(); }\nsynchronized g() {\n/* no deadlock */\nsynchronized(x) {\nsynchronized(x) { /* still no deadlock */ }\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Locking", "Header 2": "Locks", "Header 3": "java.util Locks", "path": "../pages/digitalGarden/cs/concurrentParallel/locking.mdx"}, "page_content": "Additionally to the `synchronized` keyword Java also offers some lock implementations that are more flexible. It is important to use these locks with a `try` block so that the lock can be released in the `finally` block in case any exceptions occur.  \n```java\ninterface Lock{\nvoid lock() // Acquires the lock.\nvoid lockInterruptibly() // Acquires the lock unless the current thread is interrupted.\nCondition newCondition() // Returns a new Condition instance that is bound to this Lock instance.\nboolean tryLock() // Acquires the lock only if it is free at the time of invocation.\nboolean tryLock(long time, TimeUnit unit) // Acquires the lock if it is free within the given waiting time and the current thread has not been interrupted.\nvoid unlock() // Releases the lock.\n}\n```  \nUsage Pattern:  \n```java\npublic synchronized void inc() {\nLock lock = ...;\n...\nlock.lock();\ntry {\n// access resources protected by this lock\n}\nfinally {\nlock.unlock(); // by the same thread!\n}\n}\n```  \n#### Reentrant Lock  \nThe class `ReeentrantLock` implements the `Lock` interface. It offers the same functionality as when using the synchronized mechanism with some extra functions:  \n- `int getHoldCount()` queries the number of holds on this lock by the current thread.\n- `Thread getOwner()` returns the thread that currently owns the lock, or null if not owned.\n- `Collection<Thread> getQueuedThreads()` returns a collection containing threads that are waiting to acquire this lock.\n- `int getQueueLength()` returns an estimate of the number of threads waiting to acquire this lock.  \nA fairness parameter can also be passed with the constructor to define whether the lock is fair or not. Fair locks let threads acquire the lock in the order it was requested i.e. the longest waiting thread always gets the lock (FIFO). An unfair lock is how synchronized works it lets the threads race to acquire the lock.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "STM - Software Transactional Memory", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaSTM.mdx"}, "page_content": "Up till now, we have always been writing about how we want something to work atomically and with a lot of bloat code and knowledge of what goes on in the background. For example, just a simple thread-safe transfer method can quickly become very complicated:  \n```java\npublic void transfer(Account from, Account to, double amount) throws InactiveException, OverdrawException {\nAccount x, y;\n// lexicographically order locks\nif (from.getNumber().compareTo(to.getNumber()) < 0) {\nx = from; y = to;\n} else {\nx = to; y = from;\n}\nsynchronized (x) {\nsynchronized (y) {\nfrom.withdraw(amount);\ntry {\nto.deposit(amount);\n} catch (InactiveException e) {\nfrom.deposit(amount); // if failed load money back\nthrow e;\n}\n}\n}\n}\n```  \nInstead, we would much rather just be able to say something like the following:  \n```scala\ndef transfer(from: Account, to: Account, amount: Double): Unit = {\natomic { implicit tx =>\nfrom.withdraw(amount)\nto.deposit(amount)\n}\n}\n```  \nAnd we can do something very similar to this with the software transactional memory (STM) system in scala. The STM is a coordination mechanism for shared memory and can therefore coordinate access to heap locations in a concurrent environment.  \nThe STM is heavily inspired by transactions for databases where you have the ACID principle (atomic, consistent, isolated, durable). In other words, a transaction is a sequence of reading and writing operations to shared memory that occur logically (consistent) at a single instant in time (atomic) and where the intermediate state is not visible to other transactions (isolated).  \nJust like for databases at the end of a transaction the state is checked for any conflicts. If there is a conflict the transaction is aborted and retried, if there isn't then the changes are made permanent and visible to the other transactions.  \nSo it is very similar to working with CAS:  \n```java\n@volatile\nprivate bal: Int = 0;", "type": "Document"}
{"id": null, "metadata": {"Header 1": "STM - Software Transactional Memory", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaSTM.mdx"}, "page_content": "def deposit(amount: Int): Unit = {\nwhile (true) {\nval oldBal = bal; // read current value\nval newBal = oldBal + amount; // compute new value\nif (compareAndSet(addr(bal), oldBal, newBal)) {\nreturn; // commit successful -> return\n}\n// conflict -> retry\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "STM - Software Transactional Memory", "Header 2": "ScalaSTM", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaSTM.mdx"}, "page_content": "To use ScalaSTM you need to add the dependency to your project and then import it with `import scala.concurrent.stm._`", "type": "Document"}
{"id": null, "metadata": {"Header 1": "STM - Software Transactional Memory", "Header 2": "ScalaSTM", "Header 3": "Ref and Atomic", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaSTM.mdx"}, "page_content": "ScalaSTM offers the `Ref` class which can be used as a wrapper (mutable cell) for a reference. The access to this reference is coordinated by the STM system. The reference held by the wrapper should be immutable otherwise the reference can be changed via the reference and the STM system can not coordinate these changes.  \n```scala\nval ref: Ref[Int] = Ref(1)\nval refView: Ref.View[Int] = ref.single\n```  \nSingle-operation memory transactions may be performed without an explicit atomic block using the `Ref.View` returned from `ref.single`. Otherwise, Ref is only allowed to be changed inside the static scope of an atomic block.  Reads and writes of a Ref are performed by using `x.get` and `x.set(newValue)`, or more concisely by `x()` and `x() = newValue`.  \n```scala\nobject CheatSheet extends App {\n\nval x = Ref(10) // allocate a Ref[Int]\nval y = Ref.make[String]() // type-specific default, holds no reference\nval z = x.single // Ref.View[Int]\n\n// can perform single operations on Ref.View objects\nz.set(11) // will act as if in atomic block\nprintln(z())\nval success = z.compareAndSet(11, 12)\nval old = z.swap(13) // old: Int\nprintln(old)\n\n// println(x()) can only be done in atomic block\n\natomic { implicit txn =>\nval i = x() // read\ny() = \"x was \" + i // write\nz() = 10\nval eq = atomic { implicit txn => // nested atomic\nx() == z() // both Ref and Ref.View can be used inside atomic\n}\nassert(eq)\ny.set(y.get + \", long-form access\")\n}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "STM - Software Transactional Memory", "Header 2": "ScalaSTM", "Header 3": "Ref and Atomic", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaSTM.mdx"}, "page_content": "atomic { implicit txn =>\nval i = x() // read\ny() = \"x was \" + i // write\nz() = 10\nval eq = atomic { implicit txn => // nested atomic\nx() == z() // both Ref and Ref.View can be used inside atomic\n}\nassert(eq)\ny.set(y.get + \", long-form access\")\n}\n\n// atomic transformation\nz.transform {\n_ max 20\n}\nval pre = y.single.getAndTransform {\n_.toUpperCase\n}\nval post = y.single.transformAndGet {\n_.filterNot {\n_ == ' '\n}\n}\n}\n```  \nThe atomic function is defined as `def atomic[Z](block: InTxn => Z): Z` and takes a parameter of type InTxn which provides a context for the transaction to be executed. The context has to be marked implicit as it is automatically pulled in. Luckily the atomic function is composable so we the code arrives at an atomic block it checks if it can join an existing tx or it creates a new so we can then do something like this:  \n```scala\nclass STMAccount(val id: Int) {\nprivate val balance = Ref(0d)\ndef withdraw(a: Double) {\natomic { implicit txn =>\nbalance() = balance() – a\n}\n}\ndef deposit(a: Double) {\natomic { implicit txn =>\nbalance() = balance() + a\n}\n}\n}\nclass STMBank {\ndef transfer(amount: Double, from: STMAccount, to: STMAccount) {\natomic { implicit txn =>\nto.deposit(amount)\nfrom.withdraw(amount)\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "STM - Software Transactional Memory", "Header 2": "ScalaSTM", "Header 3": "Exceptions in Atomic", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaSTM.mdx"}, "page_content": "If an exception occurs inside an atomic block it can be caught and handled inside the atomic block. But if it is not caught then the transaction is rolled back and the exception is thrown higher up.  \n```scala\nval last = Ref(\"none\")\natomic { implicit txn =>\nlast() = \"outer\"\ntry {\natomic { implicit txn =>\nlast() = \"inner\"\nthrow new RuntimeException\n}\n} catch {\ncase _: RuntimeException =>\n}\n}\n\nprintln(last.single.get) // outer because inner was rolled back\n```  \nYou do have to be aware of some things tho for example the following will only output the value 0. This is because transactions are compositional meaning the inner transactions only commit once the outer transaction has committed:  \n```scala\nObject Main extends App {\nval balance: Ref[Int] = Ref(0)\ndef pay(amount: Int) : Unit = atomic { implicit tx =>\nTxn.afterCommit(_ => println(\"Transfer:\" + amount))\nbalance += amount\nif(balance() < 0){\nthrow new RuntimeException\n}\n}\n\nval t1 = new Thread(() => { atomic { implicit tx =>\npay(2)\npay(-4)\n}})\nt1.start()\nprintln(balance.single.get)\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "STM - Software Transactional Memory", "Header 2": "ScalaSTM", "Header 3": "Lifecycle Callbacks", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaSTM.mdx"}, "page_content": "The STM also offers some callback functions for certain lifecycle states:  \n- `Txn.afterCommit(handler: Status => Unit)`\n- `Txn.afterRollback(handler: Status => Unit)`\n- `Txn.beforeCommit(handler: (InTxn) ⇒ Unit)(implicit txn: InTxn): Unit`\n- `Txn.rollback(cause: RollbackCause)(implicit txn: InTxnEnd): Nothing`\n- `Txn.retry(implicit txn: InTxn): Nothing`  \nWith the Status either being `completed` when the transaction has been rolled back or committed or `decided`.  \n```scala\ndef transfer(from: STMAccount, to: STMAccount, amount: Double) {\natomic { implicit txn =>\nto.deposit(amount)\nfrom.withdraw(amount)\nTxn.afterCommit { _ => sendMail(to.email, \"You've got $\" + amount) }\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "STM - Software Transactional Memory", "Header 2": "ScalaSTM", "Header 3": "Behind the Scenes", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaSTM.mdx"}, "page_content": "Behind the scenes, the STM system keeps a global version counter for the last successfully committed transaction. Additionally, each Ref is marked with a so-called local version stamp which is the version of the last successfully committed transaction which modified the reference. When a new transaction is started the following is done:  \n1. Transaction start: The new transaction stores the value of the global version counter locally, this is the so-called read version.\n2. Transaction body: Before a Ref is modified and read from, a local working copy is made of it and only this local copy is read from and written to. For every access of the Ref the local version stamp of the Ref is compared to the read version and if it is larger than the read version the transaction is aborted and retried.\n3. Transaction commit: All original Refs that were modified are locked (with a timeout to avoid deadlocks). Then the global version counter is incremented and copied locally for the transaction, this is the so-called write version. All Refs are then checked again and the transaction is aborted and retried if the version stamp > read version and the object is locked. Then finally the values and the write version are written to the original Refs and the locks are released.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to Concurrent Programming", "Header 2": "Moore's Law", "path": "../pages/digitalGarden/cs/concurrentParallel/introduction.mdx"}, "page_content": "Moore's law isn't really a law but rather an observation by Gordon Moore in 1965 that the number of transistors on a\nmicrochip/CPU doubles about every year which would lead to exponential growth.  \n<Image\nsrc=\"/cs/concurrentMooresLaw.png\"\nwidth={600}\n/>  \nWe can also observe this but there is a reason why people say that Moore's law is dead. Apart from the transistor\namount, everything else has slowly leveled out, meaning we can't get much more out of our CPUs, instead we can have\nmulticore CPUs which will have no impact on most current applications as they do not make use of concurrent programming.\nBut if they would, they could gain a massive speedup.  \n<Image\nsrc=\"/cs/concurrentProgrammingMeme.png\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to Concurrent Programming", "Header 2": "Amdahl's Law", "path": "../pages/digitalGarden/cs/concurrentParallel/introduction.mdx"}, "page_content": "Amdahl's law is a formula to predict the maximum speedup using a multicore CPU with $N$ processors/cores based on the\nproportion of parallelizable components of a program $p$ and the serial components $1-p$:  \n$$\nspeedup \\leq \\frac{1}{(1-p)+ \\frac{p}{N}}\n$$  \n<Image\nsrc=\"/cs/concurrentAmdahlsLawExample.png\"\nwidth={600}\n/>  \nWhen looking at the potential speedup depending on the proportion of parallelizable components and processors we can see\nafter a certain point around the 64 mark the gain becomes very little.  \n<Image\nsrc=\"/cs/concurrentAmdahlsLawWearoff.png\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to Concurrent Programming", "Header 2": "Concurrent Programming", "path": "../pages/digitalGarden/cs/concurrentParallel/introduction.mdx"}, "page_content": "When talking about programs there are three main subsets: serial programs, concurrent programs and parallel programs.  \n<Image\nsrc=\"/cs/concurrentProgramSubsets.png\"\nwidth={600}\n/>  \nConcurrent programs have multiple logical threads whereas serial programs just have one. To solve a problem concurrently\nneed to handle events that could happen at the same time. Because of this concurrent programs are often\nnon-deterministic, meaning results depend on the timing of events. Parallel programs compute components simultaneously\nso in parallel. To solve a problem with parallelism you need to break the problem down into pieces that can be done in\nparallel. Concurrent programs aren't necessarily parallel but being concurrent is a precondition for a parallel program.  \n<Image\nsrc=\"/cs/concurrentVsParallel.png\"\nwidth={300}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "Disadvantages of Locks", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "Locks are very useful and do their job well however they do have some disadvantages. Because of the context switching between threads, there can be an overhead and performance can suffer. However, probably the biggest disadvantage is contention. When a thread is waiting for a lock it cannot do anything else. If a thread that holds a lock is delayed or even ends up in a deadlock then no other thread that needs the lock can progress. This can then lead to **priority inversion** which is when a high priority thread is waiting for a lock held by a low priority thread and therefore its priority is effectively downgraded.  \nThe example below works perfectly fine but we want to remove the locks because of the previously mentioned issues. The lock for reading the value can be removed by making the value volatile so that there is a visibility guarantee. However, volatile variables do not support read-modify-write sequences which is what we are doing when incrementing the value. So we are still stuck with a lock for incrementing and we still don't have optimal performance due to the overhead of volatile variables.  \n```java\npublic final class Counter1 {\nprivate int value = 0;\npublic synchronized int getValue() { return value; }\npublic synchronized int increment() { return ++value; }\n}\npublic final class Counter2 {\nprivate volatile int value = 0;\npublic int getValue() { return value; }\npublic synchronized int increment() { return ++value; }\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "CAS - Compare and Swap", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "CPUs have an atomic instruction called compare and swap/set, `CAS(memory_location, expected_old_value, new_value)`. This operation atomically compares the content of a memory location to a given value and if they are the same modifies the content of that memory location to a given new value and returns a boolean corresponding to if the swap was done, i.e the value at the memory location was still the same as the given old value. With this operation, we can remove all of the locks in the Counter class:  \n```java\npublic final class CASCounter {\nprivate volatile int value = 0;\n\npublic int getValue() {\nreturn value;\n}\npublic int increment() {\nwhile(true) {\nint current = getValue();\nint next = current + 1;\nif (compareAndSwap(current, next)) return next;\n}\n}\n\n// Wrapper for old sun microsystems implementation\nprivate static final Unsafe unsafe = Unsafe.getUnsafe();\nprivate static final int valueOffset;\nstatic {\ntry {\nvalueOffset = unsafe.objectFieldOffset(CASCounter.class.getDeclaredField(\"value\"));\n} catch (Exception ex) { throw new Error(ex); }\n}\nprivate boolean compareAndSwap(int expectedVal, int newVal) {\nreturn unsafe.compareAndSwap(this, valueOffset, expectedVal, newVal);\n}\n}\n```  \nThis pattern is also commonly referred to as optimistic locking. It is optimistic because the code gets the old value, modifies it and optimistically hopes that in the meantime the value hasn't changed and then tries to swap the old and new value if the old value is still the same. If the value has changed in the meantime by maybe another thread then it just tries again and again until it works.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "Atomics", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "Java added Atomic Scalars which support CAS and atomic arithmetic operations for int/long. For doubles or floats etc you can use `Double.doubleToRawLongBits()` and then convert back with `Double.longBitsToDouble()`.  \n<Image\nsrc=\"/cs/concurrentJavaAtomic.png\"\n/>  \n```java\nclass AtomicInteger extends Number {\nAtomicInteger()\nAtomicInteger(int initialValue)\nboolean compareAndSet(int expect, int update)\nint incrementAndGet() int decrementAndGet()\nint getAndIncrement() int getAndDecrement()\nint addAndGet(int delta) int getAndAdd(int delta)\nint getAndSet(int newValue)\nint intValue() double doubleValue()\nlong longValue() float floatValue()\nint get() void set(int newValue)\n}\n```  \nThe Counter example would then look something like this:  \n```java\npublic final class AtomicCounter {\nprivate final AtomicInteger value = new AtomicInteger(0);\npublic int getValue() {\nreturn value.get();\n}\npublic int increment() {\nwhile (true) {\nint oldValue = value.get();\nint newValue = oldValue + 1;\nif (value.compareAndSet(oldValue, newValue)) return newValue;\n}\n}\n}\n```  \nOr even shorter:  \n```java\npublic final class AtomicCounter {\nprivate final AtomicInteger value = new AtomicInteger(0);\npublic int getValue() {\nreturn value.get();\n}\npublic int increment() {\nreturn value.incrementAndGet();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "Atomics", "Header 3": "Atomic References", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "We might not just want to work with integers, we want to be able to work with any object. For this reason, there is the AtomicReference class. For example, it can get a bit tricky when there are multiple integer values if we wanted to implement a range:  \n```java\npublic class NumberRange {\nprivate final AtomicInteger lower = new AtomicInteger(0);\nprivate final AtomicInteger upper = new AtomicInteger(0);\n\npublic int getLower() { return lower.get(); }\npublic void setLower(int newLower) {\nwhile (true) {\nint l = lower.get(), u = upper.get(); // get current values\nif (newLower > u) throw new IllegalArgumentException(); // check preconditions\nif (lower.compareAndSet(l, newLower)) return;\n}\n}\n// same for getUpper/setUpper\npublic boolean contains(int x) {\nreturn lower.get() <= x && x <= upper.get();\n}\n}\n```  \nSo instead we can work with AtomicReferences:  \n```java\npublic class NumberRange {\nprivate static class Pair {\nfinal int lower, upper; // lower <= upper\nPair(int l, int u) { lower = l; upper = u; }\n}\n\nprivate final AtomicReference<Pair> values = new AtomicReference<>(new Pair(0,0));\n\npublic int getLower(){ return values.get().lower; }\npublic void setLower(int newLower){\nwhile(true) {\nPair oldp = values.get();\nif(newLower > oldp.upper) throw new IllegalArgumentException(); // could also check preconditions in constructor\nPair newp = new Pair(newLower, oldp.upper);\nif(values.compareAndSet(oldp, newp)) return; // uses == comparison, which is why should work with immutable\n}\n}\n}\n```  \n<Callout type=\"warning\">\nBe careful when using integer literals because the JVM does some special things, like caching small integer literals which leads to the following program having unexpected behavior.  \n```java\nstatic AtomicReference<Integer> as;\npublic static void main(String[] args) throws Exception {\nnew Thread(() -> {\nas = new AtomicReference<>(1);\nas.compareAndSet(1,2);\n}).start();", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "Atomics", "Header 3": "Atomic References", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "new Thread(() -> System.out.println(as.get())).start();\n}\n```  \nWe would expect to get a NullPointerException or the value 1 but not the value 2. Because the value 1 gets auto-boxed twice with Integer.valueOf() to different objects the compareAndSet should fail. But it doesn't 2 is also a possible output because the JVM caches small integer values.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "Atomics", "Header 3": "ABA Problem", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "The ABA problem occurs in lock-free programming when a variable that was read has been changed by another thread in the following order:  \n`A -> B -> A`  \nThe CAS operation will compare its A with A and think that \"nothing has changed\" even though the second thread did work which violates that assumption. For example  \n1. Thread T1 reads value A from shared memory.\n2. T1 is put to sleep, allowing thread T2 to run.\n3. T2 modifies the shared memory value A to value B and back to A before going to sleep.\n4. T1 begins execution again, sees that the shared memory value has not changed and continues.  \nFor this reason, Java provides the AtomicStampedReference Class which holds an object reference and a stamp internally. The reference and stamp can be swapped using a single atomic compare-and-swap operation, via the compareAndSet() method.  \n```java\npublic class AtomicStampedReference<V> {\npublic AtomicStampedReference(V ref, int stamp) { ... }\npublic V getReference() { ... } // returns reference\npublic int getStamp() { ... } // returns stamp\npublic V get(int[] stampHolder) { ... } // returns both\npublic void set(V newReference, int newStamp) { ... }\npublic boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) { ... }\npublic boolean attemptStamp(V expectedReference, int newStamp) { ... }\n}\n```  \n```java\nprivate final AtomicStampedReference<Integer> account = new AtomicStampedReference<>(100, 0); // initial value=100 stamp=0\n\npublic int deposit(int funds) {\nint[] stamp = new int[1];\nwhile(true){\nint oldValue = account.get(stamp);\nint newValue = oldValue + funds;\nint newStamp = stamp[0] + 1;\nif(account.compareAndSet(oldValue, newValue, stamp[0], newStamp);)\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "Non-blocking Data structures", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "With the Atomic Scalars in Java, you can then also implement some simple data structures.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "Non-blocking Data structures", "Header 3": "Stack", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "```java\npublic class ConcurrentStack<E> {\nprivate static class Node<E> {\npublic final E item;\npublic Node<E> next;\npublic Node(E item) { this.item = item; }\n}\n\nfinal AtomicReference<Node<E>> head = new AtomicReference<>();\n\npublic void push(E item) {\nNode<E> newHead = new Node<E>(item);\nwhile(true) {\nNode<E> oldHead = head.get();\nnewHead.next = oldHead;\nif (head.compareAndSet(oldHead, newHead)) return;\n}\n}\npublic E pop() {\nwhile(true) {\nNode<E> oldHead = head.get();\nif (oldHead == null) throw new EmptyStackException();\nNode<E> newHead = oldHead.next;\nif(head.compareAndSet(oldHead, newHead)) {\nreturn oldHead.item;\n}\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Lock-Free Programming", "Header 2": "Non-blocking Data structures", "Header 3": "Queue", "path": "../pages/digitalGarden/cs/concurrentParallel/lockFreeProgramming.mdx"}, "page_content": "The tricky part of implementing a non-blocking queue is that two things need to be watched, the head and the tail. In the implementation below a dummy node is used. This then leads to there being 3 states that the tail can be in:  \n- The tail refers to the dummy i.e. to the same node as the head then the queue is empty.\n- The tail refers to the last element.\n- The tail refers to the second last element, which can only happen in the middle of an update.  \n```java\npublic class ConcurrentQueue <E> {\nprivate static class Node<E> {\nfinal E item;\nfinal AtomicReference<Node<E>> next;\npublic Node(E item, Node<E> next) {\nthis.item = item;\nthis.next = new AtomicReference<Node<E>>(next);\n}\n}\n\nprivate final Node<E> dummy = new Node<E>(null, null);\nprivate final AtomicReference<Node<E>> head = new AtomicReference<Node<E>>(dummy);\nprivate final AtomicReference<Node<E>> tail = new AtomicReference<Node<E>>(dummy);\n\npublic boolean put(E item) {\nNode<E> newNode = new Node<E>(item, null);\nwhile (true) {\nNode<E> curTail = tail.get();\nNode<E> tailNext = curTail.next.get();\nif (tailNext != null) {\n// Queue in intermediate state, advance tail\ntail.compareAndSet(curTail, tailNext);\n} else {\n// In consistent state, try inserting new node\nif (curTail.next.compareAndSet(null, newNode)) {\n// Insertion succeeded, try advancing tail\ntail.compareAndSet(curTail, newNode);\nreturn true;\n}\n}\n}\n\npublic E pop() {\nwhile(true) {\nNode<E> oldHead = head.get();\nif (oldHead == null) throw new EmptyQueueException();\nNode<E> newHead = oldHead.next.get();\nif(head.compareAndSet(oldHead, newHead)) {\nreturn oldHead.item;\n}\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Executor Framework", "path": "../pages/digitalGarden/cs/concurrentParallel/executorFramework.mdx"}, "page_content": "The Java executor framework is used to run and manage Runnable objects, so-called Tasks. It does this using so-called workers or worker threads which are most often managed as part of a ThreadPool. Depending on the configuration of the pool instead of creating new threads every time the so-called channel will try and reuse already created threads. Any excess tasks flowing into the channel that the threads in the pool can't handle at the minute are held in some form of data structure like a BlockingQueue. Once one of the threads has finished its task and gets free, it picks up the next task from the channel.  \n<Image\nsrc=\"/cs/concurrentExecutorFramework.png\"\n/>  \nThe Executor interface provides a single function `void execute(Runnable task)` which executes the given task and depending on the implementation will do this using a thread pool or a single thread etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Executor Framework", "Header 2": "Custom Executors", "path": "../pages/digitalGarden/cs/concurrentParallel/executorFramework.mdx"}, "page_content": "This is just a custom executor that uses a thread pool.  \n```java\nclass MyThreadPoolExecutor implements Executor {\nprivate final BlockingQueue<Runnable> queue = new LinkedBlockingQueue<Runnable>();\n\npublic void execute(Runnable r) { queue.offer(r); }\n\npublic MyThreadPoolExecutor(int nrThreads) {\nfor (int i = 0; i < nrThreads; i++) { activate(); }\n}\n\nprivate void activate() {\nnew Thread(() -> {\ntry {\nwhile (true) { queue.take().run(); }\n} catch (InterruptedException e) { /* die */ }\n}).start();\n}\n}\n```  \nYou can also create an executor that just executes the given task on the current thread.  \n```java\nclass DirectExecutor implements Executor {\npublic void execute(Runnable r) { r.run(); }\n}\n```  \nOr you can create an executor that creates a new thread for each task.  \n```java\nclass ThreadPerTaskExecutor implements Executor {\npublic void execute(Runnable r) {\nnew Thread(r).start();\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Executor Framework", "Header 2": "Builtin Executors", "path": "../pages/digitalGarden/cs/concurrentParallel/executorFramework.mdx"}, "page_content": "The executor framework has some built-in executors that you can access using the factory methods in the `Executors` class. All the factories return instances of the `ExecutorService` interface which extends the `Executor` interface and adds some life-cycle management methods.  \n```java\ninterface ExecutorService extends Executor {\nvoid shutdown(); // kind, finish all pending tasks, don't accept new ones\nList<Runnable> shutdownNow(); // all running tasks are interrupted, a list of the tasks that were awaiting execution\nboolean isShutdown();\nboolean isTerminated();\nboolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // blocks until all tasks completed execution after a shutdown request\n}\n```  \n- `Executors.newFixedThreadPool(int nThreads)`: Creates a thread pool that reuses a fixed number of threads operating off a shared unbounded queue. Threads that die due to an exception are replaced.\n- `Executors.newCachedThreadPool()`: Creates a thread pool that creates new threads as needed, but will reuse previously constructed threads when they are available.\n- `Executors.newSingleThreadScheduledExecutor()`: Creates an Executor that uses a single worker thread operating off an unbounded queue. The worker thread is replaced if it dies due to an exception.\n- `Executors.newScheduledThreadPool(int corePoolSize)`: Creates a thread pool that can schedule commands to run after a given delay, or to execute periodically.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Executor Framework", "Header 2": "Callable and Future", "path": "../pages/digitalGarden/cs/concurrentParallel/executorFramework.mdx"}, "page_content": "Because the runnable interface does not allow for exceptions or results we need to use something different if we wish to have this functionality. The executor framework has a few tools for this. We have the `Callable` interface which is our alternative for the `Runnable` interface and then the `Future` interface which is similiar to a promise in JavaScript and represents a future result of a task.  \n```java\ninterface Callable<V> {\nV call() throws Exception;\n}\n```  \n```java\ninterface Future<V> {\nboolean cancel(boolean mayInterruptIfRunning);\nboolean isCancelled();\nboolean isDone();\nV get() throws InterruptedException, ExecutionException, CancellationException;\nV get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, CancellationException, TimeoutException;\n}\n```  \nInstead of then using the execute function from the `Executor` interface, we have a few additional functions in the `ExecutorService` interface along with life-cycle methods.  \n```java\ninterface ExecutorService extends Executor {\n// ...lifecycle methods\n<T> Future<T> submit(Callable<T> task); // the key function\nFuture<?> submit(Runnable task);\n<T> Future<T> submit(Runnable task, T result);\n// takes a list of tasks and returns a list of the matching results\n<T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks) throws InterruptedException;\n// Executes the given tasks, returning the result of one that has completed successfully if any do.\n<T> T invokeAny(Collection<? extends Callable<T>> tasks) throws InterruptedException, ExecutionException;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Executor Framework", "Header 2": "FactorialCalculator example", "path": "../pages/digitalGarden/cs/concurrentParallel/executorFramework.mdx"}, "page_content": "In this example, each task will return the factorial of a given number.  \n```java\npublic class Main {\npublic static void main(String[] args) throws Exception {\nExecutorService executor = Executors.newFixedThreadPool(2);\n\nList<Future<Long>> resultList = new ArrayList<>();\n\nfor (long i = 1; i <= 20; i++) {\nFuture<Long> result = executor.submit(new FactorialCalculator(i));\nresultList.add(result);\n}\n\nexecutor.shutdown();\nexecutor.awaitTermination(10, TimeUnit.SECONDS);\n\nfor (int i = 0; i < resultList.size(); i++) {\nFuture<Long> result = resultList.get(i);\nLong number = null;\nnumber = result.get(); // waits for next result\nSystem.out.println(i+\"::\\t\"+number);\n}\n\nexecutor.shutdown();\n}\n\nprivate static class FactorialCalculator implements Callable<Long> {\nprivate final Long number;\n\npublic FactorialCalculator(Long number) {\nthis.number = number;\n}\n\n@Override\npublic Long call() throws Exception {\nlong result = 1;\nif (number == 0 || number == 1) {\nresult = 1;\n} else {\nfor (int i = 2; i <= number; i++) {\nresult *= i;\n}\n}\nreturn result;\n}\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Executor Framework", "Header 2": "Fork-Join", "path": "../pages/digitalGarden/cs/concurrentParallel/executorFramework.mdx"}, "page_content": "If we try and implement a divide and conquer algorithm like the merge-sort with executors we run into some issues, especially because we create a lot of threads that are waiting and not doing anything.  \n```java\npublic class MergeSortTask implements Runnable {\npublic final int[] elems, temp;\nprivate final int start, end;\n\nprivate final ExecutorService ex;\n\npublic MergeSortTask(int[] elems, ExecutorService ex) {\nthis.elems = elems;\nthis.start = 0;\nthis.end = elems.length;\nthis.temp = new int[end];\nthis.ex = ex;\n}\n\npublic MergeSortTask(int[] elems, int[] temp, int start, int end, ExecutorService es) {\nthis.elems = elems;\nthis.temp = temp;\nthis.start = start;\nthis.end = end;\nthis.ex = es;\n}\n\n@Override\npublic void run() {\nif (end - start <= 1) {\nreturn;\n} else {\nint mid = (start + end) / 2;\n\nMergeSortTask left = new MergeSortTask(elems, temp, start, mid, ex);\nMergeSortTask right = new MergeSortTask(elems, temp, mid, end, ex);\n\nFuture<?> lf = ex.submit(left);\nFuture<?> rf = ex.submit(right);\ntry {\n//print(\"Waiting for subtasks\");\nlf.get();\nrf.get();\n//print(\"Subtasks are ready\");\n} catch (Exception e) {\n}\nmerge(elems, temp, start, mid, end);\n}\n}\n\nprivate static void merge(int[] elem, int[] tmp, int leftPos, int rightPos, int rightEnd) {\nif (elem[rightPos - 1] <= elem[rightPos]) return;\n\nint leftEnd = rightPos;\nint tmpPos = leftPos;\nint numElements = rightEnd - leftPos;\n\nwhile (leftPos < leftEnd && rightPos < rightEnd)\nif (elem[leftPos] <= elem[rightPos])\ntmp[tmpPos++] = elem[leftPos++];\nelse\ntmp[tmpPos++] = elem[rightPos++];\n\nwhile (leftPos < leftEnd)\ntmp[tmpPos++] = elem[leftPos++];\n\nwhile (rightPos < rightEnd)\ntmp[tmpPos++] = elem[rightPos++];\n\nrightEnd--;\nfor (int i = 0; i < numElements; i++, rightEnd--)\nelem[rightEnd] = tmp[rightEnd];\n}\n\nprivate static int[] randomInts(int n) {\nint[] l = new int[n];\nRandom rnd = new Random();\n\nfor (int i = 0; i < l.length; i++) {\nl[i] = rnd.nextInt(1000);\n}\nreturn l;\n}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Executor Framework", "Header 2": "Fork-Join", "path": "../pages/digitalGarden/cs/concurrentParallel/executorFramework.mdx"}, "page_content": "private static int[] randomInts(int n) {\nint[] l = new int[n];\nRandom rnd = new Random();\n\nfor (int i = 0; i < l.length; i++) {\nl[i] = rnd.nextInt(1000);\n}\nreturn l;\n}\n\npublic static void main(String[] args) throws InterruptedException, ExecutionException {\nint SIZE = 4;\nint[] data = randomInts(SIZE);\n\nSystem.out.println(\"Unsorted: \" + Arrays.toString(data));\n\nExecutorService es = Executors.newCachedThreadPool();\n\nMergeSortTask ms = new MergeSortTask(data, es);\nFuture<?> f = es.submit(ms);\nf.get();\n\nes.shutdownNow();\nSystem.out.println(\"Sorted: \" + Arrays.toString(data));\n}\n}\n```  \nInstead it is better to do the work sequential after a certain threshold has been reached, this is the so-called sequential threshold.  \n```java\n...\npublic void run() {\nif(r – l <= 1000) Arrays.sort(is); return;\nelse {\nint mid = (start + end) / 2;\n\nMergeSortTask left = new MergeSortTask(elems, temp, start, mid, ex);\nMergeSortTask right = new MergeSortTask(elems, temp, mid, end, ex);\n...\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Executor Framework", "Header 2": "Fork-Join", "Header 3": "Fork-Join Framework", "path": "../pages/digitalGarden/cs/concurrentParallel/executorFramework.mdx"}, "page_content": "For this reason, there is the fork-join framework which supports the methodology of forking work and then joining it together at the end. The Framework create a limited number of worker threads according to the CPU. Then each worker thread maintains a private double-ended work queue. When forking a worker pushes the new task to the head of its queue. When the worker is waiting or idle it pops a task off the head of its queue and executes it instead of sleeping. If a worker's queue is empty, it steals a task off the tail of another randomly chosen worker.  \n<Image\nsrc=\"/cs/concurrentForkJoinStealing.png\"\nwidth={500}\n/>  \n```java\n// RecursiveAction has no result; RecursiveTask<V> returns Result V\npublic class ForkJoinMergeSort extends RecursiveAction {\npublic final int[] is, tmp;\nprivate final int l, r;\n\npublic ForkJoinMergeSort(int[] is, int[] tmp, int l, int r) {\nthis.is = is; this.tmp = tmp; this.l = l; this.r = r;\n}\n\nprotected void compute() {\nif (r - l<= 100000) Arrays.sort(is, l, r);\nelse {\nint mid = (l + r) / 2;\nForkJoinMergeSort left = new ForkJoinMergeSort(is, tmp, l, mid);\nForkJoinMergeSort right = new ForkJoinMergeSort(is, tmp, mid, r);\nleft.fork();\nright.invoke();\nleft.join();\nmerge(is, tmp, l, mid, r);\n}\n}\nprivate void merge(int[ ] es, int[ ] tmp, int l, int m, int r) { ... }\n\nprivate static int[] randomInts(int n) { ... }\n\npublic static void main(String[] args) throws InterruptedException, ExecutionException {\nint SIZE = 4;\nint[] data = randomInts(SIZE);\nint[] tmp = new int[data.length];\n\nSystem.out.println(\"Unsorted: \" + Arrays.toString(data));\n\nForkJoinPool fjPool = new ForkJoinPool();\nForkJoinMergeSort ms = new ForkJoinMergeSort(data,tmp,0,data.length);\nfjPool.invoke(ms);\nfjPool.shutdown();\nSystem.out.println(\"Sorted: \" + Arrays.toString(data));\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Safe Object Sharing", "path": "../pages/digitalGarden/cs/concurrentParallel/safeObjectSharing.mdx"}, "page_content": "There are 2 alternatives to synchronizing objects to make sure that nothing breaks when sharing objects. Either the shared object is immutable which would lead to there never being any inconsistent states between the threads. The other alternative is you just don't have a shared state variable between threads.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Safe Object Sharing", "Header 2": "Immutable Objects", "path": "../pages/digitalGarden/cs/concurrentParallel/safeObjectSharing.mdx"}, "page_content": "In principle, immutable objects aren't very complicated. You do however have to be aware of how the object is initialized when working concurrently as we don't want half- or even non-initialized objects. For example, in the example below the `account` object could be un- or partial-initialized.  \n```java\n// immutable\nfinal class Account {\nprivate int balance;\npublic Account(int balance) {\nthis.balance = balance;\n}\npublic String toString() { return \"\" + balance; }\n}\nclass Company {\nprivate Account account = null;\npublic Account getAccount() { // lazy initialization\nif(account == null) account = new Account(10000);\nreturn account;\n}\n}\n```  \nTo illustrate how `account` could break we can imagine that we have two threads, `T0` and `T1`. If we then call `T0: company.getAccount().toString();` and `T1: company.getAccount().toString();` we don't have a guaranty that we get 10000, we could also get 0. The reason for this is that there could be an interleaving between the object creation and the assignment of the `balance` field, resulting in a partial-initialized object. To fix this we could make the `account` field volatile. The happens-before relation then guarantees that fields set in the constructor are visible as the invocation of the constructor happens-before the assignment to the volatile field `account`.  \n```java\nclass Company {\nprivate volatile Account account = null; // safe publication\npublic Account getAccount() {\nif(account == null) account = new Account(10000);\nreturn account;\n}\n}\n```  \nUsing volatile for this is very expensive as we have previously seen and means that the CPU can't make performance optimizations by caching values and we only really want the functionality of volatile for the first initialization and not for any further calls of `getAccount()`. For this reason, the JMM guarantees that final fields are only visible after they have been initialized! This means that if a thread sees a reference to an Account instance, it has the", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Safe Object Sharing", "Header 2": "Immutable Objects", "path": "../pages/digitalGarden/cs/concurrentParallel/safeObjectSharing.mdx"}, "page_content": "guarantee to see all the final fields fully initialized. The JMM also guarantees that if a reference of an object is final, all referenced objects are visible after initialization if accessed over the final reference.  \n```java\nclass Account {\nprivate final int balance;\npublic Account(int balance) { this.balance = balance; }\npublic String toString() { return \"\" + balance; }\n}\n```  \nInitialization-Safety is however only guaranteed if an object is accessed after it is fully constructed. For this to be the case you can not allow the `this` reference to escape during construction. Some possible ways the `this` reference could escape:  \n- Publishing an instance of an inner class. This implicitly publishes the enclosing instance as well because the inner class instance contains a hidden reference to the enclosing instance. For example when registering an event listener from the constructor.  \n- Starting a thread within a constructor. When an object creates a thread from its constructor, it almost always shares its `this` reference with the new thread. Either explicitly, by passing it to the constructor or implicitly, because the Thread or Runnable is an inner class of the owning object. The new thread might then be able to see the owning object before it is fully constructed.  \n- Calling an alien method in the constructor. An alien methods behavior is not fully specified by the invoking class because it is either in another class or an overridable method.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Safe Object Sharing", "Header 2": "Thread Locals", "path": "../pages/digitalGarden/cs/concurrentParallel/safeObjectSharing.mdx"}, "page_content": "The [DateFormat Class](https://docs.oracle.com/javase/7/docs/api/java/text/DateFormat.html) in Java is documented not to be thread-safe. Instead, it is recommended we use a fresh instance on every invocation or a separate instance for each thread.  \n```java\npublic class BadFormatter {\nprivate static final SimpleDateFormat sdf = new SimpleDateFormat();\npublic static String format(Date d) {\nreturn sdf.format(d);\n}\n}\npublic class GoodFormatter {\npublic static String format(Date d) {\nSimpleDateFormat sdf = new SimpleDateFormat();\nreturn sdf.format(d);\n}\n}\n```  \nIn the solution above we are creating a fresh instance for each call which can be quite expensive. Instead, we can use the [ThreadLocal class](https://docs.oracle.com/javase/7/docs/api/java/lang/ThreadLocal.html). A thread-local variable provides a separate copy of its value for each thread that uses it. It, therefore, provides a mechanism to pass state down the call stack without having to explicitly define an additional method parameter.  \n```java\nclass ThreadLocal<T> {\npublic T get();\npublic void set(T value);\nprotected T initialValue();\npublic void remove();\n}\n```  \nWe could then solve the above problem like the following  \n```java\nclass ThreadLocalFormatter {\nprivate static ThreadLocal<SimpleDateFormat> local = ThreadLocal.withInitial(() -> new SimpleDateFormat());\npublic static String format(Date d) {\nreturn local.get().format(d);\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Safe Object Sharing", "Header 2": "Thread Locals", "Header 3": "ThreadLocalRandom", "path": "../pages/digitalGarden/cs/concurrentParallel/safeObjectSharing.mdx"}, "page_content": "Although the [Random class](https://docs.oracle.com/javase/8/docs/api/java/util/Random.html) is thread-safe the concurrent use of the same Random instance across threads may encounter [thread contention](https://stackoverflow.com/questions/1970345/what-is-thread-contention) and consequently have poor performance. For this reason the [ThreadLocalRandom class](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ThreadLocalRandom.html) was added.  \n```java\n// Usage\nThreadLocalRandom.current().nextInt() // current returns the current thread's ThreadLocalRandom instance\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Advantages of Scala for Concurrency", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "Scala has many advantages for why it is popular for concurrent programs, however, the main reason is that it is heavily influenced by functional languages and has a large focus on immutability which eases concurrent programming heavily. Because Scala is built up on the JVM it already supports thread-based concurrency and the executor framework. But it also builds up on that with the Software Transactional Memory (STM) system which is inspired by Clojure and Haskell. It also offers the Akka library which supports type-safe Actor-based concurrency just like in Erlang.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Option", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "An object of `Option` represents an optional value, either it is present or it isn't. Instances of Option are either an instance of `Some` or the object `None`.  \n```scala\nval o1: Option[Int] = None // same as Option(null)\nval o2 = Some(10)\nprintln(o1.isDefined)\nprintln(o1.isEmpty)\nprintln(o1.getOrElse(\"Empty\"))\nprintln(o2.get)\n```  \nAn Option is optimal for pattern matching:  \n```scala\nval result = List(1,2,3).find(i => i >= 2) match {\ncase None => \"Not Found!\"\ncase Some(elem) => \"Found: \" + elem\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Collections", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "Scala has mutable and immutable collections each in their corresponding sub-package `scala.collection.immutable` and `scala.collection.mutable`. A mutable collection can be updated or extended in place. This means when you change, add, or remove elements of a mutable collection it is done in place as a side effect. However, immutable collections never change. Operations that change, add or remove elements return a new immutable collection and leave the old collection unchanged. In scala the default is to use the immutable, if you want the mutable version you need to explicitly import it.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Collections", "Header 3": "Arrays", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "In scala, an array is a mutable sequence of objects that share the same type with a fixed length that is given when the array is instantiated.  \n```scala\nval reserved = new Array[String](3) // calls constructor in Array class\nval words = Array(\"zero\", \"one\", \"two\") // calls apply() factory in companion object\n\nfor (i <- 0 to 2) // to is a method using operator notation returning a sequence, 0.to(2)\nprintln(words(i)) // calls apply function / () operator in Array class\n\nwords(0) = \"nothing\" // shorthand for words.update(0, \"nothing\")\n\nwords.foreach(println) // shorthand for one argument with println(_)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Collections", "Header 3": "Lists and Sets", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "In scala, `List` is a concrete class, not an interface as in Java. Meaning we can create immutable or mutable List objects. The advantage of a List is that it can contain an arbitrary amount of elements because it is implemented as a linked list. But just like with arrays a List can only contain objects of the same type.  \n```scala\nval list0 = List(1, 2, 3) // List(1,2,3)\nval head = list0.head // 1, first element\nval tail = list0.tail // List(2,3), the rest\nval init = list0.init // List(1,2), all but last\nval reversed = list0.reverse // List(3,2,1)\nval list1 = 0 :: list0 // List(0,1,2,3), prepend\nval list2 = Nil // List() or List.empty\nval list3 = list0.map(i => i + 1) // List(2,3,4)\nval list4 = list0.filter(i => i > 1) // List(2,3)\nval sum0 = list0.reduce((x, y) => x + y) // 6\nval sum1 = list0.sum // 6\nval count = list0.count(i => i > 1) // 2\nval list5 = list0.zip(List('A', 'B', 'C')) // List((1,A), (2,B), (1,C))\nval list6 = list0.groupBy(i => i % 2 == 0) // Map(false -> List(1,1), true -> List(2))\nval large = list0.find(i => i > 12) // None\nval small = list0.find(i => i < 12) // Some(1)\nval list7 = list0.drop(0) // List(2,3)\nval list8 = list0.dropRight(2) // List(1), without 2 right most elements\nval list9 = list0 ::: List(3,5,6) // List(1,2,3,4,5,6)\nlist0.foreach(i => print(i + \" \")) // 1 2 3\nprintln(list0.mkString(\", \")) // 1, 2, 3\n```  \nA Set is the same as a List but can only contain unique values.  \n```scala\nval set0 = Set(1,2,3,2) // Set(1,2,3)\nval set1 = set0 + 4 // Set(1,2,3,4)\nval set2 = set0 - 1 // Set(2,3)\nval contains = set1(0) // false, same as set1.contains(0)\nval set3 = set1.filter(i => i > 2) // Set(3,4)\nval set4 = set1.map(i => i > 2) // Set(false,true)\nval subset = set2 subsetOf set0 // true\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Collections", "Header 3": "Tuples", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "Tuples are like lists but can contain different types of elements. They are commonly used for returning multiple values from a function.  \n```scala\nval pair = (99, \"Luftballons\") // is inferred to the type Tuple2[Int, String]\nval num = pair(0)\nval what = pair(1)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Collections", "Header 3": "Maps", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "```scala\nval map0 = Map(1 -> \"one\", 2 -> \"two\") // Map(1 -> \"one\", 2 -> \"two\")\nval map1 = map0 + (3 -> \"three\") // Map(1 -> \"one\", 2 -> \"two\", 3 -> \"three\")\nval map2 = map1 - 1 // Map(2 -> \"two\", 3 -> \"three\")\nval val1 = map0(1) // \"one\"\nval val0 = map0(0) // j.u.NoSuchElementException: key not found: 0\nval optVal0 = map0.get(0) // None\nval optVal1 = map0.get(1) // Some(1)\nval res = map1.filter(kv => kv._1 > 2) // Map(3 -> \"three\")\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Collections", "Header 3": "Parallel Collections", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "Parallel collections were included in the Scala standard library to enable parallel programming without users needing to know low-level details by providing a simple high-level abstraction. Some prime operations for parallelization are:  \n```scala\nval list = (1 to 10000).toList\nval res = list.map(i => i * 3)\nval even = list.filter(i => i % 2 == 0)\nval sum = list.reduce((i,j) => i + j)\n\nval par_res = list.par.map(i => i*3).seq.toList\nval par_even = list.par.filter(i => i % 2 == 0).seq.toList\nval par_sum = list.par.reduce((i,j) => i + j)\n```  \nThere are some things you do need to be aware of when using parallel collections. For example, if the collection isn't very big then the setup for parallelizing the functions might be larger than the performance gain. The other thing to be aware of is non-deterministic functions such as non-associative operations.  \n```scala\n(1 to 5).foreach(print) // 12345\n(1 to 5).par.foreach(print) // depending on execution 34512\n(1 to 1000).par.reduce(_ - _) // depending on execution -330101\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Futures", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "The problem with Javas Futures is that the get call is blocking.  \n```scala\npublic Future[String] loadHomePage() { ... }\npublic Map<String,Integer> indexContent(String content) { ... }\npublic void work() throws Exception {\n// Block current Thread until result is available\nString content = loadHomePage().get();\nMap<String, Integer> index = indexContent(content);\nSystem.out.println(index);\n}\n```  \nInstead, we would much rather make use of the observable pattern instead of having to wait for results. In Scala, by default, futures are non-blocking as they make use of callback functions instead of typical blocking operations. However, blocking is still possible but is heavily discouraged. Futures in Scala are defined as followed:  \n```scala\nobject Future {\ndef apply[T](task: => T)(implicit ec: ExecutionContext): Future[T]\n}\n// Usage:\nval inverseFuture : Future[Matrix] = Future {\nfatMatrix.inverse() // non-blocking long lasting computation\n}(executionContext)\n\n// or in short\nimport scala.concurrent.ExecutionContext.Implicits.global\nval inverseFuture : Future[Matrix] = Future {\nfatMatrix.inverse()\n}\n```  \nLet’s assume we want to fetch a list of recent posts and display them. We can register a callback by using the `onComplete[U](f: Try[A] => U]): Unit` method, where Try is very similar to `Option` and can have the value of type `Success[T]` if the future completes successfully, or a value of type `Failure[T]` otherwise.  \n```scala\nval f: Future[List[String]] = Future {\nsession.getRecentPosts()\n}\n\nf.onComplete {\ncase Success(posts) => for (post <- posts) println(post)\ncase Failure(t) => println(\"An error has occurred: \" + t.getMessage)\n}\n```  \nRegistering a `foreach` callback has the same semantics as onComplete, with the difference that the closure is only called if the future is completed successfully.  \n```scala\nval f: Future[List[String]] = Future {\nsession.getRecentPosts\n}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Scala Futures", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "f.foreach { posts =>\nfor (post <- posts) println(post)\n}\n```  \nGiven a future and a mapping function for the value of the future you can produce a new future that is completed with the mapped value once the original future is successfully completed with the `map` combinator.  \n```scala\nval rateQuote = Future {\nconnection.getCurrentValue(USD)\n}\n\nval purchase = rateQuote map { quote =>\nif (isProfitable(quote)) connection.buy(amount, quote)\nelse throw new Exception(\"not profitable\")\n}\n\npurchase foreach { amount =>\nprintln(\"Purchased \" + amount + \" USD\")\n}\n```  \nBut what happens if isProfitable returns false, hence causing an exception to be thrown? In that case purchase fails with that exception. Furthermore, imagine that the connection was broken and that getCurrentValue threw an exception, failing rateQuote. In that case we’d have no value to map, so the purchase would automatically be failed with the same exception as rateQuote.  \nIn conclusion, if the original future is completed successfully then the returned future is completed with a mapped value from the original future. If the mapping function throws an exception the future is completed with that exception. If the original future fails with an exception then the returned future also contains the same exception.  \nThe `flatmap` does basically the same thing but also wraps it into a future:  \n```scala\nval usdQuote = Future { connection.getCurrentValue(USD) }\nval chfQuote = Future { connection.getCurrentValue(CHF) }\n\nval purchase = usdQuote flatMap {\nusd =>\nchfQuote\n.withFilter(chf => isProfitable(usd, chf))\n.map(chf => connection.buy(amount, chf))\n}\n```  \nYou can as mentioned however also use blocking calls on futures:  \n```scala\nval result = Await.result(homepage, 1 second)\nval result = Await.result(homepage, Duration.Inf)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Concurrent Programming in Scala", "Header 2": "Reactive Programming with RxScala", "path": "../pages/digitalGarden/cs/concurrentParallel/scalaConcurrency.mdx"}, "page_content": "ReactiveX (Rx) is a library for composing asynchronous and event-based programs by using observable sequences. It extends the observer pattern to support sequences of data and/or events. It offers the following structures:  \nAn Observable represents an observable sequence of events much like an Iterable:  \n```scala\ntrait Observable[T] {\ndef subscribe(obs: Observer[T]): Subscription\n}\ntrait Observer[T] {\ndef onNext(t: T): Unit\ndef onCompleted(): Unit\ndef onError(t: Throwable): Unit\n}\ntrait Subscription {\ndef unsubscribe(): Unit\ndef isUnsubscribed(): Boolean\n}\n\n// Observable[String] emitting some HTML strings\ngetDataFromNetwork()\n.drop(7)\n.filter(s => s.startsWith(\"h\"))\n.take(12)\n.map(s => toJson(s))\n.subscribe(j => println(j)) // instead of foreach\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interrupts", "path": "../pages/digitalGarden/cs/concurrentParallel/interrupts.mdx"}, "page_content": "Blocking methods can potentially take forever if the condition they are waiting for never occurs which can lead to big issues. For this reason, we want a mechanism to be able to stop/cancel waiting for a given condition and continue with the program.  \nIn Java, the static function `Thread.stop()` exists but is declared deprecated as it is unsafe. It is unsafe because the Thread that it is called on releases all the locks monitors the thread was holding. Any of the objects previously protected by these released locks which were in an inconsistent state become visible to the other threads and therefore potentially result in broken behavior. So we need to use a different mechanism.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interrupts", "Header 2": "Interrupt Flag", "path": "../pages/digitalGarden/cs/concurrentParallel/interrupts.mdx"}, "page_content": "Internally every thread in Java has a boolean flag corresponding to its interrupted status. When the method `interrupt()` is called on a thread the flag is set to `true`. If the thread is blocked i.e. it is in an invocation of `wait()`, `sleep()` or `join()` the flag is consumed/reset and an `InterruptedException` is thrown. If the thread is not blocked the flag is just set and can be polled and handled by the developer. The flag can be read with the `isInterrupted()` method. There is also the static function `Thread.interrupted()` which resets the flag and returns the old value. Important to know is that if the flag is set any subsequent `wait()`, `sleep()` or `join()` on that thread will immediately throw an `InterruptedException`.  \n```java\npublic static void main(String args[]) {\nThread.currentThread().interrupt(); // true\nSystem.out.println(Thread.interrupted()); // prints true, now false\ntry {\nThread.sleep(1000);\nSystem.out.println(\"ok1\"); // prints\n} catch (InterruptedException e) {\nSystem.out.println(\"IE: \" + Thread.currentThread().isInterrupted());\n}\nThread.currentThread().interrupt(); // true\nSystem.out.println(Thread.currentThread().isInterrupted()); // prints true\ntry {\nThread.sleep(1000);\nSystem.out.println(\"ok2\"); // doesn't print\n} catch (InterruptedException e) {\nSystem.out.println(\"IE: \" + Thread.currentThread().isInterrupted()); // prints false\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interrupts", "Header 2": "Handling InterruptedException", "path": "../pages/digitalGarden/cs/concurrentParallel/interrupts.mdx"}, "page_content": "When an `InterruptedException` is thrown there are a few possible reactions all with their own benefits.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interrupts", "Header 2": "Handling InterruptedException", "Header 3": "Ignore", "path": "../pages/digitalGarden/cs/concurrentParallel/interrupts.mdx"}, "page_content": "The exception can be ignored if we know for a fact that the threads interrupt method is never called for example when it is in a local non-accessible thread class. Another use case for ignoring the exception is if we want an essential service to not be interruptable.  \n```java\ntry {\nwait();\n} catch (InterruptedException e) {\n//ignore\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interrupts", "Header 2": "Handling InterruptedException", "Header 3": "Propagate", "path": "../pages/digitalGarden/cs/concurrentParallel/interrupts.mdx"}, "page_content": "The exception can also be propagated up the call stack. Some simple cleanup can also be done in the exception handler before propagating.  \n```java\npublic synchronized void foo() throws InterruptedException {\n...\ntry {\nwait(); // wait until not full\n} catch ( InterruptedException e ) {\n/* some cleanup */\nthrow e;\n}\n...\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interrupts", "Header 2": "Handling InterruptedException", "Header 3": "Defer", "path": "../pages/digitalGarden/cs/concurrentParallel/interrupts.mdx"}, "page_content": "In some cases, it might not be possible to propagate the exception for example when the task is defined in a Runnable. Instead, we defer the handling to later point. For this, we restore the interrupted status so that code higher up on the call stack can handle the exception appropriately.  \n```java\ntry {\nwait();\n} catch (InterruptedException e) {\n// Restore the interrupted status\nThread.currentThread().interrupt();\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Interrupts", "Header 2": "Lost Wake-Up/Signal Problem", "path": "../pages/digitalGarden/cs/concurrentParallel/interrupts.mdx"}, "page_content": "A lost wake-up or signal is an event that can happen when a thread is notified with a `notify()` call and is simultaneously interrupted. This results in the notify signal getting lost and possibly leading to a deadlock as the rest of the code thinks the notify was executed without any issues. A possible scenario could look like this:  \n1. Threads t1 and t2 are waiting in a wait()\n2. Thread t3 performs a notify => t1 is selected\n3. Thread t4 interrupts t1\n1. wait called by t1 throws InterruptedException\n1. t1 does not process notification\n2. t2 does not wake up => Deadlock  \nA solution to this problem is to call `notifyAll()` or `notify()` in the ExceptionHandler.  \n```java\ncatch (InterruptedException e) {\nnotify();\nthrow e;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Tools", "path": "../pages/digitalGarden/cs/javaScript/tools.mdx"}, "page_content": "<Callout type=\"info\">\nwebpack, eslint and bable, polyfills, browser support.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Meta Programming", "path": "../pages/digitalGarden/cs/javaScript/metaProgramming.mdx"}, "page_content": "<Callout type=\"info\">\nSymbols, generators and iterators, maybe there is also more?\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "Arrays exist in JavaScript just as they do in most other programming languages, however I would describe them to be more like ArrayLists in Java as they don't have a fixed length. The precise details of how arrays are implemented are a bit more complex and depend on the EcmaScript implementation, for further details on how arrays work under the hood in JavaScript I can recommend reading [this article](https://ryanpeden.com/how-do-javascript-arrays-work-under-the-hood/).  \n```javascript\nconst cars = [\"Saab\", \"Volvo\", \"BMW\"];\ncars = []; // this would not work because const\ncars[0] = \"Mini\"; // this however would work\n\ncars.push(\"Saab\", \"Mercedes\") // push to the end, array is now longer\nconsole.log(cars.length) // 5\ncars.pop() // removes the last element\n\nconst person = [\"John\", \"Doe\", 46]; // can have values of different types\n\nconst numbers = new Array(1,2,3,4,5) // can create array like this but [] literal is preferred\nnumbers[20] = 21 // has made the array sparase, indexes 4-19 now have the value undefined\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Checking for an Array", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "Checking if a variable holds an array is not as easy as one would expect because the `typeof` operator returns \"object\". Instead, we have a few solutions:  \n1. Use the static method `Array.isArray()`.\n2. Use the `instanceof` operator.  \n```javascript\nconst cars = [\"Saab\", \"Volvo\", \"BMW\"];\nconsole.log(typeof cars); // object\nconsole.log(Array.isArray(cars)); // true\nconsole.log(cars instanceof Array); // true\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Sorting", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "JavaScript arrays can be quietly easily sorted using the `.sort()` method. However, this only really works well for strings as the sort method sorts the array alphabetically using the string representation. But it doesn't work well with other types such as numbers or objects. JavaScript also offers the reverse method which reverses the current order of the array, so if you wanted an array sorted in descending order you could first sort then reverse.  \n<Callout type=\"info\">\nImportant to know is that the sort/reverse method changes the underlying array, it doesn't return a new sorted\narray.\n</Callout>  \n```javascript\nconst cars = [\"Saab\", \"Volvo\", \"BMW\"];\nconsole.log(cars.sort()); // [BMW', 'Saab', 'Volvo']\nconsole.log(cars); // ['BMW', 'Saab', 'Volvo']\n\nconst numbers = [3, 20, 100];\nconsole.log(numbers.sort()); // [100, 20, 3]\nconsole.log(numbers.reverse()); // [3, 20, 100]\n```  \nFor objects or numbers or in general if we want more control over how the elements are sorted we can pass an optional function, the so-called compare function. This function takes 2 arguments, `a` and `b`, and compares them ( only if a or b aren't `undefined` as undefined elements are automatically put at the end of the array). If the compare function returns 0 then the 2 elements stay in the same order, if a positive value is returned then `a` is put after `b`, if a negative value is returned then `b` is put after `a`.  \n```javascript\nconst numbers = [3, 20, 100];\nconsole.log(numbers.sort()); // [100, 20, 3]\nconsole.log(numbers.sort((a,b) => a - b)); // [3, 20, 100]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Searching and Filtering", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "If you already know the element you are looking for but wish to know the index, or if the array even contains a certain element the `indexOf()` or `lastIndexOf()` methods can be used which return either the first or the last index of the passed element or if the element was not found the index -1 is returned. An index can also be specified to start the search from.  \n```javascript\nconst beasts = [\"ant\", \"bison\", \"camel\", \"duck\", \"bison\"];", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Searching and Filtering", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "console.log(beasts.indexOf(\"bison\")); //1\nconsole.log(beasts.indexOf(\"bison\", 2)); // Start from index 2, returns 4\nconsole.log(beasts.lastIndexOf(\"bison\")); // 4\n```  \n<Callout type=\"info\">\nIf you only want to know if an array has an element and don't need to know the index you can use the `includes()`\nmethod.  \n```javascript\nconst beasts = [\"ant\", \"bison\", \"camel\", \"duck\", \"bison\"];\nconsole.log(beasts.includes(\"ant\")); // true\nconsole.log(beasts.indexOf(\"ant\") !== -1); // true\n```\n</Callout>  \nInstead of looking for an index of a certain element we can also use the `findIndex()` or `findLastIndex()` methods which work very similar but instead of looking for an element are given a predicate function and return the index of the element that matches that criteria first or last.  \n```javascript\nconst beasts = [\"ant\", \"bison\", \"camel\", \"duck\", \"bison\"];\nconst firstBIndex = beasts.findIndex(beast => beast.toLowerCase().startsWith(\"b\"));\nconsole.log(firstBIndex); // 1\n```  \nWhen looking for an element that matches a certain condition you can use the `find()` method which returns the first element that matches it or undefined. If you only want to know if there is an element that matches the condition and don't need the element you can use the `some()` method.  \n```javascript\nconst array = [5, 12, 8, 130, 44];\nconst firstEven = array.find(e => e % 2 == 0);\nconsole.log(firstEven); // 12\nconsole.log(array.some(e => e % 2 == 0)); // true\n```  \nFiltering allows to only work with a subset of an array. The `filter()` method iterates over each element of the array and calls the provided predicate function once for each element. If the provided predicate function returns true for the value it is shallow copied into a new array which is then returned at the end, if it is false then it is not added to the filtered array.  \n```javascript\nconst words = [\"spray\", \"limit\", \"elite\", \"exuberant\", \"destruction\", \"present\"];", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Searching and Filtering", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "const longWords = words.filter(word => word.length > 6);\nconsole.log(longWords); // [ 'exuberant', 'destruction', 'present' ]\nconst evenWords = words.filter((word, index) => index % 2 == 0);\nconsole.log(evenWords); // [ 'spray', 'elite', 'destruction' ]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Shifting", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "The `shift()` and `unshift()` array methods almost do the same as the `pop` and `push()` methods but in the opposite way. The shift method removes the first element, whereas the pop method removes the last and the unshift method allows you to add elements to the front of an array whereas the push method lets you add to the back of the array.  \n```javascript\nconst cars = [\"Saab\", \"Volvo\", \"BMW\"];\nconsole.log(cars.shift()); // [\"Volvo\", \"BMW\"]\nconsole.log(cars.unshift(\"Saab\", \"Mini\")); // [\"Saab\", \"Mini\", \"Volvo\", \"BMW\"]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Combining", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "Elements of an array can be combined to make a new string with the `join()` method. It concatenates all of the elements string representations and\nseparates them with commas or a specified separator. If their is one element, then the seperator is not added.  \n```javascript\nconst elements = ['Fire', 'Air', 'Water'];\n\nconsole.log(elements.join()); // \"Fire,Air,Water\"\nconsole.log(elements.join('-')); // \"Fire-Air-Water\"\n```  \nYou can combine/merge two arrays into one new array with the `concat()` method.  \n```javascript\nconst array1 = ['a', 'b', 'c'];\nconst array2 = ['d', 'e', 'f'];\n\nconsole.log(array1.concat(array2)); //  ['a', 'b', 'c', 'd', 'e', 'f']\nconsole.log(array1) // unaffected, ['a', 'b', 'c']\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Slicing and Splicing", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "Using the `slice(start, end)` method you can create a new array containing a shallow copy of the specified portion of the array.\nIf start is omitted then 0 is used, and if it is negative then `start = start + array.length` which has the effect of coming from the back,\nso `start=-1` is the index of the last element. If the end is omitted or larger than `array.length` then it is just set to `array.length`.\nFor negative end indexes the same happens as with negative start indexes.  \n<Callout type=\"info\">\nThe end index is exclusive!\n</Callout>  \n```javascript\nconst animals = ['ant', 'bison', 'camel', 'duck', 'elephant'];\n\nconsole.log(animals.slice(2)); // [\"camel\", \"duck\", \"elephant\"]\nconsole.log(animals.slice(2, 4)); // [\"camel\", \"duck\"]\nconsole.log(animals.slice(-2)); // [\"duck\", \"elephant\"]\nconsole.log(animals.slice(2, -1)); // [\"camel\", \"duck\"]\nconsole.log(animals.slice()); // full shallow copy, [\"ant\", \"bison\", \"camel\", \"duck\", \"elephant\"]\n```  \nThe `splice()` method is slightly more complex than the `slice()` method but can be very useful.\nThe first parameter is the start index which works just like the start index in the slice method. The second parameter is\nthe deletion count, so the number of elements you want to delete. The last parameter can be one or multiple elements which\nwill then be added to the array beginning at the start index. So by smartly combining these arguments we can have one method\nto insert, update/replace or remove multiple elements at the same time.  \n<Callout type=\"warning\">\nSplice modifies the underlying array, so it modifies the array in place!\n</Callout>  \n```javascript\nconst months = ['Jan', 'March', 'April', 'June'];\n\nmonths.splice(1, 0, 'Feb'); // Inserts at index 1\nconsole.log(months);// [\"Jan\", \"Feb\", \"March\", \"April\", \"June\"]\n\nmonths.splice(4, 1, 'May'); // Updates/replaces 1 element at index 4\nconsole.log(months); // [\"Jan\", \"Feb\", \"March\", \"April\", \"May\"]\n\nmonths.splice(2,1);\nconsole.log(months); // [\"Jan\", \"Feb\", \"April\", \"May\"]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Map, Reduce and ForEach", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "The `map()` method creates a new array containing all the elements after being passed through the provided function.  \n```javascript\nconst arr = [1, 4, 9, 16];\n\nconst doubled = arr.map(x => x * 2);\nconsole.log(doubled); // [ 2, 8, 18, 32 ]\n\nconst square = (x) => x*x;\nconst squared = arr.map(square);\nconsole.log(squared); // [ 1, 16, 81, 256 ]\n```  \nThe `reduce()` method is often confusing for people but all it does is reduce an array to a single value by iterating\nover the array from left to right, hence the name. It takes a callback function and can optionally also receive an initial value,\nif no initial value is passed then it is just set to the first element and the first iteration is skipped.\nThe core argument of the callback function is the accumulator which holds the intermediate results of the reduction process.\nIt also has an argument for the current value and optionally the elements index.  \nIf you don't want the reduction process to go from left to right you can combine it with `reverse()` or just use `reduceRight()`.  \n```javascript\nconst arr = [1, 2, 3, 4];\n\n// 0 + 1 + 2 + 3 + 4\nlet iterations = 0;\nconst sumWithInitial = arr.reduce(\n(accumulator, currentValue) => {\niterations++\nreturn accumulator + currentValue\n},\n0\n);\nconsole.log(sumWithInitial); // 10\nconsole.log(iterations); // 4\n\n// 1 + 2 + 3 + 4\niterations = 0;\nconst sumWithoutInitial = arr.reduce(\n(accumulator, currentValue) => {\niterations++\nreturn accumulator + currentValue\n},\n)\nconsole.log(sumWithoutInitial); // 10\nconsole.log(iterations); // 3\n\n// 1 + 3\nconst sumEvenIndexes= arr.reduce(\n(accumulator, currentValue, currentIndex) => currentIndex % 2 === 0? accumulator +  currentValue : accumulator,\n0\n)\nconsole.log(sumEvenIndexes); // 4\n```  \nLastly there is the `forEach()` method which executes a function once for each element.  \n<Callout type=\"warning\">\nThe forEach method does not modify the underlying array, but the callback function can!\n</Callout>  \n```javascript\nconst arr = [1, 2, 3, 4];", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Arrays", "Header 3": "Map, Reduce and ForEach", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "arr.forEach((element, index)=>console.log(`${index}: ${element}`)) // return value is discarded and always undefined\narr.forEach((element)=> element += 1)\nconsole.log(arr); // [1, 2, 3, 4]\n\nconst animals = [{name: 'ant'}, {name: 'bison'}, {name: 'camel'}, {name: 'duck'}, {name: 'elephant'}];\nanimals.forEach((animal) => animal.name = animal.name.toUpperCase()) // modifying!\nconsole.log(animals) // all names are now in CAPS\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Sets and Maps", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "JavaScript offers a built-in object for [set semantic](../algorithmsDataStructures/sets), i.e a data structure that only holds unique elements. The set in JavaScript\nkeeps track of the insertion order so when iterating over it the order is deterministic. The set determines equality using\nthe \"SameValueZero\" Algorithm which basically says elements are equal if `===` says so apart from `NaN`s are equal which they normally are not\n(`NaN === NaN = false`) and negative zero and zero are not the same so.  \nThe EcmaScript specification requires a set to be implemented in a way that, on average, provide access times better than $O(n).\nTherefore, depending on the engine implementation it could be internally as a [hash table](../algorithmsDataStructures/hashTables)\n(with O(1) lookup), a [search tree](../algorithmsDataStructures/trees/binarySearchTrees) (with O(log(N)) lookup), or any other data structure, as long as the complexity is better than O(N).  \nThe methods available on the set are pretty self-explanatory, however it is disappointing that you need to implement your own set operators such\nas intersection, union etc. Also, the methods `values()`, `keys()` and `entries()` are all basically the same except for entries which returns an [iterator](../designPatterns/iterator),\nwhere each element is an array containing the element twice. These methods seem to just be there for consistency...  \n```javascript\nconst mySet = new Set();\n\nmySet.add(1); // { 1 }\nmySet.add(5); // { 1, 5 }\nmySet.add(5); // already contains 5, { 1, 5 }\nmySet.add(\"some text\"); // can hold multiple types, { 1, 5, 'some text' }\nconst obj = { a: 1, b: 2 };\nmySet.add(obj);\nmySet.add(obj) // already contains\nmySet.add({ a: 1, b: 2 }); // same values, but referencing different object so okay\n\nmySet.has(1); // true\nmySet.has(3); // false\nmySet.has(obj); // true\nmySet.has({a:1,b:2}) // false, different reference\n\nmySet.size; // 5\nmySet.delete(5);\nmySet.has(5); // false, 5 has been removed\nmySet.size; // 4", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Sets and Maps", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "mySet.has(1); // true\nmySet.has(3); // false\nmySet.has(obj); // true\nmySet.has({a:1,b:2}) // false, different reference\n\nmySet.size; // 5\nmySet.delete(5);\nmySet.has(5); // false, 5 has been removed\nmySet.size; // 4\n\nmySet.forEach(e => console.log(e))\n```  \nThere is also the built-in map object that allows for key-value pairs to be stored and quickly looked up. Just like with the set the specification does not say\nhow the map is implemented, but it should have a lookup that is faster than $O(n)$ so it could be a [hash table](../algorithmsDataStructures/hashTables)\n(with O(1) lookup), a [search tree](../algorithmsDataStructures/trees/binarySearchTrees) (with O(log(N)) lookup) or any other data structure fast enough.  \nFor the map object the `values()`, `keys()` and `entries()` methods make a lot more sense than for the set object, and behave in a similar self-explanatory way.  \nSets and maps can also be created from iterables i.e. an array or some other iterable object.  \n```javascript\nconst myMap = new Map([\n[\"a\", 1],\n[\"b\", 2],\n[\"c\", 3],\n]);\n\nconsole.log(myMap.get('a')); // 1\nmyMap.set('a', 97);\nconsole.log(myMap.get('a')); // 97\n\nconsole.log(myMap.size); // 3\nmyMap.delete('b');\nconsole.log(myMap.size); // 2\n\nfor ([key,value] of myMap.entries()){\nconsole.log(`${key}: ${value}`);\n}\n```  \n<Callout type=\"warning\">  \nDue to the nature of JavaScript you can assign properties to the map object, and they will actually be added and visable.\nHowever, they will not be added to the underlying data structure and are therefore useless and you should avoid accidently doing this!  \n```javascript\nconst myMap = new Map();\nmyMap['bla'] = 'blaa';\nmyMap['bla2'] = 'blaaa2';\n\nconsole.log(myMap); // {bla: 'blaa', bla2: 'blaaa2'}\nconsole.log(myMap.has('bla')); // false\nconsole.log(myMap.delete('bla')); // false\n```  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays and Iterables", "Header 2": "Sets and Maps", "Header 3": "Weak Iterables", "path": "../pages/digitalGarden/cs/javaScript/arraysIterables.mdx"}, "page_content": "WeakSets work just like sets however they can only contain objects, not primitive values. Additionally, the WeakSet only hold weak references, meaning if\nthe only reference of the object is being held by the WeakSet it can be garbage collected. The same exists for maps, the WeakMap, here however the key\nmust be an object, the value can be a primitive value, but when the key is garbage collected so is the value.  \nA possible use-case is to use a WeakSet in a recursive function to guard against circular data structures by tracking which objects have already been processed.\nThis way the object being processed can still be collected if it is no longer referenced anywhere apart from in the WeakSet.  \n```javascript\nfunction execRecursively(fn, subject, _refs = new WeakSet()) {\n// Avoid infinite recursion\nif (_refs.has(subject)) {\nreturn;\n}\n\nfn(subject);\nif (typeof subject === \"object\") {\n_refs.add(subject);\nfor (const key in subject) {\nexecRecursively(fn, subject[key], _refs);\n}\n_refs.delete(subject);\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Basics", "path": "../pages/digitalGarden/cs/javaScript/basics.mdx"}, "page_content": "<Callout type=\"info\">\nTodo on the basic types and history of ECMAScript etc. incl. let const and var and functions incl. arrow functions\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Basics", "Header 2": "let and const", "path": "../pages/digitalGarden/cs/javaScript/basics.mdx"}, "page_content": "These two keywords provide Block Scope variables (and constants) in JavaScript. Before ES6, JavaScript had only two types of scope: Global Scope and Function Scope.  \nVariables declared with the `let` keyword can have Block Scope. However variables declared inside a block can not be accessed from outside the block.  \n```jsx\n{\nvar x = 2;\n}\n// x CAN be used here\n{\nlet y = 2;\n}\n// y can NOT be used here\n```  \nDeclaring a variable with `const` is similar to `let` when it comes to Block Scope. JavaScript `const` variables must be assigned a value when they are declared. However, The keyword `const` is a little misleading. You can't change constant primitive values, but you can change the properties of a constant objects.  \n```jsx\n// You can create a const object:\nconst car = {type:\"Fiat\", model:\"500\", color:\"white\"};\n// You can change a property:\ncar.color = \"red\";\n// You can add a property:\ncar.owner = \"Johnson\";\n// You CAN'T reassign a constant object:\ncar = {type:\"Volvo\", model:\"EX60\", color:\"red\"};    // ERROR\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The Basics", "Header 2": "Arrow functions", "path": "../pages/digitalGarden/cs/javaScript/basics.mdx"}, "page_content": "Arrow functions are a different way of creating functions in JavaScript. Besides a shorter syntax, they offer advantages when it comes to keeping the scope of the `this` keyword.  \n```jsx\nfunction callMe(name) {\nconsole.log(name);\n}\n// Or\nconst callMe = function(name) {\nconsole.log(name);\n}\n// Becomes\nconst callMe = (name) => {\nconsole.log(name);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "The DOM", "path": "../pages/digitalGarden/cs/javaScript/dom.mdx"}, "page_content": "<Callout type=\"info\">\nhow does the dom work, how is it built up, how to interact with it.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Async and Promises", "path": "../pages/digitalGarden/cs/javaScript/asyncPromises.mdx"}, "page_content": "Most of the code we write is so-called synchronous code. Synchronous code is when each task is performed one after the\nother, in sequence, hence the name. This means that in synchronous code, the next task, i.e. line of code does not\nexecute until the previous one has completed.  \nHowever, asynchronous code allows for multiple tasks to be performed simultaneously, making it more efficient and faster.\nThe most common example is when making a data request from a client (browser) to a server. Whilst we are waiting for the\nresponse to arrive we don't want the website to be unusable/frozen, hence the data request/fetching is done asynchronously.  \n<Image\nsrc=\"/cs/jsAsyncVsSync.png\"\ncaption=\"The difference between synchronous and asynchronous requests.\"\nwidth={800}\n/>  \n<Callout type=\"info\">\nYou might know or have read that javascript only runs in a single thread tho, so how is asynchronous code possible?  \nIn JavaScript, the browser handles asynchronous operations by using browser threads that run independently of the\nJavaScript main thread. So instead when an asynchronous operation is initiated, the browser registers a callback\nfunction and continues executing the rest of the program. When the operation completes, the callback function is\ncalled by the browser with the result of the operation.  \n[You cand find more about this topic here](https://dev.to/bbarbour/if-javascript-is-single-threaded-how-is-it-asynchronous-56gd).\n</Callout>  \nA very simple example of async code can be made using the `setTimeout()` function which creates an async task where the\nbrowser just sleeps and then calls the callback function after a certain amount of time.  \n```javascript\nconsole.log('Fetching users...');\n\nsetTimeout(() => {\nconst users = [\n{ id: 1, name: 'Alice' },\n{ id: 2, name: 'Bob' },\n{ id: 3, name: 'Charlie' }\n];\nconsole.log('Users:', users);\n}, 2000);", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Async and Promises", "path": "../pages/digitalGarden/cs/javaScript/asyncPromises.mdx"}, "page_content": "setTimeout(() => {\nconst users = [\n{ id: 1, name: 'Alice' },\n{ id: 2, name: 'Bob' },\n{ id: 3, name: 'Charlie' }\n];\nconsole.log('Users:', users);\n}, 2000);\n\nconsole.log('Program continues to execute...');\n```  \n```filename=\"output\"\nFetching users...\nProgram continues to execute...\nUsers: [ { id: 1, name: 'Alice' }, { id: 2, name: 'Bob' }, { id: 3, name: 'Charlie' } ]\n```  \nSo we can see the that the code carried on after creating the async task. Interestingly because the way the messaging\nqueue works in javascript even if the delay was 0, and we would just have a simple console output in the callback function\nthe console output outside, i.e. the first line after the creation of the async task will always execute before the callback.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Async and Promises", "Header 2": "Promises", "path": "../pages/digitalGarden/cs/javaScript/asyncPromises.mdx"}, "page_content": "In JavaScript there is a thing callback hell. The code below does the following, whilst handling errors:  \n1. Reads the contents of file1.txt and file2.txt.\n2. Then it concatenates the content and writes the result to file3.txt.\n3. Then it reads the content of file4.txt and converts it to uppercase and writes the result to file5.txt.  \n<Callout type=\"info\">\nThe code below uses the Node.js File system API and would not work in the browser.\n</Callout>  \n```javascript\nconst fs = require('fs');\n\nfs.readFile('file1.txt', (err, data) => {\nif (err) {\nconsole.error(err);\nreturn;\n}\nconst file1Contents = data.toString();\nfs.readFile('file2.txt', (err, data) => {\nif (err) {\nconsole.error(err);\nreturn;\n}\nconst file2Contents = data.toString();\nfs.writeFile('file3.txt', file1Contents + file2Contents, (err) => {\nif (err) {\nconsole.error(err);\nreturn;\n}\nconsole.log('File3 created with the contents of file1 and file2.');\nfs.readFile('file4.txt', (err, data) => {\nif (err) {\nconsole.error(err);\nreturn;\n}\nconst file4Contents = data.toString();\nfs.writeFile('file5.txt', file4Contents.toUpperCase(), (err) => {\nif (err) {\nconsole.error(err);\nreturn;\n}\nconsole.log('File5 created with the uppercase contents of file4.');\n});\n});\n});\n});\n});\n```  \nAs you can see it gets very messy and confusing, which is why JavaScript introduced promises. To understand how promises\nwork lets first create a function that transforms the `setTimeout()` function to use promises.  \n```javascript\nconst setTimer = duration => {\nconst promise = new Promise((resolve, reject) => {\nsetTimeout(() => {\nresolve(\"Done!\");\n}, duration);\n});\nreturn promise;\n};", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Async and Promises", "Header 2": "Promises", "path": "../pages/digitalGarden/cs/javaScript/asyncPromises.mdx"}, "page_content": "setTimer(2000).then(data => {\nconsole.log(data);\n})\n```  \nThe promise object represents the eventual completion(`resolve()`) or failure(`reject()`) of an asynchronous operation\nand its result. The promise constructor takes a single argument, a function, the so-called executor which is run\nautomatically and is responsible for performing the asynchronous task. The executor takes two arguments, the resolve and\nreject callback functions which are respectively called if the operation succeeded with the result or if the\noperation failed with the error. This means that a promise object can be in one of three states:  \n- **Pending**, the initial state whilst doing the work.\n- **Fulfilled**: a successful completion of the operation, i.e. the promise has resolved and the resulting value is available.\n- **Rejected**: a failed operation, i.e. the promise has been rejected and the resulting error is available.  \nDepending on the result of a promise it can be fetched and handled with one of two methods:  \n- `then()`, when the promise is fulfilled.\n- `catch()`, when the promise is rejected.  \nBecause these methods are also encapsulated in a Promise it allows for Promises to be chained together.  \n<Callout type=\"info\">\nIf you have another `then()` block after a `catch()` the chain continues, so errors need to propagated (because the\nouter promise returns to the pending state). Only once there are no more `then()` blocks left does the outer promise\nenter the state **Settled** which can then be handled by the `finally()` method to do final cleanup work.  \n`finally()` is reached no matter if you resolved or rejected before!  \n```javascript\nconst setTimer = duration => {\nconst promise = new Promise((resolve, reject) => {\nsetTimeout(() => {\nresolve(\"Done!\");\n}, duration);\n});\nreturn promise;\n};", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Async and Promises", "Header 2": "Promises", "path": "../pages/digitalGarden/cs/javaScript/asyncPromises.mdx"}, "page_content": "setTimer(2000).then(data => {\nconsole.log(data);\nthrow new Error(\"Oh no, it's broken :(\");\n}).catch(err => {\nconsole.error(\"Catch 1\");\nconsole.log(err)\n}).then(data => {\nconsole.log(\"We continue with work\");\n}).catch(err => {\nconsole.error(\"Catch 2\");\nconsole.log(err)\n}).finally(() => {\nconsole.log(\"All done\");\n});\n```  \n```filename=\"output\"\nDone!\nCatch 1\nError: Oh no, it's broken :(\nat /tmp/jeGZmdNBei.js:12:11\nWe continue with work\nAll done\n```\n</Callout>  \nLuckily the functions `readFile()` and `writeFile()` in the \"callback hell\" example, have also been implemented to use\npromises if the import is changed slightly. We can then rewrite the above hell to this much more readable code:  \n```javascript\nconst fs = require('fs').promises;\n\nfs.readFile('file1.txt')\n.then(file1Contents => {\nreturn fs.readFile('file2.txt')\n.then(file2Contents => {\nreturn file1Contents + file2Contents;\n});\n})\n.then(file3Contents => {\nreturn fs.writeFile('file3.txt', file3Contents);\n})\n.then(() => {\nreturn fs.readFile('file4.txt');\n})\n.then(file4Contents => {\nreturn fs.writeFile('file5.txt', file4Contents.toString().toUpperCase());\n})\n.catch(err => {\nconsole.error(err);\n});\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Async and Promises", "Header 2": "Async and Await", "path": "../pages/digitalGarden/cs/javaScript/asyncPromises.mdx"}, "page_content": "As we can see error handling can still a bit annoying even with promises. The `async` and `await` keywords is a\nsyntactical sugar built on top of Promises that makes it easier to write asynchronous code that also looks more like\nsynchronous code. We especially get to see how \"synchronous esque\" the code is using `async` and `await` when handling\nerrors as we can just use `try` and `catch` with the expected behaviour.  \nTODODODODODODODODO explain the difference  \n```javascript\nfunction getPromisesData() {\nreturn fetch('https://dummyjson.com/products')\n.then(response => response.json())\n.then(data => {\nreturn data;\n})\n.catch(error => {\nconsole.error(\"Promise Error 1: \" + error);\nthrow error; // propagate\n});\n}\n\nasync function getAsyncData() {\ntry {\nconst response = await fetch('https://dummyjson.com/products');\nconst data = await response.json();\nreturn data;\n} catch (error) {\nconsole.error(\"Async Error 1: \" + error);\nthrow error; // propagate\n}\n}\n\ngetPromisesData()\n.then(data => console.log(\"Promise Data: \" + data))\n.catch(error => console.error(\"Promise Error 2: \" + error));\n\ngetAsyncData()\n.then(data => console.log(\"Async Data: \" + data))\n.catch(error => console.error(\"Async Error 2: \" + error));\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Modules", "path": "../pages/digitalGarden/cs/javaScript/modules.mdx"}, "page_content": "<Callout type=\"info\">\nwhy use modules and how to use? maybe also mention webpack.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Modules", "Header 2": "Exports and imports", "path": "../pages/digitalGarden/cs/javaScript/modules.mdx"}, "page_content": "In JavaScript projects you split your code across multiple JavaScript files, so called modules. You do this, to keep each file/module focused and manageable. To still access functionality in another file, you need export (to make it available) and import (to get access) statements.\nYou got two different types of exports: default (unnamed) and named exports:\n**Default** => `export default ...;`\n**Named** => `export const someData = ...;`\nYou can import default exports like this:  \n```jsx\nimport someNameOfYourChoice from \"./path/to/file.js\";\n```  \nNamed exports have to be imported by their name:  \n```jsx\nimport { someData } from \"./path/to/file.js\";\n```  \nA file can only contain one default and an unlimited amount of named exports. You can also mix the one default with any amount of named exports in one and the same file. When importing named exports, you can also import all named exports at once with the following syntax:  \n```jsx\nimport * as upToYou from \"./path/to/file.js\";\n```  \nThis simply bundles all exported variables/functions in one JavaScript object. You can access like this: `upToYou.someData`", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binding and This", "path": "../pages/digitalGarden/cs/javaScript/bindingThis.mdx"}, "page_content": "<Callout type=\"info\">\nTodo explain the bind and the this keyword and how they effect each other incl. arrow functions effect.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Objects, Classes and Prototypes", "path": "../pages/digitalGarden/cs/javaScript/objectsClassesPrototypes.mdx"}, "page_content": "<Callout type=\"info\">\nTodo on objects and classes and the differences. Also clearly explain how prototypes work because atm i dont know.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Objects, Classes and Prototypes", "Header 2": "Classes", "path": "../pages/digitalGarden/cs/javaScript/objectsClassesPrototypes.mdx"}, "page_content": "Classes allow you to define blueprints for JavaScript objects. With classes you can also add inheritance to JavaScript objects.  \n```jsx\nclass Human {\nspecies = \"human\";\n}\nclass Person extends Human {\nname = \"Max\";\nprintMyName = () => {\nconsole.log(this.name);\n}\n}\nconst person = new Person();\nperson.printMyName(); // prints \"Max\"\nconsole.log(person.species); // prints \"human\"\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays", "path": "../pages/digitalGarden/cs/c/arrays.mdx"}, "page_content": "In C an array is a variable that can store multiple values of the same data type. When declaring it you must define how many values the array can hold. You can then access particular elements by using indexes which start at 0. In C array out of bounds, meaning the index is not in the range of $0-length-1$, can not be checked by the compiler and therefore does not throw an error. An out of bounds exception can cause the program to crash or unexpected behavior. Arrays are initialized with the default value of that type but you can also specify specific values when initializing.  \n```c\n#include <stdio.h>\n\nvoid printIntArray(int arr[], int length){\nprintf(\"%d\", arr[0]);\nfor(int i = 1; i < length; i++) {\nprintf(\", %d\", arr[i]);\n}\nprintf(\"\\n\");\n}\n\nint main()\n{\nint empty[1];\nint marks[5] = {19, 10, 8, 17, 9};\nint otherMarks[] = {1,2,3}; // length is inferred\nint moreMarks[5] = {[2]=10, [4]=40}; // all others are 0\n\nprintIntArray(empty, 1);\nprintIntArray(marks, 5);\nprintIntArray(otherMarks, 3);\nprintIntArray(moreMarks, 5);\n\nreturn 0;\n}\n```  \n```bash filename=\"Output\"\n-410826608\n19, 10, 8, 17, 9\n1, 2, 3\n0, 0, 10, 0, 40\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays", "Header 2": "Multidimensional arrays", "path": "../pages/digitalGarden/cs/c/arrays.mdx"}, "page_content": "In C you can also create multidimensional arrays so which are arrays of arrays. Just as in other language 2D can be visualized as a table. You can also go further like 3D etc. but this can quickly get very confusing.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Arrays", "Header 2": "Pointer arithmetic", "path": "../pages/digitalGarden/cs/c/arrays.mdx"}, "page_content": "Interestingly the name of an array is also a pointer to the first element of an array which we can make use of with a concept called pointer arithmetic to iterate through the array.  \n```c\n#include <stdio.h>\n\nvoid printIntArray(int* arr, int length) {\nint *arr_end = arr + length;\nfor(int* ptr = arr; ptr < arr_end; ptr++){\nprintf(\"%p\\t%d\\n\", (void*)ptr, *ptr);\n}\n}\n\nint main()\n{\nint marks[] = {19, 10, 8, 17, 9};\nint* ptr = marks; // same as &marks[0]\nprintIntArray(ptr, 5);\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "To interact with files in C you need to have a FILE pointer a so called stream, which will let the program keep track of the memory address of the file being accessed. In C text files are sequence of characters as lines each ending with a newline (\\n). Interestingly you have already been working with file I/O since the begging as C automatically opens 3 files, the standard input (keyboard), standard output and error (both being the display). You have read from and written to these files using `scanf()` and `printf()`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "Header 2": "Opening and closing files", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "The FILE pointer points to the FILE struct which represents a stream. To be able to open a file and get a FILE pointer you need to use the `FILE *fopen(const char *restrict pathname, const char *restrict mode)` function which takes the name of the file and the mode to open it with, depending on the mode certain operations are limited. The function returns a FILE pointer or if something went wrong NULL. Once you have finished working with the file or you have reached the end of the file marked with EOF (end of file, equivalent to -1) you should close it with the `int fclose(FILE *stream)` function.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "Header 2": "Opening and closing files", "Header 3": "Modes", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "- r  - Open for reading. If the file does not exist returns NULL.\n- w  - Open for writing. If the file exists, its contents are overwritten. If the file does not exist, it will be created.\n- a  - Open for append. Data is added to the end of the file. If the file does not exist, it will be created.\n- r+ - Open for both reading and writing. If the file does not exist, returns NULL.\n- w+ - Open for both reading and writing. If the file exists, its contents are overwritten. If the file does not exist, it will be created.\n- a+ - Open for both reading and appending. If the file does not exist, it will be created.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "Header 2": "Buffers", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "Characters that are written to a stream are normally accumulated and sent to the file in a block. Similarly, streams often retrieve input from the host environment in blocks rather per character. This is called buffering.  \nThere are three different kinds of buffering strategies:  \n- Characters written to or read from an **unbuffered stream** are transmitted individually to or from the file as soon as possible.\n- Characters written to a **line buffered stream** are transmitted to the file in blocks when a newline character is encountered.\n- Characters written to or read from a **fully buffered stream** are transmitted to or from the file in blocks of arbitrary size.  \nFlushing output on a buffered stream means transmitting all accumulated characters to the file. Streams can automatically flush when following happens:  \n- When you try to do output and the output buffer is full.\n- When the stream is closed.\n- When the program terminates by calling exit.\n- When a newline is written, if the stream is line buffered.  \nIf you want to explicitly flush the buffered output you can use `int fflush (FILE *stream)`.  \nYou can bypass the stream buffering facilities altogether by using the POSIX input and output functions that operate on file descriptors instead.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "Header 2": "Reading", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "To read from a text file you have 3 options:  \n- `int fscanf(FILE *restrict stream, const char *restrict format, ...)`, the file alternative of scanf.\n- `int fgetc(FILE *stream)`, reads a single char (returns its int value) and increments the header by one.\n- `char *fgets(char *restrict s, int n, FILE *restrict stream)`, reads an entire line as a string and keeps the newline at the end.  \n```c\n#include <stdio.h>\n\nint main(void){\nchar *fileName = \"./text.txt\";\nFILE *file = fopen(fileName, \"r\");\nif(file == NULL){\nprintf(\"Failed to open %s file!\\n\", fileName);\nreturn -1;\n}\n\nchar c = '\\0';\nwhile(c != EOF){\nc = fgetc(file);\nprintf(\"%c\", c);\n}\nprintf(\"\\n\");\nrewind(file); // reset position to start\n\nchar str [100];\nwhile(fgets(str, 100, file)){\nprintf(\"%s\\n\", str);\n}\n\nrewind(file); // reset position to start\n\nint bananaCount = 0;\nfscanf(file, \"%s %s %d %s\", str, str, &bananaCount, str);\nprintf(\"bananaCount=%d\\n\", bananaCount);\n\nfclose(file);\nreturn 0;\n}\n```  \n```text filename=\"text.txt\"\nI have 3 bananas\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "Header 2": "Writing", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "Very similar to reading you have the following functions for writing  \n- `int fprintf(FILE *restrict stream, const char *restrict format, ...)`\n- `int fputc(int c, FILE *stream)`\n- `int fputs(const char *restrict s, FILE *restrict stream)`  \nImportant to note here is that the null terminator, '\\0' will not be written.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "Header 2": "Positioning", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "The file position of a stream describes where in the file the stream is currently reading or writing.  \n- `long int ftell (FILE *stream)` returns the current file position of the stream.\n- `int fseek (FILE *stream, long int offset, int whence)` is used to change the file position of the stream. The value of whence must be one of the constants SEEK_SET, SEEK_CUR, or SEEK_END, to indicate whether the offset is relative to the beginning of the file, the current file position, or the end of the file.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "Header 2": "Rename and move", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "You can rename or move files with `int rename(const char *old_filename, const char *new_filename)`. The function moves the file in between directories if needed so it can also be used to move files.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Standard File I/O", "Header 2": "Remove", "path": "../pages/digitalGarden/cs/c/standardFileIO.mdx"}, "page_content": "To remove/delete a file you can use `int remove(const char *pathname)`. if the file really is a file then `int unlink(const char *pathname)` will be called and if it is a directory `int rmdir(const char *pathname)` is called.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Flow", "path": "../pages/digitalGarden/cs/c/controlFlow.mdx"}, "page_content": "These work just as in many other languages so will not go into further detail.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Flow", "Header 3": "If/Else", "path": "../pages/digitalGarden/cs/c/controlFlow.mdx"}, "page_content": "```c\nif (test expression1) {\n// statement(s)\n}\nelse if (test expression2) {\n// statement(s)\n}\nelse {\n// statement(s)\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Flow", "Header 3": "For", "path": "../pages/digitalGarden/cs/c/controlFlow.mdx"}, "page_content": "```c\nfor (initializationStatement; testExpression; updateStatement) {\n// statements inside the body of loop\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Flow", "Header 3": "While", "path": "../pages/digitalGarden/cs/c/controlFlow.mdx"}, "page_content": "```c\nwhile (testExpression) {\n// the body of the loop\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Flow", "Header 3": "Do While", "path": "../pages/digitalGarden/cs/c/controlFlow.mdx"}, "page_content": "```c\ndo {\n// the body of the loop\n}\nwhile (testExpression);\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Flow", "Header 3": "Break and Continue", "path": "../pages/digitalGarden/cs/c/controlFlow.mdx"}, "page_content": "The `break` statement ends a loop immediately when it is encountered. The `continue` statement skips the current iteration of a loop and continues with the next iteration.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Flow", "Header 3": "Goto", "path": "../pages/digitalGarden/cs/c/controlFlow.mdx"}, "page_content": "Just dont use this.... if you need it you are doing something wrong unless you have a very very special use-case.  \n```c\n#include <stdio.h>\nint main(void)\n{\nint num, i = 1;\nprintf(\"Enter the number whose table you want to print?\");\nscanf(\"%d\", &num);\ntable:\nprintf(\"%d x %d = %d\\n\", num, i, num * i);\ni++;\nif (i <= 10)\ngoto table;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Control Flow", "Header 3": "Switch", "path": "../pages/digitalGarden/cs/c/controlFlow.mdx"}, "page_content": "Important to note here is that the values of expression and each constant-expression must have an integral type and a constant-expression must have an unambiguous constant integral value at compile time. Also the break here makes sure that it doesn't fall through to the other statements.  \n```c\nswitch (expression) {\ncase constant-expression-1:\n// statements\nbreak;\n\ncase constant-expression-t2:\n// statements\nbreak;\ndefault:\n// default statements\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "path": "../pages/digitalGarden/cs/c/functions.mdx"}, "page_content": "Just as with variables functions need to be defined before they can be used. To declare a function we use function prototypes, which include a name, the type of value it return and a list of parameters it takes. Parameters being values that the function it takes as input, also with a name and type just like variables.  \nParameters are pass by value in C, meaning that a copy of the input is made on the stack which is local to the function body. Later on you can also pass by reference using pointers.  \n```c\n#include <stdio.h>\nint addNumbers(int a, int b); // function prototype\n\nint main()\n{\nint n1 = 1, n2 = 2, sum;\n\nsum = addNumbers(n1, n2);\nprintf(\"sum = %d\",sum);\n\nreturn 0;\n}\n\nint addNumbers(int a, int b) {\nreturn a + b;\n}\n\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Pass by reference", "path": "../pages/digitalGarden/cs/c/functions.mdx"}, "page_content": "You might find yourself often swapping values between two variables which would lead you to implementing a swap function and your first attempt might look something like this  \n```c\n#include <stdio.h>\nvoid swap(int a, int b) {\nint temp = a;\na = b;\nb = temp;\nprintf(\"swap: a=%d, b=%d\\n\", a, b);\n}\nint main()\n{\nint a = 10;\nint b = 5;\n\nswap(a, b);\nprintf(\"main: a=%d, b=%d\\n\", a, b);\n\nreturn 0;\n}\n```  \nWhen executing the above code you will notice that the desired result was not reached due to functions in java being pass by value. To fix this we can use pointers and create functions which are pass by reference.  \n```c\n#include <stdio.h>\nvoid swap(int* a, int* b) {\nint temp = *a;\n*a = *b;\n*b = temp;\nprintf(\"swap: a=%d, b=%d\\n\", a, b);\n}\nint main()\n{\nint a = 10;\nint b = 5;\n\nswap(&a, &b);\nprintf(\"main: a=%d, b=%d\\n\", a, b);\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Multiple return values", "path": "../pages/digitalGarden/cs/c/functions.mdx"}, "page_content": "By using pointers as so called output parameters you can have functions return more then one value.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Pointers to functions", "Header 3": "Map", "path": "../pages/digitalGarden/cs/c/functions.mdx"}, "page_content": "We can use pointer for functions for a multitude of things for example passing a function to a map function which applies the function to every element in the array.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n\n#define LENGTH 5\n\nvoid map(int a[], int len, int (*f)(int))\n{\nfor (int i = 0; i < len; i++)\n{\na[i] = f(a[i]);\n}\n}\n\nint inc(int i)\n{\nreturn i + 1;\n}\n\nint main()\n{\nint i;\nint values[LENGTH] = {88, 56, 100, 2, 25};\n\nprintf(\"Before: \");\nfor (i = 0; i < LENGTH; i++)\n{\nprintf(\"%d \", values[i]);\n}\n\nmap(values, LENGTH, inc);\n\nprintf(\"\\nAfter: \");\nfor (i = 0; i < LENGTH; i++)\n{\nprintf(\"%d \", values[i]);\n}\n\nreturn (0);\n}\n```  \n```bash filename=\"output\"\nBefore: 88 56 100 2 25\nAfter: 89 57 101 3 26\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Pointers to functions", "Header 3": "QSort", "path": "../pages/digitalGarden/cs/c/functions.mdx"}, "page_content": "Another common use case is when you want to use the `qsort` function from the standard library to sort an array.  \n```c void qsort(void *base, size_t nitems, size_t size, int (*compar)(const void *, const void*))```  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n\n#define LENGTH 5\n\nint compareInts(const void *a, const void *b)\n{\nreturn (*(int *)a - *(int *)b);\n}\n\nint main(void)\n{\nint i;\nint values[LENGTH] = {88, 56, 100, 2, 25};\n\nprintf(\"Before: \");\nfor (i = 0; i < LENGTH; i++)\n{\nprintf(\"%d \", values[i]);\n}\n\nqsort(values, LENGTH, sizeof(int), compareInts);\n\nprintf(\"\\nAfter: \");\nfor (i = 0; i < LENGTH; i++)\n{\nprintf(\"%d \", values[i]);\n}\n\nreturn (0);\n}\n```  \n```bash filename=\"output\"\nBefore: 88 56 100 2 25\nAfter: 2 25 56 88 100\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Macros", "path": "../pages/digitalGarden/cs/c/functions.mdx"}, "page_content": "Macros are based on the define preprocessor directive, and work very similarly to functions. Just as when defining a key value pair you are limited to one line unless you use a backslash, \"\\\", at the end. Macros can be faster then normal functions because in the end they are just text substitutions and you therefore don't have the overhead when using functions like creating a new memory space. You must however be careful when using macros as they can not be debugged and because they really are just text substitute they can cause unexpected side effects.  \n```c\n#include <stdio.h>\n#define PRINT(a) \\\nprintf(\"value=%d\\n\", a);\n\n#define MAX(a, b) ((a) > (b)) ? (a) : (b)\n\nint main(void)\n{\nint a = 5;\nint b = 4;\n\nPRINT(a);            // 5\nint c = MAX(++a, b); // becomes ((++a) > (b)) ? (++a) : (b)\nPRINT(c);            // 7\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Macros", "Header 3": "Macro operators", "path": "../pages/digitalGarden/cs/c/functions.mdx"}, "page_content": "We have already seen the first one in action `\\`. Another one is `defined` which can be used to check if a symbol is already defined `#if defined(DEBUG)` which is very similar to `#ifdef`.  \n#### The # operator  \nIf you place a # in front of a parameter in a macro definition is inserts double quotes around the actual marco argument and therefore makes it to a constant string. Strings that are separated by a white space are concatenated during preprocessing so you can do something like this  \n```c\n#include <stdio.h>\n#define PRINTINT(var) printf(#var \"=%d\\n\", var)\n\nint main(void)\n{\nint count = 100;\nPRINTINT(count); // printf(\"count\" \"=%d\\n\", count); -> printf(\"count=%d\\n\", count);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "path": "../pages/digitalGarden/cs/c/pointers.mdx"}, "page_content": "Every variable is a memory location and every memory location has an address which can be accessed using the address-of operator, `&`. A pointer is a variable whose value is the address of another variable. This address is internally represented as an unsigned int on most systems however you shouldn't think of it as such (you can output it in hex using the %p format specifier). Every pointer has the type of the variable it is pointing to so the compiler knows how much memory is occupied by that variable. The `*` denotes a pointer. You can define and initialize a pointer by pointing to no location in memory with `NULL`, a so-called null pointer, which is also equivalent to 0.  \n<Image\nsrc=\"/cs/cPointers.png\"\ncaption=\"A visual representation of pointers in C. The variable `var` is stored in memory and has an address. The pointer `p` stores the address of `var` and can be dereferenced to access the value of `var`.\"\nwidth={500}\n/>  \nTo access the value the variable holds which a pointer is pointing we can dereference the pointer by using `*` again.  \nPointers are also stored in memory so they also have addresses so it is possible to output them as well. &pnumber warning by compiler because expected a pointer but it is a pointer to a pointer of itn so cast to void\\*???  \n<Callout type=\"warning\">\nYou should never dereference an uninitialized pointer as if you assign it a value it could go anywhere. You could maybe overwrite data or even cause the program to crash!\n</Callout>  \n```c\n#include <stdio.h>\nint main()\n{\nint var = 5;\nint* p_var = &var;\nprintf(\"var=%d and it's address is %p\\n\", var, (void*)&var);\nprintf(\"p_var=%p and it's address is %p and the value it points to is %d\"\n, (void*)p_var, (void*)&p_var, *p_var);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Void pointers", "path": "../pages/digitalGarden/cs/c/pointers.mdx"}, "page_content": "A void pointer can store an address of any type and can not be dereferenced as it doesn't know the size of the type it is pointing to so you must first cast it to another pointer type if you want to do so.  \n```c\n#include<stdio.h>\nint main()\n{\nint a = 10;\nvoid *ptr = &a;\nprintf(\"Address of a is %p\\n\", ptr);\nprintf(\"Value of a is %d\", *((int*)ptr));\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Const pointers", "path": "../pages/digitalGarden/cs/c/pointers.mdx"}, "page_content": "There are 3 ways you can use the `const` keyword with pointers all having different results.  \nWhen `const` is written before the type it defines a pointer to a constant value meaning we cant change the value via dereferencing. If the variable the pointer points isn't defined as a constant then the value can still be changed.  \n```c\n#include<stdio.h>\nint main()\n{\nint val = 10;\nconst int* pointer = &val;\n// *pointer = 3;  this does not work\nval = 4; // this however still does\npointer = &val;\nprintf(\"%d\", *pointer); // 4\n}\n```  \nWhen `const` is written in between the type and identifier you can not change the address the pointer points to, however you can still the change the value of the variable as this has no effect on the address.  \n```c\n#include<stdio.h>\nint main()\n{\nint val = 10;\nint* const pointer = &val;\nint otherVal = 3;\n// *pointer = &otherVal;  this does not work\n*pointer = 5; // this however still works\nprintf(\"%d\", *pointer); // 4\n}\n```  \nYou can then also combine these two concepts.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Pointers", "Header 2": "Double pointers (pointers to pointers)", "path": "../pages/digitalGarden/cs/c/pointers.mdx"}, "page_content": "You can in theory also go further then double but its just becomes a mess and shouldn't be done.  \nA common use case for double pointers is if you want to preserve the Memory-Allocation or Assignment even outside of a function call.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n\nvoid foo(int **p)\n{\nint a = 5;\n*ptr = &a;\n}\n\nint main(void)\n{\nint *p = NULL;\np = malloc(sizeof(int));\n*p = 42;\nfoo(&p);\nprintf(\"%d\\n\", *p); // 5 not 42\nfree(p);\np = NULL;\n\nreturn 0;\n}\n```  \nanother common use case is when working with strings.  \n```c\nint wordsInSentence(char **s) {\nint w = 0;\nwhile (*s) {\nw++;s++;\n}\nreturn w;\n}\n\nint main(void)\n{\nchar *word = \"foo\";\nchar **sentence;\n\nsentence = malloc(4 * sizeof *sentence); // assume it worked\nsentence[0] = word;\nsentence[1] = word;\nsentence[2] = word;\nsentence[3] = NULL;\n\nprintf(\"total words in my sentence: %d\\n\", wordsInSentence(sentence));\n\nfree(sentence);\nfree(word);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to C", "Header 2": "History", "path": "../pages/digitalGarden/cs/c/introduction.mdx"}, "page_content": "The C programming language was originally developed at Bell Labs (like so many other things) by Dennis Ritchie in 1972 to construct utilities running on Unix. Later on, it was also used to re-implement the entire kernel of the Unix operating system and has till this day been the kernel language for Unix. In 1989 C was standardized by ANSI (American National Standards Institute) to so-called ANSI C, also known as C89. Later on, in the same year, it was then adopted by the International Organization for Standardization (ISO) to create the so-called Standard C. Over the years ISO has published new standards corresponding to the years in which they were published: C90, C99, C11, C17 and now they are working on C2x.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to C", "Header 2": "Running a C Program", "path": "../pages/digitalGarden/cs/c/introduction.mdx"}, "page_content": "When writing a C program there are 4 phases of programming:  \n- Editing: Writing and modifying your code.\n- Compiling: This part is split into 2 phases:\n- Preprocessing: The code can still be modified by the compiler.\n- Compilation: The compiler checks the syntax and semantics of your code and makes sure everything is in order. The compiler then translates the code to assembly language which is then further translated into actual machine code/instructions. These machine instructions are then stored in object files that have either the extension `.obj` or `.o`.\n- Linking: The goal of this phase is to get the program into its final form for execution. The linker combines the object modules with additional libraries needed by the program to create the entire executable file which can then be run.\n- Running: This is the final phase and is self-explanatory.  \nFor more about the compilation and linking phase check out this [article](https://medium.com/@bdov_/what-happens-when-you-type-gcc-main-c-a4454564e96d).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to C", "Header 2": "Running a C Program", "Header 3": "First C Program", "path": "../pages/digitalGarden/cs/c/introduction.mdx"}, "page_content": "```c filename=\"helloWorld.c\"\n#include \"stdio.h\"\n\nint main(void)\n{\nprintf(\"Hello World\"); // prints \"Hello World\"\nreturn 0;\n}\n```  \nTo then compile and run our \"Hello World\" we can use for example the GNU Compiler Collection (gcc).  \n```bash\nfoo@bar:~$ gcc -std=c11 -pedantic -pedantic-errors -Wall -Wextra -g -o helloWorld helloWorld.c\nfoo@bar:~$ helloWorld.exe\nHello World\n```  \nThe options mean the following:  \n- `-std=c11` use C11 standard or can you use `-std=c89`, `-std=c99`, `-ansi`.\n- `-pedantic` use strict ISO C warnings.\n- `-pedantic-errors` use strict ISO C errors.\n- `-Wall` shows all warnings.\n- `-Wextra` turn on extra warnings.\n- `-g` activates the debugger.\n- `-o` the name of the executable file.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to C", "Header 2": "Commenting", "path": "../pages/digitalGarden/cs/c/introduction.mdx"}, "page_content": "Comments are the same as in many other programming languages like Java (inspired by C/C++). A single-line comment starts with `//` and is written over a line. A multi-line comment is written between `/* ... */` and can span over multiple lines.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to C", "Header 2": "Include", "path": "../pages/digitalGarden/cs/c/introduction.mdx"}, "page_content": "The `#include <stdio.h>` is a preprocessor directive that tells the compiler that we want something done in the preprocessing phase. All preprocessor directives start with a `#`. The \"include\" instruction tells the compiler to include the contents of the \"stdio.h\" file. You might notice it has the `.h` extension which means it is a header file. Header files define information about functions, so-called function prototypes which describe a function so the functions name, its arguments etc. The file we are including stands for standard input output which is part of the C standard library and we use the `printf` function of that file to write to the standard output, which is by default the console.  \nWhen specifying the file to be included you can either write it between double quotes or angle brackets. The difference between these two forms is subtle but important. If a header file is included using < >, the preprocessor will search a predetermined directory path to locate the header file (the folder for the standard library). If the header file is enclosed in \"\", the preprocessor will look first for the header file in the same directory as the source file and then in the other folders.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "To create a variable you first need a name. A variable name must start with a letter or underscore and then be followed by any combination of letters, underscores or digits as long as the name in the end isn't a reserved word like \"int\". Secondly you need a type which defines how the data is stored in memory for example `int` is an integer. When writing `int x;` you are declaring a variable which reserves the space in memory to hold the later on assigned value as it knows the amount of bytes the data type of the variable needs. Initializing a variable is giving it an initial value. This can be done as part of the declaration like for example `int a = 12;`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Basic Data Types", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "[A basic list of c data types](https://en.wikipedia.org/wiki/C_data_types#Main_types), how much memory they take up but this is very computer and compiler dependant. Closly tied to the amount of memory is of course the value range of a variable. These ranges can be checked by including [limits.h](https://pubs.opengroup.org/onlinepubs/007904975/basedefs/limits.h.html) for integer values and [float.h](https://pubs.opengroup.org/onlinepubs/007904975/basedefs/float.h.html) for float values (part of the standard library).  \nSome interesting things to note are:  \n- You can assign values using hex so `int x = 0xFFFFFF` is possible.\n- You can use scientific notation to assign values so `float x = 1.7e4` is possible.\n- You can add short, long, signed and unsigned to numerical values. `short` **might** make the types memory usage smaller, `long` **might** make it larger. `signed` is by default so has no real effect, `unsigned` means its range is only positive values and includes 0. Unless it is `int` itself the word int can be omitted so `long int` and `long` are the same. For some reason you can also do `long long` and `short short` who knows why?\n- If you want specific sized data types you can include from the standard library [stdint.h](https://pubs.opengroup.org/onlinepubs/009696899/basedefs/stdint.h.html) as to why to this doesn't exist for floats you can [read here](https://www.reddit.com/r/cpp/comments/34d7b6/why_do_we_have_intn_t_but_no_equivalent_for/).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Enums", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "An enum is a data type that only allows specific values. These values are under the hood mapped to integer constants with the first being mapped to 0 then the next is 0++ etc. if nothing else is specified.  \n```c\n#include <stdio.h>\n\nint main(void)\n{\nenum planets {mercury, venus, earth, mars, jupiter, saturn, uranus};\nenum planets home = earth;\nprintf(\"Our home is the %d. planet from the sun.\\n\", home+1);\n\nenum days {monday=1, tuesday, wednesday, thursday, friday, saturday=10, sunday};\nprintf(\"Level of motivation on a monday is %d and on a tuesday %d.\\n\", monday, tuesday);\nprintf(\"On saturday it is %d and sunday %d\\n.\", saturday, sunday);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Boolean Types", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "To have variables with boolean values in C we can use the `_Bool` data type which can have the values 0 (false) or 1 (true).  \n```c\n#include <stdio.h>\n\nint main(void)\n{\n_Bool x = 1;\n_Bool y = 0;\nif(x) {\nprintf(\"This will print!\");\n} if (!y)\n{\nprintf(\"This will also print!\");\n}\n}\n```  \nAnother way would be to use an Enum with a typedef, this takes advantage of Enums being constant integer values under the hood.  \n```c\n#include <stdio.h>\ntypedef enum { FALSE, TRUE } Boolean;\n\nint main(void)\n{\nBoolean x = TRUE;\nBoolean y = 0;\nif (x) {\nprintf(\"This will print!\");\n}\nif (!y) {\nprintf(\"This will also print!\");\n}\n}\n```  \nFrom C99 onwards you can also `#include <stdbool.h>`.  \n```c\n#include <stdio.h>\n#include <stdbool.h>\n\nint main(void)\n{\nbool x = true;\nbool y = 0;\nif (x) {\nprintf(\"This will print!\");\n}\nif (!y) {\nprintf(\"This will also print!\");\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Format Specifiers", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "There are lots format specifiers for outputting different data types. You can also use format specifiers to do cool things like adding leading or trailing zeros or only showing a certain amount of decimal points.  \n<Image\nsrc=\"/cs/cFormatSpecifiers.png\"\ncaption=\"A list of format specifiers in C.\"\nwidth={800}\n/>  \n```c\n#include <stdio.h>\n\nint main(void)\n{\nprintf(\"Characters: %c %c \\n\", 'a', 65);\nprintf(\"Preceding with blanks: %10d \\n\", 1977);\nprintf(\"Preceding with zeros: %010d \\n\", 1977);\nprintf(\"Some different radices: %d %x %o %#x %#o \\n\", 100, 100, 100, 100, 100);\nprintf(\"floats: %.2f %+.0e %E \\n\", 3.1416, 3.1416, 3.1416);\nprintf(\"Width trick: %*d \\n\", 20, 10);\nprintf(\"%s\", 0 ? \"true\" : \"false\");\nreturn 0;\n}\n```  \nYou can find more details in the [documentation of printf](https://www.cplusplus.com/reference/cstdio/printf/).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Visibility", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "All identifiers (variables, functions, classes etc.) must be defined before they can be used . Depending on where the identifier is defined they identifier has has a different visibility. Identifiers in the same block must be ambiguous and are visible in the inner blocks. An identifier from an outer block can be redefined in an inner block and can therefore be shadowed.  \n```c\n#include <stdio.h>\nint main(void)\n{\nint x = 6\n{\nint x = 9;\n{\nint x = 10;\nprintf(\"%d\", x) // 10\n}\nprintf(\"%d\", x) // 9\n}\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Visibility", "Header 3": "Global", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "If you define a variable outside all blocks then it is part of the global scope and exists as long as the program runs and can be accessed between multiple files by including the header file where it is defined and adding the `extern` keyword before it.  \n```c filename=\"main.c\"\n#include <stdio.h>\n\nint x = 5; // global\n\nint main(void)\n{\nprintf(\"%d\", x) // 5\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Visibility", "Header 3": "Static", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "By adding the `static` keyword to the global variable we can limit it's visibility to just this file.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Dynamically Allocated Memory", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "We have seen above that a lot of objects are only available inside their blocks once the block is finished they are removed. These objects are stored on the stack. If we create an object in a function we can return it and still work with it, however in C the object is copied on return which can very bad for performance if the object is very large.  \nObjects can also be stored statically meaning they are available as long as the program runs.  \n```c\n#include<stdio.h>\nint inc()\n{\nstatic int count = 0;\ncount++;\nreturn count;\n}\n\nint main()\n{\nprintf(\"%d \", inc()); // 1\nprintf(\"%d \", inc()); // 2\nreturn 0;\n}\n```  \nThe last possibility is using dynamically allocated memory. In C you can not define an array with a certain size at runtime, if we would want to do something like that we would need dynamic memory allocation. To be able to use this in C you must include `stdlib.h`. To go back on our problem of returning a created object from a function we can create the object on the stack and then just return the address of the object.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Dynamically Allocated Memory", "Header 3": "Malloc", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "The `malloc()` function, short for \"memory allocation\", is used to dynamically allocate a single large block of memory with the specified size and returns a pointer to the block.  \n:::warning\nWhen the memory is no longer needed you should release it using the `free()` function as you are otherwise using unnecessary memory. If you call free multiple times you can cause unexpected behavior which is why you should also set the pointer to NULL.\n:::  \n:::warning\nMalloc does not initialize the memory!\n:::  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n\nint main()\n{\nint* ptr;\nint n, i;\n\nprintf(\"Enter number of elements:\");\nscanf(\"%d\",&n);\nprintf(\"Entered number of elements: %d\\n\", n);\n\n// Dynamically allocate memory using malloc()\nptr = (int*)malloc(n * sizeof(int)); // returns void*\n\nif (ptr) {\nprintf(\"Memory successfully allocated using malloc.\\n\");\n\nfor (i = 0; i < n; ++i) {\nptr[i] = i + 1;\n}\n\nprintf(\"The elements of the array are: \");\nfor (i = 0; i < n; ++i) {\nprintf(\"%d, \", ptr[i]);\n}\n}\n\nfree(ptr); // free the memory!!\nptr = NULL;\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Dynamically Allocated Memory", "Header 3": "Calloc", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "The `calloc()` function, short for \"contiguous allocation\", is very similiar to the malloc function however it dynamically allocates the specified number of blocks of memory of the specified type. The most important difference however is that it initializes each block with a default value of '0'.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n\nint main()\n{\nint* ptr;\nint n, i;\n\nprintf(\"Enter number of elements:\");\nscanf(\"%d\",&n);\nprintf(\"Entered number of elements: %d\\n\", n);\n\n// Dynamically allocate memory using calloc()\nptr = (int*)calloc(n, sizeof(int));\n\nif (ptr) {\nprintf(\"Memory successfully allocated using calloc.\\n\");\n\nfor (i = 0; i < n; ++i) {\nptr[i] = i + 1;\n}\n\nprintf(\"The elements of the array are: \");\nfor (i = 0; i < n; ++i) {\nprintf(\"%d, \", ptr[i]);\n}\n}\n\nfree(ptr); // free the memory!!\nptr = NULL;\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Constant Values", "Header 3": "Define", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "With `#define` you can define key value pairs that will be substituted in the preprocessing phase. Important is that you don't write a type, an equal or a semicolon!  \n```c\n#include <stdio.h>\n#define PI 3.14\n\nint main(void)\n{\nprintf(\"%f\",PI);\nreturn 0;\n}\n```  \nYou can also conditionally define variables depending on certain compiler arguments or environment variables.  \n```c\n#include <stdio.h>\n\n#define X 2\n\n#if X == 1\n#define Y 1\n#elif X==2\n#define Y 2\n#else\n#define Y 3\n#endif\n\nint main(void)\n{\nprintf(\"%d\",Y);\nreturn 0;\n}\n```  \nYou can also execute certain code by checking if something is defined or not.  \n```c\n#include <stdio.h>\n#define UNIX 1\n\nint main()\n{\n#ifdef UNIX\nprintf(\"UNIX specific function calls go here.\\n\");\n#endif\nprintf(\"C has some weird things.\\n\");\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Constant Values", "Header 3": "Const", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "In C90 the `const` keyword was added which does not allow the value of a variable to change, making it read-only. Using const is much more flexible then define as allows you to use a data type and it is also better for performance.  \n```c\n#include <stdio.h>\n\nint main(void)\n{\nconst int PI = 3.14;\nprintf(\"%f\",PI);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Operators", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "Has the same operators as in many other languages and also work the same so not gonna go into detail. The only interesting ones to go into are below.  \n<Image\nsrc=\"/cs/cOperatorPrecedence.png\"\ncaption=\"The operator precedence in C. The lower the number the higher the precedence.\"\nwidth={700}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Operators", "Header 3": "Casting", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "conversion between different types can happen automatically (implicit) or has to be done explicit.  \nfor example double to flaot is implicit as no data is lost however double to int data is lost so it ahs to be done explicit and the decimal points are truncated  \n(int) 25.1 + (int) 27.435", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Operators", "Header 3": "Sizeof", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "The sizeof operator is very simple and just outputs how many bytes a data type or variable takes up.  \n```c\nint x = 3;\n\nprintf(\"An int takes up %ld bytes on my computer and a double %ld\", sizeof(x), sizeof(double)); // 4 and 8\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Command-line Arguments", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "When compiling you can pass arguments to the main function. The first parameter `argc` is the argument count, the second parameter `argv` is the argument vector which is an array of strings. So in other words it is an array of character arrays or an array of character pointers.  \n```c filename=\"main.c\"\n#include <stdio.h>\n\nint main(int argc, char *argv[])\n{\nprintf(\"argc=%d\\n\", argc);\n\n// the first argument is the name of the executable\nprintf(\"exe name=%s\\n\", argv[0]);\n\nfor (int i = 1; i < argc; i++)\n{\nprintf(\"argv[%d]=%s\\n\", i, argv[i]);\n}\n\nreturn 0;\n}\n```  \nTo then pass arguments you can do the following  \n```bash\nfoo@bar:~$ gcc -std=c11 -pedantic -pedantic-errors -Wall -Wextra -g -o argvExample main.c\nfoo@bar:~$ argvExample.exe arg1 arg2\nargc=3\nexe name=./argvExample\nargv[1]=arg1\nargv[2]=arg2\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Variables and Data Types", "Header 2": "Inputting Data", "path": "../pages/digitalGarden/cs/c/variablesDataTypes.mdx"}, "page_content": "The `stdio.h` file contains the `scanf()` function which reads input from the standard input stream \"stdin\", which by default is the console. The function can read and parse the input using the provided format specifier. Important to know is that it uses whitespaces to tokenize the input.  \n```c filename=\"main.c\"\n#include \"stdio.h\"\n\nint main(void)\n{\nchar str[100];\nint i;\nprintf(\"Enter a word followed by a space and a number: \");\n// provide pointers to where to store the values (remember str is actually a pointer to the first element)\nint tokensRead = scanf(\"%s %d\", str, &i);\n\nprintf(\"%d tokens were read str: %s  and i: %d\", tokensRead,str, i);\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "path": "../pages/digitalGarden/cs/c/structures.mdx"}, "page_content": "In C structures defined using the `struct` keyword are a very important concept as they allow for grouping of elements very similarly to classes in other languages they just don't include functions. For example a date, month, day, year. can then create variables as type struct date. memory is allocated 3 variables inside. can access member variables with. so `today.year` for example can also assign initialcompound literal can assign values after initilation like (struct date) `{1,2,3}` or specify the specific values with .month=9for only one time thing. can initialize structs like arrays with `{7,2,2015}`. or just the frist 2 or can do `{.month=12}`", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "Header 2": "Unnamed structs", "path": "../pages/digitalGarden/cs/c/structures.mdx"}, "page_content": "Unnamed structures can be used if you know that you only need one instance of it at all times which can be useful for constants.  \n```c\nstruct /* No name */ {\nfloat x;\nfloat y;\n} point;\n\npoint.x = 42;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "Header 2": "Array of structs", "path": "../pages/digitalGarden/cs/c/structures.mdx"}, "page_content": "Can then do all the normal things you would expect to be able to do with an array.  \n```c\nstruct Student\n{\nint rollNumber;\nchar studentName[10];\nfloat percentage;\n};\nstruct Student studentRecord[5];\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "Header 2": "Nested structs", "path": "../pages/digitalGarden/cs/c/structures.mdx"}, "page_content": "A nested structure in C is a structure within structure. One structure can be declared inside another structure in the same way structure members are declared inside a structure.  \n```c\nstruct Date\n{\nint day;\nint month;\nint year;\n};\nstruct Time\n{\nint hours;\nint minutes;\nint seconds;\n};\nstruct DateTime\n{\nstruct Date date;\nstruct Time time;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "Header 2": "Pointers to structs", "path": "../pages/digitalGarden/cs/c/structures.mdx"}, "page_content": "You can have pointers to struct variables. The important thing to know here is that there is a shorthand for accessing the data by usign the `-\\>` operator.  \n```c\n#include<stdio.h>\n\nstruct dog\n{\nchar name[10];\nchar breed[10];\nint age;\nchar color[10];\n};\n\nint main()\n{\nstruct dog my_dog = {\"tyke\", \"Bulldog\", 5, \"white\"};\nstruct dog *ptr_dog;\nptr_dog = &my_dog;\n\nprintf(\"Dog's name: %s\\n\", (*ptr_dog).name); // instead of having to do this\nprintf(\"Dog's breed: %s\\n\", ptr_dog->breed); // you can do this\nprintf(\"Dog's age: %d\\n\", ptr_dog->age);\nprintf(\"Dog's color: %s\\n\", ptr_dog->color);\n\n// changing the name of dog from tyke to jack\nstrcpy(ptr_dog->name, \"jack\");\n\n// increasing age of dog by 1 year\nptr_dog->age++;\n\nprintf(\"Dog's new name is: %s\\n\", ptr_dog->name);\nprintf(\"Dog's age is: %d\\n\", ptr_dog->age);\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Structures", "Header 2": "typdef", "path": "../pages/digitalGarden/cs/c/structures.mdx"}, "page_content": "The `typedef` keyword is used in C to assign alternative names to existing datatypes. This can be especially powerfull when combined with structs.can be used to give a type a new name. so typedef unsigned char BYTE; BYTE can then be used as an allias. this can become very powerful with structs.  \n```c\n#include <stdio.h>\n\ntypedef struct Point\n{\ndouble x;\ndouble y;\n} Point; // can have the same name\n\nstruct date\n{\nunsigned short day;\nunsigned short month;\nunsigned int year;\n};\ntypedef struct date Date;\n\ntypedef unsigned char byte;\n\nint main(void)\n{\nPoint origin = {0, 0};\nstruct date today = {1, 4, 2022};\nDate tomorrow = {2, 4, 2022};\nbyte intSize = sizeof(int);\n\nprintf(\"The origin is: (%f/%f)\\n\", origin.x, origin.y);\nprintf(\"Today is %d/%d/%d\\n\", today.day, today.month, today.year);\nprintf(\"Tommorrow is %d/%d/%d\\n\", tomorrow.day, tomorrow.month, tomorrow.year);\nprintf(\"On my computer an int takes up %d bytes.\\n\", intSize);\n\nreturn 0;\n}\n```  \n```bash filename=\"output\"\nThe origin is: (0.000000/0.000000)\nToday is 1/4/2022\nTommorrow is 2/4/2022\nOn my computer an int takes up 4 bytes.\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "path": "../pages/digitalGarden/cs/c/strings.mdx"}, "page_content": "String are stored and can be handled as arrays of chars which is why you often hear character array instead of string. In C the compiler adds at the end of each string literal the null character, '\\0' (not to be confused with NULL) so it knows where the string ends. This also means that the length of a string is always one longer then you might think it is. To get the length of a string you can implement your own function or use the built in function `strlen` provided in `string.h`.  \n```c\n#include <stdio.h>\n#include <string.h>\n\nsize_t getStringLength(char *str)\n{\nsize_t count;\nfor (count = 0; str[count] != '\\0'; ++count)\n;\nreturn count;\n}\n\nint main()\n{\nchar a[6] = {'h', 'e', 'l', 'l', 'o', '\\0'};\nchar b[] = {'h', 'e', 'l', 'l', 'o', '\\0'};\nchar c[] = \"hello\"; // string literal\nchar d[] = {\"hello\"};\nchar e[50] = \"hello\"; // to long\nchar f[5] = \"hello\";  // to short, '\\0' is not added so carefull...\n\nprintf(\"%s length=%ld strlen=%ld\\n\", a, getStringLength(a), strlen(a));\nprintf(\"%s length=%ld strlen=%ld\\n\", b, getStringLength(b), strlen(b));\nprintf(\"%s length=%ld strlen=%ld\\n\", c, getStringLength(c), strlen(c));\nprintf(\"%s length=%ld strlen=%ld\\n\", d, getStringLength(d), strlen(d));\nprintf(\"%s length=%ld strlen=%ld\\n\", e, getStringLength(e), strlen(e));\nprintf(\"%s length=%ld strlen=%ld\\n\", f, getStringLength(f), strlen(f));\nreturn 0;\n}\n```  \n```bash filename=\"output\"\nhello length=5 strlen=5\nhello length=5 strlen=5\nhello length=5 strlen=5\nhello length=5 strlen=5\nhello length=5 strlen=5\nhellohello length=10 strlen=10\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "Header 2": "Wide characters", "path": "../pages/digitalGarden/cs/c/strings.mdx"}, "page_content": "A wide character `wchar_t` is similar to char data type, except that it takes up twice the space and can take on much larger values as a result. A char can take 256 values which corresponds to entries in the ascii table. On the other hand, wide char can take on 65536 values which corresponds to unicode values. So whenever you see a function that has to do with strings or characters and there is a w then it most lightly has to do with wide characters. You can create wide string litrals just like normal string literal and then by adding the L prefix.  \n```c\n#include <stdio.h>\n\nint main()\n{\nwchar_t w = L'A';\nwchar_t *p = L\"Hello!\";\nsize_t length = wcslen(p); // can't use strlen\nprintf(\"Wide character: %c\\n\", w);\nprintf(\"Wide string with length %ld: %S\\n\", length, p);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "Header 2": "String functions", "Header 3": "Converting", "path": "../pages/digitalGarden/cs/c/strings.mdx"}, "page_content": "```c\n#include <stdio.h>\n#include <stdlib.h>\n\nint main()\n{\nchar number[] = \"123.321 hello\";\nchar *restL;\nchar *restF;\n\nint i = atoi(number); // returns 0 if fails\nlong l = atol(number);\ndouble f = atof(number);\n\nstrtol(number, &restL, 10); // third parameter is base\nstrtod(number, &restF);\n\nprintf(\"%s = %d\\n\", number, i);\nprintf(\"%s = %ld\\n\", number, l);\nprintf(\"%s = %f\\n\", number, f);\nprintf(\"%s\\n\", restL);\nprintf(\"%s\\n\", restF);\nreturn 0;\n}\n```  \n```bash filename=\"output\"\n123.321 hello = 123\n123.321 hello = 123\n123.321 hello = 123.321000\n.321 hello\nhello\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "Header 2": "String functions", "Header 3": "Comparing", "path": "../pages/digitalGarden/cs/c/strings.mdx"}, "page_content": "```c\n#include <stdio.h>\n#include <string.h>\n\nint main(void)\n{\nchar str1[] = \"Hello Earth\";\nchar str2[] = \"Hello World\";\n\nprintf(\"Compare %s with %s = %d\\n\", str1, str2, strcmp(str1, str2));\nprintf(\"Compare first 5 letters of %s with %s = %d\\n\", str1, str2, strncmp(str1, str2, 5));\n\nreturn 0;\n}\n```  \n```bash filename=\"output\"\nCompare Hello Earth with Hello World = -18\nCompare first 5 letters of Hello Earth with Hello World = 0\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Strings", "Header 2": "String functions", "Header 3": "Analyzing", "path": "../pages/digitalGarden/cs/c/strings.mdx"}, "page_content": "```c\n#include <stdio.h>\n#include <ctype.h>\n\nint main()\n{\nchar c = 'c';\n\nprintf(\"isLower: %d\\n\", islower(c));\nprintf(\"isUpper: %d\\n\", isupper(c));\nprintf(\"isAlpha: %d\\n\", isalpha(c));        // a-Z or A-Z adds 2 if lower, 1 if upper\nprintf(\"isDigit: %d\\n\", isdigit(c));        // 0-9\nprintf(\"isAlphanumeric: %d\\n\", isalnum(c)); // a-Z or A-Z or 0-9 adds 2 if lower, 1 if upper\nprintf(\"isWhitespace: %d\\n\", isspace(c));\nreturn 0;\n}\n```  \n```bash filename=\"output\"\nisLower: 1\nisUpper: 0\nisAlpha: 2\nisDigit: 0\nisAlphanumeric: 2\nisWhitespace: 0\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "In programs we care about two types of time:  \n- Real-time or also known as calendar time is a fixed time from the calendar and is used for timestamps etc.\n- Process time is the amount of time a process takes which is used for measuring performance etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Calendar Time", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "Calendar time is always in UTC(Coordinated Universal Time)/GMT(Greenwich Mean Time) no matter in which timezone the program is run. In C and most other programming languages, time is handled internally as a signed integer which is based on [Unix/epoche/POSIX time](https://en.wikipedia.org/wiki/Unix_time). The value 0 is 01.01.1970 00:00, also commonly referred to as the birth time of Unix. The integer value of time is then the number of seconds before or after this time. So if the time integer value is 60 then it corresponds to 01.01.1970 00:01.  \nThe function `time_t time(time_t *tloc);` returns the time as the number of seconds since Unix time. If tloc is non-NULL, the return value is also stored in tloc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Calendar Time", "Header 3": "Getting and Setting System Time", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "With the following functions you can get and set the time as well as a timezone of the program:  \n- `int gettimeofday(struct timeval *restrict tv, struct timezone *restrict tz);`\n- `int settimeofday(const struct timeval *tv, const struct timezone *tz);`  \nWith the corresponding structs:  \n```c\nstruct timeval {\ntime_t      tv_sec;     /* seconds */\nsuseconds_t tv_usec;    /* microseconds */\n};\nstruct timezone {\nint tz_minuteswest;     /* minutes west of Greenwich */\nint tz_dsttime;         /* type of DST correction */\n};\n```  \nHowever, the timezone structure is obsolete and should therefore normally be specified as NULL.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Calendar Time", "Header 3": "Locale", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "Locale is a set of parameters that defines the user's language, region and how numbers, dates and times etc. should be represented. You can set the program's locale with the `char *setlocale(int category, const char *locale);`. The category could be `LC_ALL`, `LC_TIME`, `LC_NUMERIC` amongst others. If you pass LC_ALL and NULL you can read the current locale.  \nSome possible locales could be: en_US, de_DE, de_CH etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Calendar Time", "Header 3": "Broken-Down Time", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "Calendar time represents absolute time as elapsed time since the epoch. This is convenient for computation but has no relation to the way people normally think of calendar time. By contrast, broken-down time is a binary representation of calendar time separated into year, month, day, and so on. Broken-down time values are not useful for calculations, but they are useful for printing human-readable time information. A broken-down time value is always relative to a choice of time zone.  \n```c\nstruct tm {\nint tm_sec; // seconds [0..60]\nint tm_min; // minutes [0..59]\nint tm_hour; // hours [0..23]\nint tm_mday; // day of the month [1..31]\nint tm_mon; // month [0..11]\nint tm_year; // years since 1900\nint tm_wday; // weekday [0..6], Sunday = 0\nint tm_yday; // day of year [0..365]\nint tm_isdst; // daylight saving time flag\n}\n```  \nWith `struct tm *gmtime(const time_t *t);` you can convert a time_t to a broken-down time in UTC. Or you can use `struct tm *localtime(const time_t *t);` to convert a time_t to a broken-down time in local time.  \nWith `time_t mktime(struct tm *timeptr);` you can convert broken-down time into time_t since the Epoch. tm_wday and tm_yday components of the structure are ignored", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Calendar Time", "Header 3": "Time and Strings", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "When not computing we want times to be in a human-readable form which is why there are lots of functions to convert times to strings:  \n- `char *ctime(const time_t *timep);` returns a 26-byte string representing the time in locale and DST: \"Wed Jun 8 14:22:34 2011\".\n- `char *asctime(const struct tm *t);` does the same as `ctime()` without changing the timezone or DST.  \nOften we also want to be able to format the string for this we can use `size_t strftime(char *restrict s, size_t max, const char *restrict format, const struct tm *restrict tm);` which formats the time and stores it in s. For example, \"%Y-%m-%dT%H:%M:%SZ\" becomes \"2018-12-29T12:17:25Z\" where Z is only if UTC.  \nThere are also cases for example when getting input from users we want to parse a string to a time for this we can use `char *strptime(const char *restrict s, const char *restrict format, struct tm *restrict tm);` which parses the string s using the format to the time and stores it in tm.  \nSome key formats being:  \n| Format                 | Description                             |\n| ---------------------- | --------------------------------------- |\n| %a / %A                | abbreviated / full weekday name         |\n| %b / %B                | abbreviated / full month name           |\n| %d / %m / %w / %W / %u | day / month / weekday / week as decimal |\n| %y / %y                | year with or without century            |\n| %H / %M / %S           | hour / minute / second as decimal       |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Calendar Time", "Header 3": "Timezones", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "Timezones define the time and region of a program. Timezones are stored in the environment variable `TZ` for Unix systems. To find out urs you can type:  \nWith the `void tzset(void);` you can initialize the timezone which in turn sets the following global variables:  \n- `extern char *tzname[2];` zone and DST zone.\n- `extern long timezone;` difference to UTC in seconds.\n- `extern int daylight;` non-null if DST.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Process Time", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "Process time is the cpu time that a process has used since creation. It consists of two parts: User CPU time which is the amount of time spent in user mode, also commonly referred to as virtual time. And System cpu time which is the amount of time spent in kernel mode whilst doing for example system calls.  \n`clock_t times(struct tms *t);` returns the number of clock ticks that have elapsed since an arbitrary point in the past and stores the current process times in the parameter.\n`clock_t clock(void);` returns an approximation of processor time used so far by the program.  \n```c\nstruct tms {\nclock_t tms_utime;  /* user time */\nclock_t tms_stime;  /* system time */\nclock_t tms_cutime; /* user time of children */\nclock_t tms_cstime; /* system time of children */\n};\n```  \n```c\n#include <time.h>\n#include <sys/times.h>\n#include <stdio.h>\n#include <unistd.h>\n\nint main() {\nlong tckPerSec = sysconf(_SC_CLK_TCK);\nlong clockPerSec = CLOCKS_PER_SEC;\nprintf(\"Ticks per second: %ld\\n\", tckPerSec);\nprintf(\"Clocks per second: %ld\\n\", clockPerSec);\nstruct tms sinceStart;\nsleep(10);\nclock_t clock = times(&sinceStart);\nprintf(\"Since start: User(%ld), System(%ld)\\n\", sinceStart.tms_utime, sinceStart.tms_stime);\nprintf(\"%ld\", clock/clockPerSec);\nreturn 0;\n}\n```  \n```c\n#include <sys/types.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <stdio.h>\n#include <sys/times.h>\n\nint main(int argc, char *argv[]) {\nstruct tms t0_buf;\nclock_t t0 = times(&t0_buf);\npid_t pid = fork();\nif (pid == 0) {\nexecvp(argv[1], argv + 1); // does not return\n}\nfflush(stdout);\nwait(NULL); // wait for child to exit\nstruct tms t1_buf;\nclock_t t1 = times(&t1_buf);\n\nlong ticks_per_s = sysconf(_SC_CLK_TCK);\nprintf(\"\\nreal \\t%ld\\nuser \\t%lfs\\nsys \\t%lfs\\n\",\n(t1 - t0) / ticks_per_s,\n(t1_buf.tms_cutime - t0_buf.tms_cutime) / (double) ticks_per_s,\n(t1_buf.tms_cstime - t0_buf.tms_cstime) / (double) ticks_per_s);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Process Time", "Header 3": "Unix Timers and Sleeping", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "With a timer, you can send notifications to a process after a certain time. Sleeping suspends the process or thread for a given time.  \n#### Interval Timer  \nWith `int setitimer(int which, const struct itimerval *restrict new_value, struct itimerval *restrict old_value);` that notifies in regular intervals. You can use the following which values:  \n- `ITIMER_REAL` counts down in real-time and sends a SIGALRM signal.\n- `ITIMER_VIRTUAL` counts down in user CPU time and sends a SIGVTALRM signal.\n- `ITIMER_PROF` counts down in system CPU time and sends a SIGPROF signal. Can be used together with the one above.  \n```c\nstruct itimerval {\nstruct timeval it_interval; /* Interval for periodic timer */\nstruct timeval it_value;    /* Time until next notification */\n};\n\nstruct timeval {\ntime_t      tv_sec;         /* seconds */\nsuseconds_t tv_usec;        /* microseconds */\n};\n```  \nYou can check the remaining time with `int getitimer(int which, struct itimerval *curr_value);`.  \n```c\n#include <signal.h>\n#include <stdio.h>\n#include <string.h>\n#include <sys/time.h>\n\nvoid timer_handler(int signum)\n{\nstatic int count = 0;\nprintf(\"timer expired %d times\\n\", ++count);\n}\n\nint main()\n{\nstruct itimerval timer;\nsignal(SIGVTALRM, timer_handler);\n\n/* Configure the timer to expire after 250 msec... */\ntimer.it_value.tv_sec = 0;\ntimer.it_value.tv_usec = 250000;\n/* ... and every 250 msec after that. */\ntimer.it_interval.tv_sec = 0;\ntimer.it_interval.tv_usec = 250000;\n/* Start a virtual timer. It counts down whenever this process is\nexecuting. */\nsetitimer(ITIMER_VIRTUAL, &timer, NULL);", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Time Measuring", "Header 2": "Process Time", "Header 3": "Unix Timers and Sleeping", "path": "../pages/digitalGarden/cs/c/systemProgramming/timeMeasuring.mdx"}, "page_content": "/* Do busy work. */\nwhile (1)\n;\n}\n```  \n#### One-time Timer - Alarm  \nWith `unsigned int alarm(unsigned int seconds);` you can set up a one-time occurring timer. When the timer expires the `SIGALRM` signal is sent. An existing timer can be removed with `alarm(0);`  \n#### Timer Precision  \nDepending on CPU use the process might only start just after being notified. This however does not influence the next signal (no delaying).  \n#### Suspend Processes - Sleeping  \nYou can suspend a process with `unsigned int sleep(unsigned int seconds);`. Internally it is implemented with `int nanosleep(const struct timespec *req, struct timespec *rem);`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "For a more in-depth explanation of how sockets work check out the [distributed systems section]().", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Sockets Interface", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "Sockets are like pipes an IPC mechanism between 2 processes that can either be on the same host or network.  \nThe communication domain of the socket defines how the socket address looks and whether the communication is local or over the network. There are the following types of sockets domains:  \n- `AF_UNIX` and `AF_LOCAL` for IPC on the same host.\n- `AF_INET` for IPv4 and `AF_INET6` for IPv6.  \nThere are two types of sockets:  \n- Stream Sockets, `SOCK_STREAM` which are reliable and bidirectional byte streams. Reliable meaning that bytes are received in the same order as they are sent and are guaranteed to arrive or an error is sent.\n- Datagram Sockets, `SOCK_DGRAM` are messaged based. They are not reliable and are connectionless. Meaning a connection between the client and server is not established and that the messages can be sent multiple times or lost and that the order of arrival is non-deterministic.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Sockets Interface", "Header 3": "System Calls", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "Just like all other IPC mechanisms, sockets use system calls to communicate:  \n- `int socket(int domain, int type, int protocol);` creates an endpoint for communication and returns a file descriptor to that endpoint. By default, protocol can be 0.\n- `int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);` \"Assigns a name to a socket\". Name being the addr and addrlen the size, in bytes, of the address structure\n- `int listen(int sockfd, int backlog);` marks the socket to be used to accept incoming connection requests.\n- `int accept(int sockfd, struct sockaddr *restrict addr, socklen_t *restrict addrlen);` extracts the first connection request from queue of pending connections for the listening socket. Creates and returns a new connected socket to be further used. The original socket is unaffected by this call.\n- `int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);` connects the socket to the address specified by addr.\n- `int close(int fd);` to close the socket and connection.  \nTo then read and write you can then use the same system calls as with pipes.  \n<Image\nsrc=\"/cs/sysSocketDiagram.png\"\ncaption=\"Flow diagram of how to create a server and client using sockets in C and the system calls used.\"\nwidth={500}\n/>  \nWhen working with Datagram sockets you don't use the listen, accept and connect system calls because the sockets are connectionless. You also don't use the read and write system calls. Instead, you use the following:  \n- `ssize_t recvfrom(int socket, void *restrict buffer, size_t length, int flags, struct sockaddr *restrict address, socklen_t *restrict address_len);`\n- `ssize_t sendto(int socket, const void *message, size_t length, int flags, const struct sockaddr *dest_addr, socklen_t dest_len);`  \n<Image\nsrc=\"/cs/sysDatagramSocketDiagram.png\"\ncaption=\"Diagram showing how datagram sockets freely send and receive messages without a connection.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Sockets Interface", "Header 3": "Addresses", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "When referring to addresses all functions take the `struct sockaddr` which is a generic structure for addresses and can parse the addresses with help of the family attribute (`AF_INET` or `AF_UNIX`).  \n```c\nstruct sockaddr {\nunsigned short   sa_family;\nchar             sa_data[14];\n};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Unix Domain Sockets", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "Unix domain sockets enable efficient communication between processes on the same host. Unix domain sockets support both stream-oriented sockets with the TCP protocol and datagram sockets with the UDP protocol (reliable compared to over the internet).  \nThe address for Unix domain sockets is a file and can be specified with the structure below. When you bind a Unix domain socket a file is created at the specified path for the socket including permissions for the owner and group (to be able to connect and write you need write and execute access). When the socket is no longer required, you must manually delete the socket file.  \n```c\nstruct sockaddr_un {\nunsigned short int sun_family; /*AF_UNIX*/\nchar sun_path[UNIX_PATH_MAX]; /*pathname*/\n};\n```  \nWhen repeatedly binding to the same path the errno `ADDRINUSE` is set.  \nExample using UDP:  \n```c filename=\"unix_server\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <string.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <sys/un.h>\n\n#define SOCK_PATH \"unix_sock_server\"\n\nint main(void)\n{\n\nint server_sock, err;\nstruct sockaddr_un server_sockaddr, client_sockaddr;\nmemset(&server_sockaddr, 0, sizeof(struct sockaddr_un));\nchar buf[256];\nmemset(buf, 0, 256);\n\nserver_sock = socket(AF_UNIX, SOCK_DGRAM, 0);\nif (server_sock == -1)\n{\nprintf(\"Failed to create server socket\");\nexit(1);\n}\n\nserver_sockaddr.sun_family = AF_UNIX;\nstrcpy(server_sockaddr.sun_path, SOCK_PATH);\nerr = bind(server_sock, (struct sockaddr *)&server_sockaddr, sizeof(server_sockaddr));\nif (err == -1)\n{\nprintf(\"Failed to bind server socket\");\nclose(server_sock);\nexit(1);\n}\n\nprintf(\"waiting to recvfrom...\\n\");\nint client_len;\nint bytes_read = recvfrom(server_sock, buf, 256, 0, (struct sockaddr *)&client_sockaddr, &client_len);\nif (bytes_read == -1)\n{\nprintf(\"Failed to read from client to server\");\nclose(server_sock);\nexit(1);\n}\nprintf(\"DATA RECEIVED = %s\\n\", buf);", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Unix Domain Sockets", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "close(server_sock);\nreturn 0;\n}\n```  \n```c filename=\"unix_client\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <string.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <sys/un.h>\n\n#define SERVER_PATH \"unix_sock_server\"\n\nint main(void)\n{\nint client_sock, err;\nstruct sockaddr_un server_sockaddr;\nmemset(&server_sockaddr, 0, sizeof(struct sockaddr_un));\nchar buf[256];\n\nclient_sock = socket(AF_UNIX, SOCK_DGRAM, 0);\nif (client_sock == -1)\n{\nprintf(\"Failed to create client socket\");\nexit(1);\n}\n\nserver_sockaddr.sun_family = AF_UNIX;\nstrcpy(server_sockaddr.sun_path, SERVER_PATH);\n\nstrcpy(buf, \"Hello from client\");\nprintf(\"Sending data...\\n\");\nerr = sendto(client_sock, buf, strlen(buf), 0, (struct sockaddr *)&server_sockaddr, sizeof(server_sockaddr));\nif (err == -1)\n{\nprintf(\"Failed to write from client to server\");\nclose(client_sock);\nexit(1);\n}\nprintf(\"Data sent!\\n\");\nclose(client_sock);\n\nreturn 0;\n}\n```  \nExample using TCP:  \n```c filename=\"unix_server\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <sys/un.h>\n\n#define SOCK_PATH \"unix_sock_server\"\n\nint main(void)\n{\nint server_sock, client_sock, err;\nstruct sockaddr_un server_sockaddr;\nstruct sockaddr_un client_sockaddr;\nmemset(&server_sockaddr, 0, sizeof(struct sockaddr_un));\nmemset(&client_sockaddr, 0, sizeof(struct sockaddr_un));\nchar buf[256];\n\n// create socket\nserver_sock = socket(AF_UNIX, SOCK_STREAM, 0);\nif (server_sock == -1)\n{\nprintf(\"Failed to create server socket\");\nexit(1);\n}\n\n// create address\nserver_sockaddr.sun_family = AF_UNIX;\nstrcpy(server_sockaddr.sun_path, SOCK_PATH);\nerr = bind(server_sock, (struct sockaddr *)&server_sockaddr, sizeof(server_sockaddr));\nif (err == -1)\n{\nprintf(\"Failed to bind server socket\");\nclose(server_sock);\nexit(1);\n}\n\nerr = listen(server_sock, 1);\nif (err == -1)\n{\nprintf(\"Failed to listen on server socket\");\nclose(server_sock);\nexit(1);\n}\nprintf(\"socket listening...\\n\");", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Unix Domain Sockets", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "err = listen(server_sock, 1);\nif (err == -1)\n{\nprintf(\"Failed to listen on server socket\");\nclose(server_sock);\nexit(1);\n}\nprintf(\"socket listening...\\n\");\n\n// accept incoming connection, store client address\nint client_len;\nclient_sock = accept(server_sock, (struct sockaddr *)&client_sockaddr, &client_len);\nif (client_sock == -1)\n{\nprintf(\"Failed to accept client on server socket\");\nclose(server_sock);\nclose(client_sock);\nexit(1);\n}\n\nprintf(\"waiting to read...\\n\");\nint bytes_read = read(client_sock, buf, sizeof(buf));\nif (bytes_read == -1)\n{\nprintf(\"Failed to read from client to server\");\nclose(server_sock);\nclose(client_sock);\nexit(1);\n}\nprintf(\"DATA RECEIVED = %s\\n\", buf);\n\nmemset(buf, 0, 256); // empty buffer\nstrcpy(buf, \"Hello From Server\");\nprintf(\"Sending data...\\n\");\nerr = write(client_sock, buf, strlen(buf));\nif (err == -1)\n{\nprintf(\"Failed to write from server to client\");\nclose(server_sock);\nclose(client_sock);\nexit(1);\n}\nprintf(\"Data sent!\\n\");\n\nclose(server_sock);\nclose(client_sock);\nreturn 0;\n}\n```  \n```c filename=\"unix_client\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <unistd.h>\n#include <sys/types.h>\n#include <sys/socket.h>\n#include <sys/un.h>\n\n#define SERVER_PATH \"unix_sock_server\"\n#define CLIENT_PATH \"unix_sock_client\"\n\nint main(void)\n{\n\nint client_sock, err;\nstruct sockaddr_un server_sockaddr;\nstruct sockaddr_un client_sockaddr;\nmemset(&server_sockaddr, 0, sizeof(struct sockaddr_un));\nmemset(&client_sockaddr, 0, sizeof(struct sockaddr_un));\n\nchar buf[256];\n\nclient_sock = socket(AF_UNIX, SOCK_STREAM, 0);\nif (client_sock == -1)\n{\nprintf(\"Failed to create client socket\");\nexit(1);\n}\n\n// setup client address\nclient_sockaddr.sun_family = AF_UNIX;\nstrcpy(client_sockaddr.sun_path, CLIENT_PATH);\n// bind\nerr = bind(client_sock, (struct sockaddr *)&client_sockaddr, sizeof(client_sockaddr));\nif (err == -1)\n{\nprintf(\"Failed to bind client socket\");\nclose(client_sock);\nexit(1);\n}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Unix Domain Sockets", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "// setup server address\nserver_sockaddr.sun_family = AF_UNIX;\nstrcpy(server_sockaddr.sun_path, SERVER_PATH);\n// connect to server\nerr = connect(client_sock, (struct sockaddr *)&server_sockaddr, sizeof(server_sockaddr));\nif (err == -1)\n{\nprintf(\"Failed to connect client to server\");\nclose(client_sock);\nexit(1);\n}\n\nstrcpy(buf, \"Hello from client\");\nerr = write(client_sock, buf, strlen(buf));\nif (err == -1)\n{\nprintf(\"Failed to write from client to server\");\nclose(client_sock);\nexit(1);\n}\nprintf(\"Data sent!\\n\");\n\n// read data from server\nprintf(\"Waiting to recieve data...\\n\");\nmemset(buf, 0, sizeof(buf)); // empty buffer\nerr = read(client_sock, buf, sizeof(buf));\nif (err == -1)\n{\nprintf(\"Failed to read from server to client\");\nclose(client_sock);\nexit(1);\n}\nprintf(\"DATA RECEIVED = %s\\n\", buf);\n\nclose(client_sock);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Unix Domain Sockets", "Header 3": "Socketpairs", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "You can create socketpairs which can then be used very similarly to pipes with the `int socketpair(int domain, int type, int protocol, int sv[2]);` function.  \n```c\nvoid child(int socket) {\nconst char hello[] = \"hello parent, I am child\";\nwrite(socket, hello, sizeof(hello)); /* NB. this includes nul */\n/* go forth and do childish things with this end of the pipe */\n}\n\nvoid parent(int socket) {\n/* do parental things with this end, like reading the child's message */\nchar buf[1024];\nint n = read(socket, buf, sizeof(buf));\nprintf(\"parent received '%.*s'\\n\", n, buf);\n}\n\nvoid socketfork() {\nint fd[2];\nstatic const int parentsocket = 0;\nstatic const int childsocket = 1;\npid_t pid;\n\n/* 1. call socketpair ... */\nsocketpair(PF_LOCAL, SOCK_STREAM, 0, fd);\n\n/* 2. call fork ... */\npid = fork();\nif (pid == 0) { /* 2.1 if fork returned zero, you are the child */\nclose(fd[parentsocket]); /* Close the parent file descriptor */\nchild(fd[childsocket]);\n} else { /* 2.2 ... you are the parent */\nclose(fd[childsocket]); /* Close the child file descriptor */\nparent(fd[parentsocket]);\n}\nexit(0); /* do everything in the parent and child functions */\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Internet Sockets", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "Internet sockets work very similarly to the Unix domain sockets. Stream sockets are based on the TCP protocol. Datagram sockets are based on the UDP protocol.  \n```c\nsvaddr.sin6_port = htons(PORT_NUM);\ninet_pton(AF_INET6, argv[1], &svaddr.sin6_addr)\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Internet Sockets", "Header 3": "Network Byte Order", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "Unfortunately, not all computers store the bytes for a multibyte value like an IP address in the same order. There are two ways to store this value:  \n- Little Endian: Low-order byte is stored on the starting address (A) and higher order byte is stored on the next address (A + 1).\n- Big Endian: High-order byte is stored on the starting address (A) and lower order byte is stored on the next address (A + 1).  \nNetwork byte order uses the big endian system. Library functions that work with IP addresses need to be converted to this system for which there are the following functions:  \n- `uint32_t ntohl(uint32_t netlong);` Network to host.\n- `uint16_t ntohs(uint16_t netshort);` Network to host.\n- `uint32_t htonl(uint32_t hostlong);` Host to Network.\n- `uint16_t htons(uint16_t hostshort);` Host to Network.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Internet Sockets", "Header 3": "Address Structures", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "To store IP addresses there are the following structs:  \n```c\n// IPv4\nstruct in_addr {\nuint32_t s_addr; // Network Byte Order\n};\nstruct sockaddr_in {\nsa_family_t sin_family; // AF_INET\nin_port_t sin_port; // Network Byte Order\nstruct in_addr sin_addr; // Internet Adresse\n};\n// IPv6\nstruct in6_addr {\nunsigned char s6_addr[16]; // IPv6 address\n};\nstruct sockaddr_in6 {\nsa_family_t sin6_family; // AF_INET6\nin_port_t sin6_port; // Port Nummer\nuint32_t sin6_flowinfo; // IPv6 Flow Info\nstruct in6_addr sin6_addr; // IPv6 Adresse\nuint32_t sin6_scope_id; // Scope ID\n};\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Internet Sockets", "Header 3": "Loopback and Wildcard Addresses", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "IPv4 Loopback 127.0.0.1 and Wildcard 0.0.0.0: INADDR_LOOPBACK, INADDR_ANY\nIPv6 Loopback (::1) and Wildcard (::): IN6ADDR_LOOPBACK_INIT, IN6ADDR_ANY_INIT", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Internet Sockets", "Header 3": "Converting Addresses", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "Converts dot notation to binary:  \n`int inet_pton(int af, const char *restrict src, void *restrict dst);`  converts the character string src into a network address structure in the af address family, then copies the network address structure to dst. The af argument must be either AF_INET or AF_INET6. dst is written in network byte order.  \nConverts binary to dot notation:  \n`const char *inet_ntop(int af, const void *restrict src, char *restrict dst, socklen_t size);` converts the network address structure src in the af address family into a character string.  The resulting string is copied to the buffer pointed to by dst, which must be a non-null pointer.  The caller specifies the number of bytes available in this buffer in the argument size", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sockets", "Header 2": "Internet Sockets", "Header 3": "Host Lookup", "path": "../pages/digitalGarden/cs/c/systemProgramming/sockets.mdx"}, "page_content": "```c\nstruct addrinfo {\nint              ai_flags;\nint              ai_family;\nint              ai_socktype;\nint              ai_protocol;\nsocklen_t        ai_addrlen;\nstruct sockaddr *ai_addr;\nchar            *ai_canonname;\nstruct addrinfo *ai_next;\n};\n```  \n`int getaddrinfo(const char *restrict node, const char *restrict service, const struct addrinfo *restrict hints, struct addrinfo **restrict res);`\nGiven node and service, which identify an Internet host and a service, getaddrinfo() returns one or more addrinfo structures, each of which contains an Internet address. After use the struct should be freed again with `void freeaddrinfo(struct addrinfo *result)`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "Threads are similar to process and allow a program to do multiple things at once by having multiple threads in it. A key difference between threads and process is however that threads share the same global memory and just have their private stack for local variables and function calls and are therefore not as expensive as process which have the big overhead of creating an entire new memory space. This is why threads are also often called lightweight processes.  \nExchanging information between process can be quiet tricky and costly because the parent and child don't share memory. However, threads share the following things between each other amongst other things  \n- PID and Parent PID\n- Open file descriptors\n- Signal handlers\n- Global memory  \nEach thread however does receive the following things for itself  \n- Thread ID\n- Signal Mask\n- Errno variable\n- Stack  \nTo work with threads you can use the Pthreads API which are also known as POSIX threads, sounds familiar... which is provided with the gcc compiler. Important to know here is that Pthreads functions don't return -1 on failure like many other functions in the standard library. Instead they return 0 on success and add the errno on failure. To be able to use the Pthreads API you need to pass the `-pthread` flag to the gcc compiler.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "Header 2": "Creating Threads", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "To create a thread you will need to use the following function  \n```c\nint pthread_create(pthread_t *restrict thread,\nconst pthread_attr_t *restrict attr,\nvoid *(*start_routine)(void *),\nvoid *restrict arg);\n```  \nThe first parameter is an integer that is used as an output parameter and is used to identify the thread in your operating system.\nThe second parameter is for specific attributes for the thread, by passing NULL you can use the default.\nThe third parameter is the function that the thread will execute once it is started.\nThe fourth parameter is used to pass arguments to the function and must be cast to a void pointer. If you want to pass multiple arguments, you would use a pointer to a struct.  \n```c\nint pthread_join(pthread_t thread, void **retval);\n```  \nA call to the join function blocks the calling thread until the thread with ID as the first parameter is terminated. You can also store the return value of the thread with the second parameter.  \nThreads can terminate in multiple ways  \n- By calling `void pthread_exit(void *retval);`\n- By letting the thread function return.\n- By calling exit which will terminate the process including all its threads.  \nInterestingly of the main thread calls pthread_exit all the other threads will continue to execute otherwise they all automatically terminate when main returns.  \n```c\n#include <stdio.h>\n#include <pthread.h>\n\n#include <stdio.h>\n#include <pthread.h>\n\nvoid *foo()\n{\nprintf(\"foo ID: %ld\\n\", pthread_self());\npthread_exit(NULL);\n}\n\nint main(void)\n{\nprintf(\"main ID: %ld\\n\", pthread_self());\npthread_t foo_t;\npthread_create(&foo_t, NULL, foo, NULL);\n\npthread_join(foo_t, NULL);\nprintf(\"done\");\n\nreturn 0;\n}\n\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "Header 2": "Passing Values", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "When creating the pthread you can pass the arguments using the fourth parameter.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <pthread.h>\n\ntypedef struct Point\n{\ndouble x;\ndouble y;\n} Point;\n\nvoid *printPoint(void *args)\n{\nPoint p = *((Point *)args);\nprintf(\"(%f, %f)\", p.x, p.y);\npthread_exit(NULL);\n}\nint main(void)\n{\npthread_t pid;\nPoint p = {2, 10};\n\npthread_create(&pid, NULL, printPoint, &p);\npthread_join(pid, NULL);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "Header 2": "Returning Values", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "You can return values from a thread with the pthread_exit function. The values you return should be on the heap otherwise you will run into problems.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <pthread.h>\n\ntypedef struct Point\n{\ndouble x;\ndouble y;\n} Point;\n\nvoid *createPoint()\n{\nPoint *p = malloc(sizeof(Point));\np->x = 3;\np->y = 7;\npthread_exit((void *)p);\n}\nint main(void)\n{\npthread_t pid;\nPoint p;\nvoid *res;\npthread_create(&pid, NULL, createPoint, NULL);\npthread_join(pid, &res);\n\np = *((Point *)res);\nfree(res);\nres = NULL;\nprintf(\"(%f, %f)\", p.x, p.y);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "Header 2": "Further Operations on Threads", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "There are further operations that you can use with threads for example:  \n- `int pthread_equal(pthread_t t1, pthread_t t2);` which compares two threads to see whether they are the same.\n- `int pthread_detach(pthread_t thread);` by default a thread runs in joinable mode. A joinable thread will not release its resources even after termination until some other thread calls `pthread_join()` with its ID. A Detached thread automatically releases its allocated resources on exit. No other thread needs to join it. Therefore there is also no way to determine its return value.\n- `int pthread_cancel(pthread_t thread);` sends a cancellation request to the thread. Whether the target thread reacts to the cancellation request depends on its cancelability state and type.\n- `int pthread_kill(pthread_t thread, int sig);` sends the signal sig to thread.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "Header 2": "Synchronization", "Header 3": "Mutex", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "Threads can have a mutual state which is useful but you need to be careful when accessing and changing this state. A critical section is a code block that uses a mutual variable and should only be executed atomically, so at once, by one thread, so that the result does not depend on the interleaving. A mutex/lock can guarantee this behavior to avoid race conditions. For more on this, there is an entire [section dedicated to concurrent programming](../../concurrentProgramming/locking).  \nMutex variables are of the type `pthread_mutex_t` and need to be initialized before they are used with `pthrad_mutex_t m = PTHREAD_MUTEX_INITIALIZER;` or the `int pthread_mutex_init(pthread_mutex_t *restrict mutex, const pthread_mutexattr_t *restrict attr);` function. To then use the lock you can use the following functions:  \n- `int pthread_mutex_lock(pthread_mutex_t *mutex);` Acquires the lock. If the lock is already in use then this function blocks till it can acquire the lock. If called repeatedly in the same thread that already has the lock a deadlock occurs.\n- `int pthread_mutex_unlock(pthread_mutex_t *mutex);` Releases the lock. If called in a thread that has already released a lock will return an error.\n- `int pthread_mutex_trylock(pthread_mutex_t *mutex);` Tries to acquire the lock. If it can't it does not block. Instead, it returns `EBUSY`.\n- `int pthread_mutex_timedlock(pthread_mutex_t *restrict mutex, const struct timespec *restrict abstime);` Tries to acquire the lock and waits for a maximum of abstime. If it couldn't get acquire the lock in the given time it returns `ETIMEDOUT`.  \nWith the timespec struct looking like this:  \n```c\nstruct timespec {\ntime_t tv_sec; // seconds\nlong tv_nsec; // nanoseconds\n};\n```  \nAn example of using locks:  \n```c\n#include <stdio.h>\n#include <string.h>\n#include <pthread.h>\n#include <stdlib.h>\n#include <unistd.h>\n\npthread_t tid[2];\nint counter;\npthread_mutex_t lock; // lock object", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "Header 2": "Synchronization", "Header 3": "Mutex", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "pthread_t tid[2];\nint counter;\npthread_mutex_t lock; // lock object\n\nvoid *incrementCounter()\n{\nfor (unsigned long i = 0; i < 1000; i++)\n{\npthread_mutex_lock(&lock);\ncounter++;\npthread_mutex_unlock(&lock);\n}\npthread_exit(NULL);\n}\n\nint main(void)\n{\nif (pthread_mutex_init(&lock, NULL) != 0) // init lock\n{\nprintf(\"\\n mutex init failed\\n\");\nreturn 1;\n}\nint i = 0;\nwhile (i < 2)\n{\nint err = pthread_create(&(tid[i]), NULL, &incrementCounter, NULL);\nif (err != 0)\nprintf(\"\\ncan't create thread :[%s]\", strerror(err));\ni++;\n}\n// wait for threads to finish\npthread_join(tid[0], NULL);\npthread_join(tid[1], NULL);\nprintf(\"Counter: %d\", counter);\npthread_mutex_destroy(&lock); // clean up lock\n\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "Header 2": "Synchronization", "Header 3": "Condition Variables", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "In C we can also use [condition variables](../../concurrentProgramming/conditionVariables) to further synchronize concurrent programs. Condition variables in C work just like in Java. `pthread_cond_signal` is the equivalant of `notify()` and `pthread_cond_broadcast` of `notifyAll()`.  \n```c\npthread_mutex_t lock; // init with PTHREAD_MUTEX_INITIALIZER\npthread_cond_t count_nonzero; // init with PTHREAD_COND_INITIALIZER\nunsigned int count;\n\ndecrementCount()\n{\npthread_mutex_lock(&lock);\nwhile (count == 0)\npthread_cond_wait(&count_nonzero, &lock);\ncount = count - 1;\npthread_mutex_unlock(&lock);\n}\n\nincrementCount(){\npthread_mutex_lock(&lock);\ncount++;\nif(count == 0){\npthread_cond_broadcast(&count_nonzero);\n// pthread_cond_signal(&count_nonzero)\n}\npthread_mutex_unlock(&lock);\n}\n````", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Threads and Synchronization with C", "Header 2": "Errno", "path": "../pages/digitalGarden/cs/c/systemProgramming/threads.mdx"}, "page_content": "To be able to set errno in a thread-safe manner we need to use a makro from the pthread library  \n```c\n# define errno (*__errno_location())\n*__errno_location() = EBUSY;\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to System Programming", "path": "../pages/digitalGarden/cs/c/systemProgramming/introduction.mdx"}, "page_content": "An operating system is a program that acts as an interface between the user and the computer hardware and controls the execution of all kinds of programs. Operating systems have kernels which are responsible for scheduling, starting and ending programs but also provide other functionalities like networking or file systems.  \n<Image\nsrc=\"/cs/sysUnixStructure.png\"\ncaption=\"The structure and layers of a Unix system.\"\nwidth={400}\n/>  \nCPUs run in two modes, [kernel and user modes](https://docs.microsoft.com/en-us/windows-hardware/drivers/gettingstarted/user-mode-and-kernel-mode) also well explained [here](https://blog.codinghorror.com/understanding-user-and-kernel-mode/). Only certain actions can be done in the kernel mode which is why there needs to be a way to interact between these two layers. This is what [system calls](https://www.ionos.com/digitalguide/server/know-how/what-are-system-calls/) are for. They allow a program to do things it can't do in the user mode like send information to the hardware etc.  \n<Image\nsrc=\"/cs/sysSystemCalls.png\"\ncaption=\"How user programs interact with the kernel which in turn interacts with the hardware through system calls.\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Introduction to System Programming", "Header 2": "POSIX", "path": "../pages/digitalGarden/cs/c/systemProgramming/introduction.mdx"}, "page_content": "POSIX stands for Portable Operating System Interface and is an API specification for system calls to maintain compatibility among operating systems. Therefore, any software that conforms to POSIX should be compatible with other operating systems that adhere to the POSIX standards. This is the reason, as to why most of the tools we use on Unix-like operating systems behave almost the same as it is pretty much POSIX compliant.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Processes", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "A program is a file containing instructions on how to create a process. Processes are instances of a running program. A program can have multiple processes and a process can be executing the same program.  \nThe kernel sees a process as a piece of user-space memory with program code, constants and initial values for variables. The kernel also keeps track of processes by storing its Process ID (PID), its virtual memory space, open file descriptors and signal handlers amongst other things.  \nThe PID is a positive integer and is used to identify a process in the system. The \"init\" process which is responsible for starting the unix operating system has the PID=1. Every process has a parent process apart from init so processes form a tree structure with init as its root. You can check this out with the `pstree` command. If a process dies it get's adopted by init so has the PID=1.  \n`pid_t getpid(void)` of current process.\n`pid_t getppid(void)` of parent process.  \n<Image\nsrc=\"/cs/sysProcessTree.png\"\ncaption=\"A visual representation of the process tree in the console.\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Processes", "Header 3": "Memory layout", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "The memory of each process is split into segments: program code, initialized data, none initialized data(bss), stack and the heap.  \n<Image\nsrc=\"/cs/sysMemoryLayout.png\"\ncaption=\"How the memory is split up in a process.\"\nwidth={600}\n/>  \nUnix and many other operating systems use virtual memory for performance reasons. When using virtual memory only a so called Page is loaded into the RAM the rest is offloaded. Along with the above mentioned data structure the kernel also keep a so called page table for each process which maps the virtual memory address space to the Page frame in the physical memory, RAM. Address spaces not in use are not mapped so if a process tries to access them you receive a so called segmentation fault (SIGSEGV).  \n<Image\nsrc=\"/cs/sysPageTable.png\"\ncaption=\"How the page table maps the virtual memory to the physical memory.\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Processes", "Header 3": "Stack and stack frames", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "Stack frames are parts of the stack which are allocated when a function is called for its arguments, local variables and CPU register copies of the external variables. If have worked with recursion you have maybe come across a \"Stackoverflow\" which can happen when the stack is full and no longer has any space.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Environment variables", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "You can create and access environment variables which are stored in the global variable `extern char **environ`. `char *getenv(const char *name)`, `int putenv(char *string)`", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Signals", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "Signals are forms of messages that go between processes. Most signals come from the kernel for example when there is input, an event or an exception occurred (division by 0 for example). A source process generates a signal until the destination process gets time from the scheduler the signal is pending. As soon as it is the process's turn the signal is delivered and it can just what to do. Either the process terminates, ignores the signal or handles it using a signal handler. Every signal has a symbol (in `signal.h`) and a number associated with the symbol for example an interrupt with CTRL+C causes the kernel to send a `SIGINT` signal. When referring to a signal you should always use its symbol as depending on the system it might have a different number. In most cases the IDs 1-31 are reserved for standard signals from the kernel and the next 32-64 are real-time signals.  \nTODOOOO\nstrsignal, psignal", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Signals", "Header 3": "Register signal handlers", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "You can register signal handlers using the `sighandler_t signal(int signum, sighandler_t handler)` function meaning your handler will look something like this `void handle(int signal)`. The return value of signal is the previously registered handler if somethign went wrong it will return `SIG_ERR`. There are some default handlers which can be used like the following, there is also `SIG_IGN` to ignore the signal. Often times however handlers just set a global flag `volatile int flag;`.  \n<Image\nsrc=\"/cs/sysSignalHandler.png\"\ncaption=\"How processes can communicate with each other using signals and signal handlers.\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Signals", "Header 3": "Send signals", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "You can send signals to other processes with `int kill(pid_t pid, int sig)`. If you use the pid=0 the signal is sent to all processes in the same group. If you use pid=-1 the signal is send to all processes it can apart from init. If you use sig=0 you can check if a signal can be sent.  \nWith `int raise(int sig)` you can send a signal to your own process which would be the same as `kill(getpid(), sig)`", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Signals", "Header 3": "Wait for signals", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "Often times you want a process to wait until it receives a signal which can be done with the `int pause(void)` function. A common thing to do after a pause would then be to check the global flags which might have been set by the signal handler.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Signals", "Header 3": "Signal masks", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "You can use masks to block certain signals from interupting a process. Signals stay pending until they are unblocked. A signal mask consists of sets of signals. You can crate a set with `int sigemptyset(sigset_t *set)` or `int sigfillset(sigset_t *set)`. You can then use `int sigaddset(const sigset_t *set, int sig)` or `int sigdelset(const sigset_t *set, int sig)` to either add or delete a signal. Once you have your signal set you can use it in the mask with `int sigprocmask(int how, const sigset_t *restrict set, sigset_t *restrict oldset)`. For the how you can use `SIG_BLOCK` which adds the signals from new to the mask, `SIG_UNBLOCK` to remove the signals from new or set it with `SIG_SETMASK`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Process lifecycle", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "With the `pid_t fork(void)` function a process can create a child process. When this is done instead of copying the entire stack, heap etc. (which would be very bad for performance especially since most child process just start another program as you will see below) the parent and child have a read only page in memory. And then id something needs to be changed the kernel makes a copy-on-write. The `void exit(int status)` function terminates a process and sends the status to any process that is suspended as it is waiting for it with `pid_t wait(int *wstatus)`. If you want a parent to wait for all its children you can use `while(wait(NULL) != -1) {}`.  \n![pageTableCopyOnWrite](/compSci/pageTableCopyOnWrite.png)  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/wait.h>\n#include <unistd.h>\n\nint main()\n{\nint id = fork(); // returns child PID for parent and for child is unassigned so 0.\nif (id == 0)\n{\nprintf(\"hello from child\\n\");\nsleep(3);\nexit(EXIT_SUCCESS);\n}\nelse\n{\nprintf(\"hello from parent\\n\");\nint status;\nwait(&status);\nprintf(\"child has terminated with %d\\n\", status);\n}\n\nprintf(\"Bye\\n\"); // without exit child would still execute this\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Processes and Signals", "Header 2": "Process lifecycle", "Header 3": "Execve and system", "path": "../pages/digitalGarden/cs/c/systemProgramming/processesSignals.mdx"}, "page_content": "With `int execve(const char *pathname, char *const argv[], char *const envp[])` you throw away all the previous program code, stack, heap etc and can load a new program in it's place. This function does not return.  \n![execve](/compSci/execve.png)  \nWith `int system(const char *command)` you can similiarly create a child process and execute and shell command. The system function takes care of all the hidden details of forking and exiting etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "The POSIX standard also defines functions/system calls for file I/O which are commonly found and used on unix systems. You have to remember that in unix everything is a file so these system calls aren't just used for text files but also multiple other things like devices, sockets etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "File descriptors", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "When a file is opened or created by a process the kernel assigns a position in an array unique to each process called the file descriptor. Each entry of this array contains a pointer to a file table which stores for each file, the file descriptor, file status flags, and offset. The file table does not itself contain the file, but instead has a pointer to another table, called the vnode table, which has vital information about the file, including its location in memory.  \nThe file descriptors are unique to a process but the integers may by reused by another process without referring to the same file or location within a file. By convention the following are however always the same  \n| File            | File Descriptor | POSIX Symbolic Constant in \"unistd.h\" |\n| --------------- | --------------- | ------------------------------------- |\n| Standard Input  | 0               | STDIN_FILENO                          |\n| Standard Output | 1               | STDOUT_FILENO                         |\n| Standard Error  | 2               | STDERR                                |  \n<Image\nsrc=\"/cs/sysFileDescriptors.png\"\ncaption=\"How files are represented and managed in Unix systems to ensure that multiple processes can access the same file without interfering with each other.\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "System Calls", "Header 3": "Opening and closing", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "You can open a file with `int open(const char *pathname, int flags, mode_t mode` from `fcntl.h` which will return the smallest int that’s not used in by the current processor as the file descriptor. Once you are done with the file you can `int close(int fd)` it which will detach the use of the file descriptor for a process. When a process terminates any open file descriptors are automatically closed by the kernel. If something goes wrong with opening a file it will return -1 and you can find the error saved under errno, a global variable.  \nPossible errors:  \n| Constant | Description                                                                       |\n| -------- | --------------------------------------------------------------------------------- |\n| EACCES   | the requested access to the file or directory is not allowed                      |\n| EISDIR   | file refers to a directory and the access requested involved writing              |\n| EMFILE   | the per-process limit on the number of open file descriptors has been reached     |\n| ENFILE   | the system-wide limit on the total number of open files has been reached.         |\n| ENOENT   | file not found and O_CREAT not speciefed                                          |\n| EROFS    | pathname refers to a file on a read-only filesystem andwrite access was requested |\n| ETXTBSY  | is busy                                                                           |  \n#### Flags  \nThe second argument consists of access, creation & status flags and is created by bitwise OR'ing ('|') the constants you want together.  \nAccess:  \n| Constant | Description                                                                           |\n| -------- | ------------------------------------------------------------------------------------- |\n| O_RDONLY | open for reading only                                                                 |\n| O_WRONLY | open for writing only                                                                 |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "System Calls", "Header 3": "Opening and closing", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "| O_RDONLY | open for reading only                                                                 |\n| O_WRONLY | open for writing only                                                                 |\n| O_RDWR   | open for reading and writing                                                          |\n| O_APPEND | append on each write                                                                  |\n| O_CREAT  | create file if it does not exist: REQUIRES mode                                       |\n| O_TRUNC  | truncate size to 0                                                                    |\n| O_EXCL   | is specified with O_CREAT, if already exists, then open() fails with the error EEXIST |\n| O_SYNC   |                                                                                       |  \n#### Modes  \nIf you used the flag O_CREAT then you must specify with the mode the permissions of the created file by bitwise OR'ing ('|') the constants you want together.  \n| Constant | Description |\n| -------- | ----------- |\n| S_IRUSR  | User-read   |\n| S_IWUSR  | User-write  |\n| S_IRGRP  | Group-read  |\n| S_IWGRP  | Group-write |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "System Calls", "Header 3": "Reading", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "The `ssize_t read(int fd, void *buf, size_t count)` function attempts to read up to count bytes from file descriptor fd into the buffer starting at buf. The read operation starts at the files offset, and increments it by the number of bytes read and returns the amount of bytes it read. If the file offset is at or past the end of file, no bytes are read, and read() returns zero.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "System Calls", "Header 3": "Writing", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "The `ssize_t write(int fd, const void *buf, size_t count)` function writes up to count bytes from the buffer starting at buf to the file referred to by the file descriptor fd. The number of bytes written may be less than count if, for example, there is insufficient space.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "System Calls", "Header 3": "Positioning", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "For each open file descriptor the kernel holds a file offset, the position where the next read or write will begin. With the `off_t lseek(int fd, off_t offset, int whence)` function you can change this offset. The value of whence must be one of the constants SEEK_SET, SEEK_CUR, or SEEK_END, to indicate whether the offset is relative to the beginning of the file, the current file position, or the end of the file.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "System Calls", "Header 3": "Truncating", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "With the `int ftruncate(int fd, off_t length)` function you can truncate the file to a size of precisely length bytes.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "System Calls", "Header 3": "Flushing", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "With the `int fsync(int fd)` function you can flush the buffers, or in other words synchronize the file states. This has the same effect as adding the O_SYNC flag when opening the file.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX File I/O", "Header 2": "Example", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixFileIO.mdx"}, "page_content": "```c\n#include <stdio.h>\n#include <fcntl.h>\n#include <unistd.h>\n#include <string.h>\n\nint main(void)\n{\nint fd = open(\"text.txt\", O_RDWR | O_CREAT | O_SYNC, S_IRUSR | S_IWUSR);\nchar *msg = \"Hello World\";\nint written = write(fd, msg, strlen(msg));\nprintf(\"wrote %d bytes to %d\\n\", written, fd);\n\nlseek(fd, -written, SEEK_CUR);\n\nchar read_msg[100];\nread(fd, read_msg, 100);\n\nprintf(\"read from file %d: \\\"%s\\\"\", fd, read_msg);\n\nreturn 0;\n}\n```  \n```bash filename=\"output\"\nwrote 11 bytes to 3\nread from file 3: \"Hello World\"\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "Interprocess comminucation - IPC is the subject of coomunicatiing, echanging data and synchronizing between to processes.  \nA kategorie of IPC are Datatransfers which use read and write system calls. The second category commincates via shared memory, without system calls and is therefore also faster.  \nDatatransfers can be byte streams, message based or use special pseduoterminals. Important with all these mechanisms is that a read operation is destructive. Meaning if data has been read then it is no longer available to the others. Synchronization is done automaticallly. If there is no data available then read operation blocks.  \nPipes, FIFOs, Stream sockets are unlimited byte streams which means that the number bytes does not matter.  \nMessage queues and datagram sockets are messaged based. Each read operation reads one message exactly how it was written. It is not possible to only read a part of a message or multiple at once.  \nShared Memory and memory mappings are fast but need to be synchronized. reading is however not destructive. Often semaphores are used.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "Header 2": "File locks", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "Work same as ReadWriteLock in Java.\ncoordiante file access. Read locks can be shared between multiples however if a process has a write lock then no other thread can have a read or write lock.\nflock and fcntl system calls???", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "Header 2": "Pipes", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "A pipe \"|\" is a form of redirection of standard output to some other destination that can be used in shells on Unix operating systems. So for example you can redirect the stdout of a command to a file or connect it to the stdin of another command. Pipes are unidirectional i.e data flows from left to right through the pipe. The command-line programs (right side) that do the further processing are referred to as filters. You can also use pipes programmatically in C for IPC.  \nread blocks, if pipe is closed returns 0/EOF.  \nJust a buffer in or file kernel memory with max capacity of 64KB. If a pipe is full write blocks until on the other end data is read.  \npipe puts 2 file descriptors into the passed array the first (index 0) being the read end of the pipe and the other file descriptor for the write end.  \nwhen finished with writing need to close it so read gets EOF and doesn't block indefinetly. If the pipe was closed on read side and the process still writes to the pipe the kernel sends a SIGPIPE signal. if the signal is ignored then write returns the error EPIPE.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n\nint main(void)\n{\nint fd[2]; // 0 = read, 1 = write end\npipe(fd);\n\nint id = fork();\n\nif (id == -1) { printf(\"Error when forking\"); return 1; }\nif (id == 0) // child process\n{\nclose(fd[0]); // close read end\nint x;\nprintf(\"CHILD: Input a number: \");\nscanf(\"%d\", &x);\nif (write(fd[1], &x, sizeof(x)) == -1) { printf(\"Error writing to pipe\"); return 3; }\nclose(fd[1]); // close write end when finished\n}\nelse // parent process\n{\nclose(fd[1]); // close write end\nint y;\nif (read(fd[0], &y, sizeof(y)) == -1) { printf(\"Error reading from pipe\"); return 3; }\nprintf(\"PARENT: You put in: %d\\n\", y);\nclose(fd[0]); // close read end when finished\n}\nreturn 0;\n}\n```  \n```bash filename=\"output\"\nCHILD: Input a number: 4\nPARENT: You put in: 4\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "Header 2": "Pipes", "Header 3": "Bidirectional Pipes", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "There can be scenarios where you want bidirectional communication between two processes using pipes. This can't be done with just one pipe for this you need two pipes between the processes.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n\nint main(void)\n{\n/* Send a number from parent to child, child processes the number and sends back to parent. */\nconst int READ_END = 0; const int WRITE_END = 1;\nint childToParent[2];\nint parentToChild[2];\nif(pipe(childToParent) == -1) { printf(\"Error creating pipe\"); return 1; }\nif(pipe(parentToChild) == -1) { printf(\"Error creating pipe\"); return 1; }\n\nint id = fork();\nif (id == -1)\n{\nprintf(\"Error when forking\");\nreturn 2;\n}\nif (id == 0) // child process\n{\nclose(childToParent[READ_END]);\nclose(parentToChild[WRITE_END]);\nint input;\nif (read(parentToChild[READ_END], &input, sizeof(input)) == -1)\n{\nprintf(\"Error when reading from parent to child\");\nreturn 3;\n}\nprintf(\"CHILD: Received: %d\\n\", input);\nint output = input * 2; // input gets doubled\nif (write(childToParent[WRITE_END], &output, sizeof(output)) == -1)\n{\nprintf(\"Error when writing from child to parent\");\nreturn 4;\n}\nprintf(\"CHILD: Sent: %d\\n\", output);\n}\nelse // parent process\n{\nclose(parentToChild[READ_END]);\nclose(childToParent[WRITE_END]);\nprintf(\"PARENT: Input a number: \");\nint input;\nscanf(\"%d\", &input);\nif (write(parentToChild[WRITE_END], &input, sizeof(input)) == -1)\n{\nprintf(\"Error when writing from parent to child\");\nreturn 5;\n}\nprintf(\"PARENT: Sent: %d\\n\", input);\nint output;\nif (read(childToParent[READ_END], &output, sizeof(output)) == -1)\n{\nprintf(\"Error when reading from child to parent\");\nreturn 6;\n}\nprintf(\"PARENT: Received: %d\\n\", output);\nclose(parentToChild[WRITE_END]);\nclose(childToParent[READ_END]);\n}\nreturn 0;\n}\n```  \n```bash filename=\"output\"\nPARENT: Input a number: 5\nPARENT: Sent: 5\nCHILD: Received: 5\nCHILD: Sent: 10\nPARENT: Received: 10\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "Header 2": "Pipes", "Header 3": "Simulating the \"|\" Pipe Operator", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "As you might have expected it is possible to simulate the pipe operator in the shell with pipes in C.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <sys/wait.h>\n\nint main(void)\n{\nconst int READ_END = 0;\nconst int WRITE_END = 1;\nint fd[2];\nif (pipe(fd) == -1)\n{\nprintf(\"Error creating pipe\");\nreturn 1;\n}\n\nint pingForkId = fork();\nif (pingForkId == -1)\n{\nprintf(\"Error when forking\");\nreturn 2;\n}\nif (pingForkId == 0) // child process\n{\ndup2(fd[WRITE_END], STDOUT_FILENO); // copies pipe write fd to stdout fd\nclose(fd[READ_END]);\nclose(fd[WRITE_END]); // can be closed as still a reference pointing to it\n\nexeclp(\"ping\", \"ping\", \"-c\", \"5\", \"google.com\", NULL); // current process code gets replaced by new process code. system() would create another child process.\n}\nint grepForkId = fork();\nif (grepForkId == -1)\n{\nprintf(\"Error when forking\");\nreturn 2;\n}\nif (grepForkId == 0) // child process\n{\ndup2(fd[READ_END], STDIN_FILENO); // copies pipe read fd to stdin fd\nclose(fd[READ_END]);\nclose(fd[WRITE_END]);\n\nexeclp(\"grep\", \"grep\", \"rtt\", NULL);\n}\n\nclose(fd[READ_END]);\nclose(fd[WRITE_END]);\n// Wait for children to terminate\nwaitpid(pingForkId, NULL, 0);\nwaitpid(grepForkId, NULL, 0);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "Header 2": "Pipes", "Header 3": "Synchronizing with Pipes", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "As mentioned above pipes don't need to transfer data, they can also just be used for synchronization between processes.  \n```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n\nint main(void)\n{\nint fd[2]; /* Process synchronization pipe */\n\nif (pipe(fd) == -1)\n{\nprintf(\"Error creating pipe\");\nreturn 1;\n}\n\nfor (int i = 0; i < 10; i++)\n{\nswitch (fork())\n{\ncase -1:\nprintf(\"Error when forking\");\nreturn 2;\ncase 0:           // child process\nclose(fd[0]); // close read end\n\n// child does some work and lets parent know when finished\nfor (int j = 0; j < 100000000; j++)\n{\n}\nprintf(\"Finished work in Process %d\\n\", i);\n// notifies parent that done by decrementing file descriptor count\nclose(fd[1]);\nexit(EXIT_SUCCESS);\ndefault:\nbreak; // parent continue with loop\n}\n}\nprintf(\"Finished creating all children\\n\");\nclose(fd[1]); // parent doesn't use write end\nint dummy;\n// blocks till all are finished and receives EOF\nif (read(fd[0], &dummy, 1) != 0)\n{\n\nprintf(\"Parent didn't get EOF\");\nreturn 3;\n}\nprintf(\"All finished\");\nreturn 0;\n}\n```  \n```bash filename=\"output\"\nFinished creating all children\nFinished work in Process 0\nFinished work in Process 1\nFinished work in Process 2\nFinished work in Process 4\nFinished work in Process 3\nFinished work in Process 5\nFinished work in Process 6\nFinished work in Process 7\nFinished work in Process 8\nFinished work in Process 9\nAll finished\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "Header 2": "FIFO files (Named Pipes)", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "pipes only work in the same hierarchy so between only between a parent and its child process or between children that share the same parent. We might want to be able to communicate between two processes that are not related. For this we have FIFOs which are a variation of pipes that work very similiarly to files, and also use a file which is why they are also often reffered to as named pipes. FIFOs work the same way as pipes so they also have unidrectional communication with \"first in first out\" semantic, hence the name.  \nYou need to create the FIFO with the `int mkfifo(const char *filepath, mode_t mode);` function just like a file, hence it also taking the same mode parameter as when working with files. Opening the FIFO with the `open()` system call in read-only mode or write-only blocks until a second process opens the same FIFO in the other mode. So the two ends of the pipe need to exist. However, you can not open a FIFO with the `O_RDWR` mode.  \n```c filename=\"fifo_read.c\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <errno.h>\n\nint main(void)\n{\nchar *fifoPath = \"myfifo\";\nif (mkfifo(fifoPath, S_IRUSR | S_IWUSR) == -1 && errno != EEXIST)\n{\nprintf(\"Error when creating FIFO file\\n\");\nreturn 1;\n}\n\nint fd = open(fifoPath, O_RDONLY); // blocks till write end is opened\nint output;\nif (read(fd, &output, sizeof(output)) == -1)\n{\nprintf(\"Error when reading from FIFO\");\nreturn 4;\n}\nprintf(\"PARENT: Received: %d\", output);\nremove(fifoPath);\nreturn 0;\n}\n```  \n```c filename=\"fifo_write.c\"\n#include <stdio.h>\n#include <stdlib.h>\n#include <unistd.h>\n#include <fcntl.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <errno.h>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "Header 2": "FIFO files (Named Pipes)", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "int main(void)\n{\nchar *fifoPath = \"myfifo\";\nif (mkfifo(fifoPath, S_IRUSR | S_IWUSR) == -1 && errno != EEXIST)\n{\nprintf(\"Error when creating FIFO file\\n\");\nreturn 1;\n}\nint fd = open(fifoPath, O_WRONLY); // blocks till other end is opened\nint input = 10;\nif (write(fd, &input, sizeof(input)) == -1)\n{\nprintf(\"Error when writing to FIFO\");\nreturn 3;\n}\nprintf(\"CHILD: Sent: %d\", input);\nremove(fifoPath);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "IPC with Pipes", "Header 2": "FIFO files (Named Pipes)", "Header 3": "Non-blocking FIFO", "path": "../pages/digitalGarden/cs/c/systemProgramming/ipcWithPipes.mdx"}, "page_content": "There might be cases where you don't want the open system call to block to avoid deadlocks. To avoid this you can add the `O_NONBLOCK` mode. Opening for read-only will succeed even if the write side hasn't been opened yet. However, opening for write only will return -1 and set `errno=ENXIO` unless the other end has already been opened.  \nUsing `O_NONBLOCK` does have an influence on reading and writing.  \nIf the buffer is empty then the read function returns -1 and sets `errno=EAGAIN`. If additionally the write end is already closed then EOF is returned.  \nIf the read end is not ready yet and the write function is used and fills the buffer then write return -1 and sets `errno=EAGAIN`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX Interprocess Communication", "Header 2": "POSIX Semaphores", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixIPC.mdx"}, "page_content": "You can find a detailed explanation of what a semaphore is [here](../../Concurrent%20Programming/9-synchronizers.md).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX Interprocess Communication", "Header 2": "POSIX Semaphores", "Header 3": "Named Semaphores", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixIPC.mdx"}, "page_content": "Named semaphores have a name and can be used by multiples process just like named pipes (FIFOs). The process that opens the semaphore but doesn't create it just needs to pass the first 2 arguments. Just like with mutexes there are in addition the functions `sem_trywait(sem_t *sem)` and `sem_timedwait(sem_t *sem, const struct timespec *abs_timeout);`.  \n```c\n#include <fcntl.h>\n#include <sys/stat.h>\n#include <semaphore.h>\n#include <stdio.h>\n\nint main(void)\n{\nchar *name = \"/my_semaphore\";                                // must start with \"/\"\"\nsem_t *sema = sem_open(name, O_CREAT, S_IRUSR | S_IRGRP, 2); // or/and O_EXCL\nsem_wait(sema);\n// sem_wait(sema); // blocks\nint current = 0;\nsem_getvalue(sema, &current);\nprintf(\"Decrease semaphore by 1, now: %d\\n\", current);\nsem_post(sema);\nsem_getvalue(sema, &current);\nprintf(\"Add semaphore by 1, now: %d\\n\", current);\nsem_close(sema);\nsem_unlink(name);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX Interprocess Communication", "Header 2": "POSIX Semaphores", "Header 3": "Unnamed Semaphores", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixIPC.mdx"}, "page_content": "Unnamed semaphores work the same way as named ones but they are in memory and can be accessed by processes and threads via shared memory. Instead of opening one you need to initialize it with the `int sem_init(sem_t *sem, int pshared, unsigned int value);` function and when you are finished with it remove it with `int sem_destroy(sem_t *sem);`. The pshared argument indicates whether this semaphore is to be shared between the threads of a process, or between processes. If pshared has the value 0, then the semaphore is shared between the threads of a process, and should be located at some address that is visible to all threads. If pshared is nonzero, then the semaphore is shared between processes, and should be located in POSIX shared memory.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "POSIX Interprocess Communication", "Header 2": "POSIX Shared Memory", "path": "../pages/digitalGarden/cs/c/systemProgramming/posixIPC.mdx"}, "page_content": "With POSIX shared memory you can have shared memory between processes without using files.  \n/// TODOOOOOO an example that actually works///  \n```c\n#include <stdio.h>\n#include <sys/mman.h>\n#include <sys/wait.h>\n#include <fcntl.h>\n#include <unistd.h>\n#include <string.h>\n\nint main(void)\n{\nchar* name = \"/my_shm1\";\nint data = 10;\n\nint fd = shm_open(name, O_CREAT | O_RDWR, S_IRUSR|S_IRGRP);\nif (fd == -1)\n{\nprintf(\"Failed to create shm object\");\nreturn 1;\n}\nftruncate(data, sizeof(int));\n// map shared memory to process address space\nvoid *addr = mmap(NULL, sizeof(int), PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);\nif (addr == MAP_FAILED)\n{\nprintf(\"Failed to map shm object\");\nreturn 2;\n}\n\nint id = fork();\nif (id == -1)\n{\nprintf(\"Failed to fork\");\nreturn 3;\n}\nif (id == 0) // child process\n{\ndata = 15;\n}\nelse // parent process\n{\nwait(NULL); // wait for update to take effect\n}\nprintf(\"data is: %d\\n\", data);\nshm_unlink(name);\nreturn 0;\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Common Problems", "path": "../pages/digitalGarden/cs/git/commonProblems.mdx"}, "page_content": "There are some common problems that you may encounter when using Git that my [basic guide](/digitalGarden/cs/git/basics) does not cover. Here are some solutions to those problems.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Common Problems", "Header 2": "Undo Unstaged Local Changes", "path": "../pages/digitalGarden/cs/git/commonProblems.mdx"}, "page_content": "To overwrite local changes:  \n```bash\ngit checkout -- <file>\n```  \nTo discard local changes to all files, permanently:  \n```bash\ngit reset --hard\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Common Problems", "Header 2": "Undo Staged Local Changes", "path": "../pages/digitalGarden/cs/git/commonProblems.mdx"}, "page_content": "To unstage the file but keep your changes:  \n```bash\ngit restore --staged <file>\n```  \nTo unstage everything but keep your changes:  \n```bash\ngit reset\n```  \nTo discard local changes to all files, permanently:  \n```bash\ngit reset --hard\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Common Problems", "Header 2": "Undo Committed Local Changes", "path": "../pages/digitalGarden/cs/git/commonProblems.mdx"}, "page_content": "This is a always a bit harder...\nTo undo last two commits, but keep your changes:  \n```bash\ngit reset HEAD~2\n```  \nTo undo last two commits, discard changes:  \n```bash\ngit reset --hard HEAD~2\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Common Problems", "Header 2": "Change Commit", "path": "../pages/digitalGarden/cs/git/commonProblems.mdx"}, "page_content": "To change the message of the last commit:  \n```bash\ngit commit --amend -m \"New message\"\n```  \nDid you forget to add a file? Just add it and amend the previous commit :)  \n```bash\ngit add forgotten_file\ngit commit --amend\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "You can find more in depth information on git in the [Pro Git book](https://git-scm.com/book/en/v2).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Initial Setup", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "The first thing you should do after installing git is set your username and email.\nThese are stored in a `gitconfig` file. There are multiple levels of configs, `--global` being for the entire system\nand `--local` only for that repository.  \n```bash\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"john@example.com\"\n```  \nTo check your settings you can use  \n```bash\ngit config --list\n```  \nTo get help for any command you can use  \n```bash\ngit <command> --help or git help <command>\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Initialize a Repository in an Existing Directory", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "Make sure this is executed inside the directory you want to have as a repository. This command will create a new\nsubdirectory named `.git` which contains all the files that are needed for maintaining your repository including but not\nlimited to the commit history.  \n*Do not delete the `.git` folder unless you know what you are doing!*  \n```bash\ngit init\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Making Changes to a Repository", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "<Image\nsrc=\"/cs/gitFileLifecycle.png\"\ncaption=\"The lifecycle and stages of a file in a git repository.\"\nwidth=\"800\"\n/>  \nAfter adding/deleting/modifying files, use `git add <directory>` or `git add <file>` to add the files to the queue to be\ncommitted.\n`git commit` commits all the added files. To be more descriptive a commit messages can also be added\nwith `git commit -m “<message>”` which is very important so that you know what was changed in that commit.  \nYou can also use the following to make life easier:  \n- `git add -A` stages all changes\n- `git add .` stages new files and modifications, without deletions\n- `git add -u` stages modifications and deletions, without new files  \nTo save even more time you can use the `git commit -a` command which is the same as executing `git add -A` followed by\na `git commit`. This can then be combined to the following  \n```bash\ngit commit -am \"<message>\"\n```  \nTo check the status of the files in your repository use`git status`.  \nTo see the history of your commits you can use `git log`.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Using a Remote Repository", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "For collaboration with other people or as a backup it is useful to have your local repository linked to a remote\nrepository, for example on a GitHub server.  \nIt is easiest to start with a repository created on the GitHub server. Cloning this repository creates a new local\nrepository that is linked to the remote one you cloned. To clone a repo\nuse `git clone https://github.com/LuciferUchiha/georgerowlands.ch.git`  \nIf you already have a local repository and want to add a remote repository you can do so the following way:  \n```bash\ngit remote add origin https://github.com/LuciferUchiha/georgerowlands.ch.git\ngit branch -M main\ngit push -u origin main\n```  \nThe first command adds the remote repo as a remote with the name `origin`.\nThe second command changes the current branch name to `main` as this is the default branch that is created on github.\nThe last command pushes to the remote repo and marks at as the upstream branch for the current branch.  \nYou can check the remote repositories you are connected to with `git remote -v`.\nNow you can edit and add files and commit your changes. Once you are done, you can *push* your changes with  \n```bash\ngit push\n```  \nBefore you continue working you should pull any changed you might have committed from another computer or that might\nhave been committed by a collaborator  \n```bash\ngit pull\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Branching", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "A branch in Git is simply a lightweight movable pointer to one of these commits. The default branch name in Git\nis `master`.\nEvery time you commit, the `master` branch pointer (`HEAD`)moves forward automatically.  \nTo create a new branch use the command:  \n```bash\ngit branch testing\n```  \nThe `git branch` command only created a new branch it didn’t switch to that branch.  \n<Image\nsrc=\"/cs/gitHeadToMaster.png\"\ncaption=\"The HEAD pointer is still pointing to the master branch which points to the latest commit.\"\nwidth=\"800\"\n/>  \nTo switch to an existing branch, you run the `git checkout testing` command.  \n<Image\nsrc=\"/cs/gitHeadToTesting.png\"\ncaption=\"The HEAD is now pointing to the testing branch.\"\nwidth=\"800\"\n/>  \nAfter changing a file and commiting, our structure could look something like this  \n<Image\nsrc=\"/cs/gitHeadAhead.png\"\ncaption=\"The HEAD and the testing branch are now ahead of the master branch.\"\nwidth=\"800\"\n/>  \nTo create and checkout a new branch at the same time:  \n```bash\ngit checkout -b testing\n```  \nTo view all branches use `git branch` without any parameters. By adding `-v` you can also see the last commit on each\nbranch. If you add `--all` you can also see the remote branches of a repository.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Merging", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "Assume our structure looks like the following  \n<Image\nsrc=\"/cs/gitBranching.png\"\ncaption=\"Two branches are based on the master branch and have been worked on.\"\nwidth=\"800\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Merging", "Header 3": "Fast-Forward", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "If we now execute the commands bellow, a so-called fast-forward merge is done which basically moves the pointer of the\nmaster branch.  \n```bash\ngit checkout master\ngit merge hotfix\n```  \n<Image\nsrc=\"/cs/gitFastForward.png\"\ncaption='The master hotfix branch can be merged by \"fast-forwarding\" the master branch pointer to the hotfix branch.'\nwidth=\"800\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Merging", "Header 3": "Three-Way", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "We can now delete the hotfix branch, checkout the issue branch and do some more work.  \n```bash\ngit branch -d hotfix\ngit checkout iss53\necho \"lots of work\" > index.html\ngit commit -a -m \"Did lots of work\"\n```  \nWe now however want our new changes to also be in the master branch, so we want to merge `iss53` into `master`.  \n<Image\nsrc=\"/cs/gitThreeWayStart.png\"\ncaption='The current structure of the branches before merging the iss53 branch into the master branch.'\nwidth=\"800\"\n/>  \n```bash\ngit checkout master\ngit merge iss53\n```  \nA fast-forward is not possible here so git does a so called three-way merge. It is called three-way because it uses the\ntwo branch heads and their common ancestor.  \n<Image\nsrc=\"/cs/gitThreeWayEnd.png\"\ncaption='The three-way merge creates a new commit that has two parents, the last commit of the master branch and the last commit of the iss53 branch.'\nwidth=\"800\"\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Git Basics", "Header 2": "Merging", "Header 3": "Merge Conflicts", "path": "../pages/digitalGarden/cs/git/basics.mdx"}, "page_content": "On the rare occasion the process above doesn’t go smoothly. If you changed the same part of the same file differently in\nthe two branches you’re merging, Git won’t be able to merge them. This is a so called merge conflict. With `git status`\nyou can see in which files there are conflicts, they will be marked as `unmerged`.  \nIf you open up such a file you will find something along the following:  \n```html filename=\"index.html\"\n<<<<<<< HEAD:index.html\n<div id=\"footer\">contact : email.support@github.com</div>\n=======\n<div id=\"footer\">\nplease contact us at support@github.com\n</div>\n>>>>>>> iss53:index.html\n```  \nTo resolve the merge conflict you need to choose one of the 2 versions you want or merge the code manually. This\nresolution has a little of each section, and the \\<, = and \\> lines have been completely removed. To finish it off use\na `git add` on the resolved file.  \n```html filename=\"index.html\"\n<div id=\"footer\">\nplease contact us at email.support@github.com\n</div>\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LaTeX Guidelines", "path": "../pages/digitalGarden/maths/latexGuidelines.mdx"}, "page_content": "This notation guide is inspired by [Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville](https://www.deeplearningbook.org/) whilst trying to stay short and compatible with [KaTeX](https://katex.org/docs/supported.html#style-color-size-and-font) (Docusaurus support) and [MathJax](https://docs.mathjax.org/en/latest/input/tex/macros/index.html) (Jupyter Notebook support). This page is also meant as a cheat sheet for me on latex structures such as matrices etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LaTeX Guidelines", "Header 2": "Numbers and Arrays", "path": "../pages/digitalGarden/maths/latexGuidelines.mdx"}, "page_content": "If we wanted tensors to also be bold we would have to write `\\mathsf{\\boldsymbol{A}}` which is sadly not nice and short.  \n| Description                                                                        | Code                          | Example                       |\n| ---------------------------------------------------------------------------------- | ----------------------------- | ----------------------------- |\n| Scalar (integer or real)                                                           | `a=2`                         | $a$                           |\n| Vector, lowercase italic bold                                                      | `\\boldsymbol{a}`              | $\\boldsymbol{a}$              |\n| Matrix, uppercase italic bold                                                      | `\\boldsymbol{A}`              | $\\boldsymbol{A}$              |\n| Tensor, uppercase sans-serif                                                       | `\\mathsf{A}`                  | $\\mathsf{A}$                  |\n| Identity matrix with $n$ rows and $n$ columns                                      | `\\boldsymbol{I}_n`            | $\\boldsymbol{I}_n$            |\n| Identity matrix with implied dimensionality                                        | `\\boldsymbol{I}`              | $\\boldsymbol{I}$              |\n| Square diagonal matrix with elements along main diagonal given by $\\boldsymbol{a}$ | `\\text{diag}(\\boldsymbol{a})` | $\\text{diag}(\\boldsymbol{a})$ |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LaTeX Guidelines", "Header 2": "Indexing", "path": "../pages/digitalGarden/maths/latexGuidelines.mdx"}, "page_content": "| Description                                                                                                | Code                   | Example                  |\n| ---------------------------------------------------------------------------------------------------------- | ---------------------- | ------------------------ |\n| Element $i$ of vector $\\boldsymbol{a}$, with indexing starting at 1                                        | `a_i`                  | $a_i$                    |\n| All elements of vector $\\boldsymbol{a}$ except for element $i$                                             | `a_{-i}`               | $a_{-i}$                 |\n| Element $i,j$ of matrix $\\boldsymbol{A}$, with $i$ corresponding to the row index and $j$ the column index | `A_{i,j}` or `a_{ij}`  | $A_{i,j}$ or $a_{ij}$    |\n| Row $i$ of matrix $\\boldsymbol{A}$                                                                         | `\\boldsymbol{A}_{i,:}` | $\\boldsymbol{A}_{i , :}$ |\n| Column $j$ of matrix $\\boldsymbol{A}$                                                                      | `\\boldsymbol{A}_{:,j}` | $\\boldsymbol{A}_{: , j}$ |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LaTeX Guidelines", "Header 2": "Linear Algebra Operations", "path": "../pages/digitalGarden/maths/latexGuidelines.mdx"}, "page_content": "| Description                                                              | Code                                 | Example                               |\n| ------------------------------------------------------------------------ | ------------------------------------ | ------------------------------------- |\n| Tranpose of matrix $\\boldsymbol{A}$                                      | `\\boldsymbol{A}^T`                   | $\\boldsymbol{A}^T$                    |\n| Moore-Penrose pseudoinverse of matrix $\\boldsymbol{A}$                   | `\\boldsymbol{A}^\\dagger`             | $\\boldsymbol{A}^\\dagger$              |\n| Hadamard (element-wise) product of $\\boldsymbol{A}$ and $\\boldsymbol{B}$ | `\\boldsymbol{A} \\odot boldsymbol{B}` | $\\boldsymbol{A} \\odot \\boldsymbol{B}$ |\n| Determinant of matrix $\\boldsymbol{A}$                                   | `\\text{det}(\\boldsymbol{A})`         | $\\text{det}(\\boldsymbol{A})$          |\n| Trace of matrix $\\boldsymbol{A}$                                         | `\\text{tr}(\\boldsymbol{A})`          | $\\text{tr}(\\boldsymbol{A})$           |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LaTeX Guidelines", "Header 2": "Sets and Graphs", "path": "../pages/digitalGarden/maths/latexGuidelines.mdx"}, "page_content": "| Description                           | Code                      | Example                   |\n| ------------------------------------- | ------------------------- | ------------------------- |\n| Set                                   | `A`                       | $A$                       |\n| Set of natural numbers, including $0$ | `\\Bbb{N}=\\{0,1,2,3,...\\}` | $\\Bbb{N}=\\{0,1,2,3,...\\}$ |\n| Set of integer numbers                | `\\Bbb{Z}`                 | $\\Bbb{Z}$                 |\n| Set of rational numbers               | `\\Bbb{Q}`                 | $\\Bbb{Q}$                 |\n| Set of irrational numbers             | `\\Bbb{I}`                 | $\\Bbb{I}$                 |\n| Set of real numbers                   | `\\Bbb{R}`                 | $\\Bbb{R}$                 |\n| Set of complex numbers                | `\\Bbb{C}`                 | $\\Bbb{C}$                 |", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Scientific Notation", "path": "../pages/digitalGarden/maths/scientificNotation.mdx"}, "page_content": "Scientific notation is a way of expressing numbers that are either very large or very small in a short and convenient\nform. The scientific notation takes the following form:  \n$$\ns \\cdot 10^n\n$$  \nwhere $s$ is the so-called **significand** or **mantissa** and is multiplied with 10 to the power of $n$, with $n$ being the so-called **exponent**. If the\nnumber is negative then a minus sign precedes the significand $s$.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n2 &= 2 \\cdot 10^0 \\\\\n300 &= 300 \\cdot 10^0 = 30 \\cdot 10^1 = 3 \\cdot 10^2 \\\\\n-10.5 &= -1.5 \\cdot 10^1\n\\end{align*}\n$$\n</Callout>  \nThe exponent of 10 indicates how many places the decimal point is moved. A positive exponent indicates that the decimal point is moved to the right, while a\nnegative exponent indicates that the decimal point is moved to the left.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n0\\textcolor{red}{.}00004 =& 0\\textcolor{red}{.}00004 \\cdot 10^0 \\\\\n=& 00\\textcolor{red}{.}0004 \\cdot 10^{-1} \\\\\n=& 000\\textcolor{red}{.}004 \\cdot 10^{-2} \\\\\n=& 0000\\textcolor{red}{.}04 \\cdot 10^{-3} \\\\\n=& 00000\\textcolor{red}{.}4 \\cdot 10^{-4} \\\\\n=& 000004\\textcolor{red}{.} \\cdot 10^{-5} \\\\\n=& 4 \\cdot 10^{-5}\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Scientific Notation", "Header 2": "Normalized Notation", "path": "../pages/digitalGarden/maths/scientificNotation.mdx"}, "page_content": "As you can see in the second example there can be ambigous representations, which is why the normalized notation was introduced.\nIn the normalized form the exponent is chosen so that the significand is at least 1 but less than 10, so $s \\in [1,10)$.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n2 &= 2 \\cdot 10^0 \\\\\n300 &= 3 \\cdot 10^2 \\\\\n-53'000 &= -5.3 \\cdot 10^4 \\\\\n0.2 &= 2 \\cdot 10^{-1}\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Scientific Notation", "Header 2": "The \"E\" Notation", "path": "../pages/digitalGarden/maths/scientificNotation.mdx"}, "page_content": "Because displaying exponents like $10^-5$ can be inconvenient to display or type on a computer or calculator, the letter \"E\" or \"e\", for \"exponent\",\nis often used to represent \"s times ten raised to the power of n\".  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n0.00004 &= 4 \\cdot 10^{-5} = 4E-5 = 4e-5 \\\\\n300 &= 3 \\cdot 10^2 = 3E2 = 3e2\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Trigonometry", "Header 2": "Einheitskreis", "path": "../pages/digitalGarden/maths/trigonometry.mdx"}, "page_content": "Der Einheitskreis ist ein Kreis mit einem Radius von 1 und dessen Mittelpunkt am Ursprung ist. Was auch heisst das jeder Punkt auf diesem Kreis genau 1 vom Ursprung entfernt ist.\n![einheitskreis](/maths/einheitskreis.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Trigonometry", "Header 2": "Einheitskreis", "Header 3": "Quadrante", "path": "../pages/digitalGarden/maths/trigonometry.mdx"}, "page_content": "Oftmals wird der Einheitskreis auch in 4 Teile geschnitten, die sogenannten Quadrante, welche durchnummeriert sind von 1 bis 4 beginnend mit oben rechts im gegen uhrzeigersin.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Trigonometry", "Header 2": "Einheiten", "Header 3": "Grad", "path": "../pages/digitalGarden/maths/trigonometry.mdx"}, "page_content": "Der **Grad** ist ein Winkelmass, das Gradmass. Beim Gradmass wird ein Kreis in 360 Teile geschnitten, 1 Grad entspricht also einem dieser Teile. Ein Kreis entspricht also 360 Grad.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Trigonometry", "Header 2": "Einheiten", "Header 3": "Radian", "path": "../pages/digitalGarden/maths/trigonometry.mdx"}, "page_content": "Der **Radian** ist ein ein Winkelmass, das Bogenmass. Beim Bogenmass betrachten wir die Länge des Bogens auf einem Kreis. 1 Radian entspricht einem Radius als Bogen. Der Umfang eines Kreises entspricht $2\\pi r$  was dann auch 360 Grad entspricht. Weil der Radius des Einheitskreises 1 ist entspricht dann der Umfang des Einheitskreises $2\\pi$.  \n![bogenmass](/maths/bogenmass.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Trigonometry", "Header 2": "Einheiten", "Header 3": "Umrechnen Grad und Radian", "path": "../pages/digitalGarden/maths/trigonometry.mdx"}, "page_content": "Wir können zwischen den 2 Winkelmasse wie gefolgt rechnen.  \n- Grad zu Radian: $x=\\alpha \\cdot \\frac{\\pi}{180}$\n- Radian zu Grad: $\\alpha=x \\cdot \\frac{180}{\\pi}$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Trigonometry", "Header 2": "Winkel im Dreieck", "Header 3": "Rechtwinklig", "path": "../pages/digitalGarden/maths/trigonometry.mdx"}, "page_content": "Bei einem rechtwinkligem Dreieck handelt es sich um ein Dreieck mit einem rechten Winkel (90 Grad).  \nWir bezeichnen die längste Seite als **Hypotenuse**. Diese Seite ist auch gegenüber vom rechten Winkel.  \nDie kürzeren Seiten nennen wir **Katheten**. In Bezug auf einen bestimmten Winkel (im Bild $\\alpha$) unterscheidet man die **Ankathete** (die dem Winkel anliegende Kathete, b) und die **Gegenkathete** (die dem Winkel gegenüberliegende Kathete, a).  \nDer **Satz von Pythagoras** besagt auch, dass die Länge der Hypotenuse die Summe der quadrierten Längen der Katheten ist. $a^2 + b^2=c^2$  \n![rechtwinkligesDreieck](/maths/rechtwinkligesDreieck.png)  \n$$\n\\begin{align*}\n\\text{Sinus von }\\alpha &=\\sin(\\alpha)=\\frac{a}{c}=\\frac{\\text{Gegenkathete}}{\\text{Hypotenuse}} &=\\frac{\\text{G}}{\\text{H}}& \\\\  \n\\text{Kosinus von }\\alpha &=\\cos(\\alpha)=\\frac{b}{c}=\\frac{\\text{Ankathete}}{\\text{Hypotenuse}} &=\\frac{\\text{A}}{\\text{H}}& \\\\  \n\\text{Tangens von }\\alpha &=\\tan(\\alpha)=\\frac{a}{b}=\\frac{\\text{Gegenkathete}}{\\text{Ankathete}} &=\\frac{\\text{G}}{\\text{A}}& \\\\  \n\\text{Kosekans von }\\alpha &=\\csc(\\alpha)=\\frac{c}{a}=\\frac{\\text{Hypotenuse}}{\\text{Gegenkathete}}&=\\frac{\\text{H}}{\\text{G}}&=\\frac{1}{\\sin(\\alpha)} \\\\  \n\\text{Sekans von }\\alpha &=\\sec(\\alpha)=\\frac{c}{b}=\\frac{\\text{Hypotenuse}}{\\text{Ankathete}} &=\\frac{\\text{H}}{\\text{A}}&=\\frac{1}{\\cos(\\alpha)} \\\\  \n\\text{Kotangens von }\\alpha &=\\cot(\\alpha)=\\frac{b}{a}=\\frac{\\text{Ankathete}}{\\text{Gegenkathete}} &=\\frac{\\text{A}}{\\text{G}}&=\\frac{1}{\\tan(\\alpha)}\n\\end{align*}\n$$  \nWeil im Einheitskreis die Hypotenuse, $c$ immer 1 ist lassen sich alle Funktionen vereinfachen und wir können folgendes daraus entnehemen  \n![sinusKosinusEinheitskreis](/maths/sinusKosinusEinheitskreis.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Trigonometry", "Header 2": "Winkel im Dreieck", "Header 3": "Allgemein", "path": "../pages/digitalGarden/maths/trigonometry.mdx"}, "page_content": "#### Cosinussatz  \n#### Sinussatz", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "Logic theory is a branch of maths that deals with formal systems of logic. Logic theory is also commonly reffered to as boolean algebra\nas it primarly deals with the boolean values of true and false.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Statements", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "A statement is something such as a sentence that is either true or false. For example, the sentence \"Zürich is in Switzerland\" is a statement because it is either true or false, but not both.\nA true/truthy value is represented by the symbol $T$ or $1$ and a false/falsy value is represented by the symbol $F$ or $0$.  \n<Callout type=\"example\">\n- \"There are more wheels then doors in the world\" is a statement because it is either true or false. However, I don't know the answer to this question.\n- \"My name is George\" is a statement (True).\n- \"The sky is green\" is a statement (False).\n- \"What time is it?\" is not a statement because it is a question.\n- \"This statement is false\" is not a statement because it is a paradox. If it is a statement then it must be false which would make it true, but if it is true then it must be false.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Statements", "Header 3": "Compound Statements", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "Statements can be combined to form compound statements. Just like in natural language, we can use words such as \"and\", \"or\" and \"not\" to combine statements, later\nwe will see that these are also the names of logical operators that are used to combine statements.  \nThe smallest unit of a compound statement is a simple statement or sometimes called an atomic statement.\nThis is a statement that cannot be broken down any further.  \n<Callout type=\"example\">\nIf we have the statement $p$ = \"Zürich is in Switzerland\" (True) and the statement $q$ = \"Zürich is the capital of Switzerland\" (False, it is Bern), we can combine them to form the compound statement:  \n\"Zürich is in Switzerland AND Zürich is the capital of Switzerland\" which is false.  \nWe can also combine them to form the compound statement:  \n\"Zürich is in Switzerland OR Zürich is the capital of Switzerland\" which is true.  \nWe can also combine them to form the compound statement:  \n\"Zürich is in Switzerland AND Zürich is NOT the capital of Switzerland\" which is true.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Statements", "Header 3": "Open Sentences", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "An open sentence is a sentence that contains a variable. The sentence is not a statement because it is not true or false until the variable is assigned a value.\nWhen the variable is assigned a value the open sentence becomes a statement. Unlike a closed/simple/definite statement that we have seen so far,\nan open sentence can be true for some values of the variable and false for other values of the variable, it only has a definite\ntruth value when the variable is assigned a value.  \n<Callout type=\"example\">\nThe sentence \"x is greater then 5\" is an open sentence because it is not true or false until we assign a value to the variable $x$.\nIf we assign the value $x = 6$ then the sentence is true, but if we assign the value $x = 4$ then the sentence is false.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Logical Operators", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "Rather then using words to express statements and combinations we can use symbols and operators. Each logical operator has a truth table that defines the output of the operator based on the input statements.  \nThese operators are not just used in logic theory but also in programming languages and circuits in the form of logical gates.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Logical Operators", "Header 3": "Conjunction (AND)", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "The conjunction operator is represented by the symbol $\\land$ and is true only if both the input statements $p$ **and** $q$ are true. Hence the name \"AND\". To remember the symbol $\\land$ think of it as a capital letter \"A\" for \"AND\".  \nThe truth table for the conjunction operator is as follows for the input statements $p$ and $q$:  \n| $p$ | $q$ | $p \\land q$ |\n| :-: | :-: | :-: |\n| $0$ | $0$ | $0$ |\n| $0$ | $1$ | $0$ |\n| $1$ | $0$ | $0$ |\n| $1$ | $1$ | $1$ |  \n<Callout type=\"example\">\n- $1 \\land 1 = 1$\n- $1 \\land 0 = 0$\n- $(0 \\land 1 \\land 1) \\land 1 = 0$ Notice that we can chain multiple conjunction operators together by going from left to right and using brackets to group the statements.\n- $(1 \\land 1 \\land 1) \\land (1 \\land 1) = 1$  \nWe can see a pattern here, the conjunction operator is only true if all the input statements are true, as soon as one of the input statements is false the output is false.  \nLet's look at the truth table for a composition of conjunction operators:  \n| $p$ | $q$ | $r$ | $p \\land q \\land r$ |\n| :-: | :-: | :-: | :-: |\n| $0$ | $0$ | $0$ | $0$ |\n| $0$ | $0$ | $1$ | $0$ |\n| $0$ | $1$ | $0$ | $0$ |\n| $0$ | $1$ | $1$ | $0$ |\n| $1$ | $0$ | $0$ | $0$ |\n| $1$ | $0$ | $1$ | $0$ |\n| $1$ | $1$ | $0$ | $0$ |\n| $1$ | $1$ | $1$ | $1$ |  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Logical Operators", "Header 3": "Disjunction (OR)", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "The disjunction operator is represented by the symbol $\\lor$ and is true if at least one of the input statements $p$ **or** $q$ is true. Hence the name \"OR\".\nTo not get $\\land$ and $\\lor$ confused just remember that $\\land$ looks like a capital letter \"A\" for \"AND\" and therefore $\\lor$ is the other one, \"OR\".  \nThe truth table for the disjunction operator is as follows for the input statements $p$ and $q$:  \n| $p$ | $q$ | $p \\lor q$ |\n| :-: | :-: | :-: |\n| $0$ | $0$ | $0$ |\n| $0$ | $1$ | $1$ |\n| $1$ | $0$ | $1$ |\n| $1$ | $1$ | $1$ |  \n<Callout type=\"example\">\n- $1 \\lor 1 = 1$\n- $1 \\lor 0 = 1$\n- $(0 \\lor 1 \\lor 1) \\lor 1 = 1$\n- $(1 \\lor 1 \\lor 1) \\lor (0 \\lor 0) = 1$  \nWe can see a pattern here, the disjunction operator is true if at least one of the input statements is true, if all the input statements are false then the output is false.  \nLet's look at the truth table for a composition of disjunction operators:  \n| $p$ | $q$ | $r$ | $p \\lor q \\lor r$ |\n| :-: | :-: | :-: | :-: |\n| $0$ | $0$ | $0$ | $0$ |\n| $0$ | $0$ | $1$ | $1$ |\n| $0$ | $1$ | $0$ | $1$ |\n| $0$ | $1$ | $1$ | $1$ |\n| $1$ | $0$ | $0$ | $1$ |\n| $1$ | $0$ | $1$ | $1$ |\n| $1$ | $1$ | $0$ | $1$ |\n| $1$ | $1$ | $1$ | $1$ |  \nLet's also look at the conjunction and disjunction operators together:  \n- $(1 \\land 1) \\lor (1 \\land 0) = 1$\n- $(1 \\land 1) \\land (1 \\land 0) = 0$\n- $(1 \\land 0) \\lor (0 \\land 1) = 0$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Logical Operators", "Header 3": "Negation (NOT)", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "The negation operator is a unary operator, meaning it only takes one input statement $p$ rather then two, like the previous operators. The negation operator is represented by the symbol $\\lnot$ and is true if the input proposition $p$ is false and false if the input proposition $p$ is true.\nHence the name \"NOT\", as it negates, inverts or flips the input statement. Sometimes the negation operator is represented by an exclamation mark $!$, as this is often the symbol used in programming languages.  \nThe truth table for the negation operator is as follows for the input statement $p$:  \n| $p$ | $\\lnot p$ |\n| :-: | :-: |\n| $0$ | $1$ |\n| $1$ | $0$ |  \n<Callout type=\"warning\">\nThe precdence of the not operator is higher then the other operators, meaning that it is evaluated first. This is similar to how in maths the brackets are evaluated first.  \nFor example, the statement $\\lnot 1 \\land 0$ is evaluated as $(\\lnot 1) \\land 0$ which is $0$. If the precedence was the other way around then the statement would be evaluated as $\\lnot (1 \\land 0)$ which is $1$.\nSo remember that the negation operator is evaluated first and depending on the desired outcome you might need to use brackets to group the statements.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Conditional Statements (IF THEN)", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "A conditional statement is often reffered to as an \"if-then\" statement or implication. The conditional operator is represented by the symbol $\\implies$ and is true if the input statement $p$ is false **or** the output statement $q$ is true.\nSo we can say that the following statements are equivalent:  \n$$\np \\implies q = \\lnot p \\lor q\n$$  \nThe conditional operator is also called the implication operator because it implies that if the input statement is true then the output statement is also true.\nThe statement \"If it is raining, then the ground is wet\" can be written using the implication operator as $r \\implies w$.\nIf $r$ is true then $w$ must also be true, but if $r$ is false then $w$ can be either true or false, i.e the ground can be wet even if it is not raining.  \nThe truth table for the Conditional operator is as follows for the input statements $p$ and $q$:  \n| $p$ | $q$ | $p \\implies q$ |\n| :-: | :-: | :-: |\n| $0$ | $0$ | $1$ |\n| $0$ | $1$ | $1$ |\n| $1$ | $0$ | $0$ |\n| $1$ | $1$ | $1$ |  \nTo remember the truth table you can either remember that the conditional operator is only false if the input statement is true and the output statement is false, otherwise it is true.  \nThe operator's direction is from left to right, meaning that the input statement is on the left and the output statement is on the right.\nHowever, we can also write the conditional operator in the other direction, i.e $q \\impliedby p$ which is equivalent to $p \\implies q$.  \n$$\np \\implies q = q \\impliedby p\n$$  \n<Callout type=\"example\">\nLet's look at the truth table for a composition including the conditional operator:  \n| $p$ | $q$ | $r$ | $(p \\implies q) \\land r$ |\n| :-: | :-: | :-: | :-: |\n| $0$ | $0$ | $0$ | $0$ |\n| $0$ | $0$ | $1$ | $1$ |\n| $0$ | $1$ | $0$ | $0$ |\n| $0$ | $1$ | $1$ | $1$ |\n| $1$ | $0$ | $0$ | $0$ |\n| $1$ | $0$ | $1$ | $0$ |\n| $1$ | $1$ | $0$ | $0$ |\n| $1$ | $1$ | $1$ | $1$ |\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Biconditional Statements (IF AND ONLY IF)", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "A biconditional statement is often reffered to as an \"if and only if\" statement. The biconditional operator is represented by the symbol $\\iff$ and is true if both the input statement $p$ **and** the output statement $q$ are true **or** both the input statement $p$ **and** the output statement $q$ are false.\nThe biconditional operator is also called the equivalence operator because it states that the input statement is equivalent to the output statement.  \nThe truth table for the Biconditional operator is as follows for the input statements $p$ and $q$:  \n| $p$ | $q$ | $p \\iff q$ |\n| :-: | :-: | :-: |\n| $0$ | $0$ | $1$ |\n| $0$ | $1$ | $0$ |\n| $1$ | $0$ | $0$ |\n| $1$ | $1$ | $1$ |  \nThe \"bi\" in biconditional means two, so we can think of the biconditional operator as two conditional operators combined together. We can\nactually show that this is the case, i.e that the biconditional operator is equivalent to two conditional operators:  \n$$\np \\iff q = (p \\implies q) \\land (q \\implies p)\n$$  \n<Callout type=\"proof\">\nLet's prove that the biconditional operator is equivalent to two conditional operators:  \n| $p$ | $q$ | $p \\implies q$ | $q \\implies p$ | $(p \\implies q) \\land (q \\implies p)$ | $p \\iff q$ |\n| :-: | :-: | :-: | :-: | :-: | :-: |\n| $0$ | $0$ | $1$ | $1$ | $1$ | $1$ |\n| $0$ | $1$ | $1$ | $0$ | $0$ | $0$ |\n| $1$ | $0$ | $0$ | $1$ | $0$ | $0$ |\n| $1$ | $1$ | $1$ | $1$ | $1$ | $1$ |  \nAs we can see the two statements are equivalent, the biconditional operator is equivalent to two conditional operators. This\nis very important when we want to prove that two statements are logically equivalent as most proofs are done using the conditional operator.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Biconditional Statements (IF AND ONLY IF)", "Header 3": "Logical Equivalence", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "In logic theory we also sometimes want to express that two statements are logically equivalent, meaning that have the same truth value.\nThis could be done by using the biconditional operator $\\iff$ which is true if both the input statements $p$ and $q$ are true **or** both the input statements $p$ and $q$ are false.  \nHowever, most often we prefer to write this using the equivalence symbol $\\equiv$ which is true if both the input statements $p$ and $q$ have the same truth value.\nSo we could change our statement from above of the conditional operator to be:  \n$$\np \\implies q \\equiv \\lnot p \\lor q\n$$  \nand the biconditional operator to be:  \n$$\np \\iff q \\equiv (p \\implies q) \\land (q \\implies p)\n$$  \nThere is no real difference between the two from my understanding. The biconditional operator used to combine two statements into one,\nwhile the equivalence operator is used to state that two statements are equivalent.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Tautologies and Contradictions", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "A tautology is a statement that is always true, no matter what the input is for the statement. A fun tautology is\nShakespeare's \"To be or not to be\" because it is always true, it is a tautology. Or the statement \"It will rain or\nit will not rain\" is also a tautology because it is always true, it cannot be both raining and not raining at the same time.  \n$$\np \\lor \\lnot p\n$$  \nTo check if a statement is a tautology we can use a truth table and check if the output is always true.  \n| $p$ | $\\lnot p$ | $p \\lor \\lnot p$ |\n| :-: | :-: | :-: |\n| $0$ | $1$ | $1$ |\n| $1$ | $0$ | $1$ |  \nOn the other hand, a contradiction is a statement that is always false, no matter what the input is for the statement.\nThe most obvious contradiction is the \"The world is round and the world is not round\", it is always false because either\nthe world is round or it is not round, it cannot be both, hence it is a contradiction.  \n$$\np \\land \\lnot p\n$$  \n| $p$ | $\\lnot p$ | $p \\land \\lnot p$ |\n| :-: | :-: | :-: |\n| $0$ | $1$ | $0$ |\n| $1$ | $0$ | $0$ |  \n<Callout type=\"example\">\nLet's find out if the statement is a tautology or a contradiction:  \n| $p$ | $q$ | $\\lnot p$ | $\\lnot q$ | $p \\lor q$ | $\\lnot p \\land \\lnot q$ | $(p \\lor q) \\lor (\\lnot p \\land \\lnot q)$ |\n| :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| $0$ | $0$ | $1$ | $1$ | $0$ | $1$ | $1$ |\n| $0$ | $1$ | $1$ | $0$ | $1$ | $0$ | $1$ |\n| $1$ | $0$ | $0$ | $1$ | $1$ | $0$ | $1$ |\n| $1$ | $1$ | $0$ | $0$ | $1$ | $0$ | $1$ |  \nAs we can see the statement is a tautology because the output is always true.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Quantifiers", "Header 3": "Nested Quantifiers", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "order matters", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "De Morgran's Laws", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "also a generalized form with quantifiers???", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Logic Theory", "Header 2": "Arguments and Inference", "path": "../pages/digitalGarden/maths/discrete/logicTheory.mdx"}, "page_content": "arguments with therefore symbol  \nmodus ponens, modus tollens, elimination  \ninstatination or something for quantifiers", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Combinatorial Principles", "path": "../pages/digitalGarden/maths/discrete/combinatorics.mdx"}, "page_content": "Combinatorial principles, addition, multiplication, piegonhole principle, inclusion-exclusion principle", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Proofs", "path": "../pages/digitalGarden/maths/discrete/proofs.mdx"}, "page_content": "This should probably be a folder and split up", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Proofs", "Header 2": "Proof by Contradiction", "path": "../pages/digitalGarden/maths/discrete/proofs.mdx"}, "page_content": "not just for conditional statements", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Proofs", "Header 2": "Prooving Biconditional Statements", "path": "../pages/digitalGarden/maths/discrete/proofs.mdx"}, "page_content": "just prove both directions", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Proofs", "Header 2": "Existence and Uniqueness Proofs", "path": "../pages/digitalGarden/maths/discrete/proofs.mdx"}, "page_content": "constructive and non-constructive proofs", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Proofs", "Header 2": "Proof by Induction", "path": "../pages/digitalGarden/maths/discrete/proofs.mdx"}, "page_content": "well ordering principle or property ???", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Proofs", "Header 2": "Combinatorial Proofs", "path": "../pages/digitalGarden/maths/discrete/proofs.mdx"}, "page_content": "??? annyoing", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Proofs", "Header 2": "Resolution Proofs", "path": "../pages/digitalGarden/maths/discrete/proofs.mdx"}, "page_content": "????", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "A **set** is a collection of distinct objects. To create a set you simply list the objects between curly brackets. The objects in a set are called **elements** or **members** of the set.\nThe **order of the elements in the set is not important**, and **an element can only appear once in a set**. If an element appears more than once, it is considered only once in the set.  \n<Callout type=\"example\">\n- The set of the first three positive numbers is $\\{1,2,3\\}$.\n- The order doesn't matter, therefore, $\\{1,2,3\\}$ is the same as $\\{3,1,2\\}$.\n- Each element can only appear once, so $\\{1,2,2,3\\}$ is the same as $\\{1,2,3\\}$.\n</Callout>  \nHowever, a set doesn't have to contain numbers. A set also doesn't have to have a certain amount of elements it could also be infinite. This allows us to differentiate between **finite** and **infinite** sets.\nA set can also have no elements at all, the so-called **empty set**, and is represented by the symbol $\\emptyset$. You may also see the notation of $\\{\\}$ or $\\phi$ (phi) for the empty set. However I will be using $\\emptyset$.  \n<Callout type=\"example\">\n- The infinite set of all positive integers is $\\{1,2,3,...\\}$.\n- The finite set of all colors in the rainbow is $\\{red, orange, yellow, green, blue, indigo, violet\\}$.\n- The set of the three primary colors is $\\{red, green, blue\\}$.\n</Callout>  \nBecause a set is a collection of distinct objects it can also contain other sets. This is called a **nested set**.  \n<Callout type=\"example\">\n- A set of sets: $\\{\\{1,2\\},\\{3,4\\}\\}$\n- A set of sets of colors: $\\{\\{red, green\\},\\{blue, yellow\\}\\}$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Cardinality", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The cardinality of a set is the number of elements/members in that set. The cardinality of the empty set is 0. The cardinality of a finite set is a positive integer. The cardinality of an infinite set is infinity.\nImportantly becuase of the definition of a set repeated elements are not counted. The cardinality of the set $A = \\{1,2,3\\}$ is denoted by:  \n$$\n|A| = 3\n$$  \nNot to be confused with the absolute value of a number.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n|\\emptyset| &= 0 \\\\\n|\\{1,2,3\\}| &= 3 \\\\\n|\\{1,2,2,3\\}| &= 3 \\\\\n|\\{\\{1,2\\},\\{3,4\\}\\}| &= 2 \\\\\n|\\{\\{\\}\\}| &= 1\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Membership", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "To indicate that an element is a member of a set the symbol $\\in$ is used. To indicate that an element is not a member of a set the symbol $\\notin$ is used.  \n<Callout type=\"example\">\nIf $A=\\{a,b,c\\}$ then:  \n- Is a member of the set: $a \\in A$ or $a \\in \\{a,b,c\\}$\n- Is not a member of the set: $d \\notin A$ or $d \\notin \\{a,b,c\\}$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Equality", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "Because sets are characterized by their elements, two sets are only equal if they have the exact same elements. More formally, two sets $A$ and $B$ are equal if and only if every element of $A$ is an element of $B$ and every element of $B$ is an element of $A$:  \n$$\nA=B \\iff \\forall x(x \\in A \\iff x \\in B)\n$$  \nFor sets to be equal, they must also have the same cardinality:  \n$$\nA=B \\implies |A| = |B|\n$$  \n<Callout type=\"example\">\n- $\\{1,2,3\\} = \\{3,2,1\\}$ because the order of the elements doesn't matter.\n- $\\{1,2,3\\} \\neq \\{1,2,3,4\\}$ because the second set has an element that is not in the first set.\n- $\\{1,2,3\\} = \\{1,2,2,3\\}$ because only the distinct elements are considered.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Venn Diagrams", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "When working with sets, it can be very helpful to visualize them with venn diagrams. The sets are usually represented by circles. By creating a venn diagram, you can easily see the relations between sets.  \n<Image\nsrc=\"/maths/setsVennDiagrams.png\"\ncaption=\"Venn diagram showing the relation between cats, dogs and birds.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sub and Superset", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "$A$ is a **subset** of a set $B$ if all elements of $A$ are also elements of $B$. $B$ is then also the so-called **superset** of $A$. This is written as:  \n$$\nA \\subseteq B \\iff \\forall x(x \\in A \\implies x \\in B)\n$$  \nThe superset relation is the opposite of the subset relation, so $B \\supseteq A$. This is written as:  \n$$\nB \\supseteq A \\iff A \\subseteq B\n$$  \nIf $C$ is not a subset of $D$, then we write $C \\nsubseteq D$, and if $D$ is not a superset of $C$, then we write $D \\nsupseteq C$.  \n<Callout type=\"example\">\n- $\\{1,2\\} \\subseteq \\{1,2,3\\}$ because all elements of the first set are also in the second set.\n- $\\{1,2,3\\} \\subseteq \\{1,2,3\\}$ because all elements of the first set are also in the second set.\n- $\\{1,2,3\\} \\nsubseteq \\{1,2\\}$ because not all elements of the first set are in the second set.\n- $\\{1,2,3\\} \\supseteq \\{1,2\\}$ because all elements of the second set are also in the first set.\n</Callout>  \nIf we analyse the cardinality of a subset and its superset, we can see that the cardinality of the subset has to be less than or equal to the cardinality of the superset:  \n$$\nA \\subseteq B \\implies |A| \\leq |B|\n$$  \nIf the symbol $\\subseteq$ is underlined, then the sets can also be equal.  \nIf $A$ is a subset of $B$ but not equal to $B$, then $A$ is called a **proper subset** of $B$.\nThe same goes for the superset relation, if $B$ is a superset of $A$ but not equal to $A$, then $B$ is called a **proper superset** of $A$.\nHowever, a set is never a proper subset or superset of itself because it is equal to itself and can therefore not be a proper subset or superset.  \n$$\nA \\subset B \\iff A \\subseteq B \\land A \\neq B\n$$  \n<Image\nsrc=\"/maths/setsSubsets.png\"\ncaption=\"Venn diagram showing the difference between a subset and a proper subset.\"\nwidth={700}\n/>  \n<Callout type=\"example\">\n- $\\{1,2\\} \\subset \\{1,2,3\\}$ because all elements of the first set are also in the second set but the sets are not equal.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sub and Superset", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "caption=\"Venn diagram showing the difference between a subset and a proper subset.\"\nwidth={700}\n/>  \n<Callout type=\"example\">\n- $\\{1,2\\} \\subset \\{1,2,3\\}$ because all elements of the first set are also in the second set but the sets are not equal.\n- $\\{1,2,3\\} \\not \\subset \\{1,2,3\\}$ because all elements of the first set are also in the second set and the sets are equal. Therefore it is not a proper subset but just a subset.\n- $\\{1,2,3\\} \\not \\subset \\{1,2\\}$ because not all elements of the first set are in the second set.\n- $\\{1,2,3\\} \\supset \\{1,2\\}$ because all elements of the second set are also in the first set but the sets are not equal.\n</Callout>  \nWe can refine the above statement regarding the cardinality of a subset if it is a proper subset:  \n$$\nA \\subset B \\implies |A| < |B|\n$$  \nBecause if the cardinality of the subset is equal to the cardinality of the superset, then the sets are equal and not a proper subset.  \n<Callout type=\"info\">\nBecause the empty set has no elements, it is a subset of every set, including itself. Therefore $\\emptyset \\subseteq A$ or $\\emptyset \\subset A$ for every set $A$.  \n- $\\emptyset \\subset \\{1,2,3\\}$\n- $\\emptyset \\subseteq \\{1,2,3\\}$\n- $\\emptyset \\subseteq \\emptyset$\n- $\\emptyset \\not \\subset \\emptyset$ because the sets are equal.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Defining a Set", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "There are multiple ways to define a set.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Defining a Set", "Header 3": "Roster Notation", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The roster or also called enumeration notation defines a set by listing its elements between curly brackets separated by commas. This is the most common way to define a set and is what I have been using in the examples above.  \n<Callout type=\"example\">\n- The set of the first three positive numbers is $\\{1,2,3\\}$.\n- The set of the first three positive even numbers is $\\{2,4,6\\}$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Defining a Set", "Header 3": "Semantic Definition", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "Sets can be described semantically using words and rules. This notation is less preffered as it can be more confusing, unclear and ambiguous.  \n<Callout type=\"example\">\nLet $A$ be the first 3 positive integers that are odd. Therefore the set is then $A=\\{1,3,5\\}$\nLet $B$ be the set of colors of the French flag. Therefore the set is then $B=\\{blue,white,red\\}$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Defining a Set", "Header 3": "Set-Builder Notation", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "This is probably the hardest notation to understand. The set-builder notation specifies a set as a selection from a larger set, determined by a condition on those elements. It commonly uses the following form:  \n$$\n\\{x \\mid P(x)\\}\n$$  \nWhere $x$ is the variable, and $P(x)$ is the statement or condition that $x$ must satisfy to be an element of the set. The vertical bar \"|\" is read as \"such that\". So the description can be read as \"the set of all elements $x$ such that $x$ satisfies $P(x)$\".  \n<Callout type=\"example\">\nIf we define the set $\\Bbb{Z}$ as the set of all integers, then the set of all positive integers that are less than 5 can be defined as:  \n$$\nF = \\{ a \\mid a \\in \\Bbb{Z} \\text{ and } 0 \\leq a \\leq 5\\}=\\{0,1,2,3,4,5\\}\n$$  \nAnother example is the set of all even numbers:  \n$$\nE = \\{ x \\mid x \\in \\Bbb{Z} \\text{ and } x \\text{ is even}\\}=\\{0,2,-2,4,-4,6,-6,...\\}\n$$  \nWe could mathmatically define when a number is even:  \n$$\nE = \\{ x \\mid x \\in \\Bbb{Z} \\text{ and } \\exists k \\in \\Bbb{Z} \\text{ such that } x=2k\\}\n$$  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sets of Numbers", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The most common use of sets is to define the different types of numbers. The following are the most common sets of numbers and their notation:  \n- **Natural Numbers** $\\Bbb{N}$: The set of positive integers. Depending on the author, this set can include 0 or not.\n- **Integers** $\\Bbb{Z}$: The set of positive and negative whole numbers, including zero.\n- **Rational Numbers** $\\Bbb{Q}$: The set of numbers that can be expressed as a fraction of two integers.\n- **Real Numbers** $\\Bbb{R}$: The set of all rational and irrational numbers.\n- **Complex Numbers** $\\Bbb{C}$: The set of numbers that can be expressed in the form $a+bi$ where $a$ and $b$ are real numbers and $i$ is the imaginary unit.  \nIf we visualize these sets with a venn diagram, we can see how they are related to each other and are all subsets of each other.  \n$$\n\\Bbb{N} \\subset \\Bbb{Z} \\subset \\Bbb{Q} \\subset \\Bbb{R} \\subset \\Bbb{C}\n$$  \n<Image\nsrc=\"/maths/setsOfNumbers.png\"\ncaption=\"Venn diagram showing the all the sets of numbers.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sets of Numbers", "Header 3": "Natural Numbers", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The natural numbers are the set of positive integers. The set of natural numbers is denoted by:  \n$$\n\\Bbb{N} =\\{0,1,2,3,...\\}\n$$  \nDepending on the author, 0 is included or not in the set of natural numbers. I prefer to include 0 in the set of natural numbers, as it makes more sense to me. This can cause confusion, so it is important to know some of the different notations used:  \n- If $\\Bbb{N} =\\{1,2,3,...\\}$ then 0 is not included in the set of natural numbers. The authors that do not include 0 in the set of natural numbers use the following notation to then include it $\\Bbb{N}_0 =\\{0,1,2,3,...\\}$.\n- If $\\Bbb{N} =\\{0,1,2,3,...\\}$ then 0 is included in the set of natural numbers. The authors that include 0 in the set of natural numbers then commonly use the following notation to exclude it $\\Bbb{N}^+ =\\{1,2,3,...\\}$.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n1 &\\in \\Bbb{N} \\\\\n0 &\\in \\Bbb{N} \\\\\n-1 &\\notin \\Bbb{N} \\\\\n\\frac{1}{2} &\\notin \\Bbb{N} \\\\\n\\sqrt{2} &\\notin \\Bbb{N} \\\\\n\\pi &\\notin \\Bbb{N} \\\\\n3i &\\notin \\Bbb{N}\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sets of Numbers", "Header 3": "Integers", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The integers are the set of positive and negative whole numbers, including zero. The set of integers is denoted by:  \n$$\n\\Bbb{Z} =\\{...,-2,-1,0,1,2,...\\}\n$$  \nMultiple restrictions can be applied to the set of integers, for example:  \n- $\\Bbb{Z}^+ =\\{1,2,3,...\\}$\n- $\\Bbb{Z}^- =\\{...,-3,-2,-1\\}$\n- $\\Bbb{Z}_0^+ =\\{0,1,2,3,...\\}$  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n1 &\\in \\Bbb{N} \\\\\n0 &\\in \\Bbb{N} \\\\\n-1 &\\in \\Bbb{N} \\\\\n\\frac{1}{2} &\\notin \\Bbb{N} \\\\\n\\sqrt{2} &\\notin \\Bbb{N} \\\\\n\\pi &\\notin \\Bbb{N} \\\\\n3i &\\notin \\Bbb{N}\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sets of Numbers", "Header 3": "Rational and Irrational Numbers", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "Rational numbers are numbers that can be expressed with a fraction of two integers. Importantly the denominator can not be zero as this is an undefined operation. Another way to remember them is that they can be written as terminating or repeating decimals. The set of rational numbers is denoted by:  \n$$\n\\Bbb{Q}=\\Big\\{\\frac{p}{q} \\mid p \\in \\Bbb{Z} \\land p,q \\in \\Bbb{Z} \\land q \\neq 0\\Big\\}\n$$  \n<Callout type=\"example\">\nThe number $0.75$ is a rational number because it can be written as a fraction of two integers but also because it is a terminating decimal:  \n$$\n0.75 = \\frac{3}{4} \\in \\Bbb{Q}\n$$  \nThis fraction is also unique to each number, as for example the fraction $\\frac{6}{8}$ can be shortened to the above\nsolution by dividing the denominator an nominator by the common factor 2.  \n$$\n\\frac{6}{8} = \\frac{3}{4} \\in \\Bbb{Q}\n$$  \nAnother example is $0.3333... = 0.\\overline{3}$, which is a repeating decimal and can be written as the following fraction:  \n$$\n0.3333... = 0.\\overline{3} = \\frac{1}{3} \\in \\Bbb{Q}\n$$\n</Callout>  \nOn the other hand, irrational numbers are numbers that cannot be expressed as a fraction of two integers. They can also not\nbe written as terminating or repeating decimals, as their decimal expansions go on infinitely without repeating. Irrational\nnumbers are usually denoted by the symbol $\\Bbb{I}$.  \n<Callout type=\"example\">\nFor example, $\\pi$, not a rational number but an irrational number:  \n$$\n\\pi = 3.14159265... \\in \\Bbb{I}\n$$  \nAnother example of an irrational number is $\\sqrt{2}$:  \n$$\n\\sqrt{2} = 1.41421356... \\in \\Bbb{I}\n$$  \nThis however does not mean that all square roots are irrational numbers!  \n$$\n\\sqrt{9} = 3 \\in \\Bbb{N}\n$$\n</Callout>  \n<Callout type=\"proof\" title=\"Proof that sqrt(2) is Irrational by Contradiction\">\nHow do we know that $\\sqrt{2}$ is irrational and therefore doesn't repeat or terminate and can't be expressed as a fraction of two integers?  \nTo prove that the square root of 2 is irrational, we need to assume that it is rational and then derive a contradiction.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sets of Numbers", "Header 3": "Rational and Irrational Numbers", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "To prove that the square root of 2 is irrational, we need to assume that it is rational and then derive a contradiction.\nThat is, we assume that $\\sqrt{2}$ can be expressed as a fraction of two integers, and then show that this assumption is wrong and leads\nto a contradiction. Suppose that $\\sqrt{2}$ is rational, and can be expressed as:  \n$$\n\\sqrt{2} = \\frac{p}{q}\n$$  \nwhere $p$ and $q$ are integers with no common factors. We can assume that $p$ and $q$ are in lowest terms, meaning that\nthey have been divided by their greatest common factor. After squaring both sides of this equation, we get:  \n$$\n2 = \\frac{p^2}{q^2}\n$$  \nThen multiplying both sides by $q^2$, we get:  \n$$\n2q^2 = p^2\n$$  \nThis means that $p^2$ is even, which implies that $p$ is also even (because the square of an odd number is odd). So we\ncan write $p$ as $p = 2k$ for some integer $k$. Substituting $p = 2k$ into the equation $2q^2 = p^2$, we get:  \n$$\n2q^2 = (2k)^2 = 4k^2\n$$  \nDividing both sides by 2, we get:  \n$$\nq^2 = 2k^2\n$$  \nThis means that $q^2$ is even, which implies that $q$ is also even (because the square of an odd number is odd). But\nthis contradicts our assumption that $p$ and $q$ have no common factors, since they both have a factor of 2. Therefore,\nour initial assumption that $\\sqrt{2}$ is rational must be false, and we conclude that $\\sqrt{2}$ is irrational.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sets of Numbers", "Header 3": "Real Numbers", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "Real numbers are the combination of rational and irrational numbers, i.e. all numbers from both sets. The set of real numbers is denoted by $\\Bbb{R}$.  \nThis can also be thought of as the set of all points on the number line.  \n<Image\nsrc=\"/maths/setsRealNumberLine.png\"\ncaption=\"Number line showing the real numbers.\"\nwidth={500}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n1 &\\in \\Bbb{N} \\\\\n0 &\\in \\Bbb{N} \\\\\n-1 &\\in \\Bbb{N} \\\\\n\\frac{1}{2} &\\in \\Bbb{N} \\\\\n\\sqrt{2} &\\in \\Bbb{N} \\\\\n\\pi &\\in \\Bbb{N} \\\\\n3i &\\notin \\Bbb{N}\n\\end{align*}\n$$\n</Callout>  \n<Callout type=\"info\">\nInfinity is not a number in the traditional sense, but rather a concept that refers to something, limitless i.e. without\nend. Therefore it is not an element of any set of numbers, including the real numbers.\n</Callout>  \n#### Heron's Method  \nUsing Heron's method we can approximate a real number by a rational number with any desired accuracy. The idea is that a square with an area of $A$ has a side length of $\\sqrt A$.  \nTo approximate $\\sqrt a$ we start by creating a rectangle which we know the area of and then slowly adjust the sides to get closer to a square with the area of $A$ and therefore the side length of $\\sqrt A$.  \nWe start with a guess $a_1$ and then calculate the next guess $a_2$ by taking the average of the previous guess and the area divided by the previous guess.  \n$$\na_2 = \\frac{a_1 + \\frac{A}{a_1}}{2}\n$$  \nThis can be repeated until the desired accuracy is reached:  \n$$\na_{n+1} = \\frac{a_n + \\frac{A}{a_n}}{2}\n$$  \n<Callout type=\"example\">\nTo approximate $\\sqrt{5}$ we start with a guess of $a_1=5$. We then calculate the next approximation:  \n$$\na_2 = \\frac{5 + \\frac{5}{5}}{2} = \\frac{5 + 1}{2} = 3\n$$  \nIf we then repeat this process we get for $a_4 = 2.234$ which is a good approximation of $\\sqrt{5}=2.236$.  \n<Image\nsrc=\"/maths/setsHeronsMethod.gif\"\ncaption=\"Animation showing Heron's method to approximate the square root of 5.\"\nwidth={500}\n/>\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Sets of Numbers", "Header 3": "Complex Numbers", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The set of complex numbers is denoted by $\\Bbb{C}$. You can see what complex numbers are and how they are used [here](/digitalGarden/maths/complexNumbers/explanation).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "There are several operations that can be performed on sets. These operations are similar to the operations that can be performed on numbers such as addition and subtraction.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "Header 3": "Intersection", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "A new set can also be constructed by determining which elements two sets have \"in common\", this results in the so called **intersection** of $A$ and $B$. This is written as $A \\cap B$. The intersection is the set of all things that are elements of both $A$ and $B$.  \n$$\nA \\cap B = \\{x \\mid x \\in A \\land x \\in B\\}\n$$  \n<Image\nsrc=\"/maths/setsIntersection.png\"\ncaption=\"Venn diagram showing the intersection of two sets.\"\nwidth={300}\n/>  \nTo remember the difference between the union and intersection symbols, you can think of the union symbol as a \"U\" for \"union\".  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\{1,2\\} \\cap \\{1,2\\} &= \\{1,2\\} \\\\\n\\{1,2\\} \\cap \\{2,3\\} &= \\{2\\} \\\\\n\\{1,2\\} \\cap \\{3,4\\} &= \\emptyset\n\\end{align*}\n$$\n</Callout>  \nWe can also observe that the cardinality of the intersection of two sets has to be less than or equal to the cardinality of the set with the least amount of elements:  \n$$\n|A \\cap B| \\leq \\min(|A|,|B|)\n$$  \n#### Disjoint Sets  \nIf $A \\cap B = \\emptyset$, then we call $A$ and $B$ **disjoint** sets. This means that the sets have no elements in common. This also means that the cardinality of the intersection of two disjoint sets is 0.\nTherefore two sets are disjoint if and only if the cardinality of their intersection is 0.  \n$$\nA \\cap B = \\emptyset \\iff |A \\cap B| = 0\n$$  \n<Callout type=\"example\">\n- $\\{1,2\\} \\cap \\{3,4\\} = \\emptyset$ so $\\{1,2\\}$ and $\\{3,4\\}$ are disjoint sets.\n- $\\{1,2\\} \\cap \\{2,3\\} = \\{2\\}$ so $\\{1,2\\}$ and $\\{2,3\\}$ are not disjoint sets.\n- $\\{1,2\\} \\cap \\emptyset = \\emptyset$ so $\\{1,2\\}$ and $\\emptyset$ are disjoint sets.\n</Callout>  \n#### Arbitrary Intersections", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "Header 3": "Union", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "Two sets can be joined together, this results in a so-called **union** of $A$ and $B$. This is written as $A \\cup B$. The union is the set containing all elements that are in $A$ or $B$ or both.  \n$$\nA \\cup B = \\{x \\mid x \\in A \\lor x \\in B\\}\n$$  \n<Image\nsrc=\"/maths/setsUnion.png\"\ncaption=\"Venn diagram showing the union of two sets.\"\nwidth={300}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\{1,2\\} \\cup \\{1,2\\} &= \\{1,2\\} \\\\\n\\{1,2\\} \\cup \\{2,3\\} &= \\{1,2,3\\} \\\\\n\\{1,2,3\\} \\cup \\{3,4,5\\} &= \\{1,2,3,4,5\\}\n\\end{align*}\n$$\n</Callout>  \nFrom the definition and image above we can also make some statements regarding the cardinality of the union of two sets. The cardinality of the union of two sets has to be greater than or equal to the cardinality of the set with the most amount of elements:  \n$$\n|A \\cup B| \\geq \\max(|A|,|B|)\n$$  \nWe can also see that the cardinality of the union of two sets can be calculated by adding the cardinalities of the two sets and then subtracting the cardinality of the intersection of the two sets. This is because the intersection is counted twice when adding the cardinalities of the two sets:  \n$$\n|A \\cup B| = |A| + |B| - |A \\cap B|\n$$  \nThis also means that if the two sets are disjoint, then the cardinality of the union of the two sets is equal to the sum of the cardinalities of the two sets:  \n$$\nA \\cap B = \\emptyset \\iff |A \\cup B| = |A| + |B|\n$$  \n#### Arbitrary Unions", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "Header 3": "Difference", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "If $A$ and $B$ are sets, then the **relative complement** (or short **difference**) of $A$ and $B$, is the set of elements in $A$ without the elements of $B$. This is written as $A \\setminus B$.  \n$$\nA \\setminus B = \\{x \\mid x \\in A \\land x \\notin B\\}\n$$  \n<Image\nsrc=\"/maths/setsDifference.png\"\ncaption=\"Venn diagram showing the difference of two sets, in this case the left set minus the right set.\"\nwidth={300}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\{1,2,3\\} \\setminus \\{2,3,4\\} &= \\{1\\} \\\\\n\\{2,3,4\\} \\setminus \\{1,2,3\\} &= \\{4\\} \\\\\n\\{1,2,3\\} \\setminus \\{1,2,3,4\\} &= \\emptyset\n\\end{align*}\n$$\n</Callout>  \nWe can observe that the cardinality of the difference of two sets has to be less than or equal to the cardinality of orginal set we are taking the difference from.  \n$$\n|A \\setminus B| \\leq |A|\n$$  \nIf $A$ and $B$ are disjoint sets, then the difference of $A$ and $B$ is just $A$. Therefore the cardinality of the difference of two disjoint sets is equal to the cardinality of the first set:  \n$$\nA \\cap B = \\emptyset \\iff |A \\setminus B| = |A|\n$$  \nWe can also see that the cardinality of the difference of two sets can be calculated by subtracting the cardinality of the intersection of the two sets from the cardinality of the first set:  \n$$\n|A \\setminus B| = |A| - |A \\cap B|\n$$  \n#### Symmetric Difference  \nThe **symmetric difference** of two sets (also known as the **disjunctive union**), is the set of elements which are in either of the sets, but not in both sets (their intersection).\nThis can be written as $A \\ominus B$.  \n$$\nA \\ominus B = (A \\cup B) \\setminus (A \\cap B) = (A \\setminus B) \\cup (B \\setminus A)\n$$  \n<Image\nsrc=\"/maths/setsSymmetricDifference.png\"\ncaption=\"Venn diagram showing the symmetric difference of two sets.\"\nwidth={300}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\{1,2,3,4\\} \\ominus \\{3,4,5,6\\} = \\{1,2,5,6\\} \\\\\n\\{7,8,9,10\\} \\ominus \\{9,10,11,12\\} = \\{7,8, 11,12\\} \\\\\n\\{1,2,3,4\\} \\ominus \\{1,2,3,4\\} = \\emptyset\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "Header 3": "Difference", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "width={300}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\{1,2,3,4\\} \\ominus \\{3,4,5,6\\} = \\{1,2,5,6\\} \\\\\n\\{7,8,9,10\\} \\ominus \\{9,10,11,12\\} = \\{7,8, 11,12\\} \\\\\n\\{1,2,3,4\\} \\ominus \\{1,2,3,4\\} = \\emptyset\n\\end{align*}\n$$\n</Callout>  \nTo calculate the cardinality of the symmetric difference of two sets we can use the following:  \n$$\n|A \\ominus B| = |A| + |B| - 2|A \\cap B|\n$$  \nThe intersection of two sets is counted twice when adding the cardinalities of the two sets, so we have to subtract it twice.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "Header 3": "Complements", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The so called **absolute complement** of $A$ (or simply the **complement of** $A$) is the set of elements not in $A$. In other words, let $U$ be a set that contains all the elements under study, also called the **universal set**.\nThe universal set is not always explicitly mentioned, but is rather implied or obvious such as one of the defined sets of numbers. The complement of $A$ is the difference between the universal set and $A$ and is denoted by $A^c$.  \n$$\nA^c = U \\setminus A\n$$  \n<Image\nsrc=\"/maths/setsAbsoluteComplement.png\"\ncaption=\"Venn diagram showing the absolute complement of the left set.\"\nwidth={300}\n/>  \n<Callout type=\"example\">\nGiven the universal set $U=\\{1,2,3,4,5,6\\}$:  \n$$\n\\begin{align*}\n\\{1,2,3\\}^c &= \\{4,5,6\\} \\\\\n\\{4,5,6\\}^c &= \\{1,2,3\\} \\\\\n\\{1,2,3,4,5,6\\}^c &= \\emptyset\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "Header 3": "Cartesian Product", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The **cartesian product** of two sets $A$ and $B$, denoted by $A \\times B$. The cartesian product is the set of all possible [ordered pairs](/digitalGarden/maths/discrete/sequences#order-pair) $(a,b)$ where $a$ is an element of $A$ and $b$ is an element of $B$.\nThis can be thought of as the set of all possible combinations of elements from the two sets i.e. if we start with the first element of the first set and then combine it with all the elements of the second set,\nthen we move to the second element of the first set and combine it with all the elements of the second set and so on.  \n$$\nA \\times B = \\{(a,b) \\mid a \\in A \\land b \\in B\\}\n$$  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\{1,2\\} \\times \\{3,4\\} &= \\{(1, 3), (1, 4), (2, 3), (2, 4)\\} \\\\\n\\{red, green\\} \\times \\{blue, yellow\\} &= \\{(red, blue), (red, yellow), (green, blue), (green, yellow)\\}\n\\end{align*}\n$$  \nThis could also be used if say we had two dice and we wanted to know all the possible outcomes of rolling them both.\nWe could then define the set $A=\\{1,2,3,4,5,6\\}$ to represent a dice and then calculate the cartesian product $A \\times A$.\nAll the possible outcomes would then be the elements of the cartesian product.\n</Callout>  \nFrom the image we can see that the cardinality of the cartesian product of two sets is as if we were calculating the area of a rectangle. The cardinality of the cartesian product of two sets is the product of the cardinalities of the two sets:  \n$$\n|A \\times B| = |A| \\cdot |B|\n$$  \n#### Cartesian Power  \nThe idea of the cartesian product can be extended to a so called **cartesian power** of a set. If we take the cartesian product of a set with itself $A \\times A$ we get the cartesian power of $A$ to the power of 2, denoted by $A^2$.  \nThis can then be generalized to the cartesian power of $A$ to the power of $n$, denoted by $A^n$.  \n$$\nA^n = A \\times A \\times A \\times ... \\times A = \\{(a_1,a_2,...,a_n) \\mid a_1,a_2,...,a_n \\in A\\}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "Header 3": "Cartesian Product", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "This can then be generalized to the cartesian power of $A$ to the power of $n$, denoted by $A^n$.  \n$$\nA^n = A \\times A \\times A \\times ... \\times A = \\{(a_1,a_2,...,a_n) \\mid a_1,a_2,...,a_n \\in A\\}\n$$  \nThis is especially useful if for example we wanted to define all the possible points in 2D space. This could be done with the cartesian power of the set of real numbers to the power of 2, $\\Bbb{R}^2$.\nWhere the first element of the ordered pair would represent the x-coordinate and the second element the y-coordinate. This could then be extended to 3D space with $\\Bbb{R}^3$ and so on hence the name cartesian power and cartesian plane.  \n<Image\nsrc=\"/maths/setsCartesianPlane.png\"\ncaption=\"The cartesian plane showing all the possible points in 2D space.\"\nwidth={300}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Set Operations", "Header 3": "Operation Properties", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "such as commutative, associative, distributive also for example when are brackets needed.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Power Set", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "The power set or also called super set of a set $A$ is the set containing all possible subsets of $A$. Importantly the empty set and $A$ itself are also elements of the power set of $A$, because these are also both subsets of $A$.\nThe power set of the set $A$ is commonly written as: $P(A)$, $\\mathscr{P}(A)$, $2^A$ or $S^A$. I will use the notation $\\mathscr{P}(A)$.  \n$$\n\\mathscr{P}(A) = \\{X \\mid X \\subseteq A\\}\n$$  \nTo create the power set of a set, we can use either a tree diagram or a hasse diagram. The tree diagram is the most common way to create a power set, but the hasse diagram is a more compact way to represent the power set.  \nFor the tree diagram, we start with the empty set and then add the elements of the original set one by one. For each element we have two choices, either include it in the subset or not.  \n<Image\nsrc=\"/maths/setsTreeDiagram.png\"\ncaption=\"Tree diagram showing the power set of the set {1,2,3}.\"\nwidth={600}\n/>  \nThis results in a binary tree with $2^n$ leaves, where $n$ is the number of elements in the original set. This also tells us that the cardinality of the power set of a set with $n$ elements is $2^n$.  \n$$\n|\\mathscr{P}(A)| = 2^{|A|}\n$$  \nTo create the hasse diagram, we start at the bottom with the empty set and then add the elements of the original set one by one. We then connect the subsets that are related to each other, i.e. if one subset is a subset of another subset.\nThis also results in a diagram where the subsets are ordered by their cardinality from top to bottom.  \n<Image\nsrc=\"/maths/setsHasseDiagram.png\"\ncaption=\"Hasse diagram showing the power set of the set {1,2,3}.\"\nwidth={300}\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\mathscr{P}(\\{1,2\\}) &= \\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}\\} \\\\\n\\mathscr{P}(\\{1,2,3\\}) &= \\{\\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\} \\\\\n\\mathscr{P}(\\emptyset) &= \\{\\emptyset\\}\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Power Set", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "$$\n\\begin{align*}\n\\mathscr{P}(\\{1,2\\}) &= \\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}\\} \\\\\n\\mathscr{P}(\\{1,2,3\\}) &= \\{\\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}, \\{1,2,3\\}\\} \\\\\n\\mathscr{P}(\\emptyset) &= \\{\\emptyset\\}\n\\end{align*}\n$$  \nImportantly the power set of the empty set is the set containing only the empty set. This also means that the cardinality of the power set of the empty set is 1 not 0.\n</Callout>  \n<Callout type=\"info\">\nIf we have $\\mathscr{P}(\\mathbb{R}^2)$, the power set of the cartesian plane, then this would be the set of all possible subsets of the cartesian plane. This would be the set of all possible shapes and areas that can be created in 2D space.\nSo an element of this power set could be a point, a line, a circle, a square, a triangle, a function and so on.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Partitions", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "A partition $P$ of a set $A$ is a set of non-empty subsets of $A$ such that every element $a \\in A$ is only exactly in one of the subsets in $P$. This means that the subsets in $P$ are disjoint and their union is equal to $A$.\nTherefore the partition of a set can be thought of as a way to divide the set into non-overlapping parts. This can be summarized as the following:  \n- P does not contain the empty set, so $\\emptyset \\notin P$.\n- The union of all the sets in $P$ is equal to $A$, so $\\bigcup_{X \\in P} X = A$.\n- The elements of $P$ don't overlap i.e the sets in $P$ are disjoint, so $X \\cap Y = \\emptyset$ for all $X,Y \\in P$.  \n<Image\nsrc=\"/maths/setsPartition.png\"\ncaption=\"Venn diagram showing a partition of a set.\"\nwidth={500}\n/>  \n<Callout type=\"example\">\nThe set $\\{1,2,3\\}$ has 5 possible partitions:  \n- $\\{\\{1\\},\\{2\\},\\{3\\}\\}$\n- $\\{\\{1,2\\},\\{3\\}\\}$\n- $\\{\\{1,3\\},\\{2\\}\\}$\n- $\\{\\{1\\},\\{2,3\\}\\}$\n- $\\{\\{1,2,3\\}\\}$  \nThe following are not partitions of \\{1,2,3\\}:  \n- $\\{\\emptyset,\\{1,3\\},\\{2\\}\\}$ is not a partition because one of its elements is the empty set.\n- $\\{\\{1,2\\},\\{2,3\\}\\}$ is not a partition because the element 2 is contained in more than one block.\n- $\\{\\{1\\},\\{2\\}\\}$ is not a partition of $\\{1,2,3\\}$ because none of its blocks contains the element 3.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Indexed Sets", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "https://faculty.etsu.edu/gardnerr/3000/notes-MR/Gerstein-2-6.pdf", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Set Theory", "Header 2": "Multisets or Bags", "path": "../pages/digitalGarden/maths/discrete/setTheory.mdx"}, "page_content": "A multiset in maths or [bag in computer science](/digitalGarden/cs/algorithmsDataStructures/bags) is a set that allows for multiple instances of the same element. This means that the elements of a multiset are not unique and can be repeated.\nThink of it is a big bag where you can throw in pretty much anything and the order doesn't matter.  \nIn maths multisets are rarely seen or used, but in computer science they are used to represent collections of objects where the order doesn't matter and the same object can appear multiple times.  \n<Image\nsrc=\"/maths/setsMultiset.png\"\ncaption=\"Comparison of a set and a multiset.\"\nwidth={600}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Intervals", "path": "../pages/digitalGarden/maths/discrete/intervals.mdx"}, "page_content": "If we have a set of real numbers, we can define a subset of this set by specifying a range of values. This subset is called an interval.\nTo define an interval, we need two real numbers, $a$ and $b$, with $a < b$.  \nWe can then define the following types of intervals:  \n- **Open interval**: We include all the numbers between $a$ and $b$, but not $a$ and $b$ themselves. This is denoted as $(a, b) = \\{x \\in \\mathbb{R} \\mid a < x < b\\}$.\n- **Closed interval**: We include all the numbers between $a$ and $b$, as well as $a$ and $b$ themselves. This is denoted as $[a, b] = \\{x \\in \\mathbb{R} \\mid a \\leq x \\leq b\\}$.\n- **Half-open interval**: We include one of the bounds but not the other. This is denoted as $[a, b) = \\{x \\in \\mathbb{R} \\mid a \\leq x < b\\}$ or $(a, b] = \\{x \\in \\mathbb{R} \\mid a < x \\leq b\\}$.\n- **Unbounded/Infinite interval**: On one or both sides, the interval extends indefinitely. This is denoted as $(-\\infty, b] = \\{x \\in \\mathbb{R} \\mid x \\leq b\\}$ if we want all numbers less or equal than $b$, or $(a, \\infty) = \\{x \\in \\mathbb{R} \\mid x > a\\}$ if we want all numbers greater than $a$.\nWe can also have intervals that are unbounded on both sides, such as $(-\\infty, \\infty) = \\mathbb{R}$.  \n<Callout type=\"info\">\nNote that the bound of the interval that is included is always the one with the square bracket if it is excluded it is the one with the round bracket.  \nFor example, $[a, b)$ includes $a$ but not $b$, while $(a, b]$ includes $b$ but not $a$.  \nAlso, note that the unbounded side with $\\infty$ is always excluded i.e. uses the round bracket.\n</Callout>  \n<Image\nsrc=\"/maths/intervalsLines.png\"\ncaption=\"All types of intervals visualized on the number line.\"\nwidth={600}\n/>  \n<Callout type=\"example\">\nExamples of intervals:\n- $(2, 5)$ is the set of all real numbers between 2 and 5, but not including 2 and 5.\n- $[3, 7]$ is the set of all real numbers between 3 and 7, including 3 and 7.\n- $(-\\infty, 4]$ is the set of all real numbers less than or equal to 4.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Intervals", "path": "../pages/digitalGarden/maths/discrete/intervals.mdx"}, "page_content": "- $(2, 5)$ is the set of all real numbers between 2 and 5, but not including 2 and 5.\n- $[3, 7]$ is the set of all real numbers between 3 and 7, including 3 and 7.\n- $(-\\infty, 4]$ is the set of all real numbers less than or equal to 4.\n- $(6, \\infty)$ is the set of all real numbers greater than 6.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "non-empty set S of objects such as numbers, functions or matrices with one or more operations $*$.  \nDepending on the properties of the operations and the set, the algebraic structure can be classified into different types.\nwhich can be useful for understanding the properties of the set and the operations and deriving proofs.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Operations", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "In mathematics, an operation is a function which takes zero or more input values (also called \"operands\" or \"arguments\") to a well-defined output value. The number of operands is the arity of the operation.  \nThe most commonly studied operations are binary operations (i.e., operations of arity 2), such as addition and multiplication, and unary operations (i.e., operations of arity 1), such as additive inverse and multiplicative inverse. An operation of arity zero, or nullary operation, is a constant.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Operations", "Header 3": "Closure", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "An operation $*$ is closed if any operation on elements of the set $S$ results in an element that is also in the set $S$.\nMore formally:  \n$$\n\\forall a, b \\in S, a * b \\in S\n$$  \n<Callout type=\"example\">\nAddition and multiplication are closed operations on the set of integers. However,\ndivision is not a closed operation on the set of integers. For example:  \n$$\n2 \\div 3 = 0.6667 \\notin \\mathbb{Z}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Operations", "Header 3": "Binary Operation", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "More specifically, a binary operation on a set is a binary operation whose two domains and the codomain are the same set, i.e\nthe operation takes two elements from the set and returns an element from the set (closure).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Operations", "Header 3": "N-ary Operation", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "Officially doesn't have to be closed.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Properties of Operations", "Header 3": "Commutativity", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "An operation $*$ is commutative if for all $a, b \\in S$:  \n$$\na * b = b * a\n$$  \nIn other words, the order of the operands does not matter.  \n<Callout type=\"example\">\nAddition and multiplication are commutative operations. For example:  \n$$\n\\begin{align*}\n2 + 3 &= 3 + 2 = 5 \\\\\n2 \\cdot 3 &= 3 \\cdot 2 = 6\n\\end{align*}\n$$  \nSubtraction and division are not commutative operations. For example:  \n$$\n\\begin{align*}\n2 - 3 &\\neq 3 - 2 \\\\\n-1 &\\neq 1 \\\\\n2 \\div 3 &\\neq 3 \\div 2 \\\\\n0.6667 &\\neq 1.5\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Properties of Operations", "Header 3": "Associativity", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "An operation $*$ is associative if for all $a, b, c \\in S$:  \n$$\na * (b * c) = (a * b) * c\n$$  \nIn other words, the grouping of the operands does not matter. So you can group the operands in any way you like\nas long as the order of the operands is preserved and there are no other operations in between.  \n<Callout type=\"example\">\nAddition and multiplication are associative operations. For example:  \n$$\n\\begin{align*}\n2 + (3 + 4) &= (2 + 3) + 4 = 9 \\\\\n2 \\cdot (3 \\cdot 4) &= (2 \\cdot 3) \\cdot 4 = 24\n\\end{align*}\n$$  \nSubtraction and division are not associative operations. For example:\n$$\n\\begin{align*}\n2 - (3 - 4) &\\neq (2 - 3) - 4 \\\\\n3 &\\neq -5 \\\\\n2 \\div (3 \\div 4) &\\neq (2 \\div 3) \\div 4\n2.666 &\\neq 0.166\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Properties of Operations", "Header 3": "Distributivity", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "An operation $*$ is distributive with respect to another operation $+$ if for all $a, b, c \\in S$:  \n$$\na * (b + c) = (a * b) + (a * c)\n$$  \nand  \n$$\n(b + c) * a = (b * a) + (c * a)\n$$  \nIn other words, the operation \"distributes\" itself over the other operation. You can think of\ndistributing as copying the operation to each of the operands of the other operation.  \n<Callout type=\"example\">\nThe operation of multiplication is distributive with respect to addition. For example:  \n$$\n2 \\cdot (3 + 4) = (2 \\cdot 3) + (2 \\cdot 4) = 14\n$$  \nThe operation of division is not distributive with respect to addition. For example:  \n$$\n2 \\div (3 + 4) \\neq (2 \\div 3) + (2 \\div 4)\n$$\n</Callout>  \n#### Left and Right Distributivity  \nAn operation $*$ is left distributive if for all $a, b, c \\in S$ the operation is distributive with respect to $+$\nwhen the operation is on the left:  \n$$\na * (b + c) = (a * b) + (a * c)\n$$  \nnot necessarily for the right:  \n$$\n(b + c) * a = (b * a) + (c * a)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Properties of Operations", "Header 3": "Existential Clauses", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "There are also operations that define existential clauses, i.e the operation requires the existence of a special\nelement that satisfies a certain property.  \n#### Identity  \n#### Inverse", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Categories of Algebraic Structures", "Header 3": "Semigroups", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "also called half-group, just need to be associative  \n#### Commutative Semigroups  \nIf it is also commutative, it is called a commutative semigroup or abelian semigroup.\nmost operations that are also commutative are either called so or abelian.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Categories of Algebraic Structures", "Header 3": "Monoids", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "Inbetween semigroups and groups,\nIs a semigroup with an identity element. Therfore obeying all the properties of a group except for the inverse property.  \nWhy are these useful especially in computer science?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Categories of Algebraic Structures", "Header 3": "Groups", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "CAIN properties, c is closure  \naccosiativy, identity, inverse  \nint with +, non-zero real numbers with multiplication  \n#### Commutative Groups  \nIf it is group and commutative, also called a abelian group  \n#### Cyclic Groups  \nis a group that is generated by a single element, composition table to see if it is cyclic", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Categories of Algebraic Structures", "Header 3": "Rings", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "Should already be a abelian group under both operations?  \ntwo operations, + and *, called addition and multiplication (don't have to be the same as the normal ones).\nneeds closure under multiplication and associative.  \n- commutative group under addition\n- associative under multiplication  \nThere is a multiplicative Identity??? unclear  \n* is distributive over +  \n#### Commutative Rings  \nif the multiplication operation is commutative", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Categories of Algebraic Structures", "Header 3": "Fields", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "integral domain with multiplicative inverse  \nalso somehow relates to vector spaces  \n#### Integral Domains  \ncommutative ring with multiplicative identity that\nalso has no zero divisors  \n#### Finite Fields  \nalso known as Galois fields if set S is finite. important in cryptography because\nof integers modulo a prime number", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Algebraic Structures", "Header 2": "Categories of Algebraic Structures", "Header 3": "Modules", "path": "../pages/digitalGarden/maths/discrete/algebraicStructures.mdx"}, "page_content": "generalization of vector spaces, but instead of scalars being in a field, they are in a ring??", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Homomorphisms", "path": "../pages/digitalGarden/maths/discrete/homomorphisms.mdx"}, "page_content": "morphe means shape in greek, so homomorphism means same shape.  \nhomomorphism is a structure-preserving map between two algebraic structures of the same type.\nHomomorphisms of vector spaces are also called linear maps  \nmap, i.e relation for $f: S \\to T$ that preserves the structure of the algebraic structure so if they both\ndefine the same operation *, then the homomorphism will preserve the operation. Preserving\nthe operation means that it doesn't matter if you apply the operation before or after the homomorphism i.e\nthe mapping. Why is this useful think of linear transformations in linear algebra?  \n$$\nf(a * b) = f(a) * f(b)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Homomorphisms", "Header 2": "Isomorphisms", "path": "../pages/digitalGarden/maths/discrete/homomorphisms.mdx"}, "page_content": "bijective homomorphism, i.e a homomorphism that is also a bijection can go back and forth between the two structures.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Partial & Total Orders", "Header 2": "Partial Order", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/partialOrders.mdx"}, "page_content": "Hasse Diagram something with anti-symmetric and transitive  \nThe word partial is used to indicate that not every pair of elements needs to be comparable; that is, there may be pairs for which neither element precedes the other.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Partial & Total Orders", "Header 2": "Partial Order", "Header 3": "Weak Partial Order", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/partialOrders.mdx"}, "page_content": "is a homogeneous relation < = on a set P that is reflexive, antisymmetric, and transitive.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Partial & Total Orders", "Header 2": "Partial Order", "Header 3": "Strong Partial Order", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/partialOrders.mdx"}, "page_content": "is a homogeneous relation < on a set P that is irreflexive, asymmetric, and transitive", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Recurrence Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/recurrenceRelations.mdx"}, "page_content": "A reccurance relation is a function that generates a sequence as a function of the index of the sequence and one\nor more previous terms of the sequence.  \n$$\na_n = f(n, a_{n-1}, a_{n-2}, \\ldots, a_{n-k}) \\quad \\text{for} \\quad n \\geq k\n$$  \nwhere $n$ is the index of the sequence and $a_n$ is the $n$th term of the sequence. The elements\n$a_{n-1}, a_{n-2}, \\ldots, a_{n-k}$ are the previous terms of the sequence.  \nIf the reccurance relation only depends on the previous term, then it is called a first order reccurance relation\nand can be written as:  \n$$\n\\begin{align*}\na_n = &f(n, a_{n-1}) \\quad \\text{for} \\quad n \\geq 1 \\\\\n&f: \\mathbb{N} \\times X \\rightarrow X\n\\end{align*}\n$$  \nwhere $X$ is the set of all possible values of the sequence. More generally, a reccurance relation of order $k$\ncan be written as:  \n$$\n\\begin{align*}\na_n = &f(n, a_{n-1}, a_{n-2}, \\ldots, a_{n-k}) \\quad \\text{for} \\quad n \\geq k \\\\\n&f: \\mathbb{N} \\times X^k \\rightarrow X\n\\end{align*}\n$$  \n<Callout type=\"warning\">\nWhen defining a reccurance relation, it is important to specify the initial conditions of the sequence.\nIf the reccurance relation is of order $k$, then the first $k$ terms of the sequence must be specified.\n</Callout>  \n<Callout type=\"example\">\nThere are some very common and well known reccurance relations. For example, the Fibonacci sequence is defined by the\nsecond order reccurance relation:  \n$$\n\\begin{align*}\na_0 &= 0 \\\\\na_1 &= 1 \\\\\na_n &= a_{n-1} + a_{n-2}\n\\end{align*}\n$$  \nwhere $F_n$ is the $n$th term of the Fibonacci sequence.  \nA popular way to visualize reccurance relations is to draw a call or recursion tree, which\nshows the recursive calls made by the function. The call tree for the Fibonacci sequence is shown below:  \n<Image\nsrc=\"/maths/relationsFibonacci.png\"\ncaption=\"The call tree for the Fibonacci sequence.\"\nwidth={700}\n/>  \nIf we wanted to code the Fibonacci sequence, we could use the following recursive function:  \n```java\npublic int fib(int n) {\nif (n <= 1)\nreturn n;\nreturn fib(n - 1) + fib(n - 2);\n}\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Recurrence Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/recurrenceRelations.mdx"}, "page_content": "width={700}\n/>  \nIf we wanted to code the Fibonacci sequence, we could use the following recursive function:  \n```java\npublic int fib(int n) {\nif (n <= 1)\nreturn n;\nreturn fib(n - 1) + fib(n - 2);\n}\n```  \nHowever, this function can be shown to have a time complexity of $O(2^n)$, which is very slow. Instead we\ncan use dynamic programming to improve the time complexity to $O(n)$ as shown and discussed [here](/digitalGarden/cs/algorithmsDataStructures/dynamicProgramming/introduction).  \nAnother example is the factorial function, which is defined by the first order reccurance relation:  \n$$\n\\begin{align*}\na_0 &= 1 \\\\\na_n &= n \\cdot a_{n-1}\n\\end{align*}\n$$  \nwhere $n!$ is the factorial of $n$. Or as is more commonly written:  \n$$\n0! = 1 \\quad \\text{and} \\quad n! = n \\cdot (n-1)!\n$$  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "In mathematics we use many symbols such as $\\leq$, $\\geq$, $<$, $>$, $=$, $\\neq$ to represent the relationship between two objects. These symbols are called relations or relational operators because they convey the relationship between two objects.\nThese realtions are defined on a specific set. For example, the relation $\\leq$ is defined on the set of real numbers $\\mathbb{R}$. To define a relation, we need to specify a set of order pairs that relates a set $A$'s elements to each other i.e. is a subset of $A \\times A$:\nThese order pairs are special as $(a, b)$ is used to define the relationship between $a$ and $b$, so we can say that \"$a$ relates to $b$\".  \n$$\nR \\subseteq \\{(a, b) \\in A \\times A\\}\n$$  \nBecause the relations is defined on the same set twice it is called a homogeneous relation, from the Greek word \"homo\" meaning \"same\" or \"identical\".  \nLets now put this all together and define a relation on the set $A = \\{1, 2, 3, 4\\}$:  \n$$\nR = \\{(1, 1), (1, 3), (3, 1), (3, 3), (2, 2), (2, 4), (4, 2), (4, 4)\\} \\subseteq A \\times A\n$$  \nIf we then take an element of the realtion such as $(1, 1)$ it means that $1$ relates to $1$ and $(1, 3)$ means that $1$ relates to $3$. The relation shown above can be thought of as \"has the same parity\" as $1$ and $3$ are both odd and $2$ and $4$ are both even.\nThis relationship can also be denoted as followed:  \n$$\n(a, b) \\in R \\iff aRb\n$$  \nThis means that if the order pair $(a, b)$ is in the relation $R$, then $a$ relates to $b$, so $aRb$. If they are not in the relation, then $a$ does not relate to $b$ and we can denote this as $a \\cancel{R} b$.  \nThere are many ways to show relations such as a table or a directed graph, sometimes also called a digraph, arrow diagram or mapping diagram. The table below shows the relation $R$ defined above:  \n| $R$ | 1 | 2 | 3 | 4 |\n|---|---|---|---|---|\n|1|1|0|1|0|\n|2|0|1|0|1|\n|3|1|0|1|0|\n|4|0|1|0|1|  \nThe directed graph of the relation $R$:  \n<Image\nsrc=\"/maths/relationsDiagram.png\"\ncaption=\"A directed graph of the relation above.\"", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "| $R$ | 1 | 2 | 3 | 4 |\n|---|---|---|---|---|\n|1|1|0|1|0|\n|2|0|1|0|1|\n|3|1|0|1|0|\n|4|0|1|0|1|  \nThe directed graph of the relation $R$:  \n<Image\nsrc=\"/maths/relationsDiagram.png\"\ncaption=\"A directed graph of the relation above.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Empty Relation", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "The empty relation is a relation that has no elements in it. This means that no elements in the set relate to each other. The empty relation is denoted by $\\emptyset$ or $\\{\\}$.\nIn other words if:  \n$$\nR = \\emptyset\n$$  \nThen for all elements $a, b \\in A$:  \n$$\n(a, b) \\notin R \\text{ or } a \\cancel{R} b\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Universal Relation", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "The universal relation is a homogeneous relation that contains all possible order pairs of the set $A$. This means that every element in the set relates to every other element. The universal relation is denoted by $A \\times A$.  \n$$\nR = A \\times A\n$$  \nThis means that for all elements $a, b \\in A$:  \n$$\n(a, b) \\in R \\text{ or } aRb\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Properties of Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "Depending on the relationship between the elements of a set, a relation can have different properties.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Properties of Relations", "Header 3": "Reflexive", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "A relation $R$ is reflexive if every element in the set $A$ relates to itself.  \n$$\n\\forall a \\in A, (a, a) \\in R\n$$  \nAn example of a reflexive relation is the relation $\\leq$ on the set of real numbers $\\mathbb{R}$ as every number is less than or equal to itself.  \nWe can see in the table below that every element has a \"1\" in the diagonal and therefore the relation is reflexive.\nThe same can be seen in the diagram where every element has a loop to itself.  \n<Image\nsrc=\"/maths/relationsReflexive.png\"\ncaption=\"An example of a reflexive relation.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Properties of Relations", "Header 3": "Irreflexive", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "A relation $R$ is irreflexive if no element in the set $A$ relates to itself.  \n$$\n\\forall a \\in A, (a, a) \\notin R\n$$  \nAn example of an irreflexive relation is the relation $<$ on the set of real numbers $\\mathbb{R}$ as no number is less than itself.  \nWe can see in the table below that every element has a \"0\" in the diagonal and therefore the relation is irreflexive.\nThe same can be seen in the diagram where no element has a loop to itself.  \n<Image\nsrc=\"/maths/relationsIrreflexive.png\"\ncaption=\"An example of an irreflexive relation.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Properties of Relations", "Header 3": "Symmetric", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "A relation $R$ is symmetric if for every element $(a, b)$ in the relation, $(b, a)$ is also in the relation.  \n$$\n\\forall a, b \\in A, (a, b) \\in R \\implies (b, a) \\in R\n$$  \nAn example of a symmetric relation is the relation $=$ on the set of real numbers $\\mathbb{R}$ as if $a = b$ then $b = a$.  \nWe can see in the table below that if $(a, b)$ is in the relation then $(b, a)$ is also in the relation and therefore the relation is symmetric.\nThis means that the table is symmetrical or mirrored along the diagonal.\nThe same can be seen in the diagram where if there is an arrow from $a$ to $b$ then there is an arrow from $b$ to $a$.  \n<Image\nsrc=\"/maths/relationsSymmetric.png\"\ncaption=\"An example of a symmetric relation.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Properties of Relations", "Header 3": "Antisymmetric", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "A relation $R$ is antisymmetric if for every element $(a, b)$ in the relation, $(b, a)$ is not in the relation unless $a = b$.  \n$$\n\\forall a, b \\in A, \\big[(a, b) \\in R \\land (b, a) \\in R\\big] \\implies a = b\n$$  \nAn example of an antisymmetric relation is the relation $\\geq$ on the set of real numbers $\\mathbb{R}$ as if $a \\geq b$ and $b \\geq a$ then $a = b$.  \nWe can see in the table below that if $(a, b)$ is in the relation and $(b, a)$ is also in the relation then the corresponding diagonal element is a \"1\" and therefore the relation is antisymmetric.\nThe same can be seen in the diagram that there are no arrows from $a$ to $b$ and $b$ to $a$ only one way arrows and loops.  \n<Image\nsrc=\"/maths/relationsAntisymmetric.png\"\ncaption=\"An example of an antisymmetric relation.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Properties of Relations", "Header 3": "Asymmetric", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "A relation $R$ is asymmetric if for every element $(a, b)$ in the relation, $(b, a)$ is not in the relation, even if $a = b$. So it can be thought of as a stricter version of antisymmetric or a combination of antisymmetric and irreflexive.  \n$$\n\\forall a, b \\in A, (a, b) \\in R \\implies (b, a) \\notin R\n$$  \nAn example of an asymmetric relation is the relation $<$ on the set of real numbers $\\mathbb{R}$ as if $a < b$ then $b \\nless a$.  \nWe can see in the table below that if $(a, b)$ is in the relation then $(b, a)$ is not in the relation and all the diagonal elements are \"0\" and therefore the relation is asymmetric.\nThe same can be seen in the diagram that there are no arrows from $a$ to $b$ and $b$ to $a$ and no loops.  \n<Image\nsrc=\"/maths/relationsAsymmetric.png\"\ncaption=\"An example of an asymmetric relation.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Properties of Relations", "Header 3": "Transitive", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "A relation $R$ is transitive if for every element $(a, b)$ and $(b, c)$ in the relation, $(a, c)$ is also in the relation.  \n$$\n\\forall a, b, c \\in A, \\big[(a, b) \\in R \\land (b, c) \\in R\\big] \\implies (a, c) \\in R\n$$  \nAn example of a transitive relation is the relation $\\leq$ on the set of real numbers $\\mathbb{R}$ as if $a \\leq b$ and $b \\leq c$ then $a \\leq c$.  \nWe can see in the table below that if $(a, b)$ and $(b, c)$ are in the relation then $(a, c)$ is also in the relation and therefore the relation is transitive.\nThe same can be seen in the diagram that if there is an arrow from $a$ to $b$ and $b$ to $c$ then there is an arrow from $a$ to $c$.  \n<Image\nsrc=\"/maths/relationsTransitive.png\"\ncaption=\"An example of a transitive relation.\"\nwidth={500}\n/>  \n<Callout type=\"info\">\nImportantly note if for example $(a,b) \\in R$ and $(b,a) \\in R$ then $(a,a)$ and $(b,b)$ must also be in the relation for it to be transitive.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Relations between Sets", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "Relations can also be defined between two different sets, this is called a heterogeneous relation, from the Greek word \"hetero\" meaning \"different\". This means that the relation $R$ is a subset of the Cartesian product of the two sets $A$ and $B$, rather than between the cartesian product of the same set $A$.  \n$$\nR \\subseteq A \\times B\n$$  \nThe first set $A$ is called the domain and the second set $B$ is called the codomain. This means that the relation assigns elements of the domain to elements of the codomain.\nThis can be shown in a directed graph where the elements of the domain are on the left and the elements of the codomain are on the right.  \n<Image\nsrc=\"/maths/relationsHeteregenous.png\"\ncaption=\"An example of a relation between two different sets.\"\nwidth={500}\n/>  \nDepending on the relationship between the elements of the two sets not all elements of the codomain are assigned to an element of the domain. The elements of the codomain that are assigned to an element of the domain are called the range of the relation.\nSo in the above diagram the domain is $\\{1,2,5,7\\}$ and the codomain is $\\{a,c,m,n\\}$ and the range of the relation is $\\{a,c,n\\}$.  \n<Callout type=\"info\">\nWhen defining a relation between two differenet sets the order matters. This is obvious as if we have a relation between two sets $A$ and $B$ then the order pair $(a, b)$ is not the same as $(b, a)$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Identity Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "The identity relation is a special type of relation that is only defined for homogenous relations. The identity relation is a relation where every element in the set relates to itself and no other elements.\nIt is a reflexive, symmetric and transitive relation and is denoted by $I_A$ where $A$ is the set that the relation is defined on.  \n$$\nI_A = \\{(a, a) \\in A \\times A\\}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Inverse Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "The inverse of a relation $R$ is a relation that is the opposite of $R$ i.e. it \"undo's\" the relationship between the elements of the set. The inverse of a relation $R$ is denoted by $R^{-1}$.  \n$$\nR^{-1} = \\{(b, a) \\in A \\times A | (a, b) \\in R\\}\n$$  \nThis means that if $(a, b)$ is in the relation $R$ then $(b, a)$ is in the inverse relation $R^{-1}$. In other words the domain and codomain of the realtion $R$ is swapped in the inverse relation $R^{-1}$ and become the codomain and domain respectively.  \nWhen visualising the inverse relation in a directed graph the arrows are simply reversed.  \n<Image\nsrc=\"/maths/relationsInverse.png\"\ncaption=\"On the left is the original relation p and on the right is its inverse relation q.\"\nwidth={700}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Relations", "Header 2": "Composition of Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/relations.mdx"}, "page_content": "Relations can also be combined or chained together to form a new relation. This is called the composition of relations. The composition of two relations $R$ and $S$ is denoted by $R \\circ S$ and is defined as:  \n$$\nR \\circ S = \\{(a, c) \\in A \\times C | \\exists b \\in B, (a, b) \\in R \\land (b, c) \\in S\\}\n$$  \nThis means that if there is an element $(a, b)$ in the relation $R$ and an element $(b, c)$ in the relation $S$ then there is an element $(a, c)$ in the composition of the two relations $R \\circ S$.  \nWhen visualising the composition of relations in a directed graph the arrows are simply chained together.  \n<Image\nsrc=\"/maths/relationsComposition.png\"\ncaption=\"An example of the composition of two relations.\"\nwidth={600}\n/>  \n<Callout type=\"info\">\nImportantly note that the composition of relations is not commutative i.e. $R \\circ S \\neq S \\circ R$.  \nAlso note that the relations are denoted from left to right i.e. $R \\circ S$ means that $R$ is applied first and then $S$ is applied. Unlike functions where $f \\circ g$ means that $g$ is applied first and then $f$ is applied.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "You have most likely seen functions before. They are a fundamental concept in mathematics and are also often seen in the real world.\nFunctions describe relationships between things. For example, the cost of something might be a function of the number of items bought\nor the distance you travel might be a function of the time and speed you travel at.  \n<Image\nsrc=\"/maths/functionsInputOutput.png\"\ncaption=\"A function takes an input and maps it to an output.\"\nwidth={300}\n/>  \nTo be more formal and precise a function is a relation between two sets.\nThe first set is the input set, also called the domain, and the second set is the output set, also called the codomain.\nWe then also provide a rule that assigns each element in the domain to exactly one element in the codomain.  \n$$\nf: A \\to B, x \\mapsto f(x)\n$$  \nWhere $f(x)$ is the rule that assigns each element in the domain to an element in the codomain.\nFor example we can make define the following rule:  \n$$\nf: \\mathbb{R} \\to \\mathbb{R}, x \\mapsto 2x + 1\n$$  \nAbove we have a function $f$ that maps the elements in the set $A$ to the elements in the set $B$. The\nrule that assigns each element in the domain to an element in the codomain is $x \\mapsto 2x + 1$, which means that\neach element $x$ in the domain is mapped to $2x + 1$ in the codomain. The subset of elements in the codomain that are actually mapped to are called the image or range of the function.  \n<Callout type=\"warning\">\nNot every relation is a function. A relation is a function if each element in the domain is mapped to exactly one element in the codomain.  \n<SideBySideBlock>\n<Block>\n<Image\nsrc=\"/maths/functionsRelation.png\"\ncaption=\"A relation that is a function.\"\nwidth={250}\n/>\n</Block>\n<Block>\n<Image\nsrc=\"/maths/functionsNonRelation.png\"\ncaption=\"A relation that is not a function.\"\nwidth={250}\n/>\n</Block>\n</SideBySideBlock>  \nThe relation on the right is not a function because the element $2$ in the domain is mapped to two elements in the codomain, $B$ and $C$,", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "caption=\"A relation that is not a function.\"\nwidth={250}\n/>\n</Block>\n</SideBySideBlock>  \nThe relation on the right is not a function because the element $2$ in the domain is mapped to two elements in the codomain, $B$ and $C$,\nwhich would make the function ambiguous. It is also not a function because the elements $3$ and $4$ in the domain are not mapped to any element in the codomain.  \nImportant is also to notice that a function can have multiple input elements\nmap to the same output element, but not the other way around. An example of such a function is $f: \\mathbb{R} \\to \\mathbb{R}, x \\mapsto x^2$ where\nboth $-2$ and $2$ are mapped to $4$.\n</Callout>  \nYou may however have seen functions written in different ways. The function above could also be written as:  \n$$\n\\begin{align*}\ny = 2x + 1 \\\\\nf(x) = 2x + 1 \\\\\ny = f(x) = 2x + 1 \\\\\n\\end{align*}\n$$  \nIn the above example the domain and codomain are not explicitly defined. The domain and codomain is often assumed to be the set of all real numbers, unless otherwise specified.  \n<Callout type=\"warning\">\nHowever, depending on the function it is important to remove 0 from the domain.\nFor example, the function $f(x) = \\frac{1}{x}$ is not defined for $x = 0$ because of the division by zero.  \nTherefore the domain of the function $f(x) = \\frac{1}{x}$ must be $\\mathbb{R} \\setminus \\{0\\}$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Visualising Functions", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "Like with everything, visualising functions can help us understand them better.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Visualising Functions", "Header 3": "Value Tables", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "Show an example with value table", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Visualising Functions", "Header 3": "Graphs", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "show a plot of a function and explain x and y with desmos iframe. Also show the meaning of y-axis cutoff and slope. and the meaning of a slope and y-axis when drawing\nie the steps and where the line crosses the y-axis.  \nFormally the graph of the function $f: A \\to B$ is the set of all points $(x, f(x))$ where $x \\in A$:  \n$$\nG = \\{(x, f(x)) | x \\in A\\}\n$$  \nWhere $G$ is the graph of the function $f$ and each point $(x, f(x))$ is a point on the graph.  \nThe number of elements in the tuple $(x, f(x))$ corresponds to the number of dimensions of the function. For example, a function $f: \\mathbb{R} \\to \\mathbb{R}$ is a 2-dimensional function\nand can be visualised in a 2-dimensional plane. A function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ is a 3-dimensional function and can be visualised in a 3-dimensional space.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Linear vs Non-Linear Functions", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "for example polynomial functions, exponential functions, logarithmic functions, trigonometric functions, etc.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Functions vs Equations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "What is the difference between function and equation.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Properties of Functions", "Header 3": "Injective", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "A function $f: A \\to B$ is injective or one-to-one if each element in the domain is mapped to a unique element in the codomain,\ni.e no two elements in the domain are mapped to the same element in the codomain. Because each element in the domain\nis mapped to a unique element in the codomain, the function is one-to-one so i.e each unique element in the domain is mapped to a unique element in the codomain.\nMore formally, a function $f: A \\to B$ is injective if:  \n$$\n\\forall x, y \\in A, f(x) = f(y) \\implies x = y\n$$  \nor equivalently the contrapositive:  \n$$\n\\forall x, y \\in A, x \\neq y \\implies f(x) \\neq f(y)\n$$  \nVisually there are no two points on the graph of the function that have the same y-coordinate. This also means\nthat $|A| \\leq |B|$ because each element in the domain is mapped to a unique element in the codomain.  \n<Image\nsrc=\"/maths/functionsInjective.png\"\ncaption=\"Example of injective and non-injective functions and their graphs.\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Properties of Functions", "Header 3": "Surjective", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "A function $f: A \\to B$ is surjective or onto if each element in the codomain is mapped to by at least one element in the domain,\ni.e the range of the function is equal to the codomain. More formally, a function $f: A \\to B$ is surjective if:  \n$$\n\\forall y \\in B, \\exists x \\in A, f(x) = y\n$$  \nVisually there are no points on the graph of the function that are not mapped to by an element in the domain.\nThis also means that $|A| \\geq |B|$ because each element in the codomain is mapped to by at least one element in the domain.  \n<Image\nsrc=\"/maths/functionsSurjective.png\"\ncaption=\"Example of surjective and non-surjective functions and their graphs.\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Properties of Functions", "Header 3": "Bijective", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "A function $f: A \\to B$ is bijective if it is both injective and surjective. This means that\nan element in the domain is mapped to precisely one element in the codomain and all elements in the codomain are mapped to one element in the domain.  \nMore formally, a function $f: A \\to B$ is bijective if:  \n$$\n\\begin{align*}\n\\forall x, y \\in A, f(x) = f(y) \\implies x = y \\\\\n\\forall y \\in B, \\exists x \\in A, f(x) = y\n\\end{align*}\n$$  \nVisually there are no two points on the graph of the function that have the same y-coordinate and there are no points on the graph that are not mapped to by an element in the domain.\nThis also means that $|A| = |B|$ because each element in the domain is mapped to a unique element in the codomain and each element in the codomain is mapped to by an element in the domain.  \n<Image\nsrc=\"/maths/functionsBijective.png\"\ncaption=\"Example of bijective and non-bijective functions and their graphs.\"\nwidth={800}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Composition of Functions", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "<Image\nsrc=\"/maths/functionsComposition.png\"\ncaption=\"Composition of two functions.\"\nwidth={400}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Functions", "Header 2": "Inverse Functions", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/functions.mdx"}, "page_content": "must be bijective  \n<Image\nsrc=\"/maths/functionsInverse.png\"\ncaption=\"Visualisation of a function and its inverse.\"\nwidth={600}\n/>  \n<Image\nsrc=\"/maths/functionsInverseGraph.png\"\ncaption=\"The graph of a function and its inverse.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Equivalence Relations", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/equivalenceRelations.mdx"}, "page_content": "A relation $R$ is an equivalence relation if it is reflex, symmetric and transitive. To show that the relation\n$R$ is an equivalence relation, we can write instead of $aRb$, $a \\equiv b$ or $a \\sim b$ if the relation is clear and\n$a$ is related to $b$. If the relation is not clear, we can write $a \\equiv_R b$ or $a \\sim_R b$.  \nTherefore we know that a relation $R$ is an equivalence relation if it satisfies the following properties:  \n- $\\forall a \\in A, a \\equiv_R a$ (reflexive)\n- $\\forall a, b \\in A, a \\equiv_R b \\implies b \\equiv_R a$ (symmetric)\n- $\\forall a, b, c \\in A, a \\equiv_R b \\land b \\equiv_R c \\implies a \\equiv_R c$ (transitive)  \nThis makes sense if we think of we compare it to our normal understandin of \"equivalence\":\n- We say that two things that are the same are equivalent to each other (reflexive).\n- If something is equivalent to something else, then that something else is equivalent to the first thing (symmetric).\n- If something is equivalent to something else, and that something else is equivalent to another thing, then the first thing is equivalent to the other thing (transitive).  \n<Callout type=\"example\">\nFor the set $A = \\{1, 2, 3\\}$, the relation $R = \\{(1, 1), (2, 2), (3, 3), (2,3), (3, 2)\\}$ is an equivalence relation.  \n- The relation $R$ is reflexive because $(1, 1), (2, 2), (3, 3) \\in R$.\n- The relation $R$ is symmetric because $(2, 3), (3, 2) \\in R$. $(1,2)$ and $(2,1)$ do not have to be in $R$.\n- The relation $R$ is transitive because $(2, 3), (3, 2) \\in R$ and $(2, 2), (3, 3) \\in R$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Equivalence Relations", "Header 2": "Equivalence Classes", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/equivalenceRelations.mdx"}, "page_content": "Given an equivalence relation $R$ on a set $A$, the equivalence class is a subset of $A$ that contains all the elements that are equivalent to a given element $a \\in A$.\nThe equivalence class of $a$ is denoted by $[a]$ if the relation is clear, or $[a]_R$ if the relation is not clear.\nMore formally, the equivalence class of $a$ is defined as:  \n$$\n[a]_R = \\{b \\in A | a \\equiv_R b\\}\n$$  \nBecause of the reflexive, symmetric and transitive properties of the equivalence relation, the equivalence classes partition the set $A$ into subsets\nthat must either be equal or disjoint. So for any elements $a, b \\in A$:  \n- If $a \\equiv_R b$, then $[a]_R = [b]_R$\n- Or if $a \\not\\equiv_R b$, then $[a]_R \\cap [b]_R = \\emptyset$.  \n<Callout type=\"example\">\nFor the set $A = \\{1, 2, 3\\}$ and the equivalence relation $R = \\{(1, 1), (2, 2), (3, 3), (2,3), (3, 2)\\}$, the equivalence classes are:  \n- $[1]_R = \\{1\\}$\n- $[2]_R = \\{2, 3\\}$\n- $[3]_R = \\{2, 3\\}$  \nAs we can see, $[2]_R = [3]_R$ and $[1]_R \\cap [2]_R = \\emptyset$ and therefore the equivalence classes partition the set $A$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Equivalence Relations", "Header 2": "Equivalence Classes", "Header 3": "Equivalence Classes as Modulo", "path": "../pages/digitalGarden/maths/discrete/relationsFunctions/equivalenceRelations.mdx"}, "page_content": "interesting but not really important", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Factorial", "path": "../pages/digitalGarden/maths/commonFunctions/factorial.mdx"}, "page_content": "The factorial is a function that is defined only for non-negative integers $n$. The factorial of a number is written as\n$n!$, and is the product of all positive integers less than or equal to $n$. Important to note is that $0! = 1$ and $1! = 1$.\nFor $n > 1$, the factorial is defined as:  \n$$\nn! = n \\cdot (n-1) \\cdot (n-2)\\cdot ... \\cdot 2 \\cdot 1\n$$  \nor alternatively,  \n$$\nn! = \\prod_{x=1}^{n}{x}\n$$  \n<Callout type=\"example\">\n| $n$ | $n!$ |\n| --- | --- |\n| $0$ | $1$ |\n| $1$ | $1$ |\n| $2$ | $2 \\cdot 1 = 2$ |\n| $3$ | $3 \\cdot 2 \\cdot 1 = 6$ |\n| $4$ | $4 \\cdot 3 \\cdot 2 \\cdot 1 = 24$ |\n| $5$ | $5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1 = 120$ |\n| $n$ | $n \\cdot (n-1) \\cdot (n-2)\\cdot ... \\cdot 2 \\cdot 1$ |\n</Callout>  \n<Callout type=\"proof\" title=\"Why is $0! = 1$?\">\nThere are many reasons why $0!$ must be $1$, most of them being related to combinatorics and counting problems. However, there is\nan intuitive explanation that can be given. Notice above that $n! = n \\cdot (n-1)!$. This means that the factorial of a number\nis the product of that number and the factorial of the number that is one less than it.  \nFor the above to hold true for $1!$ we need to have $0! = 1$ because if $0!=0$, then $1!$ would be:  \n$$\n1! = 1 \\cdot (1-1)! = 1 \\cdot 0! = 1 \\cdot 0 = 0\n$$  \nand all other factorials would also be $0$ as well. Therefore, $0!$ must be $1$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binomial Coefficient", "path": "../pages/digitalGarden/maths/commonFunctions/binomialCoefficient.mdx"}, "page_content": "The binomial coefficient is a function that often appears in various areas of maths but especially\nin combinatorics and probability theory.  \nThe formula for a binomial coefficient, or simply a binom, is defined as:  \n$$\n\\binom{n}{k}=\\frac{n!}{k!(n-k)!}\n$$  \nWhere $n$ and $k$ are non-negative integers and $n!$ is the [factorial](/digitalGarden/maths/commonFunctions/factorial) of $n$.  \nWhen $k$ is $0$ the binomial coefficient resolves to $1$:  \n$$\n\\binom{n}{0} = \\frac{n!}{0!(n-0)!} = \\frac{n!}{n!} = 1\n$$  \nWhen $k$ is equal to $n$ the binomial coefficient also resolves to $1$:  \n$$\n\\binom{n}{n} = \\frac{n!}{n!(n-n)!} = \\frac{n!}{n!} = 1\n$$  \nWhen calculating binomials by hand, it is often helpful to simplify the calculation by canceling out terms in the\nfactorials.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\binom{5}{2} &= \\frac{5!}{2!(5-2)!} = \\frac{5*4*3*2*1}{2*1*3*2*1} = \\frac{5*4}{2*1} = 10 \\\\\n\\binom{12}{5} &= \\frac{12!}{5!(12-5)!} = 792 \\\\\n\\binom{7}{1} &= \\frac{7!}{1!(7-1)!} = 7\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binomial Coefficient", "Header 2": "For Probability", "path": "../pages/digitalGarden/maths/commonFunctions/binomialCoefficient.mdx"}, "page_content": "<Callout type=\"todo\">\nThis will link to combinatorics\n</Callout>  \nIn probability theory, a binomial indicates how many ways you can choose $k$ objects from a set of $n$ different objects\n(without replacement and without regard to order). Formally, the binomial coefficient is the number of $k$-element\nsubsets of an $n$-element set.  \nOn calculators, you often find the English abbreviation \"nCr\", which stands for \"n choose r\" and corresponds to the binomial.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binomial Coefficient", "Header 2": "For Binomial Terms", "path": "../pages/digitalGarden/maths/commonFunctions/binomialCoefficient.mdx"}, "page_content": "A binomial term has the following form:  \n$$\n(x + y)^n , n \\in \\mathbb{N}\n$$  \nThe binomial coefficients give the coefficients of the expansion of a binomial term. This is done using the following formula:  \n$$\n(x + y)^n = \\sum_{k=0}^{n}{\\binom{n}{k}x^{n-k}y^k}\n$$  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n(x+y)^3 &= \\binom{3}{0} x^{3} + \\binom{3}{1} x^{2}y + \\binom{3}{2} xy^{2} + \\binom{3}{3} y^{3} \\\\\n&=x^3+3x^2y+3xy^2+y^3\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Binomial Coefficient", "Header 2": "Pascal's Triangle", "path": "../pages/digitalGarden/maths/commonFunctions/binomialCoefficient.mdx"}, "page_content": "The Pascal's triangle is a graphical representation of binomial coefficients, which also simplifies calculation.\nHere, a triangle is drawn where the upper variable $n$ of the binomial coefficient corresponds to the row index and the\nlower variable $k$ corresponds to the column index.  \n<Image\nsrc=\"/maths/pascalsTriangle.png\"\ncaption=\"Pascal's Triangle, left the binoms, right the values.\"\nwidth=\"700\"\n/>  \nThe calculation of the values is then very simple. First, for all elements at the edge of the triangle, you set a 1,\nbecause a binom with 0 or n as the lower variable resolves to 1. Then, every other value is the sum of the values\ndirectly above it.  \n<Image\nsrc=\"/maths/pascalsTriangleAnimated.gif\"\ncaption=\"Animation of how Pascal's Triangle is built up.\"\nwidth=\"300\"\n/>  \nThis derivation comes from the equation:  \n$$\n\\binom{n+1}{k+1}=\\binom{n}{k} + \\binom{n}{k+1}\n$$  \n<Callout type=\"example\" title=\"Example of Pascal's Triangle derivation\">\nWe will take the value in the middle of the third row of the image above.  \n$$\n\\begin{align*}\n\\binom{2}{1} &= \\binom{1}{0} + \\binom{1}{1} \\\\\n\\frac{2!}{1!(2-1)!} &= \\frac{1!}{0!(1-0)!} +\n\\frac{1!}{1!(1-1)!} \\\\\n\\frac{2}{1!1!} &= \\frac{1}{1!} + \\frac{1}{1!0!} \\\\\n\\frac{2}{1} &= \\frac{1}{1} + \\frac{1}{1} \\\\\n2 &= 1 + 1\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Absolute Value", "path": "../pages/digitalGarden/maths/commonFunctions/absolute.mdx"}, "page_content": "The absolute value of a real number $x$ is denoted by $|x|$ and is defined as follows:  \n$$\n|x| = \\begin{cases}\nx & \\text{if } x \\geq 0 \\\\\n-x & \\text{if } x < 0\n\\end{cases}\n$$  \nIn other words, if the number is positive, the absolute value is the number itself, and if the number is negative,\nthe absolute value is the number with the sign flipped i.e. the positive version of the number.  \n<Image\nsrc=\"/maths/absoluteValueNumberLine.png\"\ncaption='The absolute value can also be thought of as the \"distance\" of a number from zero on the number line.'\nwidth=\"700\"\n/>  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n|3| &= 3 \\\\\n|-3| &= 3 \\\\\n|0| &= 0 \\\\\n|-\\frac{1}{2}| &= \\frac{1}{2} \\\\\n|5 - 7| &= 2 \\\\\n|-3.14| &= 3.14\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "<Callout type=\"todo\">\nThis needs to be translated\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Motivation", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "In der Menge der natürlichen Zahlen $\\mathbb{N}= \\{1, 2, 3, . . .\\}$ sind die Operationen subtrahieren wie multiplizieren abgeschlossen. Das heisst zwei natürliche\nZahlen addiert oder multipliziert ergeben wieder eine natürliche Zahl.  \nWill man nun auch noch die Operation subtrahieren einführen, so hat man das\nProblem, dass für nicht alle zwei natürlichen Zahlen es eine natürliche\nDifferenz gibt z.B. $2-4 \\notin \\mathbb{N}$ und mehr generell $a-b \\notin \\mathbb{N}$ mit $b \\geq a$.  \nUm nun auch uneingeschränkt subtrahieren zu können, brauchen wir eine andere Zahlenmenge, für die die Operation subtrahieren abgeschlossen ist. Am besten eine\nZahlenmenge in der die natürlichen Zahlen schon enthalten sind und die alten Operationen weiterhin abgeschlossen sind, eine sogenannte Mengenerweiterung. Diese neue Zahlenmenge sind die ganzen Zahlen $\\mathbb{Z} = \\{. . . ,−3,−2,−1, 0, 1, 2, 3, . . .\\}$.  \nNun wollen wir auch noch Multiplikation umkehren also die Operation dividieren ausführen. Für gewisse Zahlenpaare ist dies möglich wie z.B. $4 \\div 2=2$ aber schon bei $1 \\div 4$ sind wir an unsere Grenzen gestossen. Wir brauchen also wieder eine sinnvolle Mengenerweiterung.\nDiese Erweiterung finden wir in der Menge der rationalen Zahlen $\\mathbb{Q} = \\{x|x=\\frac{z}{n}, z \\in \\mathbb{Z}, n\\in \\mathbb{N}\\}$.  \nNun können wir schon einige Operationen ausführen aber auch hier können wir schnell Defizite bemerken z.B. bei der Nutzung von Pythagoras theorem ($a^2+b^2=c^2$) wenn $a,b=1$ weil $\\sqrt{2}$ unendliche viele Kommastellen hat. Auch hier brauchen wir eine sinnvolle Mengenerweiterung zu den reelen Zahlen. Ein grosser Teil der Analysis benutzt die reelen Zahlen aber es gibt immer noch Operationen die nicht abgeschlossen sind für reele Zahlen. Betrachten wir z.B. folgende Gleichung  \n$$\n\\begin{align*}\nx^2 &= 0 \\\\\nx^2 &= -1 \\\\\nx &= \\sqrt{-1} = ?\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Imaginäre Einheit", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Wir definieren die Zahl $i$ als imaginäre Einheit  \n$$\ni=\\sqrt{-1}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Imaginäre Einheit", "Header 3": "Potenzen der imaginären Einheit", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "$$\n\\begin{align*}\ni   &= \\sqrt{-1} \\\\\ni^2 &= (\\sqrt{-1})^2 =-1 \\\\\ni^3 &= i^2i=(-1)i = -i \\\\\ni^4 &= i^2i^2 = (-1)(-1)=1 \\\\\ni^5 &= i^4i=i\n\\end{align*}\n$$  \nWir sehen also ein wiederholendes Muster und können also mehr allgemein formulieren  \n$$\ni^{k+4n}=i^k, \\forall k,n \\in \\mathbb{Z}\n$$  \nsomit könnn wir z.B für $i^{69}=i^{1+68}=i^{1+4\\cdot17}=i$  \n<Callout type=\"example\">\nBeispiel Gleichungen mit komplexe Zahlen lösen  \n$$\n\\begin{align*}\nx^2 + 4 &= 0 \\\\\nx^2 &= -4 \\\\\nx_{1,2}=\\pm \\sqrt{-4} = \\pm \\sqrt{4} \\sqrt{-1} &= \\pm 2i\n\\end{align*}\n$$\n</Callout>  \n<Callout type=\"example\">\nBeispiel Gleichungen mit komplexe Zahlen lösen  \n$$\n\\begin{align*}\nx^2 + x + 1 &= 0 \\\\\n(x+\\frac{1}{2})(x+\\frac{1}{2}) + \\frac{3}{4} &= 0 \\\\\n(x + \\frac{1}{2})^2 &= -\\frac{3}{4} \\\\\nx + \\frac{1}{2} &= \\pm \\sqrt{-\\frac{3}{4}} \\\\\nx &= -\\frac{1}{2} \\pm \\sqrt{-\\frac{3}{4}} \\\\\nx &= -\\frac{1}{2} \\pm \\sqrt{\\frac{3}{4}}\\sqrt{-1} \\\\\nx_{1,2} &= -\\frac{1}{2} \\pm \\frac{\\sqrt{3}}{2}i\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Komplexe Zahlen", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Wir definieren die komplexe Zahl $z$ wie gefolgt  \n$$\nz=a+bi \\text{ mit } a,b \\in \\mathbb{R}\n$$  \nwir bezeichnen $a$ als Realteil und $b$ als Imaginärteil. Oftmals wird diese Schreibform als kartesische Form, arithmetische Form oder algebraische Form. Oftmals begegnet man auch die folgende Schreibweise für den Realteil: $a=Re(z)=\\Re(z)$ und für den Imaginärteil: $b=Im(z)=\\Im(z)$. Die Menge der komplexen Zahlen wird wie gefolgt definiert  \n$$\n\\mathbb{C}=\\{z|z=a+bi \\text{ mit } a,b \\in \\mathbb{R}\\}\n$$  \nDie Menge der komplexen Zahlen enthält immer noch die Menge der reelen Zahlen. Dies ist einfach zu sehen, da sich jede reele Zahl $r$ als komplexe Zahl $z = r + 0i$ schreiben lässt somit ist $\\mathbb{R} \\subset \\mathbb{C}$  \nEine gute Zusammenfassung zu komplexen Zahlen und ihre Operationen findets du [hier](https://www.youtube.com/watch?v=zB2VwWzpYx4) und [hier](https://www.youtube.com/watch?v=TSeC_2D8xNs&t=12s)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Komplexe Zahlen", "Header 3": "Gleichheit von Komplexen Zahlen", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Zwei komplexe Zahlen $z_1 = a + bi$ und $z_2 = c + bi$ mit $a, b, c, d \\in \\mathbb{R}$ sind genau dann gleich, wenn ihre Real- und Imaginärteile übereinstimmen.  \n$$\na+bi=c+di \\iff a=c \\land b=d\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Komplexe Zahlen", "Header 3": "Operationen mit Komplexen Zahlen", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "#### Addieren und subtrahieren  \nBeginnen wir mal mit der Addition und betrachten wir alle Variablen als reele Zahlen.  \n$$\ns=z_1 + z_2 = (a+bi) + (c+di)=(a+c)+i(b+d)\n$$  \nWir können also definieren, dass wir zwei komplexe Zahlen addieren/subtrahieren, indem wir ihre Real- und Imaginärteile addieren/subtrahieren.  \n$$(a+bi)\\pm (c+di)=(a \\pm c) + i(c\\pm d)$$  \n<Callout type=\"example\">\nBeispiel komplexe Zahlen addieren/subtrahieren  \n$$\n\\begin{align*}\n(1+2i) + (4+5i) &= (1+4) + i(2+5) = 5 + 7i \\\\\n(1-2i) + (-4 + 5i) &= (1-4)+i(-2+5)=-3 + 3i \\\\\n(1+2i)-(4+5i) &= (1-4) + i(2-5) = -3 -3i\n\\end{align*}\n$$\n</Callout>  \n#### Multiplizieren  \nBei der Multiplikation gehen wir genau gleich vor  \n$$\n\\begin{align*}\nz_1 \\cdot z_2 &= (a + bi)(c+di)= ac + adi + bci +i^2bd \\\\\n(\\text{weil } i^2=-1) &= ac + adi + bci -bd \\\\\n&= (ac - bd) + i(ad + bc)\n\\end{align*}\n$$  \nso können wir die Multiplikation von zwei komplexe Zahlen wie gefolgt definieren  \n$$\n(a+bi)(c+di)=(ac-bd) + i(ad+bc)\n$$  \n<Callout type=\"example\">\nBeispiel komplexe Zahlen multiplizieren  \n$$\n\\begin{align*}\n(1 + 2i)(3 − 4i)&=((1 \\cdot 3)−(2 \\cdot (−4)))+i((1 \\cdot (−4))+(2 \\cdot 3)) \\\\\n&= 11 + 2i \\\\\n(7 + 0i)(−8 + 0i)&=((7 \\cdot(−8)) − (0 \\cdot 0)) + i((7 \\cdot 0) + (0 \\cdot (−8))) \\\\\n&= −56 + 0i \\\\\n(a + ib)(a − ib) &= a^2 − i^2b^2 + i(a \\cdot(−b) + ab) \\\\\n&= a^2 + b^2\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Komplexe Zahlen", "Header 3": "Potenzieren", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Weil wir schon wissen wir wir komplexe Zahlen multiplizieren können, können wir sie auch potenzieren mit der allgemeinen binomische Formel  \n$$\n(a +bi)^n = \\sum_{k=0}^{n}{\\binom{n}{k} a^{n-k}(bi)^k}\n$$  \n<Callout type=\"example\">\nBeispiel komplexe Zahlen multiplizieren  \n$$\n\\begin{align*}\n(1+2i)^5 &= \\sum_{k=0}^{5}{\\binom{5}{k}1^{5-k}(2i)^k} \\\\\n&= 1+5(2i)+10(2i)^2+10(2i)^3+5(2i)^4+(2i)^5 \\\\\n\\text{ersetzen mit potenzwerte} &= 1+10i-40-80i+80+32i \\\\\n&= 41 -38i\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Komplexe Zahlen", "Header 3": "Komplexe Konjugation", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Zwei komplexe Zahlen die sich nur im Vorzeichen im Imaginärteil unterscheiden, nennt man konjugiert komplex. Wir schrieben dies wie gefolgt  \n$$\n\\begin{align*}\nz = (a + bi) \\\\\nz \\text{ komplex konjugiert} = \\overline{z} = (a-bi)\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Komplexe Zahlen", "Header 3": "Dividieren", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Bei der Division haben wir einen imaginären Teil im Nenner, dies wollen wir aber nicht. dank dem 3-ten Binom lässt sich jedoch der Nenner reel machen. Damit wir im Nenner ein 3-tes Binom haben erweitern wir den Bruch im Nenner und Zähler mit dem Nenner komplex konjugiert.  \n$$\n\\begin{align*}\n\\frac{z_1}{z_2}&=\\frac{z_1\\overline{z_2}}{z_2\\overline{z_2}} \\\\\n\\text{Als Erweiterung}&=\\frac{a+bi}{c+di} = \\frac{a+bi}{c+di}\\frac{c-di}{c-di}=\\frac{(ac+bd)+i(bc-ad)}{c^2+d^2} \\\\\n\\text{Als Formel}&= \\frac{ac+bd}{c^2+d^2} +i \\frac{bc-ad}{c^2+d^2}\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Gauss'sche Zahlenebene", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Wir können jede komplexe Zahl $z=a +bi$ mit $a,b \\in \\mathbb{R}$ eindeutig identifizieren. Wir können jetzt auch jeder komplexe Zahl ein geordnetes Zahlenpaar $(a,b) \\in \\mathbb{R}^2$ zuweisen, was dann einem Punkt im kartesischen Koordinatensystem entspricht wenn man $a=x$ und $b=y$ . So haben wir eine Möglichkeit komplexe Zahlen im zweidimensionalen, reellen Raum darzustellen. Wir haben also  \n$$\nf: \\mathbb{C} \\to \\mathbb{R}^2, a+bi \\mapsto (a,b)\n$$  \nDiese Abbildung nennt man Komplexe oder Gauss'sche Zahlenebene. **Achtung!!!** wenn wir komplexe Zahlen darstellen dann werden oft Zeiger verwendet welche nicht mit Vektoren zu verwirren sind da auf Vektoren andere Operationen definiert sind auch wenn sie sehr ähnlich sind und oftmals auch sehr ähnlich funktionieren.  \n![complexNumberCartesian](/maths/complexNumberCartesian.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Gauss'sche Zahlenebene", "Header 3": "Betrag einer komplexen Zahl", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Der Betrag einer komplexen Zahl $z = a + bi \\in \\mathbb{C}$ entspricht der Entfernung des Punktes $(a,b) \\in \\mathbb{R}^2$ vom Ursprung oder die Länge des Zeigers. Dies können wir so berechnen $|z|=\\sqrt{z \\overline{z}}=\\sqrt{a^+b^2} \\in \\mathbb{R}$ oder mit der Formel  \n$$\n|z|=|a+bi|=\\sqrt{a^2+b^2}\n$$  \n<Callout type=\"example\">\nBeispiel Betrag einer komplexen Zahlen  \n$$\n|1-2i|=\\sqrt{1^2+(-2)^2}=\\sqrt{1+4}=\\sqrt{5}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Gauss'sche Zahlenebene", "Header 3": "Argument einer komplexen Zahl", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Das Argument, Winkel oder Phasenwinkel einer komplexen Zahl $z = a + bi \\in \\mathbb{C}$ entspricht dem Winkel gegenüber der x-Achse. Wir definieren das Argument $\\arg(z)$ wie gefolgt  \n$$\n\\arg(z)=\\arg(a+bi)=\n\\begin{cases}\n\\arctan(\\frac{b}{a}) &a > 0 \\\\\n\\pi + \\arctan(\\frac{b}{a}) &a < 0 \\\\\n\\frac{\\pi}{2} &a=0,b > 0 \\\\\n-\\frac{\\pi}{2} &a=0,b < 0 \\\\\n\\text{undefined} &a=0,b=0\n\\end{cases}\n$$  \nOder auch kürzer aber weniger verbreitet  \n$$\n\\varphi=arg(z)=\\arg(a+bi)=\n\\begin{cases}\n\\arccos(\\frac{a}{r}) &\\text{für } b \\geq 0 \\\\\n-\\arccos(\\frac{a}{r}) &\\text{für } b < 0\n\\end{cases}\n$$  \n<Callout type=\"example\">\nBeispiel Argument einer komplexen Zahlen  \n$$\n\\arg(1-2i)=\\arctan(\\frac{-2}{1}) = -1.1071rad = -1.1071rad \\cdot \\frac{180}{/pi} = -63.4321575^{\\circ}\n$$  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Gauss'sche Zahlenebene", "Header 3": "Komplexe Zahlen im Polarkoordinatensystem", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Punkte in einem Koordinatensystem müssen nicht unbedingt im kartesischen\nKoordinatensystem sein. Wir können auch polare Koordinaten verwenden, wir müssen es dann nur Umwandeln. Dafür verwenden wir Betrag: $r=|z|$ und Argument: $\\varphi=\\arg(z)$ der komplexen Zahl.  \n![complexNumberPolar](/maths/complexNumberPolar.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Gauss'sche Zahlenebene", "Header 3": "Goniometrische Darstellung", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Aus der Umrechnung von kartesische Koordinaten zu polare Koordinaten sehen wir, dass wir jede komplexe Zahl auch eindeutig schreiben können als  \n$$\nz=r(\\cos(\\varphi) + i \\sin(\\varphi)) = r \\text{ cis}(\\varphi)\n$$  \nwobei $r=|z| \\in \\mathbb{R}_0^+$ und $\\varphi=\\arg(z) \\in [0,2\\pi)$. Dies nennen wir die goniometrsiche Form.  \n<Callout type=\"example\">\nBeispiel Goniometrische Darstellung einer komplexen Zahlen  \n$$\n\\begin{align*}\nz &= 1 + \\sqrt{3}i \\\\\n&\\Rightarrow r=|z|=2, \\varphi=\\arg(z)=\\frac{\\pi}{3} \\\\\n&\\Rightarrow z=2(\\cos(\\frac{\\pi}{3})+i \\sin(\\frac{\\pi}{3})) = 2 \\text{ cis}(\\frac{\\pi}{3})\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Rechenoperationen in der Gauss’schen Zahlenebene", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Schauen wir uns nun an wie die oben schon definierten Rechenoperationen von komplexen Zahlen auf der Gauss'schen Zahlenebene aussehen und ob wir sie vielleicht verbessern können.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Rechenoperationen in der Gauss’schen Zahlenebene", "Header 3": "Komplexe Konjugation und Negation", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Schauen wir uns zuerst einmal die Komplexe Konjugation und Negation an.  \n![complexNumberConjugate](/maths/complexNumberConjugate.png)  \nWir sehen, dass die komplexe Konjugation eine Spiegelung an der x-Achse bewirkt. Wir sehen auch, dass eine negation eine Spiegelung am Ursprung bewirkt.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Rechenoperationen in der Gauss’schen Zahlenebene", "Header 3": "Addieren/subtrahieren", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Die Addition/Subtraktion von zwei komplexen Zahlen lässt sich wie die Addition zweier Vektoren interpretieren. Achtung sie sind aber nicht Vektoren sondern Zeiger!  \n![complexNumberAdditionSubtraction](/maths/complexNumberAdditionSubtraction.png)  \nDie Berechnung der Summe ist immer am einfachsten in der arithmetischen Form die goniometrische Form eignet sich nicht dafür.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Rechenoperationen in der Gauss’schen Zahlenebene", "Header 3": "Multiplizieren", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Wenn wir mit in der goniometrischer Darstellungsform eine Multiplikation probieren können wir auf folgendes stossen mit ein wenig komplizierter Auflösungen  \n$$\n\\begin{align*}\nz_1 \\cdot z_2 &= (r_1(\\cos(\\varphi_1) + i \\sin(\\varphi_1)))(r_2(\\cos (\\varphi_2) + i \\sin (\\varphi_2))) \\\\\n&= r_1 \\cdot r_2(\\cos(\\varphi_1)\\cos(\\varphi_2)−\\sin(\\varphi_1)\\sin (\\varphi_22)) \\\\\n&\\hspace{1cm} + i(\\cos(\\varphi_1)\\sin(\\varphi_2)+\\sin(\\varphi_1)\\cos (\\varphi_2)) \\\\\n&= r_1 \\cdot r_2(\\cos(\\varphi_1 + \\varphi_2) + i \\sin(\\varphi_1 + \\varphi_2)) \\\\\n&= r_1 \\cdot r_2 \\text{cis}(\\varphi_1 + \\varphi_2)\n\\end{align*}\n$$  \nWir können als mit der goniometrischer Darstellungsform viel einfacher und schneller multiplizieren wenn wir die folgender Formel verwenden  \n$$\nz_1 \\cdot z_2 = (r_1 \\text{cis}(\\varphi_1))(r_2 \\text{cis}(\\varphi_2))=r_1r_2\\text{cis}(\\varphi_1 + \\varphi_2)\n$$  \nWird die komplexe Zahl $z_1 = a + ib = r \\text{ cis}(\\varphi)$ als Zeiger in der\nGauss’schen Zahlenebene interpretiert, so bewirkt die Multiplikation von $z_1$ mit der\nkomplexen Zahl $z_2 = s \\text{ cis} (\\alpha)$ eine Drehstreckung des Zeigers $z_1$. Der Zeiger wird dabei um den Faktor $s$ gestreckt und um den Winkel $\\alpha$ (im gegenuhrzeigersinn) gedreht!  \n![komplexeZahlenMutiplikation](/maths/complexNumberMultiplication.png)  \n<Callout type=\"example\">\nInteressante Beispiele bei der Multiplikation  \nDas Produkt der beiden komplexen Zahlen $z_1 = a + bi = r \\text{cis}(\\varphi)$ und $z_2 = \\overline{z_1} = a − bi = r cis (−\\varphi)$ ergibt  \n$$\nz_1z_2 =r \\text{ cis}(\\varphi) \\cdot r \\text{ cis}(−\\varphi)= r\\cdot r \\text{ cis}(\\varphi − \\varphi) = r^2 \\text{ cis}(0) = r^2\n$$  \nDas Produkt der beiden komplexen Zahlen $z_1 = a + bi = r \\text{cis}(\\varphi)$ und $z_2 = 1 \\text{cis}(\\alpha)$ ergibt  \n$$\nz1_z2 = r \\text{ cis}(\\varphi) 1 \\cdot \\text{ cis}(\\alpha) = r \\text{ cis}(\\varphi + \\alpha)\n$$  \nWas auf der Gauss’schen Zahlenebene nur eine Drehung des Zeigers bewirkt.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Rechenoperationen in der Gauss’schen Zahlenebene", "Header 3": "Division", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Wenn wir mit in der goniometrischer Darstellungsform eine Division probieren so sehen wir, dass es sehr viel einfacher und kürzer ist als in der algebraischen Form.  \n$$\n\\frac{z_1}{z_2}=\\frac{r_1 \\text{cis}(\\varphi_1)}{r_2 \\text{cis}(\\varphi_2)}=\\frac{r_1}{r_2} \\text{cis}(\\varphi_1 - \\varphi_2)\n$$  \nDer Betrag der Division ist also gleich der Division der Beträge und das Argument der Division entspricht der Differenz der Argumente.  \n![komplexeZahlenDividieren](/maths/complexNumberDivision.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Rechenoperationen in der Gauss’schen Zahlenebene", "Header 3": "Satz von Moivre", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Als nächstes wollen wir die Operationen Potenzieren und Radizieren betrachten. Dafür werden wir den Satz von Moivre verwenden.  \n#### Satz von Moivre Teil 1, potenzieren  \nWeil die Multiplikation in der goniometrischen Darstellung einfacher zu berechnen\nist als in der algebraischen Form, wollen wir die Potenzen ebenfalls in goniometrischer Darstellung definieren.  \n$$\n\\begin{align*}\nz^2 &= (r \\text{ cis}(\\varphi))^2=(r \\text{ cis}(\\varphi))(r \\text{ cis}(\\varphi)) = r \\cdot r \\text{ cis}(\\varphi + \\varphi) \\\\\n&= r^2 \\text{ cis}(2\\varphi)\n\\end{align*}\n$$  \nDaraus können wir dann die folgende Formel bauen für ganzzählige Potenzen $n \\in \\mathbb{Z}$  \n$$\nz^n= (r \\text{ cis}(\\varphi))^n=r^n \\text{ cis}(n\\varphi)\n$$  \nmehr dazu findest du [hier](https://www.youtube.com/watch?v=G_FRNyHpzrk)  \n#### Satz von Moivre Teil 2, radizieren (wurzelziehen)  \nIn der reelen Zahlenmenge hat die Rechnung $x^4=1$ nur zwei Resultate $x_{1,2}=\\pm1$. Jedoch gibt es in der komplexen Zahlenmenge für diese Rechnung 4 Resultate zwar $1,-1,i$ und $-i$. Dies gibt uns ein kleines Problem weil wir nicht mehr einfach $\\pm$ hinschrieben können wenn wir die Wurzel ziehen. Damit wir alle Lösungen bekommen müssen wir die Periodizität ausnutzen. Wir wollen also folgendes problem lösen  \n$$\nz^n= a + bi = r \\text{ cis}(\\varphi) \\iff z=\\sqrt[n]{r \\text{ cis}(\\varphi)}\n$$  \nWir wissen, dass das radizieren eine spezielle Form von potenzieren ist mit Dezimalzahlen\n$\\sqrt[3]{x}=(x)^{\\frac{1}{3}}$ nun müssen wir noch die Periodizität einbringen dann bekommen wir die folgende Formel  \n$$\nz^n= r \\text{ cis}(\\varphi) \\iff z_k=\\sqrt[n]{r} \\text{ cis}(\\varphi + 2\\pi k)^{\\frac{1}{n}}=\\sqrt[n]{r} \\text{ cis}(\\frac{\\varphi}{n} + \\frac{2\\pi}{n} k)\n$$  \nwobei $k=0,1,...n-1$  \nmehr dazu findest du [hier](https://www.youtube.com/watch?v=BKdqTn2iO4s)  \n<Callout type=\"example\">\nBeispiel komplexe Zahlen radizieren  \n$$\n\\begin{align*}\nz^2 &= i = 0 + 1i = 1 \\text{ cis}(\\frac{\\pi}{2}) \\iff z=\\sqrt{i} =  \\sqrt{1 \\text{ cis}(\\frac{\\pi}{2})} \\\\", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Rechenoperationen in der Gauss’schen Zahlenebene", "Header 3": "Satz von Moivre", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "<Callout type=\"example\">\nBeispiel komplexe Zahlen radizieren  \n$$\n\\begin{align*}\nz^2 &= i = 0 + 1i = 1 \\text{ cis}(\\frac{\\pi}{2}) \\iff z=\\sqrt{i} =  \\sqrt{1 \\text{ cis}(\\frac{\\pi}{2})} \\\\\n&\\Rightarrow z_0 =  1 \\text{ cis}(\\frac{\\pi}{4}) \\\\\n&\\Rightarrow z_1 =  1 \\text{ cis}(\\frac{\\pi}{4}+\\frac{2\\pi}{2})= 1 \\\\\n\\text{ cis}(\\frac{5\\pi}{4})\n\\end{align*}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Exponential/Euler Form", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Bevor wir mit dem Logarithmieren starten können benötigen wir noch eine weitere Darstellungsform für die komplexen Zahlen. Dies ist die sogenannte Exponentialform auch oft Eulerform gennant. Um diese Darstellungsform herzuleiten arbeiten wir mit Potenzreihen.  \nEin gutes Video findest du auch [hier](https://www.youtube.com/watch?v=TGJHnQY9cjA)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Exponential/Euler Form", "Header 3": "Euler'sche Formel", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "$$\ne^{ix}=\\cos(x)+i\\sin(x)=\\text{cis}(x)\n$$  \nSomit können wir dann komplexe Zahlen darstellen  \n$$\nz=re^{i\\varphi}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Exponential/Euler Form", "Header 3": "Logarithmieren", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Mit der exponentialform können wir nun logarithmieren.  \nWenn $z=-5+5i=5\\sqrt{2}\\text{ cis}(\\frac{3\\pi}{4})=5\\sqrt{2}e^{i\\frac{3\\pi}{4}}$. Dann wollen wir folgendes ausrechnen  \n$$\n\\begin{align*}\n\\ln(z)&=\\ln(5\\sqrt{2}e^{i\\frac{3\\pi}{4}}) \\\\\n&= \\ln(5\\sqrt{2})+\\ln(e^{i\\frac{3\\pi}{4}}) \\\\\n&= \\ln(5\\sqrt{2})+i\\frac{3\\pi}{4}\n\\end{align*}\n$$  \nWir sehen, dass Der Logarithmus einer komplexen Zahl $z \\neq 0$   ist wieder eine\nkomplexe Zahl. Dabei ist der Realteil des Resultats gleich dem Logarithmus des Betrags und der Imaginärteil gleich dem Exponenten (plus ein ganzzahliges Vielfaches von 2$\\pi$ wegen der Periodizität).  \n$$\n\\ln(z)=\\ln(re^{i\\varphi})=\\ln(r)+i\\varphi\n$$  \n<Callout type=\"example\">\nBeispiel logarithmieren mit Basiswechsel  \nWir wollen $\\log_2(\\sqrt{2}-\\sqrt{2}i)$ berechnen dafür verwandeln wir wieder die Zahl zuerst in die Exponentialform $\\sqrt{2}-\\sqrt{2}i=2e^{i\\frac{7\\pi}{4}}$  \n$$\n\\log_2(2e^{i\\frac{7\\pi}{4}})=\\frac{\\ln(2e^{i\\frac{7\\pi}{4}})}{\\ln(2)}=1+i\\frac{\\frac{7\\pi}{4}}{ln(2)}\n$$  \n</Callout>  \n<Callout type=\"example\">\nBeispiel negative Zahlen logarithmieren  \nWir wollen $\\ln(-1)$ berechnen, mit reelen Zahlen ist es nicht möglich aber vielleicht ist es mit komplexen Zahlen möglich. Dafür müssen wir die Zahl zuerst in die Exponentialform umwandeln $-1=-1+0i=e^{i\\pi}$  \n$$\n\\ln(-1)=\\ln(e^{i\\pi}) = i\\pi\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Complex Numbers", "Header 2": "Exponential/Euler Form", "Header 3": "Komplexe Zahlen im Exponenten", "path": "../pages/digitalGarden/maths/complexNumbers/explanation.mdx"}, "page_content": "Neben dem Logarithmieren liefert die Exponentialform eine Möglichkeit des Potenzierens mit der komplexen Zahl im Exponenten.  \n$$\ne^z=e^{a+bi}=e^a e^{bi}= e^a \\text{ cis}(b)\n$$  \n<Callout type=\"example\">\nBeispiel komplexe Zahl im Exponenten  \n$$\ne^{1+i\\pi}=e \\text{ cis}(\\pi)=-e\n$$  \n$$\n2^i=e^{\\ln(2^i)}=e^{i \\ln(2)}=\\text{cis}(\\ln(2))\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "General Information", "Header 3": "Imaginary Unit", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "$$\n\\begin{align*}\ni &= \\sqrt{-1} \\\\\ni^2 &= -1 \\\\\ni^3 &= -i \\\\\ni^4 &= 1 \\\\\ni^5 &= i \\\\\ni^{k+4n} &= i^k \\\\\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "General Information", "Header 3": "Cartesian/Arithmetic/Algebraic Form", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "$$\nz=a+bi\n$$  \nWhere $a,b \\in \\mathbb{R}$ are the real and imaginary parts of the complex number:\n- The real part, $a=Re(z)=\\Re(z)$.\n- And the imaginary part, $b=Im(z)=\\Im(z)$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "General Information", "Header 3": "Set of Complex Numbers", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "$$\n\\mathbb{C}=\\{z|z=a+bi\\text{ mit }a,b \\in \\mathbb{R}\\}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "General Information", "Header 3": "Equality of Complex Numbers", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Complex numbers are equal if and only if their real and imaginary parts are equal  \n$$\na+bi=c+di \\iff a=c \\land b=d\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "General Information", "Header 3": "Complex/Gaussian Plane", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "A unique mapping and identification of complex numbers as points.  \n$$\nf: \\mathbb{C} \\to \\mathbb{R}^2, a+bi \\mapsto (a,b)\n$$  \n#### Magnitude/Modulus of a Complex Number  \nThe modules/magnitude of a complex number is the distance of the point from the origin, i.e. the length of the vector.  \n$$\nr=|z|=|a+bi|=\\sqrt{z\\overline{z}}=\\sqrt{a^2+b^2}\n$$  \nArgument of a Complex Number\nThe angle of the vector with respect to the x-axis.  \n$$\n\\varphi = \\arg(z) = \\arg(a+bi) = \\begin{cases}\n\\arctan(\\frac{b}{a}) &a > 0 \\\\\n\\pi + \\arctan(\\frac{b}{a}) &a < 0 \\\\\n\\frac{\\pi}{2} &a=0,b > 0 \\\\\n-\\frac{\\pi}{2} &a=0,b < 0 \\\\\n\\text{undefined} &a=0,b=0\n\\end{cases}\n$$  \nAlternatively, but less common:  \n$$\n\\varphi=arg(z)=\\arg(a+bi) = \\begin{cases}\n\\arccos(\\frac{a}{r}) &\\text{für } b \\geq 0 \\\\\n-\\arccos(\\frac{a}{r}) &\\text{für } b < 0\n\\end{cases}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "General Information", "Header 3": "Polar Form", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Using the absolute value and argument, a complex number can also be represented in the polar coordinate system.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "General Information", "Header 3": "Trigonometric/Goniometric Form", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "From the conversion from Cartesian to polar coordinates, we notice that we can also uniquely write and identify complex\nnumbers in another way:  \n$$\nz=r(\\cos(\\varphi) + i \\sin(\\varphi))=r \\text{ cis}(\\varphi)\n$$  \nwhere  $r=|z| \\in \\mathbb{R}_0^+$ and $\\varphi=\\arg(z) \\in [0,2\\pi)$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "General Information", "Header 3": "Exponential/Euler Form", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "From the power series of $e^x,\\sin(x)$ and $\\cos(x)$, we can form the Euler form:  \n$$\ne^{ix}=\\cos(x)+i\\sin(x)=\\text{cis}(x)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Negation", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Negation corresponds to reflection across the origin in the complex plane. If $z= a+bi$ then  \n$$\n-z=-a-bi\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Complex Conjugation", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Complex conjugation is negation of only the imaginary part and corresponds to reflection across the x-axis. If $z=a+bi$\nthen the  komplexe Konjugation is  \n$$\n\\overline{z}=a-bi\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Addition/Subtraction", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Addition/subtraction is best done in Cartesian form, where the two vectors are added/subtracted.  \n$$\ns=z_1 + z_2=(a+bi)\\pm (c+di)=(a \\pm c) + i(c\\pm d)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Multiplication", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Multiplication can be done in all forms, but it is easiest and fastest in the trigonometric or Euler form. In Cartesian\nform, multiplication becomes complicated with more than two factors.  \n- Cartesian form: $z_1 \\cdot z_2=(a+bi)(c+di)=(ac-bd) + i(ad+bc)$\n- Trigonometric form: $z_1 \\cdot z_2 = (r_1 \\text{cis}(\\varphi_1))(r_2 \\text{cis}(\\varphi_2))=r_1r_2\\text{cis}(\\varphi_1 + \\varphi_2)$\n- Euler form: $z_1 \\cdot z_2 = r_1 e^{i\\varphi_1} \\cdot r_2 e^{i\\varphi_2}=r_1 r_2 e^{i(\\varphi_1 + \\varphi_2)}$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Exponentiation", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Using the [binomial formula](../functions/binomialCoefficient#for-binomial-terms), we can raise complex numbers to a\npower using Cartesian form, but because multiplication is so much easier in the other forms, exponentiation is also much\neasier in those forms.  \n- Cartesian form: $z^n=(a +bi)^n = \\sum_{k=0}^{n}{\\binom{n}{k} a^{n-k}(bi)^k}$\n- Trigonometric form: Using De Moivre's theorem $z^n= (r \\text{ cis}(\\varphi))^n=r^n \\text{ cis}(n\\varphi)$\n- Euler form: $z^n=(re^{i\\varphi})^n=r^n e^{in\\varphi}$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Division", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Division can also be done in all forms, but like multiplication, it is best not to use Cartesian form.  \n- Cartesian form: $\\frac{z_1}{z_2}=\\frac{a+bi}{c+di}=\\frac{ac+bd}{c^2+d^2} +i \\frac{bc-ad}{c^2+d^2}$\n- Trigonometric form: $\\frac{z_1}{z_2}=\\frac{r_1 \\text{cis}(\\varphi_1)}{r_2 \\text{cis}(\\varphi_2)}=\\frac{r_1}{r_2} \\text{cis}(\\varphi_1 - \\varphi_2)$\n- Euler form: $\\frac{z_1}{z_2}=\\frac{r_1 e^{i\\varphi_1}}{r_2 e^{i\\varphi_2}}=\\frac{r_1}{r_2} e^{i(\\varphi_1 - \\varphi_2)}$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Roots of Complex Numbers", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "The process of taking roots is more complicated with complex numbers than with real numbers. Fortunately, taking roots\nis the inverse function of raising to a power, and we can convert roots to powers as follows: $\\sqrt[4]{x}=(x)^{\\frac{1}{4}}$.\nThus, we can proceed as we did with real numbers. However, when raising to a power, we did not have to take into account\nthe periodicity of the cosine and sine functions, but when taking roots, we must do so because\n$\\cos(\\frac{\\varphi+2\\pi k}{n})$ for $k=0,1,...,n-1$ gives exactly $n$ different values. Of course, we can also convert\nthe formulas below into trigonometric form.  \n$$\n\\sqrt[n]{re^{i\\varphi}}=\\sqrt[n]{r}(\\cos(\\frac{\\varphi}{n})+i \\sin(\\frac{\\varphi}{n}))=\\sqrt[n]{r}e^{i\\frac{\\varphi}{n}}\n$$  \nTo calculate all solutions, we use the following formula  \n$$\nz_k=\\sqrt[n]{r} e^{i(\\frac{\\varphi}{n}+\\frac{2\\pi k}{n})}\n$$  \nwhere $k=0,1,...n-1$ and we call the solution $z_0$ the principal value of the root.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Logarithm", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "Thanks to the exponential/euler form, we can also take logarithms.  \n$$\n\\ln(z)=\\ln(re^{i\\varphi})=\\ln(r)+i\\varphi\n$$  \nIf the natural logarithm is not used, we can make a base change at the end.  \n$$\n\\log_2(2e^{i\\frac{7\\pi}{4}})=\\frac{\\ln(2e^{i\\frac{7\\pi}{4}})}{\\ln(2)}=1+i\\frac{\\frac{7\\pi}{4}}{ln(2)}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Cheatsheet", "Header 2": "Operations", "Header 3": "Complex Numbers in the Exponent", "path": "../pages/digitalGarden/maths/complexNumbers/cheatsheet.mdx"}, "page_content": "We can also solve problems where the complex number is used as an exponent.  \n$$\ne^{1+\\pi i}=e^1e^{\\pi i}=e e^{(\\pi i)}\n$$  \n$$\n2^i=e^{\\ln(2^i)}=e^{i \\ln(2)}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "You can think of a matrix as rectangle of objects arranged in rows and columns, like a chessboard. For computer scientist you can also imagine a matrix as a\n2D array. A matrix with $m$ rows and $n$ columns is called an $m \\times n$ matrix. The number of rows and columns therefore define the size or dimensions of a matrix.\nThe objects in a matrix or commonly reffered to as elements or entries. A matrix containing only real numbered elements is called a real matrix and can be defined as follows:  \n$$\n\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\n$$  \nThe elements of a matrix are indexed by their row and then there column. So $a_{ij}$ or sometimes also $(\\boldsymbol{A})_{ij}$is the element in the $i$th row and $j$th column. This means that a matrix can also be\ndefined as $\\boldsymbol{A} = [a_{ij}]_{1 \\leq i \\leq m, 1 \\leq j \\leq n}$.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\boldsymbol{A} = \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\\\\n\\boldsymbol{B} = \\begin{bmatrix}1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}\n\\end{align*}\n$$\n</Callout>  \n<Callout type=\"warning\">\nThe number of rows and columns of a matrix are always written in the order of rows first and columns second! So a matrix with 2 rows and 3 columns is written\nas a $2 \\times 3$ matrix, not a $3 \\times 2$ matrix, so $\\boldsymbol{A} \\in \\mathbb{R}^{2 \\times 3}$.\n</Callout>  \nA matrix with only one row is called a row vector or [n-tuple]() and a matrix with only one column is called a column vector. When talking about vectors\nwe usually refer to column vectors. A matrix that only has one row and one column can be thought of as a normal number and is called a scalar and therefore omit the matrix brackets.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "we usually refer to column vectors. A matrix that only has one row and one column can be thought of as a normal number and is called a scalar and therefore omit the matrix brackets.\nTo differentiate between the three types of matrices we usually use bold capital letters for matrices, bold lower case letters for column vectors and normal lower case letters for scalars.  \n<Callout type=\"example\">\nA column vector, i.e. ur usual vector, can be written as follows:  \n$$\n\\boldsymbol{v} \\in \\mathbb{R}^{n \\times 1} = \\begin{bmatrix}v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\n$$  \nA row vector can be written as follows:  \n$$\n\\boldsymbol{v} \\in \\mathbb{R}^{1 \\times n} = \\begin{bmatrix}v_1 & v_2 & \\cdots & v_n \\end{bmatrix}\n$$  \nA scalar can be written as follows:  \n$$\n\\boldsymbol{v} \\in \\mathbb{R}^{1 \\times 1} = \\begin{bmatrix}v \\end{bmatrix} = v\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Square Matrix", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "As you can imagine a square matrix is a matrix where the number of rows and columns are equal. The number of rows or columns $n$\nis reffered to as the order of the square matrix. So a square matrix of order $n$ is an $n \\times n$ matrix, i.e.\n$\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$. Square matrices have a number of useful properties and are therefore often used in many different applications.  \n$$\n\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n} =\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn}\n\\end{bmatrix}\n$$  \n<Callout type=\"example\">\n$$\n\\boldsymbol{A} =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Zero Matrix", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "If a matrix only contains elements that are zero it is called a zero matrix. A zero matrix is denoted as $\\boldsymbol{O}$ as the\ndimensions are usually implied by the context. If you want to be explicit you can also write $\\boldsymbol{O}_{m \\times n}$ where $m$ is the number of rows and $n$ is the number of columns.  \n$$\n\\boldsymbol{O} = \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n$$  \nIf the matrix is a column vector we usually use the lower case letter $\\boldsymbol{o}$ to denote a zero vector.  \n$$\n\\boldsymbol{o} = \\begin{bmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Diagonal Matrix", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "A diagonal matrix is a square matrix where all the elements that are not on the main diagonal are zero.\nThe main diagonal is the diagonal that goes from the top left to the bottom right of the matrix, in other words\nwhere the row and column index are equal, i.e. $a_{ij}$ where $i = j$.  \nSo a diagonal matrix is where $a_{ij} = 0$ for all $i \\neq j$ and $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$.\nIf a diagonal matrix is defined by the following elements $d_{11}, d_{22}, \\ldots, d_{nn}$ then the matrix can be written as:  \n$$\n\\boldsymbol{D} = \\text{diag}(d_{11}, d_{22}, \\ldots, d_{nn}) = \\begin{bmatrix}\nd_{11} & 0 & \\cdots & 0 \\\\\n0 & d_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & d_{nn}\n\\end{bmatrix}\n$$  \n<Callout type=\"example\">\n$$\n\\boldsymbol{D} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{bmatrix} = \\text{diag}(1, 2, 3)\n$$\n</Callout>  \nThe definition of a diagonal matrix can also be extended to non-square matrices. In this case the matrix is still a diagonal matrix if all the elements that are not on the main diagonal are zero.  \n<Callout type=\"example\">\nThe following non-square matrix are still diagonal matrices:  \n$$\n\\boldsymbol{D}_1 =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & -3 \\\\\n0 & 0 & 0\n\\end{bmatrix} \\text{ and } \\boldsymbol{D}_2 =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 4 & 0 & 0 & 0 \\\\\n0 & 0 & -3 & 0 & 0\n\\end{bmatrix}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Identity Matrix", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "An identity matrix sometimes also called unit matrix or identity of order $n$ is a square diagonal matrix where all the diagonal elements are equal to $1$. The identity matrix is often denoted as $\\boldsymbol{I}$ or $\\boldsymbol{I}_n$\nwhere $n$ is the number of rows and columns in the matrix.  \n<Callout type=\"example\">\n$$\n\\boldsymbol{I_3} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} = \\text{diag}(1, 1, 1)\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Triangular Matrix", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "A triangular matrix is a square matrix where all the elements above or below the main diagonal are $0$. If all the elements above the diagonal are $0$ it is called a **lower triangular matrix** becuase it only has elements below the\ndiagonal that are non-zero:  \n$$\n(\\boldsymbol{L})_{ij} = 0 \\text{ for all } i < j\n$$  \nIf all the elements below the diagonal are $0$ it is called a **upper triangular matrix** because it only has elements above the diagonal that are\nnon-zero:  \n$$\n(\\boldsymbol{U})_{ij} = 0 \\text{ for all } i > j\n$$  \n<Callout type=\"example\">\nA lower triangular matrix $\\boldsymbol{L}$:  \n$$\n\\boldsymbol{L} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n2 & 3 & 0 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n$$  \nAn upper triangular matrix $\\boldsymbol{U}$:  \n$$\n\\boldsymbol{U} =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 4 & 5 \\\\\n0 & 0 & 6\n\\end{bmatrix}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Matrix Addition", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "Two matrices can be added together if they have the same dimensions, i.e. $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ and $\\boldsymbol{B} \\in \\mathbb{R}^{m \\times n}$.\nThe addition of two matrices is defined as the element-wise addition of the matrices:  \n$$\n\\boldsymbol{A} + \\boldsymbol{B} = \\begin{bmatrix}\na_{11} + b_{11} & a_{12} + b_{12} & \\cdots & a_{1n} + b_{1n} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\cdots & a_{2n} + b_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} + b_{m1} & a_{m2} + b_{m2} & \\cdots & a_{mn} + b_{mn}\n\\end{bmatrix}\n$$  \nOr more formally:  \n$$\n\\boldsymbol{A} + \\boldsymbol{B} = (\\boldsymbol{A} + \\boldsymbol{B})_{ij} = (\\boldsymbol{A})_{ij} + (\\boldsymbol{B})_{ij}\n$$  \n<Callout type=\"example\">\n$$\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} + \\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix} = \\begin{bmatrix}\n8 & 10 & 12 \\\\\n14 & 16 & 18\n\\end{bmatrix}\n$$\n</Callout>  \n#### Additive Identity  \nAs you can expect the addition of any matrix with the null matrix results in the original matrix:  \n$$\n\\boldsymbol{A} + \\boldsymbol{O} = \\boldsymbol{A}\n$$  \n#### Additive Inverse  \nWe can also expect to find a matrix $\\boldsymbol{B}$ that when added to $\\boldsymbol{A}$ results in the null matrix:  \n$$\n\\boldsymbol{A} + \\boldsymbol{B} = \\boldsymbol{O}\n$$  \nThe matrix $\\boldsymbol{B}$ is called the additive inverse of $\\boldsymbol{A}$ and is denoted as $-\\boldsymbol{A}$ where\neach element in the matrix is defined as the negative of the corresponding element in the matrix $\\boldsymbol{A}$:  \n$$\n(-\\boldsymbol{A})_{ij} = -(\\boldsymbol{A})_{ij}\n$$  \nBy using the additive inverse we can also subtract two matrices. The subtraction of two matrices is defined as the addition of the first matrix and the additive inverse of the second matrix:  \n$$\n\\boldsymbol{A} - \\boldsymbol{B} = \\boldsymbol{A} + (-\\boldsymbol{B})\n$$  \n<Callout type=\"example\">\n$$\n\\boldsymbol{A} - \\boldsymbol{B} = \\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix} - \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Matrix Addition", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "$$\n\\boldsymbol{A} - \\boldsymbol{B} = \\boldsymbol{A} + (-\\boldsymbol{B})\n$$  \n<Callout type=\"example\">\n$$\n\\boldsymbol{A} - \\boldsymbol{B} = \\begin{bmatrix}\n7 & 8 & 9 \\\\\n10 & 11 & 12\n\\end{bmatrix} - \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} = \\begin{bmatrix}\n7 - 1 & 8 - 2 & 9 - 3 \\\\\n10 - 4 & 11 - 5 & 12 - 6\n\\end{bmatrix} = \\begin{bmatrix}\n6 & 6 & 6 \\\\\n6 & 6 & 6\n\\end{bmatrix}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Scalar Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "A matrix can be multiplied by a scalar, i.e. a constant. This is defined as the element-wise multiplication of the matrix with the scalar:  \n$$\ns \\cdot \\boldsymbol{A} = \\begin{bmatrix}\ns \\cdot a_{11} & s \\cdot a_{12} & \\cdots & s \\cdot a_{1n} \\\\\ns \\cdot a_{21} & s \\cdot a_{22} & \\cdots & s \\cdot a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ns \\cdot a_{m1} & s \\cdot a_{m2} & \\cdots & s \\cdot a_{mn}\n\\end{bmatrix}\n$$  \nOr more formally:  \n$$\ns \\cdot \\boldsymbol{A} = (s \\cdot \\boldsymbol{A})_{ij} = s \\cdot (\\boldsymbol{A})_{ij}\n$$  \n<Callout type=\"example\">\n$$\n5 \\cdot \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} = \\begin{bmatrix}\n5 \\cdot 1 & 5 \\cdot 2 & 5 \\cdot 3 \\\\\n5 \\cdot 4 & 5 \\cdot 5 & 5 \\cdot 6\n\\end{bmatrix} = \\begin{bmatrix}\n5 & 10 & 15 \\\\\n20 & 25 & 30\n\\end{bmatrix}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Matrix Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "Matrix multiplication is a bit more complex than addition and scalar multiplication. One would think that multiplying two matrices together would be as simple as multiplying\neach element of the first matrix with the corresponding element in the second matrix and would therefore just like addition be an element-wise operation and only defined for\nmatrices with the same dimensions. However, this is not the case, this would be the [Hadamard product]() of two matrices. The reason for a more complex\ndefinition is due to relationship that the matrix multiplication has with linear transformations to which we will come later.  \nMatrix multiplication is only defined for matrices where the number of columns in the first matrix is equal to the number of rows in the second matrix.\nWe will see why this is the case later. The result of multiplying two matrices together is a new matrix where the number of rows is equal to the number of rows in the first matrix and the number of\ncolumns is equal to the number of columns in the second matrix. So the dimensions in a matrix multiplication are defined as follows:  \n$$\n\\boldsymbol{A} \\in \\mathbb{R}^{\\color{red}{m} \\times \\color{blue}{n}} \\text{ and } \\boldsymbol{B} \\in \\mathbb{R}^{\\color{blue}{n} \\times \\color{green}{p}} \\Rightarrow \\boldsymbol{C} = \\boldsymbol{A} \\cdot \\boldsymbol{B} \\in \\mathbb{R}^{\\color{red}{m} \\times \\color{green}{p}}\n$$  \n<Image src=\"/maths/matrixMultiplicationDimensions.png\"\ncaption=\"Dimensions of a matrix multiplication visualized.\"\nwidth={300}\n/>  \nThe actual calculation of the elements in the resulting matrix is a bit more complex. The element in the $i$-th row and $j$-th column in the resulting matrix is defined as\nthe sum of the products of the elements in the $i$-th row of the first matrix and the $j$-th column of the second matrix. So the element $c_{ij}$ in the resulting matrix is calculated as follows:  \n$$\nc_{ij} = a_{i1} \\cdot b_{1j} + a_{i2} \\cdot b_{2j} + \\cdots + a_{im} \\cdot b_{mj} = \\sum_{k=1}^m a_{ik} \\cdot b_{kj}\n$$  \nor more formally:  \n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Matrix Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "$$\nc_{ij} = a_{i1} \\cdot b_{1j} + a_{i2} \\cdot b_{2j} + \\cdots + a_{im} \\cdot b_{mj} = \\sum_{k=1}^m a_{ik} \\cdot b_{kj}\n$$  \nor more formally:  \n$$\n\\boldsymbol{C} =\n\\boldsymbol{A} \\cdot \\boldsymbol{B} = (\\boldsymbol{A} \\cdot \\boldsymbol{B})_{ij} = \\sum_{k=1}^m (\\boldsymbol{A})_{ik} \\cdot (\\boldsymbol{B})_{kj}\n= \\sum_{k=1}^m a_{ik} \\cdot b_{kj}\n$$  \n<Image src=\"/maths/matrixMultiplication.png\"\ncaption=\"Matrix multiplication visualized.\"\nwidth={400}\n/>  \nIn the visualization above the elements would be calculated as follows:  \n$$\n\\begin{align*}\nc_{12} = a_{11} \\cdot b_{12} + a_{12} \\cdot b_{22} \\\\\nc_{33} = a_{31} \\cdot b_{13} + a_{32} \\cdot b_{23}\n\\end{align*}\n$$  \n<Callout type=\"example\">  \nI suggest to start with the first row of the first matrix and the first column of the second matrix and calculate the element in the resulting matrix.\nThen move on to the second column of the second matrix and calculate the element in the resulting matrix.\nOnce you have calculated all the elements in the first row of the resulting matrix move on to the second row of the first matrix and repeat the process.  \n$$\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} = \\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix}\n$$\n</Callout>  \n#### Commutative Property  \nMatrix multiplication is not commutative so:  \n$$\n\\boldsymbol{A} \\cdot \\boldsymbol{B} \\neq \\boldsymbol{B} \\cdot \\boldsymbol{A}\n$$  \nThis means that the order in which you multiply\nis important! This already becomes apparent when you look at the dimensions of the matrices. If you multiply a $2 \\times 3$ matrix with a $3 \\times 2$ matrix you get a\n$2 \\times 2$ matrix. However, if you multiply a $3 \\times 2$ matrix with a $2 \\times 3$ matrix you get a $3 \\times 3$ matrix.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} &= \\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n7 & 8 \\\\", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Matrix Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "<Callout type=\"example\">\n$$\n\\begin{align*}\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} &= \\begin{bmatrix}\n58 & 64 \\\\\n139 & 154\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n7 & 8 \\\\\n9 & 10 \\\\\n11 & 12\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} &= \\begin{bmatrix}\n39 & 54 & 69 \\\\\n49 & 68 & 87 \\\\\n59 & 82 & 105\n\\end{bmatrix}\n\\end{align*}\n$$  \nEven if the dimensions of the matrices are the same the result of the matrix multiplication can be different depending on the order of the matrices.  \n$$\n\\begin{align*}\n\\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix} &= \\begin{bmatrix}\n19 & 22 \\\\\n43 & 50\n\\end{bmatrix} \\\\\n\\begin{bmatrix}\n5 & 6 \\\\\n7 & 8\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n1 & 2 \\\\\n3 & 4\n\\end{bmatrix} &= \\begin{bmatrix}\n23 & 34 \\\\\n31 & 46\n\\end{bmatrix}\n\\end{align*}\n$$  \nFor the element $c_{11}$ we can clearly see why the order of the matrices is important:  \n$$\n\\begin{align*}\nc_{11} &= 1 \\cdot 5 + 2 \\cdot 7 = 19 \\\\\nc_{11} &= 5 \\cdot 1 + 6 \\cdot 3 = 23 \\\\\n19 &\\neq 23\n\\end{align*}\n$$\n</Callout>  \nThere are however some special matrices that are commutative to each other.\nThese matrices are called **commutative matrices** or **commute** to each other so:  \n$$\n\\boldsymbol{A} \\cdot \\boldsymbol{B} = \\boldsymbol{B} \\cdot \\boldsymbol{A}\n$$  \n<Callout type=\"example\">\nFor the three matrices $\\boldsymbol{A}$, $\\boldsymbol{B}$ and $\\boldsymbol{C}$:  \n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n2 & 6 \\\\\n1 & 7\n\\end{bmatrix} \\quad \\boldsymbol{B} = \\begin{bmatrix}\n-3 & -1 \\\\\n2 & 1\n\\end{bmatrix} \\quad \\boldsymbol{C} = \\begin{bmatrix}\n15 & 6 \\\\\n1 & 20\n\\end{bmatrix}\n$$  \nWe can calculate the following:  \n$$\n\\begin{align*}\n\\boldsymbol{AB} = \\begin{bmatrix}\n6 & 4 \\\\\n11 & 6\n\\end{bmatrix} \\quad \\boldsymbol{BA} = \\begin{bmatrix}\n-7 & -25 \\\\\n5 & 19\n\\end{bmatrix} \\\\\n\\boldsymbol{AC} = \\begin{bmatrix}\n36 & 132 \\\\\n22 & 146\n\\end{bmatrix} \\quad \\boldsymbol{CA} = \\begin{bmatrix}\n36 & 132 \\\\\n22 & 146\n\\end{bmatrix} \\\\", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Matrix Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "6 & 4 \\\\\n11 & 6\n\\end{bmatrix} \\quad \\boldsymbol{BA} = \\begin{bmatrix}\n-7 & -25 \\\\\n5 & 19\n\\end{bmatrix} \\\\\n\\boldsymbol{AC} = \\begin{bmatrix}\n36 & 132 \\\\\n22 & 146\n\\end{bmatrix} \\quad \\boldsymbol{CA} = \\begin{bmatrix}\n36 & 132 \\\\\n22 & 146\n\\end{bmatrix} \\\\\n\\end{align*}\n$$  \nWe can see that the matrices $\\boldsymbol{A}$ and $\\boldsymbol{C}$ commute to each other.  \n$$\n\\boldsymbol{A} \\cdot \\boldsymbol{C} = \\boldsymbol{C} \\cdot \\boldsymbol{A}\n\\quad \\text{and} \\quad\n\\boldsymbol{A} \\cdot \\boldsymbol{B} \\neq \\boldsymbol{B} \\cdot \\boldsymbol{A}\n$$\n</Callout>  \n#### Associative Property  \nMatrix multiplication is associative so:  \n$$\n(\\boldsymbol{A} \\cdot \\boldsymbol{B}) \\cdot \\boldsymbol{C} = \\boldsymbol{A} \\cdot (\\boldsymbol{B} \\cdot \\boldsymbol{C})\n$$  \nThis mean that the order in which you group the matrices in a multiplication does not matter. However,\nthis does not mean that the order in which you multiply the matrices does not matter. As we have seen before matrix multiplication is not commutative.  \nThe associative property is very useful for linear transformations as it allows you to combine multiple transformations into a single transformation.  \n<Callout type=\"proof\">\nWe have the following matrices:  \n$$\n\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n} \\quad \\boldsymbol{B} \\in \\mathbb{R}^{n \\times p} \\quad \\boldsymbol{C} \\in \\mathbb{R}^{p \\times q}\n$$  \nThen we can see that the following two equations become the same:  \n$$\n\\begin{align*}\n((\\boldsymbol{A}  \\boldsymbol{B}) \\boldsymbol{C})_{ik} &=\n\\sum_{l=1}^p (\\boldsymbol{A}  \\boldsymbol{B})_{il}  c_{lk} =\n\\sum_{l=1}^p \\sum_{j=1}^n a_{ij} b_{jl}  c_{lk} \\\\\n(\\boldsymbol{A} (\\boldsymbol{B} \\boldsymbol{C}))_{ij} &=\n\\sum_{j=1}^n a_{ij} (\\boldsymbol{B} \\boldsymbol{C})_{jk} =\n\\sum_{j=1}^n \\sum_{l=1}^p a_{ij} b_{jl}  c_{lk}\n\\end{align*}\n$$\n</Callout>  \n#### Distributive Property  \nThe matrix multiplication is left distributive over addition and right distributive over addition so:  \n$$\n\\begin{align*}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Matrix Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "\\sum_{j=1}^n \\sum_{l=1}^p a_{ij} b_{jl}  c_{lk}\n\\end{align*}\n$$\n</Callout>  \n#### Distributive Property  \nThe matrix multiplication is left distributive over addition and right distributive over addition so:  \n$$\n\\begin{align*}\n\\boldsymbol{A} \\cdot (\\boldsymbol{B} + \\boldsymbol{C}) = \\boldsymbol{A} \\cdot \\boldsymbol{B} + \\boldsymbol{A} \\cdot \\boldsymbol{C} \\\\\n(\\boldsymbol{A} + \\boldsymbol{B}) \\cdot \\boldsymbol{C} = \\boldsymbol{A} \\cdot \\boldsymbol{C} + \\boldsymbol{B} \\cdot \\boldsymbol{C}\n\\end{align*}\n$$  \n<Callout type=\"proof\">\nFor the matrices $\\boldsymbol{A}$, $\\boldsymbol{B}$ and $\\boldsymbol{C}$:  \n$$\n\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n} \\quad \\boldsymbol{B} \\in \\mathbb{R}^{m \\times n} \\quad \\boldsymbol{C} \\in \\mathbb{R}^{n \\times p}\n$$  \nThen we can see that the following two equations become the same:  \n$$\n\\begin{align*}\n((\\boldsymbol{A} + \\boldsymbol{B}) \\cdot \\boldsymbol{C})_{ij} &= \\sum_{k=1}^n (\\boldsymbol{A} + \\boldsymbol{B})_{ik} c_{kj} \\\\\n&= \\sum_{k=1}^n (a_{ik} + b_{ik}) c_{kj} = \\sum_{k=1}^n a_{ik} c_{kj} + b_{ik} c_{kj} = \\sum_{k=1}^n a_{ik} c_{kj} + \\sum_{k=1}^n b_{ik} c_{kj} \\\\\n&= (\\boldsymbol{A} \\cdot \\boldsymbol{C})_{ij} + (\\boldsymbol{B} \\cdot \\boldsymbol{C})_{ij}\n\\end{align*}\n$$\n</Callout>  \n#### Multiplicative Identity  \nWith regards to matrix multiplication the identity matrix is a special matrix.\nThe identity matrix functions as the multiplicative identity or also called the neutral element for matrix multiplication.\nMeaning that if you multiply any matrix with the identity matrix you get the same matrix back.\nThe identity matrix is also commute to any matrix.  \n$$\n\\boldsymbol{I} \\cdot \\boldsymbol{A} = \\boldsymbol{A} \\cdot \\boldsymbol{I} = \\boldsymbol{A}\n$$  \nThe reason as to why the identity matrix functions as the multiplicative identity can quite easily be seen if you\nthink of the first row in the identity matrix as selecting all the elements in the first row of the second matrix.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Matrix Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "$$  \nThe reason as to why the identity matrix functions as the multiplicative identity can quite easily be seen if you\nthink of the first row in the identity matrix as selecting all the elements in the first row of the second matrix.\nThen the second row in the identity matrix selects all the elements in the second row of the second matrix and so on. So the identity matrix just selects the elements in the matrix.  \n<Callout type=\"example\">\n$$\n\\boldsymbol{I} \\cdot {A} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n$$  \nThe same is true for the other way around:  \n$$\n\\boldsymbol{A} \\cdot {I} =\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\n$$\n</Callout>  \n#### Permutation Matrix  \n#### Strassen's Algorithm  \n#### Winograd's Algorithm", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Transpose", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "The transpose of a matrix is a matrix where the rows and columns are swapped. The transpose of the matrix $\\boldsymbol{A}$ is written as $\\boldsymbol{A}^T$.\nFormally the element in the $i$-th row and $j$-th column in the matrix $\\boldsymbol{A}$ becomes the element in the $j$-th row and $i$-th column in the\nmatrix $\\boldsymbol{A}^T$.  \n$$\n\\boldsymbol{A} = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix} \\Rightarrow \\boldsymbol{A}^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1} \\\\\na_{12} & a_{22} & \\cdots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{bmatrix}\n$$  \nOr also more formally:  \n$$\n(\\boldsymbol{A}^T)_{ij} = (\\boldsymbol{A})_{ji}\n$$  \nThis results in the transpose of a row vector being a column vector and the transpose of a column vector being a row vector. The same goes for a matrix\nwhere the transpose of a matrix results in a matrix with its dimensions swapped. So the transpose of a $m \\times n$ matrix is a $n \\times m$ matrix.  \n<Callout type=\"example\">\nTransposing a matrix:  \n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} \\Rightarrow \\boldsymbol{A}^T = \\begin{bmatrix}\n1 & 4 \\\\\n2 & 5 \\\\\n3 & 6\n\\end{bmatrix}\n$$  \nTransposing a column vector is very useful when you want to write vectors in a readable way in a text,\nas you can see. Most commonly in textbooks when a vector is defined it is defined as a transposed column vector.  \n$$\n\\boldsymbol{v} = \\begin{bmatrix}\nv_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\n\\end{bmatrix} \\Rightarrow \\boldsymbol{v}^T = \\begin{bmatrix}\nv_1 & v_2 & \\cdots & v_n\n\\end{bmatrix}\n$$  \nTransposing a row vector:  \n$$\n\\boldsymbol{v} = \\begin{bmatrix}\nv_1 & v_2 & \\cdots & v_n\n\\end{bmatrix} \\Rightarrow \\boldsymbol{v}^T = \\begin{bmatrix}\nv_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\n\\end{bmatrix}\n$$\n</Callout>  \n#### Properties of Transpose", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Transpose", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "$$  \nTransposing a row vector:  \n$$\n\\boldsymbol{v} = \\begin{bmatrix}\nv_1 & v_2 & \\cdots & v_n\n\\end{bmatrix} \\Rightarrow \\boldsymbol{v}^T = \\begin{bmatrix}\nv_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n\n\\end{bmatrix}\n$$\n</Callout>  \n#### Properties of Transpose  \nThere are a few useful properties of the transpose of a matrix that are worth remembering.\nThist first ist that if a matrix is transposed twice it is the same as the original matrix:  \n$$\n(\\boldsymbol{A}^T)^T = \\boldsymbol{A}\n$$  \n<Callout type=\"example\">\n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} \\Rightarrow \\boldsymbol{A}^T = \\begin{bmatrix}\n1 & 4 \\\\\n2 & 5 \\\\\n3 & 6\n\\end{bmatrix} \\Rightarrow (\\boldsymbol{A}^T)^T = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix} = \\boldsymbol{A}\n$$\n</Callout>  \nFurther ones are:  \n$$\n\\begin{align*}\n(\\boldsymbol{A} + \\boldsymbol{B})^T &= \\boldsymbol{A}^T + \\boldsymbol{B}^T \\\\\n(\\boldsymbol{A} \\cdot \\boldsymbol{B})^T &= \\boldsymbol{B}^T \\cdot \\boldsymbol{A}^T\n\\end{align*}\n$$  \n#### Symmetric Matrix  \nIf a matrix is equal to its transpose it is called a symmetric matrix. So $\\boldsymbol{A} = \\boldsymbol{A}^T$ and for each element in the matrix $a_{ij} = a_{ji}$.\nAs you can quite easily imagine a prerequisite for a matrix to be symmetric is that it is a square matrix as otherwise the transpose of the matrix would have different dimensions.  \nAnother thing that makes sense is that a diagonal matrix is always symmetric as all the elements that are not on the diagonal are zero.  \n<Callout type=\"example\">\nA symmetric matrix:  \n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 5 \\\\\n3 & 5 & 6\n\\end{bmatrix} \\Rightarrow \\boldsymbol{A}^T = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 5 \\\\\n3 & 5 & 6\n\\end{bmatrix} = \\boldsymbol{A}\n$$  \nA diagonal matrix is also symmetric:  \n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{bmatrix} \\Rightarrow \\boldsymbol{A}^T = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{bmatrix} = \\boldsymbol{A}\n$$\n</Callout>  \n##### Skew-Symmetric Matrix", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Transpose", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "$$\n\\boldsymbol{A} = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{bmatrix} \\Rightarrow \\boldsymbol{A}^T = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{bmatrix} = \\boldsymbol{A}\n$$\n</Callout>  \n##### Skew-Symmetric Matrix  \nA skew-symmetric matrix is a matrix where the elements of the matrix are equal to the negative of the elements in the transpose of the matrix.\nSo $\\boldsymbol{A} = -\\boldsymbol{A}^T$ and for each element in the matrix $a_{ij} = -a_{ji}$.  \n<Callout type=\"example\">\n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n0 & 2 & -3 \\\\\n-2 & 0 & 5 \\\\\n3 & -5 & 0\n\\end{bmatrix} \\Rightarrow \\boldsymbol{A}^T = \\begin{bmatrix}\n0 & -2 & 3 \\\\\n2 & 0 & -5 \\\\\n-3 & 5 & 0\n\\end{bmatrix} = -\\boldsymbol{A}\n$$\n</Callout>  \n#### Transposing on a Computer  \nWhen you want the transpose of a matrix you don't actually need to perform any operations. You can just change the way you access the elements of the matrix. Rather than\naccessing the elements row by row you can access them column by column.  \n<Image src=\"/maths/matrixTransposeOnComputers.png\"\ncaption=\"Reading a matrix and its transpose.\"\nwidth={200}\n/>  \nDepending on the size of the matrix and how many times you need to access the elements of the matrix this can be a lot faster than actually transposing the matrix. However,\nif you need to access the elements of the matrix multiple times it is probably faster to transpose the matrix first and then access the elements due to memory locality.  \nTo transpose a square matrix in-place you can use the following algorithm which you can think of as swapping the elements of the matrix along the diagonal:  \n```java\nfor (int i = 0; i < n; i++) {\nfor (int j = i + 1; j < n; j++) {\nswap(A[i][j], A[j][i]);\n}\n}\n```  \nFor a non-square matrix you need to use a slightly more complex algorithm.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Inverse", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "The inverse of a matrix is a matrix that when multiplied with the original matrix results in the identity matrix. The inverse of a matrix is denoted as $\\boldsymbol{A}^{-1}$.  \n$$\n\\boldsymbol{A} \\cdot \\boldsymbol{A}^{-1} = \\boldsymbol{A}^{-1} \\cdot \\boldsymbol{A} = \\boldsymbol{I}\n$$  \nNot all matrices have an inverse. A matrix that has an inverse is called an regular/invertible/non-singular matrix. A matrix that does not have an inverse is called a degenerate/singular/non-invertible matrix.\nIf a matrix is invertible then the inverse is unique.  \nSimilarily the inverse of a matrix has some properties that are worth remembering:  \n$$\n\\begin{align*}\n(\\boldsymbol{A}^{-1})^{-1} &= \\boldsymbol{A} \\\\\n(\\boldsymbol{A} \\cdot \\boldsymbol{B})^{-1} &= \\boldsymbol{B}^{-1} \\cdot \\boldsymbol{A}^{-1} \\\\\n(\\boldsymbol{A} + \\boldsymbol{B})^{-1} &\\neq \\boldsymbol{A}^{-1} + \\boldsymbol{B}^{-1}\n\\end{align*}\n$$  \n<Callout type=\"example\">\nThe following matrix is invertible:  \n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n1 & 2 & 1 \\\\\n4 & 4 & 5 \\\\\n6 & 7 & 7\n\\end{bmatrix} \\quad \\text{with} \\quad \\boldsymbol{A}^{-1} = \\begin{bmatrix}\n-7 & -7 & 6 \\\\\n2 & 1 & -1 \\\\\n4 & 5 & -4\n\\end{bmatrix}\n$$  \nThe following simple matrix is not invertible:  \n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n2 & 4 \\\\\n2 & 4\n\\end{bmatrix}\n$$\n</Callout>  \n#### Calculating the Inverse  \n<Callout type=\"todo\">\nHow to calculate the inverse of a matrix and know if a matrix is invertible.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Rank", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "<Callout type=\"todo\">\nRanks seem a bit annoying but not hugely complex. also somehow related to the determinant and the inverse.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Frobenius Norm", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "The Frobenius norm is a way to measure the size of a matrix. It is defined as the square root of the sum of the squares of all the elements in the matrix. So for a matrix\n$\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ the Frobenius norm is defined as follows:  \n$$\n\\|\\boldsymbol{A}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n a_{ij}^2}\n$$  \nYou can also think of it as just taking the matrix and flattening it into a vector and then calculating the length of that vector, i.e. the Euclidean/L2 norm of the vector.  \n<Callout type=\"example\">\nIf we define the matrix $\\boldsymbol{A}$:  \n$$\n\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\n$$  \nThen the Frobenius norm of $\\boldsymbol{A}$ is:  \n$$\n\\|\\boldsymbol{A}\\|_F = \\sqrt{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2} = \\sqrt{91} \\approx 9.539\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Trace", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "The trace of a matrix is the sum of all the diagonal elements in the matrix and is denoted as $\\text{tr}(\\boldsymbol{A})$. Because it is the sum of the diagonal elements\nit is only defined for square matrices, i.e. $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$:  \n$$\n\\text{tr}(\\boldsymbol{A}) = \\sum_{i=1}^n a_{ii} = a_{11} + a_{22} + \\cdots + a_{nn}\n$$  \n<Callout type=\"example\">\n$$\n\\text{tr}(\\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}) = 1 + 5 + 9 = 15\n$$\n</Callout>  \n<Callout type=\"todo\">\nAdd properties of the trace. and proof that it is the sum of the eigenvalues.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Matrix Operations", "Header 3": "Determinant", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "<Callout type=\"todo\">\nThis section is still a work in progress!\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Matrices", "Header 2": "Orthogonal / Orthonormal Matrix", "path": "../pages/digitalGarden/maths/linearAlgebra/matrices.mdx"}, "page_content": "Very unclear what the difference is between these two. I think an orthogonal matrix is a matrix where the columns are orthogonal to each other but don't have to be normalised.\nAnd an orthonormal matrix is a matrix where the columns are orthogonal to each other and are normalised, i.e. have a length of $1$.  \n<Callout type=\"todo\">\nThis section is still a work in progress!\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linear Maps", "path": "../pages/digitalGarden/maths/linearAlgebra/linearMaps.mdx"}, "page_content": "also known as linear transformations, are functions between vector spaces that preserve the vector space operations of addition and scalar multiplication. In other words, a linear map is a function that takes in vectors and outputs vectors.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linear Maps", "Header 2": "Matrices as Linear Maps", "path": "../pages/digitalGarden/maths/linearAlgebra/linearMaps.mdx"}, "page_content": "Also relates back to inverse matrices, as the inverse of a matrix is the matrix that undoes the transformation of the original matrix.  \nso linear maps are bijective, and thus invertible.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linear Equations", "path": "../pages/digitalGarden/maths/linearAlgebra/linearEquations.mdx"}, "page_content": "ax=b  \nand the number of solutions. Not vector version.  \nthen y=mx+c and the number of solutions and the graphical representation.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Linear Equations", "Header 2": "System of Linear Equations", "path": "../pages/digitalGarden/maths/linearAlgebra/linearEquations.mdx"}, "page_content": "general system and the rules for the number of solutions.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "The gaussian elimination algorithm, sometimes called the row reduction algorithm, is an algorithm that was first developed to solve systems of linear equations by using some\nsimple operations on the rows of the matrix. The allowed operations are:  \n- Swapping two rows\n- Multiplying a row by a non-zero number\n- Adding a row to another row. This can also be combined with the above operation of multiplying a row to be able to add a multiple of a row to another. The multiple can also be negative resulting in subtracting a multiple of one row from another row.  \nThese operations are allowed because they do not change the solution set of the system of linear equations. When solving a system of linear equations the conventional way\nby using algebra you are also performing the same operations but in a different way.  \n<Callout type=\"example\">\nGiven the system of linear equations:  \n$$\n\\begin{vmatrix}\n2x - 2y + 4z = 6 \\\\\n-5x + 6y -7z = -7 \\\\\n3x + 2y + z = 9\n\\end{vmatrix}\n$$  \nYou would probably solve it by trying to eliminate one variable at a time. For example let's solve the first equation for $x$:  \n$$\n\\begin{align*}\n2x = 2y - 4z + 6 \\\\\n\\rightarrow x = y - 2z + 3\n\\end{align*}\n$$  \nNow we can substitute this into the second equation:  \n$$\n\\begin{align*}\n-5x + 6y -7z = -7 \\\\\n& \\rightarrow -5(y - 2z + 3) + 6y - 7z = -7 \\\\\n& \\rightarrow -5y + 10z - 15 + 6y - 7z = -7 \\\\\n& \\rightarrow y + 3z = 8\n\\end{align*}\n$$  \nand the third equation:  \n$$\n\\begin{align*}\n3x + 2y + z = 9 \\\\\n& \\rightarrow 3(y - 2z + 3) + 2y + z = 9 \\\\\n& \\rightarrow 3y - 6z + 9 + 2y + z = 9 \\\\\n& \\rightarrow 5y - 5z = 0\n\\end{align*}\n$$  \nThe above step could also have been achieved by addiding/subtracting the correct multiple of the first equation to the second and third equation, which is what the gaussian elimination algorithm does.\nTo remove the $x$ variable from the second equation we could subtract the first equation multiplied by $-\\frac{5}{2}$ from the second equation:  \nFirst we multiply the first equation by $-\\frac{5}{2}$:  \n$$\n\\begin{align*}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "To remove the $x$ variable from the second equation we could subtract the first equation multiplied by $-\\frac{5}{2}$ from the second equation:  \nFirst we multiply the first equation by $-\\frac{5}{2}$:  \n$$\n\\begin{align*}\n2x - 2y + 4z = 6 \\\\\n& \\rightarrow -\\frac{5}{2}(2x - 2y + 4z) = -\\frac{5}{2} \\cdot 6 \\\\\n& \\rightarrow -5x + 5y - 10z = -15\n\\end{align*}\n$$  \nNow we subtract this from the second equation:  \n$$\n\\begin{align*}\n-5x + 6y -7z = -7 \\\\\n& \\rightarrow -5x + 6y -7z - (-5x + 5y - 10z) = -7 - (-15) \\\\\n& \\rightarrow y + 3z = 8\n\\end{align*}\n$$  \nAnd we have the same result as before. This process can be repeated for the third equation to get the same result as before.\nWe can then also repeat the process to eliminate the $y$ variable from the third equation by adding/subtracting the correct multiple of the second equation to the third equation.  \nThen to finally solve for the variables we can perform back substitution, i.e take the found value for $z$ and substitute it into the second equation to solve for $y$ and then substitute the found value for $y$ and $z$ into the first equation to solve for $x$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Row Echelon Form", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "Above we have seen the idea of the algorithm but before looking at the algorithm, we need to define what it means for a matrix to be in row echelon form. A matrix is in row echelon form if it satisfies the following conditions:  \n- All rows that only contain zeros are at the bottom of the matrix.\n- The first nonzero element in each row, called the leading entry or pivot, is to the right of all the pivots of the rows above it.  \n<Callout type=\"example\">\nIn row echelon form:  \n$$\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n0 & 1 & 2 & 3 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$  \nNotice that in the third row, the pivot is not directly to the right of the pivot in the second row, but it is to the right it doesn't matter if there are zeros in between.  \nNot in row echelon form:  \n$$\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n0 & 1 & 2 & 3 \\\\\n0 & 1 & 2 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$  \nBecause in the third row, the pivot is not to the right of the pivot in the second row.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Row Echelon Form", "Header 3": "Reduced Row Echelon Form", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "Reduced row echelon form is a stricter form of row echelon form. A matrix is in reduced row echelon form if it satisfies the following conditions:  \n- It is in row echelon form.\n- The pivot element in each row is equal to 1.  \n<Callout type=\"example\">\n$$\n\\begin{bmatrix}\n1 & 0 & 3 & 2 \\\\\n0 & 1 & 0 & 5 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Solving Linear Systems", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "Orignally the gaussian elimination algorithm was developed to solve systems of linear equations.  \nSAY SOMETHING ABOUT USUALLY IT IS USED FOR SQUARE MATRICES  \nBefore the algorithm can be used to solve a system of linear equations,\nthe system must be transformed into a matrix, to be more specific, an augmented matrix. An augmented matrix is a matrix that contains the coefficients of the variables\non the left side of the vertical line and the constants on the right side of the vertical line.  \n<Callout type=\"example\">\nThe system of linear equations:  \n$$\n\\begin{vmatrix}\n2x + 3y = 6 \\\\\nx - y = \\frac{1}{2}\n\\end{vmatrix}\n$$  \nHas the coefficient matrix:  \n$$\n\\begin{bmatrix}\n2 & 3 \\\\\n1 & -1\n\\end{bmatrix}\n$$  \nAnd the augmented matrix:  \n$$\n\\left[\\begin{array}{cc|c}\n2 & 3 & 6 \\\\\n1 & -1 & \\frac{1}{2}\n\\end{array}\\right]\n$$  \n</Callout>  \nOnce we have the augmented matrix, we can use the gaussian elimination algorithm to solve the system of linear equations. We solve the system of linear equations by\nusing the allowed operations to transform the augmented matrix into the row echelon form. Once the augmented matrix is in row echelon form, we can easily solve the\nsystem of linear equations by back substitution.  \n<Callout type=\"info\" title=\"Why does it work?\">\nThe gaussian elimination algorithm works because the allowed operations do not change the solution set of the system of linear equations. Simply changing the order\nof the equations in the system is obvious why it doesn't change the solution set. Adding an equation to another equation is allowed because imagine\nyou have the following system of linear equations:  \n$$\n\\begin{vmatrix}\n5x + 3y = 200 \\\\\n2x + y = 100\n\\end{vmatrix}\n$$  \nYou can't simple add 100 becuase you have to add the same amount to both sides of the equation and then the equation would no longer be in its standard form and by\ntransforming it into its standard form you would have to subtract 100 from both sides of the equation which would result in a hole lot of nothing.  \n$$\n\\begin{vmatrix}", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Solving Linear Systems", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "transforming it into its standard form you would have to subtract 100 from both sides of the equation which would result in a hole lot of nothing.  \n$$\n\\begin{vmatrix}\n5x + 3y + 100 = 300 \\\\\n2x + y = 100\n\\end{vmatrix}\n$$  \nHowever, what you can do is add on the right side 100 to the first equation $2x + y$ to left side because it is equal to 100. You can think of it as adding 100 to both sides\nbut whilst keeping the equation in its standard form.  \n$$\n\\begin{vmatrix}\n5x + 3y + 2x + y = 200 + 100 \\\\\n2x + y = 100\n\\end{vmatrix} \\rightarrow\n\\begin{vmatrix}\n7x + 4y = 300 \\\\\n2x + y = 100\n\\end{vmatrix}\n$$  \nMultiplying an equation by a nonzero number is allowed because it is the same as multiplying both sides of the equation by the same number, it doesn't change the\nlinear equation at all.  \nCombining these two operations, we can see that we can add a multiple of one equation to another equation and it will not change the solution set!\n</Callout>  \nSo now that we know why the gaussian elimination algorithm works, let's actually solve our system of linear equations from above using it.  \n<Callout type=\"example\">\n$$\n\\begin{align*}\n\\left[\\begin{array}{cc|c}\n2 & 3 & 6 \\\\\n1 & -1 & \\frac{1}{2}\n\\end{array}\\right]& \\rightarrow R_1 \\leftrightarrow R_2 \\\\\n\\rightarrow \\left[\\begin{array}{cc|c}\n1 & -1 & \\frac{1}{2} \\\\\n2 & 3 & 6\n\\end{array}\\right]& \\rightarrow R_2 = R_2 - 2R_1  \\\\\n\\rightarrow \\left[\\begin{array}{cc|c}\n1 & -1 & \\frac{1}{2} \\\\\n0 & 5 & 5\n\\end{array}\\right]&\n\\end{align*}\n$$  \nAnd we have our augmented matrix in row echelon form!  \nWhy did we first swap the rows? Well, we did that because we wanted to make the first pivot 1. We could have\nalso multiplied the first row by $\\frac{1}{2}$ but that would have resulted in a fraction and we don't want that.  \nWhy did we subtract 2 times the first row from the\nsecond row? Well, we did that because we wanted to make the second pivot 0.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Solving Linear Systems", "Header 3": "Back Substitution", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "Now that we have our augmented matrix in row echelon form, we can solve the system of linear equations by back substitution. Back substitution is the process of\nsolving for the variables starting from the bottom of the matrix and working our way up. We can do this in two ways, either by using algebra or by using the\ngaussian elimination even further.  \n<Callout type=\"example\" title=\"Using Algebra\">\nWe can take the bottom row of the augmented matrix and quickly solve it for $y$ thanks to the row echelon form by dividing both sides by 5.  \n$$\n5y = 5 \\rightarrow y = 1\n$$  \nNow that we know $y = 1$, we can substitute it into the first row of the augmented matrix and solve for $x$.  \n$$\nx - 1 = \\frac{1}{2} \\rightarrow x = \\frac{3}{2}\n$$\n</Callout>  \n<Callout type=\"example\" title=\"Using Gaussian Elimination\">\nTo completely solve the system of linear equations, we can use the gaussian elimination algorithm to further transform the augmented matrix from row echelon form\nto reduced row echelon form. This will make it easier to solve for the variables.  \n$$\n\\begin{align*}\n\\left[\\begin{array}{cc|c}\n1 & -1 & \\frac{1}{2} \\\\\n0 & 5 & 5\n\\end{array}\\right]& \\rightarrow R_2 = \\frac{1}{5}R_2 \\\\\n\\rightarrow \\left[\\begin{array}{cc|c}\n1 & -1 & \\frac{1}{2} \\\\\n0 & 1 & 1\n\\end{array}\\right]& \\rightarrow R_1 = R_1 + R_2 \\\\\n\\rightarrow \\left[\\begin{array}{cc|c}\n1 & 0 & \\frac{3}{2} \\\\\n0 & 1 & 1\n\\end{array}\\right]&\n\\end{align*}\n$$  \nAnd now we have our augmented matrix in reduced row echelon form! We can now easily see that $x = \\frac{3}{2}$ and $y = 1$ and that is it!\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Solving Linear Systems", "Header 3": "Free Variables", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "the columns that do not contain a pivot are called free variables. The number of free variables is equal to the number of columns minus the number of pivots.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Solving Linear Systems", "Header 3": "Compacted Form", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "especially when working with large matrices, it can be useful to compact the matrix by only looking at the columns that still need to be processed.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Calculating the Rank", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "<Callout type=\"todo\">\nTODO in reduced row echelon form, the number of pivots is the rank of the matrix, i.e. number of non-zero rows. Proof column rank = row rank\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Calculating the Rank", "Header 3": "Number of Solutions", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "using the rank of the matrix we can determine the number of solutions to a system of linear equations.  \nMeaning of regular and singular matrices???  \nAlso something about homogenous systems and non-homogenous systems. Trivial solutions and non-trivial solutions.  \nConsistency conditions", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Calculating the Inverse", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "The gaussian elimination method can be extended to calculate the inverse of a matrix. Because the inverse of a matrix is only defined for square matrices,\nthis algorithm only works for square matrices such as $2 \\times 2$ matrices, $3 \\times 3$ matrices, $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$.  \nJust like when solving a system of linear equations, we first transform the matrix into an augmented matrix. However, instead of having the constants on the right side\nof the vertical line we add the identity matrix $\\boldsymbol{I}$ of the same size as the matrix on the right side of the vertical line.  \n<Callout type=\"example\">\nGiven the matrix:  \n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n2 & 4 & -2 \\\\\n4 & 9 & -3 \\\\\n-2 & -3 & 7\n\\end{bmatrix}\n$$  \nWe can calculate the inverse of $\\boldsymbol{A}$ by transforming it into the following augmented matrix:  \n$$\n[\\boldsymbol{A}|\\boldsymbol{I}] =\\left[\\begin{array}{ccc|ccc}\n2 & 4 & -2 & 1 & 0 & 0 \\\\\n4 & 9 & -3 & 0 & 1 & 0 \\\\\n-2 & -3 & 7 & 0 & 0 & 1\n\\end{array}\\right]\n$$  \n</Callout>  \nThe goal of the algorithm is now to transform the left side of the augmented matrix into the identity matrix $\\boldsymbol{I}$. We are still only allowed to perform\nthe same operations as before:  \n- Swapping two rows\n- Multiplying a row by a nonzero number\n- Adding a multiple of one row to another row, the multiple can also be negative resulting in subtracting a multiple of one row from another row.  \nImportant it is important that when we perform an operation on the left side of the augmented matrix, we also perform the same operation on the right side!  \nBy then transforming the left hand side firts into row echelon form and then into reduced row echelon form and then lastly performing back substitution\nwe should get on the left hand side the identity matrix $\\boldsymbol{I}$ and on the right hand side the inverse of the matrix $\\boldsymbol{A}$ so that we have:  \n$$\n[\\boldsymbol{I}|\\boldsymbol{A}^{-1}]\n$$  \nWhich fullfills the definition of the inverse of a matrix:  \n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Calculating the Inverse", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "$$\n[\\boldsymbol{I}|\\boldsymbol{A}^{-1}]\n$$  \nWhich fullfills the definition of the inverse of a matrix:  \n$$\n\\boldsymbol{A} \\boldsymbol{A}^{-1} = \\boldsymbol{I}\n$$  \n<Callout type=\"example\">\n$$\n\\begin{align}\n[\\boldsymbol{A}|\\boldsymbol{I}]= \\left[\\begin{array}{ccc|ccc}\n2 & 4 & -2 & 1 & 0 & 0 \\\\\n4 & 9 & -3 & 0 & 1 & 0 \\\\\n-2 & -3 & 7 & 0 & 0 & 1\n\\end{array}\\right] & \\rightarrow R_1 = \\frac{1}{2}R_1 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 2 & -1 & \\frac{1}{2} & 0 & 0 \\\\\n4 & 9 & -3 & 0 & 1 & 0 \\\\\n-2 & -3 & 7 & 0 & 0 & 1\n\\end{array}\\right] & \\rightarrow R_2 = R_2 - 4R_1 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 2 & -1 & \\frac{1}{2} & 0 & 0 \\\\\n0 & 1 & 1 & -2 & 1 & 0 \\\\\n-2 & -3 & 7 & 0 & 0 & 1\n\\end{array}\\right] & \\rightarrow R_3 = R_3 + 2R_1 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 2 & -1 & \\frac{1}{2} & 0 & 0 \\\\\n0 & 1 & 1 & -2 & 1 & 0 \\\\\n0 & 1 & 5 & 1 & 0 & 1\n\\end{array}\\right] & \\rightarrow R_3 = R_3 - R_2 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 2 & -1 & \\frac{1}{2} & 0 & 0 \\\\\n0 & 1 & 1 & -2 & 1 & 0 \\\\\n0 & 0 & 4 & 3 & -1 & 1\n\\end{array}\\right] & \\rightarrow R_3 = \\frac{1}{4}R_3 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 2 & -1 & \\frac{1}{2} & 0 & 0 \\\\\n0 & 1 & 1 & -2 & 1 & 0 \\\\\n0 & 0 & 1 & \\frac{3}{4} & -\\frac{1}{4} & \\frac{1}{4}\n\\end{array}\\right] & \\rightarrow R_2 = R_2 - R_3 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 2 & -1 & \\frac{1}{2} & 0 & 0 \\\\\n0 & 1 & 0 & -\\frac{11}{4} & \\frac{5}{4} & -\\frac{1}{4} \\\\\n0 & 0 & 1 & \\frac{3}{4} & -\\frac{1}{4} & \\frac{1}{4}\n\\end{array}\\right] & \\rightarrow R_1 = R_1 + R_3 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 2 & 0 & \\frac{5}{4} & -\\frac{1}{4} & \\frac{1}{4} \\\\\n0 & 1 & 0 & -\\frac{11}{4} & \\frac{5}{4} & -\\frac{1}{4} \\\\\n0 & 0 & 1 & \\frac{3}{4} & -\\frac{1}{4} & \\frac{1}{4}\n\\end{array}\\right] & \\rightarrow R_1 = R_1 - 2R_2 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 0 & 0 & \\frac{27}{4} & -\\frac{11}{4} & \\frac{3}{4} \\\\\n0 & 1 & 0 & -\\frac{11}{4} & \\frac{5}{4} & -\\frac{1}{4} \\\\", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Gaussian Elimination", "Header 2": "Calculating the Inverse", "path": "../pages/digitalGarden/maths/linearAlgebra/gaussianElimination.mdx"}, "page_content": "\\end{array}\\right] & \\rightarrow R_1 = R_1 - 2R_2 \\\\  \n\\rightarrow \\left[\\begin{array}{ccc|ccc}\n1 & 0 & 0 & \\frac{27}{4} & -\\frac{11}{4} & \\frac{3}{4} \\\\\n0 & 1 & 0 & -\\frac{11}{4} & \\frac{5}{4} & -\\frac{1}{4} \\\\\n0 & 0 & 1 & \\frac{3}{4} & -\\frac{1}{4} & \\frac{1}{4}\n\\end{array}\\right]\n\\end{align}\n$$  \nThis gives us the inverse of the matrix $\\boldsymbol{A}$:  \n$$\n\\boldsymbol{A}^{-1} = \\begin{bmatrix}\n\\frac{27}{4} & -\\frac{11}{4} & \\frac{3}{4} \\\\\n-\\frac{11}{4} & \\frac{5}{4} & -\\frac{1}{4} \\\\\n\\frac{3}{4} & -\\frac{1}{4} & \\frac{1}{4}\n\\end{bmatrix} = \\frac{1}{4} \\begin{bmatrix}\n27 & -11 & 3 \\\\\n-11 & 5 & -1 \\\\\n3 & -1 & 1\n\\end{bmatrix} = \\begin{bmatrix}\n6.75 & -2.75 & 0.75 \\\\\n-2.75 & 1.25 & -0.25 \\\\\n0.75 & -0.25 & 0.25\n\\end{bmatrix}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hadamard Product", "path": "../pages/digitalGarden/maths/linearAlgebra/hadamardProduct.mdx"}, "page_content": "The hadamard product is how some people might first think matrix multiplication works, which is wrong.  \nThe hadamard product is denoted using $\\boldsymbol{A} \\odot \\boldsymbol{B}$ and is also commonly known as the element-wise matrix multiplication which perfectly describes how it works. It is defined for two matrices $\\boldsymbol{A}$ and $\\boldsymbol{B}$ of the same dimensions and results in another matrix $\\boldsymbol{C}$ that has the same dimension as the two input matrices so $\\boldsymbol{A, B, C} \\in \\mathbb{R}^{N \\times M}$. So each element at $i,j$ in the resulting matrix is the product of the elements at $i,j$ of the two input matrices.  \n$$\n\\boldsymbol{A} \\odot \\boldsymbol{B}=\n\\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix} \\odot\n\\begin{bmatrix}\nb_{11} & b_{12} & b_{13} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33}\n\\end{bmatrix} =\n\\begin{bmatrix}\na_{11}b_{11} & a_{12}b_{12} & a_{13}b_{13} \\\\\na_{21}b_{21} & a_{22}b_{22} & a_{23}b_{23} \\\\\na_{31}b_{31} & a_{32}b_{32} & a_{33}b_{33}\n\\end{bmatrix}\n$$  \n<Callout type=\"example\">\n$$\n\\begin{bmatrix}\n1 & 4 & 7 \\\\\n2 & 3 & 8 \\\\\n9 & 5 & 6\n\\end{bmatrix} \\odot\n\\begin{bmatrix}\n6 & 5 & 8 \\\\\n3 & 2 & 9 \\\\\n7 & 4 & 1\n\\end{bmatrix} =\n\\begin{bmatrix}\n6 & 20 & 56 \\\\\n6 & 6 & 72 \\\\\n63 & 20 & 6\n\\end{bmatrix}\n$$\n</Callout>  \nBeginners in numpy often write the following lines of code to multiply two matrices  \n```python\nimport numpy as np", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Hadamard Product", "path": "../pages/digitalGarden/maths/linearAlgebra/hadamardProduct.mdx"}, "page_content": "A = np.array([[1,4,7],[2,3,8],[9,5,6]])\nB = np.array([[6,5,8],[3,2,9],[7,4,1]])\nA * B\n```\n<div className=\"code-output-wrapper\">\n```\narray([[ 6, 20, 56],\n[ 6,  6, 72],\n[63, 20,  6]])\n```\n</div>  \nBut as might notice this is the hadamard product/element-wise multiplication. You would get the same result if you used the [`np.multiply(A,B)`](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html) function as this is the function that is called under the hood when using the `*` operator. If you really did want to multiply $\\boldsymbol{A}$ with $\\boldsymbol{B}$ you would need to use the `@` operator or [`np.matmul(A,B)`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) function.  \nIf you try to calculate a hadamard product in numpy where the matrices do not have the same shape you will probably get the following error:  \n`ValueError: operands could not be broadcast together with shapes (2,2) (3,3)`  \nSo you can see that the hadamard product only works if A and B are of the same dimension/shape. However, from the error you can also see that there is an exception to this definition \"if the matrices can be broadcast together\". To find out more about broadcasting and how it works check out [this page](./broadcasting.md).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LU Decomposotion", "path": "../pages/digitalGarden/maths/linearAlgebra/luDecomposition.mdx"}, "page_content": "Basically the goal of the LU decomposition is to factorize a matrix into a product of a lower triangular matrix L and an upper triangular matrix U.  \n$$\nA = LU\n$$  \nWhere A is the matrix we want to factorize, L is the lower triangular matrix and U is the upper triangular matrix.  \nThe U matrix is the equivalent to the matrix we get from the Gaussian elimination method, and the L matrix is the matrix that contains the multipliers that we used to eliminate the elements below the diagonal.  \nSo in other words we can perform the gaussian elimination method with one matrix multiplication by choosing the correct L matrix.  \nJust like with the Gaussian elimination method, we can use the LU decomposition to solve systems of linear equations.\nThe resulting matrices are not unique like with the Gaussian elimination method.\nHowever, the same L can be used to solve multiple systems of equations for the same A matrix and different b vectors.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LU Decomposotion", "Header 3": "Partial Pivoting", "path": "../pages/digitalGarden/maths/linearAlgebra/luDecomposition.mdx"}, "page_content": "The matrix A can be permuted with a permutation matrix P to swap rows.  \n$$\nPA = LU\n$$  \nWhere P is a permutation matrix, A is the matrix we want to factorize, L is the lower triangular matrix and U is the upper triangular matrix.  \nThis is called partial pivoting because we only swap rows.  \nWhat is the optimal strategy for choosing the pivot element? Maybe this should actually be written in the gaussian elimination method.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LU Decomposotion", "Header 3": "Full Pivoting", "path": "../pages/digitalGarden/maths/linearAlgebra/luDecomposition.mdx"}, "page_content": "The matrix A can be permuted with a permutation matrix P to swap rows and columns.  \n$$\nPAQ = LU\n$$  \nWhere P and Q are permutation matrices, A is the matrix we want to factorize, L is the lower triangular matrix and U is the upper triangular matrix.  \nThis is called full pivoting because we swap rows and columns.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "LU Decomposotion", "Header 3": "LDU Decomposition", "path": "../pages/digitalGarden/maths/linearAlgebra/luDecomposition.mdx"}, "page_content": "The LU decomposition can be extended to the LDU decomposition where D is a diagonal matrix.  \n$$\nA = LDU\n$$  \nWhere A is the matrix we want to factorize, L is the lower triangular matrix, D is the diagonal matrix and U is the upper triangular matrix.  \nWhat are the advantages?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "SVD - Singular Value Decomposition", "path": "../pages/digitalGarden/maths/linearAlgebra/svd.mdx"}, "page_content": "The eigendecomposition only works for square matrices, the singular value decomposition, short SVD, is a generalization of the eigendecomposition allowing it to be used for rectangular matrices. Singular value decomposition uses 3 matrices just like the eigendecomposition.  \n$$\n\\boldsymbol{A}=\\boldsymbol{U}\\Sigma\\boldsymbol{V}^T\n$$  \nThe first matrix $\\boldsymbol{U}$ is the so-called left singular value matrix which is an orthogonal matrix meaning $\\boldsymbol{UU}^T=\\boldsymbol{I}$, the second matrix $\\Sigma$ is the singular value matrix which is very just like the matrix containing the eigenvalues in the eigendecomposition a diagonal matrix. The last matrix $\\boldsymbol{V}^T$ is the right singular value matrix which is also an orthogonal matrix. To find the values we can do the following transformations which make it very similar to the eigendecomposition.  \n$$\n\\begin{align*}\n\\boldsymbol{A}^T\\boldsymbol{A}=\\boldsymbol{V}\\Sigma^T\\boldsymbol{U}^T\\boldsymbol{U}\\Sigma\\boldsymbol{V}^T \\\\\n\\boldsymbol{A}^T\\boldsymbol{A}=\\boldsymbol{V}(\\Sigma^T\\Sigma)\\boldsymbol{V}^T\n\\end{align*}\n$$  \nBecause $\\Sigma$ is a diagonal matrix the multiplication with its transpose results again in a diagonal matrix. Which gives it the same form as the eigendecomposition. To get the matrix $\\boldsymbol{U}$ we can do something very similar.  \n$$\n\\begin{align*}\n\\boldsymbol{A}\\boldsymbol{A}^T=\\boldsymbol{U}\\Sigma\\boldsymbol{V}^T\\boldsymbol{V}\\Sigma^T\\boldsymbol{U}^T \\\\\n\\boldsymbol{A}\\boldsymbol{A}^T=\\boldsymbol{U}(\\Sigma\\Sigma^T)\\boldsymbol{U}^T\n\\end{align*}\n$$  \n```python\nimport numpy as np", "type": "Document"}
{"id": null, "metadata": {"Header 1": "SVD - Singular Value Decomposition", "path": "../pages/digitalGarden/maths/linearAlgebra/svd.mdx"}, "page_content": "A = np.array([[-5,2,3], [2, 5, 1], [-3,1,-5]])\ne_values, e_vectors = np.linalg.eigh(A.T@A) # @ is same as np.matmul\nSigma = np.diag(np.sqrt(e_values))\nV = e_vectors\nU = []\nfor i in range(0, len(e_values)):\nu_i = A@V[:,i]/np.linalg.norm(A@V[:,i])\nU.append(u_i)\nU@Sigma@V.T\n```\n<div className=\"code-output-wrapper\">\n```\narray([[-5.18214154,  1.89367286,  2.74943852],\n[ 1.95955405,  5.01675209,  1.2880268 ],\n[-2.7028794 ,  1.11633398, -5.07755598]])\n```\n</div>  \nWe can see that we lose some precision due to floating number operations but these can be fixed using the round operation.  \n```python\nnp.round(U@Sigma@V.T)\n```\n<div className=\"code-output-wrapper\">\n```\narray([[-5.,  2.,  3.],\n[ 2.,  5.,  1.],\n[-3.,  1., -5.]])\n```\n</div>  \nWe can also just use the built in svd of numpy which is more efficient and accurate.  \n```python\nU, Sigma, Vh = np.linalg.svd(A)\nU@np.diag(Sigma)@Vh\n```\n<div className=\"code-output-wrapper\">\n```\narray([[-5.,  2.,  3.],\n[ 2.,  5.,  1.],\n[-3.,  1., -5.]])\n```\n</div>  \nSolving equations using svd???? Or only because we can solve equations using moore penrose and one way to compute is with SVD.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "What is a Vector?", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "Depending on the field you are working in, a vector can mean different things. In computer science, a vector is just a list of numbers, i.e. a 1D array.  \nIn maths, a vector can be thought of as an arrow in space that have starting and ending point, also reffered to as the tail and the head of the vector. Most commonly\nvectors are denoted by a lowercase letter either in bold or with an arrow above it, e.g. $\\vec{v}$ or $\\boldsymbol{v}$. If the vector is defined by two points, $A$ and $B$,\nthen the vector is denoted as $\\vec{AB}$.  \n<Callout type=\"todo\">\nNot sure yet how I want to nicely formulate and show all this. I want to show that moving the vector around doesn't change it i.e it is only defined by it's direction and length.\nBut I also want to show ortsvektor (position vector)? How does 3blue1brown do it? Also define components\n</Callout>  \nA vector therefore has a direction and a length/magnitude.\nThis arrow can be moved around, but it's direction and length will remain the same. Vectors are easily visualized in 2D and 3D space, but can be extended to any number of dimensions.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Norms", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "A norm is a function that assigns a non-negative length or size to each vector. There are many different types of norms, but the most common ones are the $L_1$ and $L_2$ norms,\nalso known as the Manhattan and Euclidean norms respectively. The $L_p$ norm is a generalization of the $L_1$ and $L_2$ norms. We denote a vector's norm by writting it in between\ntwo vertical bars, e.g. $\\|\\boldsymbol{x}\\|$, and the subscript denotes the type of norm, e.g. $\\|\\boldsymbol{x}\\|_1$ or $\\|\\boldsymbol{x}\\|_2$ etc. If\nthe subscript is omitted, then the $L_2$ norm is assumed.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Norms", "Header 3": "Manhattan Norm", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "The Manhattan norm or $L_1$ norm is defined as the sum of the [absolute values]() of the vector's components.  \nIt is called the Manhattan norm because it can be thought of as the distance\nbetween two points along the axis of a rectangular grid, like the streets of Manhattan or any other city with a grid-like structure.  \n$$\n\\|\\boldsymbol{x}\\|_1 = |x_1| + |x_2| + \\dots + |x_n| = \\sum_{i=1}^n |x_i|\n$$  \n<Image\nsrc=\"/maths/vectorManhattenNorm.png\"\ncaption=\"No matter how we move along the roads of Manhattan, the distance between two points is always the same.\"\nwidth={400}\n/>  \n<Callout type=\"example\">\nIf $\\boldsymbol{x}$ is defined as:  \n$$\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n$$  \nthen the $L_1$ norm of $\\boldsymbol{x}$ is:  \n$$\n\\|\\boldsymbol{x}\\|_1 = |1| + |2| + |3| = 6\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Norms", "Header 3": "Euclidean Norm", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "As the name suggests, the Euclidean norm or $L_2$ norm is the distance between two points in Euclidean space, i.e. the straight line distance between two points.\nFor the 2D case, the Euclidean norm is the length of the hypotenuse of a right-angled triangle with the vector's components as the other two sides, i.e.\nthe Pythagorean theorem.  \n$$\n\\|\\boldsymbol{x}\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} = \\sqrt{\\sum_{i=1}^n x_i^2}\n$$  \n<Image\nsrc=\"/maths/vectorEuclideanNorm.png\"\ncaption=\"We can see the 2D case of the Euclidean norm and the 3D case of the Euclidean norm.\"\nwidth={400}\n/>  \n#### Schwarz Inequality  \n$$\n|\\boldsymbol{x} \\cdot \\boldsymbol{y}| \\leq \\|\\boldsymbol{x}\\|_2 \\|\\boldsymbol{y}\\|_2\n$$  \n#### Triangle Inequality  \n$$\n\\[|| \\boldsymbol{x} + \\boldsymbol{y} ||\\] \\leq \\[|| \\boldsymbol{x} ||\\] + \\[|| \\boldsymbol{y} ||\\]\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Norms", "Header 3": "P-Norm", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "The idea of the $L_p$ norm is to generalize the $L_1$ and $L_2$ norms. The $L_p$ norm is defined as:  \n$$\n\\|\\boldsymbol{x}\\|_p = \\left(|x_1|^p + |x_2|^p + \\dots + |x_n|^p\\right)^{\\frac{1}{p}} = \\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}}\n$$  \nAn arbitrary norm is rarely used in practice, most commonly the $L_1$ and $L_2$ norms are used. For some use-cases the $L_\\infty$ norm is used, which is defined as:  \n$$\n\\|\\boldsymbol{x}\\|_\\infty = \\max_i |x_i|\n$$  \nIn other words, the $L_\\infty$ norm is vector component with the largest absolute value.  \n<Callout type=\"example\">\nIf $\\boldsymbol{x}$ is defined as:\n$$\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}\n$$  \nthen the $L_4$ norm of $\\boldsymbol{x}$ is:  \n$$\n\\|\\boldsymbol{x}\\|_4 = \\left(|1|^4 + |2|^4 + |3|^4\\right)^{\\frac{1}{4}} = \\left(1 + 16 + 81\\right)^{\\frac{1}{4}} = 4\n$$  \nand the $L_\\infty$ norm of $\\boldsymbol{x}$ is:  \n$$\n\\|\\boldsymbol{x}\\|_\\infty = \\max_i |x_i| = \\max\\{1, 2, 3\\} = 3\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Operations on Vectors and Matrices", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "All the vector operations can be thought of as matrix operations, because a vector can be thought of as a matrix with one column.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Operations on Vectors and Matrices", "Header 3": "Vector Addition", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "To add two vectors together, we simply add the corresponding components of the vectors together i.e. we just add element-wise. This also means that the two vectors\nmust have the same number of components/dimensions. So we can add two vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$ together as follows:  \n$$\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} + \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix} = \\begin{bmatrix}\nx_1 + y_1 \\\\\nx_2 + y_2 \\\\\n\\vdots \\\\\nx_n + y_n\n\\end{bmatrix}\n$$  \n<Callout type=\"example\">\n$$\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix} + \\begin{bmatrix}\n4 \\\\\n5 \\\\\n6\n\\end{bmatrix} = \\begin{bmatrix}\n1 + 4 \\\\\n2 + 5 \\\\\n3 + 6\n\\end{bmatrix} = \\begin{bmatrix}\n5 \\\\\n7 \\\\\n9\n\\end{bmatrix}\n$$\n</Callout>  \nWe can also visualize vector addition nicely in 2D and 3D space. We can think of vector addition as moving the tail of one vector to the head of the other vector. The\nresulting vector is the vector that starts at the tail of the first vector and ends at the head of the second vector.  \n<Image\nsrc=\"/maths/vectorAddition.png\"\ncaption=\"Vector addition in 2D.\"\nwidth={400}\n/>  \nFrom the image above we can also see that the order in which we add the vectors does not matter, i.e. the vector addition is commutative.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Operations on Vectors and Matrices", "Header 3": "Scalar Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "We can multiply a vector by a scalar, i.e. a number, by multiplying each component of the vector by the scalar. So if we have a vector $\\boldsymbol{x}$ and a scalar $c$,\nthen we can multiply them together as follows:  \n$$\nc\\boldsymbol{x} = c \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} = \\begin{bmatrix}\ncx_1 \\\\\ncx_2 \\\\\n\\vdots \\\\\ncx_n\n\\end{bmatrix}\n$$  \n<Callout type=\"example\">\n$$\n2\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix} = \\begin{bmatrix}\n2 \\cdot 1 \\\\\n2 \\cdot 2 \\\\\n2 \\cdot 3\n\\end{bmatrix} = \\begin{bmatrix}\n2 \\\\\n4 \\\\\n6\n\\end{bmatrix}\n$$\n</Callout>  \nWe can also visualize scalar multiplication nicely in 2D and 3D space. We can think of scalar multiplication as stretching or shrinking the vector by the scalar. This is\nwhy scalar multiplication is also called vector scaling and the number is called the scalar. If the scalar is negative, then the vector will be flipped around, i.e. it will\npoint in the opposite direction.  \n<Image\nsrc=\"/maths/vectorScalarMultiplication.png\"\ncaption=\"Scalar multiplication of a vector in 2D.\"\nwidth={400}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Operations on Vectors and Matrices", "Header 3": "Subtraction", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "Subtracting two vectors is the same as adding the first vector to the negative of the second vector, i.e. multiplying the second vector by $-1$. So if we have two vectors\nwe can subtract them as follows:  \n$$\n\\boldsymbol{x} - \\boldsymbol{y} = \\boldsymbol{x} + (-\\boldsymbol{y})\n$$  \n<Callout type=\"example\">\n$$\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix} - \\begin{bmatrix}\n4 \\\\\n5 \\\\\n6\n\\end{bmatrix} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix} + \\begin{bmatrix}\n-4 \\\\\n-5 \\\\\n-6\n\\end{bmatrix} = \\begin{bmatrix}\n1 + (-4) \\\\\n2 + (-5) \\\\\n3 + (-6)\n\\end{bmatrix} = \\begin{bmatrix}\n-3 \\\\\n-3 \\\\\n-3\n\\end{bmatrix}\n$$\n</Callout>  \nWhen visualizing the subtraction of two vectors, we can think of it in two ways. The first way is to think of it as adding the negative of the second vector to the first\nvector, i.e. moving the tail of the second vector to the head of the first vector. We can also visualize it by moving the tail of the second vector to the tail of the first\nvector, and then drawing a vector from the head of the second vector to the head of the first vector.  \n<Image\nsrc=\"/maths/vectorSubtraction.png\"\ncaption=\"Vector subtraction in 2D.\"\nwidth={400}\n/>  \nI prefer the first way of visualizing vector subtraction, because it is easier to see that the order in which we subtract the vectors matters. When using the second way you\ncan easily draw the vectors in the wrong order, which will give you the wrong result (negative of the correct result).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Operations on Vectors and Matrices", "Header 3": "Inner/Dot Product", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "The dot product is a way of multiplying two vectors together to get a scalar. The dot product is also called the inner product or the scalar product because it results in a\nscalar, not to be confused with the scalar multiplication of a vector! The dot product of two vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$ is denoted as\n$\\boldsymbol{x} \\cdot \\boldsymbol{y}$ and is defined as:  \n$$\n\\boldsymbol{x} \\cdot \\boldsymbol{y} = x_1y_1 + x_2y_2 + \\dots + x_ny_n = \\sum_{i=1}^n x_iy_i\n$$  \nAs we can see it is the sum of the products of the corresponding components of the two vectors meaning that the two vectors must have the same number of components/dimensions.  \n<Callout type=\"example\">\n$$\n\\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix} \\cdot \\begin{bmatrix}\n4 \\\\\n5 \\\\\n6\n\\end{bmatrix} = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 = 4 + 10 + 18 = 32\n$$\n</Callout>  \nThe dot product is also commutative, meaning that the order in which we multiply the vectors together does not matter $\\boldsymbol{x} \\cdot \\boldsymbol{y} =\n\\boldsymbol{y} \\cdot \\boldsymbol{x}$.  \nJust as we can think of the matrix multiplication as the dot product of the rows of the first matrix and the columns of the second matrix, we can also think of the dot as\nthe matrix multiplication of a $1 \\times n$ matrix and a $n \\times 1$ matrix. So if we have two vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$, then we can think of the two\nvectors as matrices, transpose one of them and then multiply them together as follows:  \n$$\n\\boldsymbol{x} \\cdot \\boldsymbol{y} = \\boldsymbol{x}^T\\boldsymbol{y} = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n$$  \nWe can also visualize the dot product nicely in 2D and 3D space. The dot product of two vectors is the cosine of the angle between the two vectors multiplied by the length of\nthe two vectors if we place the tails at the same point. So if we have two vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$, then we can calculate the dot product as follows:  \n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Operations on Vectors and Matrices", "Header 3": "Inner/Dot Product", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "the two vectors if we place the tails at the same point. So if we have two vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$, then we can calculate the dot product as follows:  \n$$\n\\boldsymbol{x} \\cdot \\boldsymbol{y} = \\|\\boldsymbol{x}\\| \\|\\boldsymbol{y}\\| \\cos(\\theta)\n$$  \nwhere $\\theta$ is the angle between the two vectors. We can also calculate the angle between the two vectors by rewriting the equation above as follows:  \n$$\n\\theta = \\cos^{-1}\\left(\\frac{\\boldsymbol{x} \\cdot \\boldsymbol{y}}{\\|\\boldsymbol{x}\\| \\|\\boldsymbol{y}\\|}\\right)\n$$  \nWhere $\\cos^{-1}$ is the inverse cosine function, also called the arccosine function.  \n<Image\nsrc=\"/maths/vectorDotProductAngle.png\"\ncaption=\"Calculating the angle between two vectors using the dot product.\"\nwidth={400}\n/>  \n<Callout type=\"example\">\nIf $\\boldsymbol{x}$ and $\\boldsymbol{y}$ are defined as:  \n$$\n\\boldsymbol{x} = \\begin{bmatrix}\n3 \\\\\n-2\n\\end{bmatrix} \\quad \\text{and} \\quad \\boldsymbol{y} = \\begin{bmatrix}\n1 \\\\\n7\n\\end{bmatrix}\n$$  \nthen the angle between $\\boldsymbol{x}$ and $\\boldsymbol{y}$ is:  \n$$\n\\begin{align*}\n\\theta &= \\cos^{-1}\\left(\\frac{\\boldsymbol{x} \\cdot \\boldsymbol{y}}{\\|\\boldsymbol{x}\\| \\|\\boldsymbol{y}\\|}\\right) \\\\\n&= \\cos^{-1}\\left(\\frac{3 \\cdot 1 + (-2) \\cdot 7}{\\sqrt{3^2 + (-2)^2} \\sqrt{1^2 + 7^2}}\\right) \\\\\n&= \\cos^{-1}\\left(\\frac{3 - 14}{\\sqrt{9 + 4} \\sqrt{1 + 49}}\\right) \\\\\n&= \\cos^{-1}\\left(\\frac{-11}{\\sqrt{13} \\sqrt{50}}\\right) \\\\\n&= 115.6^\\circ\n\\end{align*}\n$$\n</Callout>  \n#### Orthogonal Vectors  \nWe call two vectors orthogonal if the angle between them is 90 degrees, i.e. they are perpendicular to each other. If two vectors are orthogonal, then their dot product is\nzero, because $\\cos(90) = 0$. So if we have two vectors $\\boldsymbol{x}$ and $\\boldsymbol{y}$, then we can check if they are orthogonal as follows:  \n$$\n\\boldsymbol{x} \\cdot \\boldsymbol{y} = 0\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Operations on Vectors and Matrices", "Header 3": "Matrix Vector Multiplication", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "#### Linear Combinations  \nas a matrix times a vector can relate this back to a coefficient matrix and a solution vector.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Operations on Vectors and Matrices", "Header 3": "Normalization", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "Normalizing means to bring something into some sort of normal or standard state. In the case of vectors, normalizing means to scale the vector in a way that it's length is\nequal to one. Often we denote a normalized vector by adding a hat to the vector, e.g. $\\hat{\\boldsymbol{x}}$ is the normalized vector of $\\boldsymbol{x}$. So we can\nsay if $\\|\\boldsymbol{x}\\| = 1$, then $\\boldsymbol{x}$ is normalized. From this definition we can see that to normalize a vector, we simply divide the vector by it's length,\ni.e. we divide the vector by a scalar. So if we have a vector $\\boldsymbol{x}$, then we can normalize it as follows:  \n$$\n\\hat{\\boldsymbol{x}} = \\frac{\\boldsymbol{x}}{\\|\\boldsymbol{x}\\|_2}\n$$  \nThis normalized vector will have the same direction as the original vector, but it's length will be equal to one. By eliminating the length of the vector, we can uniquely\nidentify a vector by it's direction. This is useful because we can now compare vectors based on their direction, without having to worry about their length. All these\nnormalized vectors are also called unit vectors and if they are placed at the origin in 2D they span the unit circle.  \n<Image\nsrc=\"/maths/vectorNormalization.png\"\ncaption=\"We can see that the normalized vectors all have the same length, but different directions.\"\nwidth={400}\n/>  \n#### Orthonormal Vector  \nWe can now combine the idea of orthogonal vectors and normalized vectors to get orthonormal vectors. Orthonormal vectors are vectors that are orthogonal to each other and\nhave a length of one.  \n<Image\nsrc=\"/maths/vectorOrthonormal.png\"\ncaption=\"The difference between orthogonal and orthonormal vectors.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Linear Independence", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "Two vectors are linearly independent if neither of them can be written as a linear combination of the other. In other words, two vectors are linearly independent if\nthey are not scalar multiples of each other. It is however easier to find to define and check for linear dependence. The vectors $\\boldsymbol{a}$ and $\\boldsymbol{b}$\nare linearly dependent if:  \n$$\n\\boldsymbol{a} = c\\boldsymbol{b} \\quad \\text{for some } c \\in \\mathbb{R}\n$$  \nthis can also be written as:  \n$$\n\\boldsymbol{a} - c\\boldsymbol{b} = \\boldsymbol{0}\n$$  \nwhere $\\boldsymbol{0}$ is the zero vector. This means that the vectors $\\boldsymbol{a}$ and $\\boldsymbol{b}$ are linearly dependent\nif they are collinear, i.e. they lie on the same line. The two equations above can also be used to define linear independence, we just replace the equal sign with a\nnot equal sign.  \n<Image\nsrc=\"/maths/vectorLinearDependence2D.png\"\ncaption=\"The left two vectors are linearly independent, while the right two vectors are linearly dependent.\"\nwidth={400}\n/>  \n<Callout type=\"example\">\nIf $\\boldsymbol{a}$ and $\\boldsymbol{b}$ are defined as:  \n$$\n\\boldsymbol{a} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix} \\quad \\text{and} \\quad \\boldsymbol{b} = \\begin{bmatrix}\n2 \\\\\n4 \\\\\n6\n\\end{bmatrix}\n$$  \nthen $\\boldsymbol{a}$ and $\\boldsymbol{b}$ are linearly dependent because:  \n$$\n\\boldsymbol{a} = 2\\boldsymbol{b}\n$$  \nHowever, if $\\boldsymbol{a}$ and $\\boldsymbol{b}$ are defined as:  \n$$\n\\boldsymbol{a} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix} \\quad \\text{and} \\quad \\boldsymbol{b} = \\begin{bmatrix}\n2 \\\\\n3 \\\\\n4\n\\end{bmatrix}\n$$  \nthen $\\boldsymbol{a}$ and $\\boldsymbol{b}$ are linearly independent because no scalar multiple of $\\boldsymbol{b}$ can be equal to $\\boldsymbol{a}$.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Linear Independence", "Header 3": "Linear Independence of More Than Two Vectors", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "Never used this.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Basis", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "unit vectors, basis of a vector space, standard basis, orthonormal basis  \nalso in quantum computing", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vectors", "Header 2": "Span", "path": "../pages/digitalGarden/maths/linearAlgebra/vectors.mdx"}, "page_content": "is the set of all possible linear combinations of the vectors in the set.\neither plane or line or nothing in 2d space.  \nWhat about in 3d? with 2 vectors? some plane  \ngoes back to linear independence", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Eigenvalues and Eigenvectors", "path": "../pages/digitalGarden/maths/linearAlgebra/eigenvaluesEigenvectors.mdx"}, "page_content": "Before we talk about eigenvalues and eigenvectors let us just remind ourselves that vectors can be transformed using matrices. For example we can rotate a vector using the rotation matrix:  \n$$\n\\begin{bmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta &  \\cos\\theta \\\\\n\\end{bmatrix}\\begin{bmatrix}\nx \\\\\ny \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx' \\\\\ny' \\\\\n\\end{bmatrix}\n$$  \n<Image\nsrc=\"/maths/vectorTransformationRotation2D.png\"\ncaption=\"Rotating a 2D vector by the angle theta\"\nwidth={400}\n/>  \nOr we can use a matrix to scale a vector:  \n$$\n\\begin{bmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n\\end{bmatrix}\\begin{bmatrix}\n4 \\\\\n3 \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n8 \\\\\n6 \\\\\n\\end{bmatrix}\n$$  \n<Image\nsrc=\"/maths/vectorTransformationScaling2D.png\"\ncaption=\"Scaling a 2D vector, in this case doubling its length\"\nwidth={300}\n/>  \nNow let us go back to eigenvalues and eigenvectors. An eigenvector $\\boldsymbol{v}$ of a square matrix $\\boldsymbol{A}$ is defined as a non-zero vector such that the multiplication with $\\boldsymbol{A}$ only changes the scale of the vector it does not change the direction. The scalar $\\lambda$ is called the eigenvalue.  \n$$\n\\boldsymbol{Av}=\\lambda \\boldsymbol{v}\n$$  \nBecause there would be an infinite amount of solutions we limit the magnitude of the vector to $\\parallel\\boldsymbol{v}\\parallel_2=1$.  \nLet us look at an example of how to calculate the eigenvector and eigenvalue of  \n$$\n\\boldsymbol{A}=\n\\begin{bmatrix}\n0 & 1 \\\\\n-2 & -3 \\\\\n\\end{bmatrix}\n$$  \nFor this we can rewrite the problem and solve the following equations:  \n$$\n\\begin{align*}\n\\boldsymbol{Av}=\\lambda \\boldsymbol{v} \\\\\n\\boldsymbol{Av} - \\lambda \\boldsymbol{v} = 0 \\\\\n\\boldsymbol{Av} - \\lambda \\boldsymbol{Iv} = 0\n(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} = 0\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Eigenvalues and Eigenvectors", "path": "../pages/digitalGarden/maths/linearAlgebra/eigenvaluesEigenvectors.mdx"}, "page_content": "$$\n\\begin{align*}\n\\boldsymbol{Av}=\\lambda \\boldsymbol{v} \\\\\n\\boldsymbol{Av} - \\lambda \\boldsymbol{v} = 0 \\\\\n\\boldsymbol{Av} - \\lambda \\boldsymbol{Iv} = 0\n(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} = 0\n\\end{align*}\n$$  \nFor there to be a solution where $\\boldsymbol{v}$ is non-zero then the following must be true and which then must lead to the characteristic polynomial of $\\boldsymbol{A}$. Solving the characteristic polynomial equaling 0 we can get between 0 and $n$ eigenvalues with $n$ being the number of dimensions of $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$:  \n$$\n\\begin{align*}\ndet(\\boldsymbol{A}-\\lambda\\boldsymbol{I}) &= 0 \\\\\ndet\\big(\n\\begin{bmatrix}\n0 & 1 \\\\\n-2 & -3 \\\\\n\\end{bmatrix}\n- \\begin{bmatrix}\n\\lambda & 0 \\\\\n0 & \\lambda \\\\\n\\end{bmatrix}\n\\big) &= 0 \\\\\ndet\\big(\n\\begin{bmatrix}\n-\\lambda & 1 \\\\\n-2 & -3-\\lambda \\\\\n\\end{bmatrix}\n\\big) &= \\lambda^2+3\\lambda+2=0 \\\\\n&\\lambda_1 = -1,\\,\\lambda_2 = -2\n\\end{align*}\n$$  \nNow that we have the eigenvalues all we need to do is calculate the eigenvectors corresponding to each eigenvalue.  \n$$\n\\begin{align*}\n(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} &= 0 \\\\\n\\big(\\begin{bmatrix}\n0 & 1 \\\\\n-2 & -3 \\\\\n\\end{bmatrix}\n- \\begin{bmatrix}\n-1 & 0 \\\\\n0 & -1 \\\\\n\\end{bmatrix} \\big)\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\n\\end{bmatrix} &= 0 \\\\\n\\begin{bmatrix}\n1 & 1 \\\\\n-2 & -2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nv_1 \\\\\nv_2 \\\\\n\\end{bmatrix} &= 0 \\\\\n\\begin{bmatrix}\nv_1 + v_2 \\\\\n-2v_1 -2v_2 \\\\\n\\end{bmatrix} &= 0 \\\\\n&\\Rightarrow v_1 = -v_2\n\\end{align*}\n$$  \nSo we know $v_1 = -v_2$ since we constrict ourselves to vectors with a magnitude of 1 so $\\sqrt{v_1^2 + (-v_1)^2}=1$ we get for eigenvalue $\\lambda_1=-1$ the eigenvector  \n$$\n\\boldsymbol{v}=\n\\begin{bmatrix}\n0.707107 \\\\\n-0.707107 \\\\\n\\end{bmatrix}\n$$  \nWe can also calculate this using the following numpy code:  \n```python\nimport numpy as np", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Eigenvalues and Eigenvectors", "path": "../pages/digitalGarden/maths/linearAlgebra/eigenvaluesEigenvectors.mdx"}, "page_content": "A = np.array([[0, 1], [-2, -3]])\ne_values, e_vectors = np.linalg.eig(A)\nprint(f\"Eigenvalues: {e_values}\")\nprint(f\"Eigenvectors: {e_vectors}\")\n```\n<div className=\"code-output-wrapper\">\n```\nEigenvalues: [-1. -2.]\nEigenvectors: [[ 0.70710678 -0.4472136 ]\n[-0.70710678  0.89442719]]\n```\n</div>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Eigenvalues and Eigenvectors", "Header 2": "Properties", "path": "../pages/digitalGarden/maths/linearAlgebra/eigenvaluesEigenvectors.mdx"}, "page_content": "We can use the eigenvalues and eigenvectors of the matrix $\\boldsymbol{A}$ to find out a lot about it  \n- The trace of $\\boldsymbol{A}$ is the sum of its eigenvalues $tr(\\boldsymbol{A})=\\sum_{i=1}^{n}{\\lambda_i}$.\n- The determinant of $\\boldsymbol{A}$ is the product of its eigenvalues $det(\\boldsymbol{A})=\\prod_{i=1}^{n}{\\lambda_i}$.\n- The rank of $\\boldsymbol{A}$ is amount of non-zero eigenvalues.  \n```python\nprint(f\"Trace: {np.trace(A)}\")\nprint(f\"Determinant: {np.linalg.det(A)}\")\nprint(f\"Rank: {np.linalg.matrix_rank(A)}\")\n```\n<div className=\"code-output-wrapper\">\n```\nTrace: -3\nDeterminant: 2.0\nRank: 2\n```\n</div>  \nIf $\\boldsymbol{A}$ is a diagonal matrix then the eigenvalues are just the diagonal elements.  \n```python\nD = np.diag([1, 2, 3])\ne_values, e_vectors = np.linalg.eig(D)\nprint(f\"Eigenvalues: {e_values}\")\nprint(f\"Eigenvectors: {e_vectors}\")\n```\n<div className=\"code-output-wrapper\">\n```\nEigenvalues: [1. 2. 3.]\nEigenvectors: [[1. 0. 0.]\n[0. 1. 0.]\n[0. 0. 1.]]\n```\n</div>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Eigenvalues and Eigenvectors", "Header 2": "Trick for 2 by 2 Matrices", "path": "../pages/digitalGarden/maths/linearAlgebra/eigenvaluesEigenvectors.mdx"}, "page_content": "As presented in this [video by 3Blue1Brown](https://www.youtube.com/watch?v=e50Bj7jn9IQ) there is a cool formula that can be used to calculate the eigenvalues of a $2 \\times 2$ matrix such as $\\boldsymbol{A}=\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}$. It rests upon two properties that have already been mentioned above:  \n- The trace of $\\boldsymbol{A}$ is the sum of its eigenvalues $tr(\\boldsymbol{A})=\\sum_{i=1}^{n}{\\lambda_i}$. So in other words $a + d = \\lambda_1 + \\lambda_2$. We can also reform this to get the mean value of the two eigenvalues: $\\frac{1}{2}tr(\\boldsymbol{A})=\\frac{a+d}{2}=\\frac{\\lambda_1 + \\lambda_2}{2}=m$\n- The determinant of $\\boldsymbol{A}$ is the product of its eigenvalues $det(\\boldsymbol{A})=\\prod_{i=1}^{n}{\\lambda_i}$. So in other words $ad - bc = \\lambda_1 \\cdot \\lambda_2 = p$.  \n$$\n\\lambda_1, \\lambda_2 = m \\pm \\sqrt{m^2 - p}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Eigenvalues and Eigenvectors", "Header 2": "Eigendecomposition", "path": "../pages/digitalGarden/maths/linearAlgebra/eigenvaluesEigenvectors.mdx"}, "page_content": "The eigendecomposition is a way to split up **square** matrices into 3 matrices which can be useful in many applications. Eigendecomposition can be pretty easily derived from the above since it lead to the following equations:  \n$$\n\\begin{align*}\n\\boldsymbol{A}= \\begin{bmatrix}5 & 2 & 0\\\\ 2 & 5 & 0\\\\ 4 & -1 & 4\\end{bmatrix} \\\\\n\\boldsymbol{A}\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix} = 7 \\cdot \\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix} \\\\\n\\boldsymbol{A}\\begin{bmatrix}0\\\\ 0\\\\ 1\\end{bmatrix} = 4 \\cdot \\begin{bmatrix}0\\\\ 0\\\\ 1\\end{bmatrix} \\\\\n\\boldsymbol{A}\\begin{bmatrix}-1\\\\ 1\\\\ 5\\end{bmatrix} = 3 \\cdot \\begin{bmatrix}-1\\\\ 1\\\\ 5\\end{bmatrix}\n\\end{align*}\n$$  \nInstead of holding this information in three separate equations we can combine them to one equation using matrices. We combine the eigenvectors to a matrix where each column is a eigenvector and we create a diagonal matrix with the eigenvalues (by convention in order of small to large):  \n$$\n\\begin{align*}\n\\boldsymbol{A}\\begin{bmatrix}\n1 & 0 & -1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 5\n\\end{bmatrix}\n= \\begin{bmatrix}\n1 & 0 & -1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\n7 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 3\n\\end{bmatrix}\n\\end{align*} \\\\\n\\boldsymbol{AX}=\\boldsymbol{X}\\Lambda \\\\\n\\boldsymbol{AXX}^{-1}=\\boldsymbol{X}\\Lambda\\boldsymbol{X}^{-1} \\\\\n\\boldsymbol{A}=\\boldsymbol{X}\\Lambda\\boldsymbol{X}^{-1}\n$$  \nIf $\\boldsymbol{A}$ is a symmetric matrix then $\\boldsymbol{Q}$ is guaranteed to be an orthogonal matrix because it is the eigenvectors of $\\boldsymbol{A}$ concatenated. Because $\\boldsymbol{Q}$ is orthogonal $\\boldsymbol{Q}^{-1} = \\boldsymbol{Q}^T$ which leads to the formula being simplified to  \n$$\n\\boldsymbol{A}=\\boldsymbol{X}\\Lambda\\boldsymbol{X}^T\n$$  \n```python\nA = np.array([[5, 2, 0], [2, 5, 0], [4, -1, 4]])\nA\n```\n<div className=\"code-output-wrapper\">\n```\narray([[ 5,  2,  0],\n[ 2,  5,  0],\n[ 4, -1,  4]])\n```\n</div>  \n```python\nX = np.array([[1, 0, -1], [1, 0, 1], [1, 1, 5]])\nLambda = np.diag([7, 4, 3])\ninverse = np.linalg.inv(X)\nnp.matmul(np.matmul(X, Lambda), inverse)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Eigenvalues and Eigenvectors", "Header 2": "Eigendecomposition", "path": "../pages/digitalGarden/maths/linearAlgebra/eigenvaluesEigenvectors.mdx"}, "page_content": "```\narray([[ 5,  2,  0],\n[ 2,  5,  0],\n[ 4, -1,  4]])\n```\n</div>  \n```python\nX = np.array([[1, 0, -1], [1, 0, 1], [1, 1, 5]])\nLambda = np.diag([7, 4, 3])\ninverse = np.linalg.inv(X)\nnp.matmul(np.matmul(X, Lambda), inverse)\n```\n<div className=\"code-output-wrapper\">\n```\narray([[ 5.,  2.,  0.],\n[ 2.,  5.,  0.],\n[ 4., -1.,  4.]])\n```\n</div>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vector Spaces", "path": "../pages/digitalGarden/maths/linearAlgebra/vectorSpaces.mdx"}, "page_content": "a vector space (also called a linear space) is a set whose elements, often called vectors, can be added together and multiplied (\"scaled\") by numbers called scalars.\nMore generally scalars can be elements of a field, but in this case we will only consider real numbers i.e\na real vector space rather then if we were to consider complex numbers a complex vector space.  \ntakes binary operation for vector addition and a binary function for scalar multiplication.  \nMust fullfill the following properties:  \nAn equivalent definition of a vector space can be given, which is much more concise but less elementary:\nthe first four axioms (related to vector addition) say that a vector space is an abelian group under\naddition, and the four remaining axioms (related to the scalar multiplication) say that this operation defines a\nring homomorphism from the field F into the endomorphism ring of this group.  \nSubtraction of two vectors can be defined as the addition of the first vector with the additive inverse of the second vector.  \nEven more concisely, a vector space is a module over a field.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Vector Spaces", "Header 2": "Row and Column Space", "path": "../pages/digitalGarden/maths/linearAlgebra/vectorSpaces.mdx"}, "page_content": "<Callout type=\"todo\">\nRow and column space probably does not belong here and is not completely clear and finished.\n</Callout>  \nIn some cases it is useful to think of a matrix as a collection of column vectors. So a matrix with $n$ columns can be thought of as $n$ column vectors concatenated together.\nThis is also called the column space of a matrix.  \n<Image src=\"/maths/matrixColumnSpace.png\"\ncaption=\"A matrix as a collection of column vectors, i.e. a column space.\"\nwidth={300}\n/>  \nAnother way is to think of a matrix as a collection of row vectors. So a matrix with $m$ rows can be thought of as $m$ row vectors stacked on top of each other. This is also\ncalled the row space of a matrix.  \n<Image src=\"/maths/matrixRowSpace.png\"\ncaption=\"A matrix as a collection of row vectors, i.e. a row space.\"\nwidth={300}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Broadcasting", "path": "../pages/digitalGarden/maths/linearAlgebra/broadcasting.mdx"}, "page_content": "Broadcasting is something special that computer scientists make use of when working with tensors such as scalars, vectors and matrices. It's very useful for computer scientists but it is not really mathematical. It allows libraries like numpy to perform arithmetic operations (element-wise addition or multiplication, also known as [hadamard product](./hadamardProduct.md)) although the arrays have different shapes. If the arrays meet certain constraints then the smaller array is “broadcast” across the larger array so that they have compatible shapes to perform the operation. Let's look at some simple examples of how [broadcasting in numpy](https://numpy.org/doc/stable/user/basics.broadcasting.html) works. We might often want to perform the element-wise multiplication between two different arrays, in this case, vectors of the same shape this works fine.  \n```python\nimport numpy as np\n\na = np.array([1.0, 2.0, 3.0])\nb = np.array([2.0, 2.0, 2.0])\na * b\n```\n<div className=\"code-output-wrapper\">\n```\narray([2., 4., 6.])\n```\n</div>  \nBut what about multiplying a vector with a scalar? This is defined mathematically but in numpy these two arrays do not have the same shape yet it still works.  \n```python\nb = 2\na * b\n```\n<div className=\"code-output-wrapper\">\n```\narray([2., 4., 6.])\n```\n</div>  \nThe result is the same as where `b` was an array consisting of only the value 2. So in this case numpy transformed the integer 2 to `np.array([2])` to perform the operation but it also performed broadcasting which we can think of as the array containing the single value 2 being stretched to the same shape as `a`. The new elements in `b` are just copies of the original scalar.  \n<Image\nsrc=\"/maths/broadcasting.png\"\ncaption='A visualization of the \"stretching\" when broadcasting an array.'\nwidth={500}\n/>  \n<Callout type=\"info\">\nThe stretching analogy is only conceptual. NumPy is smart enough to use the original scalar value without actually making copies so that broadcasting does not waste memory.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Broadcasting", "Header 2": "Conditions for Broadcasting", "path": "../pages/digitalGarden/maths/linearAlgebra/broadcasting.mdx"}, "page_content": "When checking to see if two arrays have the same shape or have compatible numpy starts with the rightmost dimension and works its way left comparing them element-wise. For two dimensions to be compatible and therefore be broadcasted the following conditions need to be met for the pairs of dimensions:  \n1. The dimensions are equal.\n2. One of the dimensions is 1 and the other is 1 (first condition) or more.  \n<Callout type=\"info\">\nArrays do not need to have the same number of dimensions. For example if you have an RGB image so a 256x256x3 array of intensity values, and you want to scale each color (red, green, blue) in the image by a different value, you can multiply the image by a one-dimensional array with 3 values.  \n```\nImage  (3d array): 256 x 256 x 3\nScale  (1d array):             3\nResult (3d array): 256 x 256 x 3\n```\n</Callout>  \n<Callout type=\"example\">  \nThe following more complex example can still be broadcast.  \n```\nA      (4d array):  8 x 1 x 6 x 1\nB      (3d array):      7 x 1 x 5\nResult (4d array):  8 x 7 x 6 x 5\n```  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Broadcasting", "Header 2": "Outer Product Using Broadcasting", "path": "../pages/digitalGarden/maths/linearAlgebra/broadcasting.mdx"}, "page_content": "Broadcasting provides a convenient way of taking the outer product of two vectors.  \n<Image\nsrc=\"/maths/broadcastingOuterProductVector.png\"\ncaption=\"The efficient calculation of the outer product of two vectors with broadcasting.\"\nwidth={550}\n/>  \n```python\na = np.array([0.0, 10.0, 20.0, 30.0]).reshape((4,1))\nb = np.array([1.0, 2.0, 3.0]).reshape((1,3))\na*b\n```\n<div className=\"code-output-wrapper\">\n```\narray([[ 0.,  0.,  0.],\n[10., 20., 30.],\n[20., 40., 60.],\n[30., 60., 90.]])\n```\n</div>  \nHowever, it is not limited to the outer product of two vectors, it can be used to compute any outer product of two tensors.  \n<Image\nsrc=\"/maths/broadcastingOuterProduct3Vectors.png\"\ncaption=\"Using three vectors to calculate a 3D tensor.\"\nwidth={800}\n/>  \n```python\nc = np.array([1.0, 2.0, 3.0])\na = a.reshape((4,1,1))\nb = b.reshape((1,3,1))\nc= c.reshape(1,1,3)\na*b*c\n\n```\n<div className=\"code-output-wrapper\">\n```\narray([[[  0.,   0.,   0.],\n[  0.,   0.,   0.],\n[  0.,   0.,   0.]],\n\n[[ 10.,  20.,  30.],\n[ 20.,  40.,  60.],\n[ 30.,  60.,  90.]],\n\n[[ 20.,  40.,  60.],\n[ 40.,  80., 120.],\n[ 60., 120., 180.]],\n\n[[ 30.,  60.,  90.],\n[ 60., 120., 180.],\n[ 90., 180., 270.]]])\n```\n</div>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Independent Random Variables", "Header 2": "Unabhängigkeit von Zufallsvariablen", "path": "../pages/digitalGarden/maths/probabilityStatistics/independentRandomVariables.mdx"}, "page_content": "Eine unendliche Folge von Zufallsvariablen heisst stochastisch unabhängig, wenn jede endliche Teilfolge davon stochastisch unabhängig ist.\nMathematisch ausgedrückt:  \n$$\nP(X_1 \\in A_1,...X_n \\in A_n)=P(X_1 \\in A_1) \\cdot ... \\cdot P(X_n \\in A_n)\n$$  \n<Callout type=\"example\" title=\"Beispiel Abhängigkeit von Zufallsvariablen\">\nWir würfeln mit einem fairen Würfel dreimal.\nDie Zufallsvariable $X$ zählt die Anzahl an gewürfelten Einsen.\nDie Zufallsvariable $Y$ zählt die Anzahl an Vieren in den ersten 2 Würfe.  \nDann sind $X$ und $Y$ nicht stochastisch unabhängig, weil  \n$$\nP(X=3,Y=2)=0 \\neq P(X=3)\\cdot P(Y=2)\n$$\n</Callout>  \n<Callout type=\"example\" title=\"Beispiel Unabhängigkeit von Zufallsvariablen\">\nPerson A kommt zu einem zufälligen Zeitpunkt zwischen 12:00 und 12:45, Person B unabhängig davon zwischen 12:15 und 13:00 in ein Café.  \n- X: Ankunftszeit von Person A $X \\sim U[0,45]$\n- Y: Ankunftszeit von Person B $Y \\sim U[15,60]$  \n$$\nP(X\\leq 30,Y\\leq 30)=\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Independent Random Variables", "Header 2": "Unabhängigkeit von Zufallsvariablen", "Header 3": "Faltung", "path": "../pages/digitalGarden/maths/probabilityStatistics/independentRandomVariables.mdx"}, "page_content": "#### Diskrete Zufallsvariablen  \nEs seien $X, Y$ unabhängige diskrete Zufallsvariablen und Zähldichten $f_X, f_Y$ Dann hat die Summe $X+Y$ die Zähldichte  \n$$\nf_{X+Y}(z)=\\sum_{x_i \\in X}{f_X(x_i) \\cdot f_y(z-x_i)}\n$$  \nDaraus können wir auch folgendes ableiten  \n$$\nX\\sim Poi(\\lambda_1), Y \\sim Poi(\\lambda_2) = X+Y \\sim Poi(\\lambda_1 + \\lambda_2)\n$$  \n$$\nX\\sim Bin(n_1,p), Y \\sim Bin(n_2,p) = X+Y \\sim Bin(n_1+n_2,p)\n$$  \n#### Stetige Zufallsvariablen  \n$$\nf_{X+Y}(z)=\\int_{x_i=-\\infty}^{\\infty}{f_X(x_i) \\cdot f_y(z-x_i) dx}\n$$  \n##### Additionstheorem Normalverteilung  \nEs seien $X_1, X_2,...,X_n$ unabhängige, normal verteilte Zufallsvariablen eines Zufalls-experimentes mit Erwartungswerten\n$\\mu_i$ und Standardabweichungen $\\sigma_i$, mit $a_i, a_2,...,a_n \\in \\mathbb{R}$ dann ist  \n$$\nY=a_i X_1 + a_2 X_2 +...+a_n X_n\n$$  \nmit dem Erwartungswert $a_i \\mu + a_2 \\mu +...+a_n \\mu$ und die Varianz  $a_i^2 \\sigma^2 + a_2^2 \\sigma^2 +...+a_n^2 \\sigma^2$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Oftmals wird dieses Thema auch als Warteschlangentheorie bezeichnet. In der klassischen Warteschlangentheorie geht man davon aus, dass sich im System eine Grenzverteilung eingestellt hat wofür wir bestimmte Kenngrössen bestimmen.  \n1. Verteilung der Anzahl Kunden im System.\n2. Länge der Warteschlange $N_Q$.\n3. Anzahl der Kunden die gerade bearbeitet werden $N_s$.\n4. Erwartete Wartezeit eines Kunden in der Schlange $W_q$.\n5. Verweilzeit eines Kunden im System $D$ (Warten + Abarbeitung).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "Kendall-Notation", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Um Bediensysteme zu beschreiben verwenden wir die Kendall-Notation. Dabei werden die Charakterisierungen in eines bestimmte Reihenfolge geschrieben.  \n$$A|B|s|c|R$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "Kendall-Notation", "Header 3": "A - Art des Ankunftprozesses", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Hier gibt es 3 beliebte Optionen:  \n- $D$ für deterministisch, also das die Ankünfte der Kunden zu festen Zeitpunkten stattfinden.\n- $G$ für Generelle Annahmen, also das nicht bekannt ist über die Ankünfte der Kunden.\n- $M$ für Markov-Eigenschaft, also ist die Wartezeit auf den nächsten Kunden exponentialverteilt weil es das no-memory-property besitzt so wie Markow-Ketten.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "Kendall-Notation", "Header 3": "B - Art des Bedienvorgangs", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Auch hier gibt es die 3 beliebten Optionen:  \n- $D$ für deterministisch, also die Dauer zur Abarbeitung eines Kunden ist nicht zufällig.\n- $G$ für Generelle Annahmen, also das nicht bekannt ist die Dauer der Abarbeitung der Kunden.\n- $M$ für Markov-Eigenschaft, also das die Dauer der Abarbeitung eines Kunden ist exponentialverteilt.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "Kendall-Notation", "Header 3": "S - Anzahl Server", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Ist entweder eine ganze Zahl grösser gleich 1 oder $\\infty$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "Kendall-Notation", "Header 3": "C - Capacity", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Die grösse  des Warteraums welche entweder eine ganze Zahl grösser gleich 0 ist oder $\\infty$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "Kendall-Notation", "Header 3": "R - Reihenfolge der Bedienung", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Hier gibt es auch wieder 3 beliebte Optionen:  \n- **FIFO** first in first out\n- **LIFO** last in first out\n- **SIRO** service in random order", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "Kendall-Notation", "Header 3": "Spezialfälle", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Der Standardfall ist FIFO welcher oft weggelassen wird, ebenso bei $c=\\infty$.  Wenn $s=\\infty$ entfällt die Angabe von C, weil kein Warteraum benötigt wird.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "M|M|s|c-Systeme", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Wir beginnen mit der Grenzverteilung von einem M|M|1|0-System und leiten dann daraus die Formeln für M|M|s|c-Systeme.  \nWir gehen davon aus das die Wartezeit exponentialverteilt ist mit dem Parameter $\\lambda$(Ankunfts-rate) und das die Dauer zur Abarbeitung eines Kunden auch exponentialverteilt ist mit dem Parameter $\\mu$(Bedien-rate). Als Einheit nehmen wir Stunden, also kommen pro Stunde im Schnitt $\\lambda$ Kunden.  \nWir haben einen Server und keinen platz im Warteraum. Das heisst, dass das System sich in zwei verschiedene Zustände befinden kann (1: Kund wird bearbeitet oder 0: Kein Kunde wird bearbeitet). Wird ein Kunde gerade bearbeitet, so werden weitere Kunden abgewiesen.  \nWir wählen dann ein Zeitintervall $h$ in Stunden, welche so klein ist (limes gegen 0), dass in diesem Zeitintervall höchstens ein Kunde ankommt und höchstens ein Kunde abgearbeitet wird. Da im Zeitintervall $h$ also entweder ein Kunde kommt oder nicht muss die Wahrscheinlichkeit für das Ankommen eines Kunden $\\lambda \\cdot h$ sein damit immer noch $\\lambda$ Kunden pro Stunde kommen. Analog ist die Wahrscheinlichkeit, dafür dass ein Kunde welcher gerade  bearbeitet wird fertiggestellt wird$\\mu \\cdot h$. Daraus erhalten wir dann  \n![bediensystemeGraph](/maths/bediensystemeGraph.png)  \nDaraus erhalten wir dann die folgende Formeln für die Elemente der Grenzverteilung $\\pi^*=(p_0,p_1)$  \n$$p_0=\\frac{(1-\\lambda h)\\cdot \\mu}{(1-\\lambda h)\\cdot \\mu + \\lambda} \\text{ mit } {h\\to 0} = \\frac{\\mu}{\\mu + \\lambda}$$  \n$$p_1=1-p_0=\\frac{\\lambda}{\\mu + \\lambda}$$  \nDie Wahrscheinlichkeit, dass der Server belegt ist, beträgt also $\\frac{\\lambda}{\\lambda + \\mu}$  \nIn einem M|M|s|c-System sieht das dann so aus  \n![bediensystemeFormeln](/maths/bediensystemeFormeln.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "M|M|s|c-Systeme", "Header 3": "Kennzahlen", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "#### Erwartete Warteschlangenlänge  \nFür die Erwartete Warteschlangenlänge wobei die Warteschlange die Länge $j-s$ hat, wenn $j$ Kunden im System sind und dafür beträgt die Wahrscheinlichkeit $p_j$ ergibt sich:  \n$$E(N_Q)=\\sum_{j=s+1}^{s+c}{(j-s)p_j}$$  \nDies lässt sich dann umformulieren zu  \n$$\nE(N_Q)=\\begin{cases}\n\\frac{p_0\\cdot(\\frac{\\lambda}{\\mu})^s \\cdot \\frac{\\lambda}{s\\mu}\\cdot (1+c)\\cdot c}{2\\cdot s!} \\quad & if \\lambda=s\\mu\\\\\n\\frac{p_0\\cdot(\\frac{\\lambda}{\\mu})^s \\cdot \\frac{\\lambda}{s\\mu}[1-(\\frac{\\lambda}{s\\mu})^{c+1} - (1-\\frac{\\lambda}{s\\mu})\\cdot(c+1)\\cdot(\\frac{\\lambda}{s\\mu})^c]}{s!\\cdot (1-\\frac{\\lambda}{s\\mu})^2} \\quad & if \\lambda \\neq s\\mu\n\\end{cases}\n$$  \n#### Erwartete bediente Kunden  \nWenn $0\\leq j < s$ Kunden im System sind mit Wahrscheinlichkeit $p_j$, werden $j$ Kunden bedient. Wenn mindestens $s$ Kunden im System sind mit der Wahrscheinlichkeit $\\sum_{j=s}^{s+c}{p_j}$ werden $s$ Kunden bedient.  \nVereinfachen kann man es zu  \n$$E(N_s)=\\frac{\\lambda}{\\mu}\\cdot (1-p_{s+c})$$  \n#### Erwartete Wartezeit  \nAus der Formel von Little, welche besagt, dass die erwartete Anzahl von Kunden in einem System gleich dem Produkt ihrere durchschnittlichen Ankunfts-rate und ihre erwartete Verweildauer im System ist.  \n$$E(W_Q)=\\frac{E(N_Q)}{\\lambda \\cdot (1-p_{s+c})}$$  \n#### Erwartete Verweilzeit  \nFür die erwartete Verwilzeit eines Kunden im System ergibt sich  \n$$E(D)=E(W_Q)+\\frac{1}{\\mu}$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Queueing theory", "Header 2": "M|M|s|c-Systeme", "Header 3": "Kennzahlen mit unendlich Warteraum", "path": "../pages/digitalGarden/maths/probabilityStatistics/queueingTheory.mdx"}, "page_content": "Wenn $\\lambda \\geq s\\mu$ existiert keine Gleichverteilung weil mehr Anfragen rein kommen also von den Servern bearbeitet werden können und somit geht die Wartezeit gegen unendlich. Sonst gibt es die folgende Grenzverteilung  \n![bediensystemeFormelnUnendlich](/maths/bediensystemeFormelnUnendlich.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Die Kombinatorik ist ein Bereich der Mathematik, der sich mit der Anzahl der Möglichkeiten befasst, wie man Objekte auf\nverschiedene Arten auswählen, ordnen oder gruppieren kann.  \nZum Beispiel, wenn man aus einer Gruppe von 5 Personen 2 auswählt, gibt es insgesamt 10 Möglichkeiten, dies zu tun.  \nDie Kombinatorik hilft uns, die Anzahl der möglichen Ergebnisse zu berechnen, während die Wahrscheinlichkeitsrechnung\nuns sagt, wie wahrscheinlich es ist, dass bestimmte Ergebnisse eintreten. Wenn man die Anzahl der möglichen Ergebnisse\nkennt, kann man die Wahrscheinlichkeit eines bestimmten Ergebnisses berechnen.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Urnenmodel", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Das Urnenmodell ist eine verbreitete Veranschaulichung, um viele Probleme der Wahrscheinlickeitsrechnung zu lösen.\nDafür stellen wir uns vor wir haben eine Urne mit $n$ **verschiedene** Kugeln, die sich z.B. in ihrer Farbe oder einer\nBeschriftung unterscheiden und ziehen dann davon $k$ Kugeln. Dabei wird noch unterschieden, ob wir nach dem Ziehen einer\nKugel, die Kugel wieder zurücklegen oder nicht.  \nOftmals wird auch das zufällige Ziehen der Kugeln eine **Stichprobe** genannt.  \n![urnenModell](/maths/urnenModell.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Permutation", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Jede mögliche **Anordnung** von $n$ Kugeln heisst eine **Permutation** der $n$ Kugeln. Die Anzahl der Permutationen\nhängt noch davon ab, ob alle Kugeln verschieden sind (Ohne Wiederholung), oder ob es gewisse Kugeln mehrmals hat\n(Mit Wiederholung). Diese Unterscheidung macht man, weil die Permutationen nicht verschieden sind, wenn man die Kugeln\ndie gleich sind vertauscht.  \n![permutationMitWiederholung](/maths/permutationMitWiederholung.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Permutation", "Header 3": "Ohne Wiederholung", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Sind alle Kugeln verschieden, also gibt es keine Wiederholungen, dann ist die Anzahl der verschiedenen möglichen\nPermutationen, und somit auch Anordnungen  \n$$\nPer(n)=n!\n$$  \nWir ziehen also aus einer Urne mit $n$ Kugeln $n$-mal eine Kugel daraus. Wir können uns also vorstellen das wir für den\nersten Zug $n$ mögliche Kugeln, weil sich noch alle in der Urne befinden. Danach für den zweiten Zug gibt es nur noch\n$n-1$ mögliche Kugeln, die wir aus der Urne ziehen können, weil wir eine schone herausgenommen haben. Das geht weiter so\nbis keine Kugel mehr in der Urne sind. Am Schluss wird dann alles multipliziert was dann\n$n \\cdot (n-1) \\cdot (n-2)\\cdot ... \\cdot 2 \\cdot 1 = n!$ ergibt.  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/permutation-ohne-wiederholung-1071).  \n<Callout type=\"example\" title=\"Beispiel Permutation ohne Wiederholung\">\nAuf einem Regal sollen 3 verschiedene Bücher, $b_1, b_2, b_3$ angeordnet werden.  \nEs gibt dann $3!=6$ verschiedene Anordnungen.\n$(b_1,b_2,b_3),(b_1,b_3,b_2),(b_2,b_1,b_3),(b_2,b_3,b_1),(b_3,b_1,b_2),(b_3,b_2,b_1)$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Permutation", "Header 3": "Mit Wiederholung", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Wenn sich unter den Kugeln $n_1,n_2,...,n_k$ gleiche Kugeln befinden, also es Wiederholungen hat, so ist die Anzahl der\nverschiedenen Anordnungsmöglichkeiten  \n$$\nPer(n;n_1,n_2,...,n_k) = \\frac{n!}{n_1!n_2!...n_k!}\n$$  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/permutation-mit-wiederholung-1070).  \n<Callout type=\"example\" title=\"Beispiel Permutation mit Wiederholung\">\nIn einer Urne befinden sich 6 Kugeln, darunter sind 3 weisse, 2 graue und 1 schwarz. Die gleichfarbigen Kugeln sind\nnicht von einander unterscheidbar.  \nEs gibt dann $\\frac{6!}{3!2!1!} = \\frac{6!}{3!2!} = 60$ verschiedene Anordnungsmöglichkeiten.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Kombination", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Bei der **Kombination** werden nacheinander $k$ Kugeln aus einer Urne mit $n$ Kugeln gezogen. Dabei schauen wir nicht\nauf die Reihenfolge, in der wir die Kugeln ziehen, dass heisst, wenn wir 2 Kugeln ziehen und wir zuerst eine Schwarze und\ndann eine Weisse ziehen zählen wir als das gleiche Resultate wie wenn wir zuerst eine Weisse und dann eine Schwarze\ngezogen hätten.  \nHier unterscheiden wir auch wieder zwischen 2 Fälle, ob wir nach dem Ziehen die Kugel wieder in die Urne zurücklegen\noder nicht.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Kombination", "Header 3": "Ohne Zurücklegen", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Hier lautet also die genaue Fragestellung \"Auf wie vielen verschiedenen Arten können wir $k$ Kugeln aus einer Urne mit $n$\nverschiedenen Kugeln ziehen, ohne sie nach dem Ziehen zurückzulegen und ohne die Reihenfolge in der wir die Kugeln\nziehen, zu beachten.\"  \nDieses Problem lässt sich ziemlich gut zu einer Permutation umwandeln. Wir können uns vorstellen, dass wir jede gezogene\nKugel als 1 markieren und die anderen als 0. So bekommen wir zum Schluss eine Binärzahl mit $k$ mal eine 0 und $n-k$\nmal eine 1.\nWir können uns nun die Frage stellen, wie viele verschiedene Anordnungsmöglichkeiten gibt es für die $n$ Zahlen mit\n$k$ und $n-k$ gleiche Zahlen.  \n$$\nC(n;k) = Per(n;k,(n-k)) = \\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$  \nWas genau dem Binomialkoeffizienten entspricht.  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/ziehen-ohne-zuruecklegen-1077).  \n<Callout type=\"example\" title=\"Beispiel Kombination ohne zurücklegen\">\nIm Lotto gibt es 49 Zahlen, davon werden 6 ohne wiederholung gezogen und die Reihenfolge der Zahlen wird nicht\nbeachtet.  \nSo gibt es $\\binom{49}{6}=13'983'816$ verschiedene Kombinationen\n</Callout>  \n<Callout type=\"example\" title=\"Enigma Beispiel Kombination ohne zurücklegen\">\nUm eine Enigma maschine zu betätigen müssen 3 Rotoren von 5 ausgewählt werden. Das Militär hat sogar 8 Rotoren zur\nAuswahl.  \n$\\binom{5}{3}=10$ verschiedene Kombinationen  \n$\\binom{8}{3}=56$ verschiedene Kombinationen  \nWie man sieht Steigt die Zahl drastisch wenn man mehr Rotoren zur Auswahl hat.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Kombination", "Header 3": "Mit Zurücklegen", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Wenn wir nach dem Ziehen die Kugeln wieder zurücklegen, dann kann es sein, dass eine Kugel mehrmals verwendet wird.\nDabei kommen wir auf  \n$$\nC_w(n;k) = \\frac{(n+k-1)!}{k!(n-k)!} = \\binom{n+k-1}{k}\n$$  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/ziehen-mit-zurucklegen-ohne-reihenfolge-1074).  \n<Callout type=\"example\" title=\"Beispiel Kombination mit zurücklegen\">\nWie viele Kombinationsmöglichkeiten gibt es beim dreimaligen Würfeln?  \nVergleicht man die drei Würfe mit der Anzahl der zu ziehenden Kugeln $k$ und die sechs möglichen Ergebnisse, nämlich\ndie\nWürfelaugen 1 bis 6, mit der Gesamtzahl der Kugeln $n$, erhält man folgende Anzahl möglicher Ergebnisse:  \n$$\n\\binom{6 + 3 - 1}{3}=\\binom{8}{3}=56\n$$\n</Callout>  \n<Callout type=\"example\" title=\"Gummibärchen-Orakel Beispiel Kombination mit zurücklegen\">\nBeim sogenannten Gummibärchen-Orakel haben wir eine Tüte mit Gummibärchen. Wir wissen nicht die Anzahl der\nGummibärchen\naber das es sie in 5 verschiedene Farben gibt. Wir nehmen aus der Tüte 5 Gummibärchen. Die Frage ist demnach\nwie viele Farbkombinationen kann man ziehen.  \n$$\n\\binom{5 + 5 - 1}{5}=\\binom{9}{5}=126\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Variation", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Bei der **Variation** haben wir genau die gleichen Überlegungen wie bei der Kombination, nur berücksichtigen wir jetzt\ndie Reihenfolge, in der wir die Kugeln ziehen.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Variation", "Header 3": "Ohne Zurücklegen", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Da wir $k$-mal aus einer Urne mit $n$ Kugeln ziehen und sie nicht zurücklegen haben wir eigentlich eine Kombination.\nNur berücksichtigen wir jetzt die Reihenfolge. Also kommt die Frage noch auf wie viele Arten können wit $k$ Kugeln\nanordnen, eine Permutation mit $k!$  verschiedene Anordnungen. Somit kommen wir auf  \n$$\nV(n;k) = k! \\cdot C(n;k) = k! \\cdot \\frac{n!}{k!(n-k)!} = \\frac{n!}{(n-k)!}\n$$  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/ziehen-ohne-zurucklegen-mit-reihenfolge-1073).  \n<Callout type=\"example\" title=\"Beispiel Variation ohne zurücklegen\">\nBeim Pferdewetten muss in der sogenannten \"Dreierwette\" die Reihenfolge der ersten 3 Pferde die ins Ziel laufen\nkorrekt\nangegeben werden. Die Frage ist nun wie viele Dreierwetten gibt es wenn das Rennen 10 Pferde hat.  \nEs gibt also $\\frac{10!}{(10 - 3)!} = \\frac{10!}{7!} = 8 \\cdot 9 \\cdot 10 = 720$ verschiedene Dreierwetten.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Variation", "Header 3": "Mit Zurücklegen", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "Da wir $k$-mal ziehen können wir uns vorstellen, dass wir $k$ Stellen haben und weil wir nach jeder Ziehung die Kugel\nwieder in die Urne legen haben wie bei jeder Ziehung $n$ mögliche Kugeln, die wir ziehen könnten. Daraus lässt sich folgen  \n$$\n{V_w(n;k)} = {n \\cdot n ... \\cdot n = n^k}\n$$  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/ziehen-mit-zurucklegen-mit-reihenfolge-1072).  \n<Callout type=\"example\" title=\"Beispiel Variation mit zurücklegen\">\nBei einem Zahlenschloss hat es vier Ringe die je Zehn Ziffern haben. So gibt es $10^4=10000$ verschiedene\nVariationen, die Zahlen 0000 bis 9999.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Combinatorics", "Header 2": "Zusammenfassung", "path": "../pages/digitalGarden/maths/probabilityStatistics/combinatorics.mdx"}, "page_content": "![kombinatorikUebersicht](/maths/kombinatorikUebersicht.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Zufallsexperiment", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Ein **Zufallsexperiment** ist ein Experiment, welches beliebig oft wiederholt werden kann und zu einem oder mehrere\nErgebnisse führt, welche sich gegenseitig ausschliessen. Beim Durchführen eines Zufallsexperiments lässt sich ein Ergebnis\nnicht voraussagen, sondern ist zufallsbedingt.  \nEin paar einfache Beispiele für ein Zufallsexperiment wären eine Münze oder einen Würfel zu werfen. Genau so ist das\nZiehen einer Kugel aus einer Urne zufallsbedingt, sofern man nicht in die Urne schauen kann und eine Kugel auswählt.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Elementarereignis", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Die möglichen sich aber gegenseitig ausschliessende Ergebnisse heissen **Elementarereignisse**. Mit **gegenseitig\nausschliessend** heisst, dass sie nicht gleichzeitig passieren können z.B. kann eine Münze nicht gleichzeitig auf Kopf\nund Zahl landen. Elementarereignisse werden beschreiben mit  \n$$\n\\omega_1,\\omega_2,\\omega_3,...\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Ergebnismenge", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Die **Ergebnismenge** beschreibt alle möglichen Ergebnisse, und ist somit die Menge aller Elementarereignisse und wird\ngeschrieben als $\\Omega$. Dabei wird noch zwischen **endlichen** und **abzählbar-unendliche** Ergebnismengen unterschieden.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Ergebnismenge", "Header 3": "Endliche Ergebnismenge", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Enthält nur endlich viele Elementarereignisse  \n$$\n\\Omega = \\{\\omega_1,\\omega_2,\\omega_3,...\\omega_n\\}\n$$  \n<Callout type=\"example\" title=\"Beispiel endliche Ergebnismenge\">\nBeim Wurf eines Würfels sind 6 Augenzahlen möglich somit ist $\\omega_i = i$ für $i=(1,2,...,6)$ und  \n$$\n\\Omega = \\{1,2,3,4,5,6\\}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Ergebnismenge", "Header 3": "Abzählbar-unendliche Ergebnismenge", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Enthält unendlich viele Elementarereignisse, die wir aber wie die natürlichen Zahlen durchnummerieren können.  \n<Callout type=\"example\" title=\"Beispiel abzählbar-unendliche Ergebnismenge\">\nWir werfen einen Würfel so lange bis wir zum ersten Mal eine 6 bekommen. Theoretisch kann dies unendliche lange dauern,\naber wir können zählen bei welchem Wurf wir zum ersten Mal die 6 bekommen.  \nAlso haben wir $\\omega_i = i$ für $i=(1,2,...)$ und  \n$$\n\\Omega = \\{1,2,3,...\\}\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Ereignis", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Ein **Ereignis** ist eine Zusammenfassung von Elementarereignisse. Anders gesagt ist ein Ereignis eine Teilmenge der Ergebnismenge $A \\subseteq \\Omega$.  \n<Callout type=\"example\" title=\"Beispiel Ereignisse beim würfeln\">\nWir haben festgelegt das beim würfeln eines sechsseitigen Würfels $\\Omega = \\{1,2,3,4,5,6\\}$ ist. Wir können nun z.B. folgende Teilmengen konstruieren.  \n- Würfeln einer geraden Zahl: $A=\\{2,4,6\\}$\n- Würfeln einer durch 3 teilbare Zahl: $B=\\{3,6\\}$\n- Würfeln einer Zahl grösser als 2: $C=\\{3,4,5,6\\}$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Ereignis", "Header 3": "Unmögliches Ereignis", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Entspricht ein Ereignis der leeren Menge $\\emptyset$ so redet man vom sogenannten **unmöglichem Ereignis**, welches nie eintreten wird.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Ereignis", "Header 3": "Sicheres Ereignis", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Enthält ein Ereignis alle Elementarereignisse der Ergebnismenge also ist $A=\\Omega$ so redet man vom sogenannten **sicherem Ereignis**,\nwelches garantiert immer eintreten wird.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Verknüpfung von Ereignissen", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Wie wir gesehen haben, sind Ereignisse eigentlich nur Mengen, dass heisst wir können auch Mengenoperationen auf sie durchführen.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Verknüpfung von Ereignissen", "Header 3": "Vereinigung", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Bei der Vereinigung von Ereignissen $A \\cup B$ kann man aussagen, dass entweder tritt $A$ oder $B$ ein oder $A$ und $B$ treten gleichzeitig ein.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Verknüpfung von Ereignissen", "Header 3": "Durchschnitt", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Der Durchschnitt der Ereignisse $A \\cap B$ besagt, dass $A$ und $B$ gleichzeitig eintreten.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Verknüpfung von Ereignissen", "Header 3": "Kompliment", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Das Kompliment zu $A$ also $A^c$ was aber auch oftmals als $\\overline{A}$ geschrieben wird, besagt, dass A nicht eintritt.  \n<Callout type=\"example\" title=\"Beispiel verknüpfung von Ereignissen beim würfeln\">\nWenn wir beim würfeln sagen, dass wir die Ereignisse \"würfeln einer geraden Zahl\" und \"würfeln einer ungeraden Zahl\" haben,\nalso $A=\\{2,4,6\\}, B=\\{1,3,5\\}.  \nWir können nun also sagen  \n- $\\overline{A}=B$\n- $\\overline{B}=A$\n- $A \\cup B = \\Omega$\n- $A \\cap B = \\emptyset$  \nWas auch alles Sin macht wenn man es sich überlegt.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "De Morgan's Laws", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "De Morgan's Laws funktioniert auch mit Ereignisse  \n$$\n\\begin{align*}\n\\overline{A \\cup B} &= \\overline{A} \\cap \\overline{B} \\\\\n\\overline{A \\cap B} &= \\overline{A} \\cup \\overline{B}\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Laplace-Experimente", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Wenn bei einem Zufallsexperiment alle Elementarereignisse die gleiche Wahrscheinlichkeit haben einzutreten, also das\nalle ereignisse **gleich möglich** sind, reden wir, von einem Laplace-Experiment. Man redet hier auch oft von einer Gleichverteilung.  \nBei einer Ergebnismenge $|\\Omega|=m$ also mit $m$ gleich möglichen Elementarereignisse redet man von einem\n**Laplace-Raum**. Dabei haben alle Elementarereignisse $\\omega_i$ die gleiche Wahrscheinlichkeit, die sogenannte **Zähldichte**.  \n$$\nP(\\{\\omega_i\\}) = p(\\omega_i)= \\frac{1}{m} \\text{ mit }i=1,2,...,m\n$$  \nDas heisst die Wahrscheinlichkeit für ein Ereignis $A$ ist definiert als  \n$$\nP(A) = \\sum_{\\omega_i \\in A}{p(\\omega_i)} = |A| \\cdot \\frac{1}{m} = \\frac{|A|}{m}\n$$  \nMan kann es auch ein wenig ausführlicher definieren als  \n$$\nP(A) = \\frac{|A|}{|\\Omega|} = \\frac{\\text{Anzahl Ergebnisse wo A eintritt}}{\\text{Anzal aller möglichen Ergebnisse}}\n$$  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/laplace-experiment-1109).  \n<Callout type=\"example\" title=\"Beispiel Laplace-Experiment würfeln\">  \nBeim Wurf eines Würfels haben alle 6 Augenzahlen die gleiche Wahrscheinlichkeit somit handelt es sich um ein\nLaplace-Experiment.  \nFür jedes Elementarereignis gilt also $p(\\omega_i) = \\frac{1}{6}$  \nFür das Ereignis \"gerade Augenzahl\" also $A = \\{2,4,6\\}$ ist die Wahrscheinlichkeit somit  \n$$\nP(A)=\\frac{3}{6} = \\frac{1}{2} = 50\\%\n$$  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Wahrscheinlichkeitsaxiome von Kolmogoroff", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Wir definieren nun den Begriff Wahrscheinlichkeit ein wenig genauer, dazu verwenden wir ein paar Axiome (Grundsätze).\nDie Wahrscheinlichkeit ist eine Funktion $P$ welches jedem Ereignis $E \\subseteq \\Omega$ eine Zahl $P(E) \\in [0,1]$ zuordnet.  \n$$\nP: 2^{\\Omega} \\mapsto [0,1]\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Wahrscheinlichkeitsaxiome von Kolmogoroff", "Header 3": "Axiom 1", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "$P(E)$ ist eine nicht-negative Zahl, die höchstens gleich 1 ist  \n$$\n0 \\leq P(E) \\leq 1\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Wahrscheinlichkeitsaxiome von Kolmogoroff", "Header 3": "Axiom 2", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Für das sichere Ereignis, $\\Omega$ gilt  \n$$\nP(\\Omega)=1\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Wahrscheinlichkeitsaxiome von Kolmogoroff", "Header 3": "Axiom 3", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Für paarweise sich gegenseitig ausschliessende Ereignisse $A_1,A_2,A_3,...$ z.B. die Elementarereignisse gilt  \n$$\nP(A_1 \\cup A_2 \\cup A_3, ...) = P(A_1)+P(A_2)+P(A_3)+ ...\n$$  \nauch der sogenannte Additionssatz für sich gegenseitig ausschliessende Ereignisse.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Folgerungen aus den Wahrscheinlichkeitsaxiomen", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Für das zu $A$ komplementäre Ereignis $\\overline{A}$ gilt  \n$$\nP(\\overline{A})= 1-P(A)\n$$  \nFür das unmögliche Ereignis, $\\emptyset$ gilt  \n$$\nP(\\emptyset)=0\n$$  \nweil es das komplementäre Ereignis zum sicheren Ereignis $\\Omega$ ist.  \nFür sich 2 gegenseitig ausschliessende Ereignisse $A$ und $B$ gilt  \n$$\nP(A \\cup B)= P(A) + P(B)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Additionssatz", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Für sich 2 gegenseitig ausschliessende Ereignisse $A$ und $B$ gilt anhand des 3. Axiom  \n$$\nP(A \\cup B)= P(A) + P(B)\n$$  \nSomit haben wir die Wahrscheinlichkeit für wenn $A$ oder $B$ eintritt, da sie nicht gleichzeitig eintreten können weil\nsie sich gegenseitig ausschliessen.  \nSind aber $A \\cup B \\neq \\emptyset$, also schliessen sie sich nicht gegenseitig aus, dann gilt der folgende allgemeine\nAdditionssatz  \n$$\nP(A \\cup B)= P(A) + P(B) - P(A \\cap B)\n$$  \nDieser kann auch für sich gegenseitig ausschliessende Ereignisse verwendet werden da dann\n$P(A \\cap B) = P(\\emptyset) = 0$ und wir somit 0 subtrahieren würden.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Bedingte Wahrscheinlichkeit", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Oftmals interessiert uns die Wahrscheinlichkeit für das Eintreten des Ereignisses $B$ unter der **Voraussetzung** oder\n**Bedingung**, dass $A$ bereits eingetreten ist.  \nWir nennen diese Wahrscheinlichkeit die **bedingte Wahrscheinlichkeit von B unter (Der Bedingung) A** und definieren sie als  \n$$\nP(B | A)= {P(A \\cap B) \\over P(A)}\n$$  \nwobei $P(A) \\neq 0$ ist.  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/bedingte-wahrscheinlichkeit-1110).  \n<Callout type=\"example\" title=\"Beispiel bedingte Wahrscheinlichkeit würfeln\">  \nWir würfeln mit 2 Würfeln und interessieren uns für die Würfe wo die Augensumme 8 ist. Die Frage die wir uns nun\nstellen\nist was die Wahrscheinlichkeit, bei so einem Wurf ist, dass beide Augenzahlen gerade sind.  \n- $A = \\text{Die Augensumme ist 8}$\n- $B = \\text{Die Augenzahlen beider Würfel sind gerade}$  \n**1. Lösungsweg ohne Formel**  \nDas Ereignis $A$ wird durch die 5 Elementarereignisse $(2,6),(3,5),(4,4),(5,3),(6,2)$ gegeben.\nDaraus sehen wir, dass bei 3 beide Zahlen gerade sind. Also haben wir  \n$$\nP(B | A) = {3 \\over 5}\n$$  \n**2. Lösungsweg mit Formel**  \nWir wissen das es 36 gleich mögliche Elementarereignisse gibt. Daraus können wir zählen, dass $P(A)= {5 \\over 36}$\nund\n$P(A \\cap B)={3 \\over 36}$.  \n$$\nP(B | A) ={{3 \\over 36} \\over {5 \\over 36}} = {3 \\over 5}\n$$  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Multiplikationssatz", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Wenn wir die Definitionsgleichung der bedingten Wahrscheinlichkeit nach $P(A \\cap B)$ auflösen erhalten wir den folgenden Multiplikationssatz  \n$$\nP(A \\cap B)=P(A) \\cdot P(B|A)\n$$  \nweil $A \\cap B = B \\cap A$ gilt daher auch  \n$$\nP(A) \\cdot P(B|A) = P(B) \\cdot P(A|B)\n$$  \n<Callout type=\"example\" title=\"Beispiel Multiplikationssatz\">  \nIn einer Urne sind 6 Kugeln, 4 weiss, 2 schwarz. Wir entnehmen der Urne 2 Kugeln nach einander Ohne zurückzulegen.\nMit\nwelcher wahrscheinlichkeit sind beide weiss?  \n- $A=$ erste Kugel ist weiss\n- $B=$ zweite Kugel ist weiss  \n$$\nP(A)= {4\\over 6} = {2 \\over 3} , P(B|A)= {3\\over 5}\n$$  \nUns interessiert nun die Wahrscheinlichkeit, dass $A$ und $B$ gleichzeitig eintreten also $P(A \\cap B)$ welches wir\nmit dem Multiplikationssatz berechnen können.  \n$$\nP(A \\cap B) = {{2\\over 3} \\cdot {3 \\over 5}} = {2 \\over 5}\n$$  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Stochastische unabhängigkeit", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Es kann sein, dass die Wahrscheinlichkeit eines Ereignisses $B$ von einem anderen Ereignis $A$ abhängen kann. Dies\nführte uns zu der bedingten Wahrscheinlichkeit.  \nWenn dies jedoch nicht der Fall ist, also wenn die Ereignisse nicht voneinander abhängen, dann bezeichnen wir solche\nEreignisse als **stochastisch unabhängig** und somit gilt dann  \n$$\nP(A | B) = P(A) \\text{und} P(B | A) = P(B)\n$$  \nAus dem Multiplikationssatz wird dann  \n$$\nP(A \\cap B)=P(A) \\cdot P(B|A)=P(A) \\cdot P(B)\n$$  \nWir können also definieren das zwei Ereignisse stochastisch unabhängig sind wenn  \n$$\nP(A \\cap B)= P(A) \\cdot P(B)\n$$  \n<Callout type=\"example\" title=\"Beispiel stochastische unabhängige Münzenwurfe\">  \nEine Münze wird 3x geworfen und wir betrachten die folgende Ereignisse:  \n- $A=$ Zahl beim 1. Wurf\n- $B=$ Zahl beim 2. Wurf\n- $C=$ Kopf beim 3. Wurf  \nSie sind alle stochastisch unabhängig, den sie sind völlig unabhängig von einander.  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Mehrstufige Zufallsexperimente", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Bei einem Mehrstufige Zufallsexperimente werden mehrere Zufallsexperimente nacheinander ausgeführt. Diese werden oftmals\ndurch Baumdiagramme (Ereignisbäume) dargestellt. Dabei unterscheidet man zwischen Endergebnisse und Zwischenergebnisse.  \nWir definieren noch folgende Regeln:  \n1. Die Wahrscheinlichkeiten entlang eines Pfades werden miteinander multipliziert.\n2. Führen mehrere Pfade zum gleichen Endergebnis, so addiert man ihre Wahrscheinlichkeiten.  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/baumdiagramm-1107).  \n<Callout type=\"example\" title=\"Beispiel Mehrstufiges Zufallsexperiment\">  \nIn einer Urne befinden sich 6 Kugeln, 2 weiss und 4 schwarz. Wir entnehmen nacheinander ganz zufällig 2 Kugeln ohne\nzurücklegen, somit 2 Stufen und stellen uns die Frage mit welcher Wahrscheinlichkeit erhalten wir 2 gleichfarbige\n$A$ oder 2 verschiedenfarbige Kugeln $B$?  \n**1. Stufe:**  \n- $P(W) = {2 \\over 6} = {1 \\over 3}$\n- $P(S) = {4 \\over 6} = {2 \\over 3}$  \n**2. Stufe:**\nNach der 1. Ziehung sind nurnoch 5 Kugeln in der Urne, entweder wurde eine schwarze oder eine weisse entzogen.\nFalls es eine Weisse war haben wir:  \n- $P(W|W) = {1 \\over 5}$\n- $P(S|W) = {4 \\over 5}$  \nFalls es ein Schwarze war haben wir:  \n- $P(W|S) = {2 \\over 5}$\n- $P(S|S) = {3 \\over 5}$  \nSomit ergeben sich folgende Resultate:  \nDie Pfade wo es gleichfarbige Kugeln sind:  \n$$\nP(A)=P(WW) + P(SS) = {1 \\over 3} \\cdot {1 \\over 5} + {2 \\over 3} \\cdot {2 \\over 5} = {7 \\over 15}\n$$  \nDie Pfade wo es verschiedenfarbige Kugeln sind:\n$$\nP(A)=P(WS) + P(SW) = {1 \\over 3} \\cdot {4 \\over 5} + {2 \\over 3} \\cdot {2 \\over 5} = {8 \\over 15}\n$$  \n![mehrstufigeZufallsexperimente](/maths/mehrstufigeZufallsexperimente.png)  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Totale Wahrscheinlichkeit", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Die **totale Wahrscheinlichkeit** für das Eintreten des Ereignisses $B$ wobei $A_i$ die möglichen Zwischenereignisse\nauf dem Weg zum Ereignis $B$ ist  \n$$\nP(B)= \\sum_{i=1}^{n}{P(A_i)\\cdot P(B|A_i)}\n$$  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/satz-der-totalen-wahrscheinlichkeit-1111)\nund [hier](https://www.youtube.com/watch?v=0iJSW0VNddo).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Bayes' theorem", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Unter der Voraussetzung, dass das Ereignis $B$ bereits eingetreten ist, gilt dann für die Wahrscheinlichkeit, dass\ndieses Ereignis über das Zwischenereignis $A_j$ die Bayessche Formel  \n$$\nP(A_j|B)= {P(A_j \\cup B) \\over P(B)} = {P(A_j) \\cdot P(B | A_j) \\over P(B)}\n$$  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/satz-von-bayes-1113)\nund [hier](https://www.youtube.com/watch?v=wUDxQFbXqjA).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Geburtstagsparadox", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Das Geburtstagsparadox ist ein Beispiel dafür, dass bestimmte Wahrscheinlichkeiten intuitiv häufig falsch geschätzt werden.  \nWir stellen uns die Frage \"Was ist die Wahrscheinlichkeit, dass mindestens zwei Personen am selben Tag Geburtstag haben\nin einer Gruppe von $k$ personen?\".  \nUm dieses Problem anzugehen, schauen wir zuerst an, was die Wahrscheinlichkeit ist, dass die personen **nicht** am selben Tag geburtstag haben:  \nBei 2 Leuten: ${365 \\over 365} \\cdot {364 \\over 365}$\nBei 3 Leuten: ${365 \\over 365} \\cdot {364 \\over 365}\\cdot {363 \\over 365}$\netc.\nDiese Zahl wird näher zu 0 und nun können wir unsere Frage beantworten  \n$$\nP(gleich)=1-P(ungleich) \\Leftrightarrow P(A)=1- \\frac{365 \\cdot (365-1)\\cdot...\\cdot (365-n+1)}{365^n}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Probability", "Header 2": "Bernoulli-Experiment", "path": "../pages/digitalGarden/maths/probabilityStatistics/probability.mdx"}, "page_content": "Ein Bernoulli-Experiment ist ein Zufallsexperiment mit genau 2 möglichen Ergebnisse, Treffer oder nicht Treffer.  \nEin häufiges Beispiel dafür ist das Werfen eines Würfels. Wir interessieren uns nur, ob wir eine 6 bekommen beim\nWürfeln. Das heisst, wenn wir eine 6 würfeln betrachten wir es als ein Treffer alle andere Ergebnisse fassen wir zusammen als kein Treffer.  \nAnders als bei einem Laplace-Experiment müssen wie man oben sieht die Wahrscheinlichkeiten der Ergebnisse gleich sein.\nIm obigen Beispiel ist die Wahrscheinlichkeit für ein Treffer $\\frac{1}{6}$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Zufallsvariablen", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Eine Zufallsvariable $X$ ist eine Funktion welche jedem Elementarereignis $\\omega \\in \\Omega$ genau eine reele Zahl\nzuordnet. Wir unterscheiden noch dabei, ob eine Zufallsvariable **diskret** oder **stetig** ist. Hier werden wir diskrete\nZufallsvariablen anschauen später werden wir noch stetige dazu nehmen.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Zufallsvariablen", "Header 3": "Diskrete Zufallsvariablen", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Eine diskrete Zufallsvariable kann endlich viele oder abzählbar unendliche viele Werte annehmen.\nGute Beispiel dafür sind Ergebnisse beim Würfeln, Anzahl Münzwurfe bis zum ersten Mal Kopf etc.  \nEine gute Videoerklärung dazu gibt es auch [hier](https://studyflix.de/statistik/diskrete-zufallsvariablen-1114).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Dichte/Wahrscheinlichkeits-Funktion", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Bei einer diskreten Zufallsvariable gehört zu jedem Wert $x_i$ eine bestimmte Wahrscheinlichkeit $P(X=x_i)$. Diese\nBeziehung lässt sich gut mit einer sogenannten Verteilungstabelle oder einem Stabsdiagramm (Wahrscheinlichkeitsdiagramm)\nvisualisieren, dabei gilt  \n$$\nf(x_i)=p_i \\text{ wobei } p_i \\in [0,1]\n$$  \nEine Dichtefunktion ist auch normiert, das heist, dass alle Wahrscheinlichkeiten der verschiedenen Werte $x_i$ der\nZufallsvariable $X$ zusammen 1 ergeben.  \n$$\n\\sum_{x_i \\in X}{f(x_i)}=1\n$$  \nVerteilungstabelle:  \n![verteilungsTabelle](/maths/verteilungsTabelle.png)  \nStabsdiagramm:  \n![stabdiagram](/maths/stabdiagram.png)  \nmehr dazu findest du [hier](https://studyflix.de/statistik/stetige-dichtefunktion-und-verteilungsfunktion-1080).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Verteilungsfunktion", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Die Verteilungsfunktion $F(x)$ einer Zufallsvariable $X$ ist die Wahrscheinlichkeit dafür, dass die Zufallsvariable $X$\neinen Wert, der kleiner oder gleich $x$ annimmt.  \n$$\nF(x)=P(X \\leq x) = \\sum_{x_i \\leq x}{f(x_i)}\n$$  \nDie Wahrscheinlichkeit bei einer diskreten Zufallsvariable $P(a < X \\leq b)$ ist gegeben durch $F(b)-F(a)$  \n![verteilungsFunktion](/maths/verteilungsFunktion.png)  \nmehr dazu findest du [hier](https://studyflix.de/statistik/diskrete-dichte-und-verteilungsfunktion-1115).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Erwartungswert", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Der Erwartungswert ist der Wert, den wir im Durchschnitt erwarten können und ist definiert als  \n$$\nE(X)=\\sum_{x_i \\in X}{x_i\\cdot f(x_i)}=\\sum_{x_i \\in X}{x_i \\cdot P(X=x_i)}\n$$  \n<Callout type=\"example\" title=\"Beispiel Erwartungswert würfeln\">\nBeim Wurf eines Würfels mit $X=$Augenzahl ist  \n$$\nE(X)=3.5\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Erwartungswert", "Header 3": "Erwartungswerte addieren", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "$$\nE(X+Y) = E(X)+E(Y)\n$$  \n<Callout type=\"example\" title=\"Beispiel Erwartungswerte addieren\">\nWir werfen eine Münze, solange bis zum ersten Mal Kopf, dann würfeln wir, solange bis eine 6 kommt.  \nWie hoch ist die totale erwartete Anzahl würfe?  \n$$\nX \\sim Geo(1/2), Y \\sim Geo(1/6), E(X+Y)=E(X)+E(Y)=8\n$$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Erwartungswert", "Header 3": "Erwartungswert, Skalar multiplizieren", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "$$\nE(aX) = aE(X)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Erwartungswert", "Header 3": "Erwartungswert, Skalar addieren", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "$$\nE(X+c) = E(X)+c\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Erwartungswert", "Header 3": "Erwartungswert, Funktion anwenden", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "$$\nE(g(X))=\\sum_{x_i \\in X}{g(x_i)\\cdot P(X=x_i)}\n$$  \nFür all Funktionen $g: \\mathbb{R} \\mapsto \\mathbb{R}$  \n<Callout type=\"example\" title=\"Beispiel Erwartungswert Funktion anwenden\">\nEin Computerhändler hat 3 Computer für 500 CHF pro Stück gekauft, die er für 1000 CHF vor Neujahr verkaufen will.  \nDer händler weiss, dass er alle nicht verkauften Computer nach Neujahr garantiert für 200 CHF an ein Unternehmen verkaufen kann.  \nDer Händler denkt, wenn $X$ die Anzahl verkaufte Computer entspricht, dass er die folgende Verteilung hat.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Varianz", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Wenn wir den Erwartungswert von $X$ als $\\mu$ beschreiben, dann ist die Varianz gegeben durch  \n$$\nV(X)=\\sigma^2(X)=\\sum_{x_i \\in X}{(x_i - \\mu)^2 \\cdot f(x_i)}=\\sum_{x_i \\in X}{(x_i - \\mu)^2 \\cdot P(X=x_i)}\n$$  \noder auch in kurz  \n$$\nV(X) = \\sigma^2(X) = E(X^2) - E(X)^2\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Varianz", "Header 3": "Varianz, Skalar addieren", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "$$\nV(X+c) = V(X)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Varianz", "Header 3": "Varianz, Skalar multiplizieren", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "$$\nV(aX)=a^2V(X)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Standardabweichung", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Die Standardabweichung ist gegeben durch  \n$$\n\\sigma(X)=\\sqrt{V(X)}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Diskrete Verteilungen", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Nun schauen wir uns ein paar Verteilungen, an die häufig vorkommen, wenn man mit diskreten Zufallsvariablen arbeitet.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Diskrete Verteilungen", "Header 3": "Binomial-Verteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Die Binomialverteilung der Zufallsvariable $X$ ist die Anzahl Treffer bei der $n$-maligen unabhängigen Durchführung eines\nExperiments mit 2 Elementarereignisse, Treffer und kein Treffer wobei $p$ die Wahrscheinlichkeit für einen Treffer ist.  \n- Wir schreiben dann $X \\sim Bin(n,p)$\n- Die Dichtefunktion von $X$ ist $f(k)=\\binom{n}{k}p^k(1-p)^{n-k}$ wobei $k$ die Anzahl benötigter treffer ist.\n- $E(X) = n \\cdot p$\n- $V(X) = n \\cdot p \\cdot (1-p)$  \nIn Matlab haben wir die Funktionen:  \n- Dichtefunktion $binopdf(k,n,p)$ wobei $pdf$ English ist und für \"probability density function\" steht\n- Verteilungsfunktion $binocdf(k,n,p)$ wobei $cdf$ English ist und für \"cumulative distribution function\" steht  \nmehr dazu findest du [hier](https://studyflix.de/statistik/binomialverteilung-1118).  \n<Callout type=\"example\" title=\"Beispiel Binomialverteilung\">\nEin Multiple Choice Test besteht aus 12 Fragen mit je 4 möglichen Antworten wovon immer genau 1 richtig ist. Der Test\nwird durch Erraten ausgefüllt. Wie gross ist die Wahrscheinlichkeit für mehr als 8 richtige Antworten?  \n$$\nX \\sim Bin(12,1/4)\n$$  \nMit $P(X \\geq 9) = P(X=9)+...+P(X=12)=1 - P(x \\leq 8)$  \nUnd somit dann $1-binocdf(8,12,1/4)$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Diskrete Verteilungen", "Header 3": "Bernoulli-Verteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Die Bernoulli-Verteilung ist eine spezielle Form der Binomialverteilung wobei $n=1$. Wir können dann alles ein wenig vereinfachen.  \n- Wir schreiben dann $X \\sim B(p)$\n- Die Dichtefunktion von $X$ ist $f(0)= 1-p, f(1)=p$\n- $E(X) = p$\n- $V(X) = p \\cdot (1-p)$  \nmehr dazu findest du [hier](https://studyflix.de/statistik/bernoulliverteilung-1117).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Diskrete Verteilungen", "Header 3": "Geometrische-Verteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Die Geometrische Verteilung der Zufallsvariable $X$ ist die Anzahl der Versuche bis zum ersten Treffer bei der\nwiederholten unabhängigen Durchführung eines Experiments mit 2 Elementarereignisse, Treffer und kein Treffer wobei $p$\ndie Wahrscheinlichkeit für einen Treffer ist.  \n- Wir schreiben dann $X \\sim Geo(p)$\n- Die Dichtefunktion von $X$ ist $f(k)=\\binom{n}{k}p\\cdot (1-p)^{k-1}$ wobei $k$ bedeutet, dass die ersten $k-1$\nVersuche kein Treffer waren aber der $k$-te Versuch ein Treffer ist.\n- $E(X) = \\frac{1}{p}$\n- $V(X) = \\frac{1-p}{p^2}$  \nIn Matlab haben wir die Funktionen:  \n- Dichtefunktion: $geopdf(k-1,p)$\n- Verteilungsfunktion $geoocdf(k-1,p)$  \nmehr dazu findest du [hier](https://studyflix.de/statistik/geometrische-verteilung-1120).  \n<Callout type=\"example\" title=\"Beispiel Geometrische Verteilung\">\nWir würfeln solange bis eine sechs kommt. Wie hoch ist die Wahrscheinlichkeit, dass dies im zehnten Versuch passiert?  \n$X \\sim Geo(1/6)$\nMit $P(X = 10) = (\\frac{5}{6})^9 \\cdot \\frac{1}{6} = geopdf(9,1/6) \\approx 3.2$%\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Diskrete Verteilungen", "Header 3": "Hypergeometrische-Verteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Die Hypergeometrische Verteilung der Zufallsvariable $X$ ist die Verteilung, die beim $n$-maligen Ziehen ohne\nZurücklegen und ohne Reihenfolge aus einer Urne mit $N$ Kugeln, von denen $M$ eine spezielle Eigenschaft haben und wo\ndie Anzahl der gezogenen Kugeln mit dieser speziellen Eigenschaft gezählt werden.  \n- Wir schreiben dann $X \\sim Hyp(N,M,n)$\n- Die Dichtefunktion von $X$ ist $f(k)=\\binom{M}{k} \\cdot \\frac{\\binom{N-M}{n-k}}{ \\binom{N}{n}}$ wobei $N$ die\nGesamtanzahl der Kugeln ist, $M$ die Anzahl mit der speziellen Eigenschaft. $n$ ist dann der Umfang der Stichprobe also\ndie Anzahl der entnommenen Kugeln und $k$ die Anzahl angestrebte Kugeln mit der speziellen Eigenschaft.\n- $E(X) = n \\cdot \\frac{M}{N}$\n- $V(X) = n \\cdot \\frac{M}{N} \\cdot (1 - \\frac{M}{N}) \\cdot \\frac{N-n}{N-1}$  \nIn Matlab haben wir die Funktionen:  \n- Dichtefunktion: $hygepdf(k,N,M,n)$\n- Verteilungsfunktion $hygecdf(k,N,M,n)$  \nmehr dazu findest du [hier](https://www.youtube.com/watch?v=BoPYslAe8sg).  \n<Callout type=\"example\" title=\"Beispiel Hypergeometrische Verteilung\">\nDas perfekte Beiepiel dafür ist Lotto, wobei wir 49 nummerierte Kugeln haben, 6 davon werden gezogen, welche in diesem\nFalle unsere spezielle Kugeln sind. Wir dürfen 6 Zahlen aufschreiben, also sind das unsere Kugeln die wir\nherausnehmen ohne zurücklegen oder die Reihenfolge zu beachten. Was ist nun die Wahrscheinlichkeit das wir 4 von\nden 6 richtig haben?  \n$X \\sim Hyp(49,6,6)$\n$hygepdf(4,49,6,6) = \\frac{645}{665896} \\approx 0.09686$%\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Discrete Random Variables", "Header 2": "Diskrete Verteilungen", "Header 3": "Poisson-Verteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/discreteRandomVariables.mdx"}, "page_content": "Die Poisson-Verteilung kommt bei Zufallsvariablen zum Einsatz, welche die Anzahl der Ereignisse einer bestimmten Art\nin einem Zeit- und/oder Ortsintervall beschreiben die Anzahl dieses Ereignisses entspricht $\\lambda$. Diese Ereignisse\nsind oftmals \"seltene\" Ereignisse z.B.  \n- $X$ Anzahl Druckfehler auf einer Seite eines Buchs\n- $X$ Anzahl Unfälle an einem Wochenende in einem Skigebiet\n- $X$ Anzahl falsch gewählter Telefon-Nummern an einem Tag\n- $X$ Anzahl Erdbeben in einem Jahr in einer bestimmten Region.  \nDies sind nur ein paar Beispiele der Poisson-Verteilung, sie ist einer der wichtigsten Verteilungen die wir kennen.  \n- Wir schreiben dann $X \\sim Poi(\\lambda)$\n- Die Dichtefunktion von $X$ ist$f(k)=\\frac{\\lambda^k}{k!} \\cdot e^{-\\lambda}$\n- $E(X) = \\lambda$\n- $V(X) =\\lambda$  \nIn Matlab haben wir die Funktionen:  \n- Dichtefunktion: poisspdf(k,$\\lambda$)\n- Verteilungsfunktion poisscdf(k,$\\lambda$)  \nmehr dazu findest du [hier](https://studyflix.de/statistik/poissonverteilung-1121)  \n<Callout type=\"example\" title=\"Beispiel Poisson-Verteilung\">\nDer Druchschnitt der Anzahl Druckfehler pro Seite ist 0.4.\nDann ist $X \\sim Poi(0.4)$ ein gutes Modell.\nDamit erhalten wir:  \n- $P(X=0)=poisspdf(0,0.4)=67.03$%\n- $P(X=2)=poisspdf(2,0.4)=5.36$%\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Covariance", "path": "../pages/digitalGarden/maths/probabilityStatistics/covariance.mdx"}, "page_content": "!!!!!!!!!!!!!This is very short and should prob explain KL-divergence since I have no clue how it works!!!!!!!!!!!!!  \nDie Kovarianz ist wie die Varianz und der Erwartungswert eine Kennzahl. Die Kovarianz kann genutzt werden für die\nÜberprüfung, ob es zwischen zwei Zufallsvariablen lineare Zusammenhänge gibt oder nicht. In anderen worte, ob die\neine Zufallsvariable mit der anderen etwas zu tun hat. Die Kovarianz kann jedoch keine genaue Aussage zur Abhängigkeit machen!  \n$$\nCov(X,Y) = E((X - E(X))*(Y-E(Y)))\n$$  \nOder mehr mathematisch ausgedruckt aber nur für diskrete Zufallsvariablen mit $N$ werte:  \n$$\nCov(X,Y) = \\frac{1}{N} \\sum_{i=1}{n}{(x_i - E(X))\\cdot(y_i - E(Y))}\n$$  \n![kovarianz](/maths/kovarianz.gif)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Covariance", "Header 2": "Multivariate Normalverteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/covariance.mdx"}, "page_content": "!!!!!!!!!!!!!No Idea why I put this here!!!!!!!!!!!!!  \nDie multivariate Normalverteilung ist sehr ähnlich wie die eindimensionale Normalverteilung. Sie verallgemeinert\ndie eindimensionale Normalverteilung auf mehrere Dimensionen. Anstatt Erwartungswert und Standardabweichung hat die\nmultivariate Normalverteilung folgende Parameter:  \n- Der Erwartungswertvektor $\\mu$\n- Die Kovarianzmatrize $\\Sigma$\n- Wir schreiben dann für eine $p$-dimensionale Zufallsvariable $X \\sim N_p[\\mu,\\Sigma]$\n- Die Dichte von $X$ ist $f_{\\mu,\\Sigma}(x)= \\frac{1}{\\sqrt{(2\\pi)^p \\mathbb{det}(\\sigma)}} e^{-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)}$  \nGenau wie bei der eindimensionalen Normalverteilung haben die Parameter einen Einfluss auf die Form der Verteilung,\nvor allem die Kovarianzmatrize.  \nMit einer zweidimensionalen Matrize $\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\n\\end{bmatrix}$ sieht die Verteilung so aus:  \n![einfacheMultiNormalverteilung](/maths/einfacheMultiNormalverteilung.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Markov Chains", "Header 2": "Stochastische Prozesse", "path": "../pages/digitalGarden/maths/probabilityStatistics/markovChains.mdx"}, "page_content": "Ein stochastischer Prozess beschreibt die Zustände eines Systems zu einem bestimmten Zeitpunkt welche vom Zufall beeinflusst sind.  \nFür jeden Zeitpunkt $t \\in T \\subset \\mathbb{R}$ beschreibt die Zufallsvariable $X_t: \\Omega \\mapsto I$ den Zustand eines Systems zum Zeitpunkt $t$. Dann heisst $(X_t, t\\in T)$ oder kurz $(X_t)$ **stochastischer Prozess mit Zustandsraum** $I$. Wir beschränken uns hier auf Prozesse mit diskreter Zeit und diskretem Zustandsraum, also sind beide endlich abzählbar ($\\mathbb{N}, \\mathbb{Z}$).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Markov Chains", "Header 2": "Markow-Kette", "path": "../pages/digitalGarden/maths/probabilityStatistics/markovChains.mdx"}, "page_content": "Bei vielen Systemen hängt der Folgezustand nur vom aktuellen Zustand ab, und nicht noch von allen Zuständen davor. Solche Prozesse nennt man Markow-Ketten. Mathematische ausgedrückt sieht das dann so aus für $(X_n,n\\in\\mathbb{N})$  \n$$P(X_n=i_n | X_{n-1}=i_{n-1},...,X_0=i_o)=P(X_n=i_n | X_{n-1}=i_{n-1})$$  \nhierbei sind alle $i_0,...,i_n \\in I$. Ein Beispiel hier wäre, dass wenn ein Server ohne Probleme läuft, dann hat er mit 0.9 Wahrscheinlichkeit am nächsten Tag auch kein Problem. Hat er jedoch ein Problem dann hat er mit 0.5 Wahrscheinlichkeit am nächsten Tag immer noch ein Problem.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Markov Chains", "Header 2": "Markow-Kette", "Header 3": "Homogene Markow-Kette, HMK", "path": "../pages/digitalGarden/maths/probabilityStatistics/markovChains.mdx"}, "page_content": "Wenn nicht nur die ganze Historie sondern auch der Tag keine Rolle spielt dann ist es eine **homogene Markow-Kette** (HMK). Dies wäre nicht der Fall wenn die Wahrscheinlichkeit, dass am Sonntag der Server noch Probleme hat, wenn er am Samstag schon ein Problem hat anders ist also alle andere Tage (Obwohl dies sehr wahrscheinlich der Fall ist weil niemand am Sonntag arbeitet um den Server zu reparieren). Eine Markow-Kette ist also homogen, wenn die Übergangswahrscheinlichkeiten für alle $n$ gleich sind.  \n$$P(X_n=j|X_{n-1}=i), (i,j\\in I)$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Markov Chains", "Header 2": "Markow-Kette", "Header 3": "Übergangs-Matrix / Graph", "path": "../pages/digitalGarden/maths/probabilityStatistics/markovChains.mdx"}, "page_content": "In diesem Fall heisst die Matrix $P=(p_{ij})$ eine Übergangsmatrix wobei $P_{ij}=P(X_n=j|X_{n-1}=i)$ (Zeile=$i$ und Spalte=$j$). Wichtig hierbei ist, dass die Summe der Reihen 1 ergeben weil die Wahrscheinlichkeiten normalisiert sind.  \n![markowKetteUebergangsmatrix](/maths/markowKetteUebergangsmatrix.png)  \nOftmals werden Markow-Ketten auch mit Hilfe von einem Übergangsgraph dargestellt.  \n![markowKetteUebergangsgraph](/maths/markowKetteUebergangsgraph.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Markov Chains", "Header 2": "Markow-Kette", "Header 3": "Zustände in der Zukunft", "path": "../pages/digitalGarden/maths/probabilityStatistics/markovChains.mdx"}, "page_content": "Mit der Übergangsmatrix können wir Zustände des Systems in der Zukunft berechnen solange wir den Startwert kennen in dem wir folgendes machen  \n$$P(X_{n+m}=j|X-n=i) \\text{ ist der ij-te Eintrag der Matrix } P^m$$  \nJedoch ist der Startwert nicht immer bekannt dieser kann auch zufällig sein. Zum Beispiel kann ein Server mit 1% Wahrscheinlichkeit beim liefern schon kaputt gehen. Man hat also eine Startverteilung $P(X_0=i)$ für alle $i \\in I$. Einen nicht zufälligen also festen Startwert $X_0=s$ kann man auch so modellieren $P(X_0=s)=1$ für all andere $i\\neq s,$ $P(X_0=i)=0$.  \nDer Vektor mit den Einträgen $P(X_0=i)$ für alle $i \\in I$ bezeichnet man meistens mit $\\pi_0$. Mit $\\pi_n$ bezeichnet man den Vektor mit den Einträgen für $P(X_n=i)$ für alle $i \\in I$. Daraus folgt dann  \n$$\\pi_n=\\pi_0 \\cdot P^n$$  \nFür unseres vorherige Beispiel erhalten wir also die Startverteilung $\\pi_0=(0.99,0.01)$ daraus können wir dann berechnen was die Verteilung am ersten Tag, am vierten etc ist.  \n- $\\pi_1=\\pi_0 \\cdot P = (0.99 \\cdot 0.9 + 0.01 \\cdot 0.5, 0.99 \\cdot 0.1 + 0.01 \\cdot 0.5)=(0.896, 0.104)$\n- $\\pi_4=\\pi_0 \\cdot P^4=(0.8434, 0.1566)$\n- $\\pi_364=\\pi_0 \\cdot P^{364}=(0.8333, 0.1667)$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Markov Chains", "Header 2": "Markow-Kette", "Header 3": "Reguläre HMK", "path": "../pages/digitalGarden/maths/probabilityStatistics/markovChains.mdx"}, "page_content": "Die Frage, ob der Server an einem konkreten Tag Probleme hat, ist aber eigentlich gar nicht so wichtig. Viel wichtiger ist die Frage, an wie vielen Tagen der Server langfristig Probleme hat. Dazu schauen wir uns das Verhalten von $\\pi_n$ über die Zeit an.  \n![markowKetteTimeGraph](/maths/markowKetteTimeGraph.png)  \nWir sehen also das nach einer Phase beträgt die Wahrscheinlichkeit, das der Server an einem beliebigen Tag ein Problem hat, 16.667%. Mit einem anderen Startwert erhalten wir.  \n![markowKetteTimeGraph2](/maths/markowKetteTimeGraph2.png)  \nDie Konvergenz, unabhängig von der Startverteilung, ist kein Zufall, dies gilt für jede **reguläre** HMK. Eine HMK mit Übergangsmatrix $P$ heisst regulär wenn ein $n$ existiert, so dass all Einträge von $P^n$ grösser als 0 sind.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Markov Chains", "Header 2": "Markow-Kette", "Header 3": "Grenzverteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/markovChains.mdx"}, "page_content": "Zu jeder regulären HMK existiert eine sogenannte Grenzverteilung(auch stationäre oder Gleichgewichtsverteilung) $\\pi^*= (\\pi_1^*,...,\\pi_m^*)$. Diese Grenzverteilung hat die folgende Eigenschaften  \n- Für jede Startverteilung $\\pi_0$ gilt $\\lim_{n \\to \\infty}{\\pi_0\\cdot P^n=\\pi^*}$\n- $\\pi^*\\cdot P = \\pi^*$\n- Die Zeilen der Matrix $P^n$ konvergieren gegen die Grenzverteilung $\\pi^*$  \nUm die Gleichgewichtsverteilung zu bestimmen muss man das folgende Gleichungssystem lösen:  \n- Normierung Bedingung (N): $\\sum_{i\\in I}{\\pi_i^*}=1$\n- Gleichgewicht Bedingung (G): $\\pi^*\\cdot P = \\pi^*$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Eine stetige Zufallsvariable kann unendlich viele nicht Abzählbare Werte annehmen. Stetige Zufallsvariablen entstehen\nmeist durch einen Messvorgang. Unabhängig von der Messgenauigkeit kann eine stetige Zufallsvariable innerhalb eines\nIntervalls unendlich viele Werte annehmen aber einen genauen Wert zu messen ist nicht wirklich möglich.  \nDie Wahrscheinlichkeit, dass eine stetige Zufallsvariable $X$ einen exakten Wert $x_i$ annimmt ist gleich 0. Die Begründung\ndafür ist, dass die Wahrscheinlichkeit das eine Person genau 180 cm gross ist, 0 ist. Grund dafür ist, dass die Person\n180.000000000000001cm gross sein könnte, was nicht das Gleiche ist wie 180 cm. Deshalb macht es mehr Sin, dass $X$ einen\nWert in einem Interval $[a,b]$ annimmt z.B. $[179.5,180.5]$.  \nMehr zu stetigen zufallsvariablen findest du [hier](https://www.youtube.com/watch?v=g9acHn8zSr8).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Dichtefunktion", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Die Wahrscheinlichkeiten von Stetigen Zufallsvariablen werden durch die Fläche unter der Dichtefunktion $f(x)$ für alle\n$x$ der Zufallsvariable $X$ beschrieben  \n$$\nP(a \\leq X \\leq b) = \\int_{a}^{b}{f(x) dx}\n$$  \n![stetigeDichtefunktion](/maths/stetigeDichtefunktion.png)  \nDie Gesamtfläche unter der Dichtefunktion muss gleich 1 sein sonst ist sie nicht normalisiert  \n$$\nP(-\\infty < X < \\infty)=\\int_{-\\infty}^{\\infty}{f(x) dx}=1\n$$  \nWeil die Wahrscheinlichkeit von einem genauen Wert 0 ist haben abgeschlossene und offene Intervalle dieselben\nWahrscheinlichkeiten, dies kann man dann später gut ausnutzen Fragestellungen um zu formulieren.  \n$$\nP(a \\leq X \\leq b)=P(a < X \\leq b)=P(a \\leq X < b)=P(a < X < b)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Erwartungswert", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Genau wie bei diskreten Zufallsvariablen kann man den Erwartungswert mit einer Formel berechnen.  \n$$\nE(X)=\\int_{-\\infty}^{\\infty}{x \\cdot f(x) dx}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Varianz", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Ebenso die Varianz  \n$$\nE(X)=\\int_{-\\infty}^{\\infty}{(x-E(X))^2 \\cdot f(x) dx}\n$$  \nOder kurz geschrieben genau gleich wie bei diskreten Zufallsvariablen  \n$$E(X^2)-E(X)^2$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Standardabweichung", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Und genau gleich wie bei diskreten Zufallsvariablen die Standardabweichung  \n$$\n\\sigma(X)=\\sqrt{V(X)}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Verteilungsfunktion", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Die Verteilungsfunktion ist definiert als  \n$$\nF(x)=P(X \\leq x)=\\int_{-\\infty}^{x}{f(y) dy}\n$$  \nUnd hat die follgende Eigneschaften:  \n- $P(a \\leq X \\leq b)=F(b)-F(a)$\n- $F'(x)=f(x)$ wenn $f$ stetig ist.\n- $\\lim_{x\\to\\infty} F(x)=1$\n- $\\lim_{x\\to - \\infty} F(x)=0$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Stetige Verteilungen", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Nun schauen wir uns ein paar Verteilungen an, die häufig vorkommen, wenn man mit stetigen Zufallsvariablen arbeitet.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Stetige Verteilungen", "Header 3": "Stetige Gleichverteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Die **stetige Gleichverteilung** wird auch oft **Uniformverteilung** genannt. Sie hat auf dem Intervall $[a,b]$ eine konstante\nWahrscheinlichkeitsdichte, dass heisst das alle Teilintervalle gleicher Länge dieselbe Wahrscheinlichkeit besitzen.  \n- Wir schreiben dann $X \\sim U[a,b]$\n- Die Dichte von $X$ ist $f(x)=\\frac{1}{b-a}$ wobei $a\\leq x\\leq b$ dies kommt davon, weil die Dichte normalisiert\nist und die Fläche 1 ergeben muss.\n- $E(X) = \\frac{a+b}{2}$\n- $V(X) = \\frac{1}{12}(b-a)^2$  \n![Stetige_Gleichverteilung_Dichte](/maths/stetigeGleichverteilungDichte.png)  \nIn Matlab haben wir die Funktionen:  \n- Dichte: $unifpdf(x,a,b)$\n- Verteilungsfunktion $unifcdf(x,a,b)$  \nMehr dazu findest du [hier](https://www.youtube.com/watch?v=SzvXAJmVPdI) und [hier](https://studyflix.de/mathematik/stetige-gleichverteilung-1081).  \n<Callout type=\"example\" title=\"Beispiel stetige Gleichverteilung\">\nEine Person kommt zu einem zufälligen Zeitpunkt zum Bahnhof. Der Zug fährt einmal pro Stunde. Wie hoch ist die Wahrscheinlichkeit,\ndass man höchstens 10 Minuten warten muss? Was ist die erwartete Wartezeit im Durchschnitt?  \n$X$: Wartezeit, dann ist $X \\sim U[0,60]$  \nWir erhalten also: $f(x)=\\frac{1}{b-a}=\\frac{1}{60}$ und somit dann $F(X)=\\frac{1}{60}x=\\frac{x}{60}$  \n$$\nP(X \\leq 10)=\\int_{0}^{10}{f(x) dx}=\\int_{0}^{10}{\\frac{1}{60} dx} = \\frac{x}{60} \\Big|_{0}^{10}=\\frac{10}{60}-\\frac{0}{60}=\\frac{1}{6}\n$$  \nUnd die erwartete Wartezeit ist $E(X)=\\frac{a+b}{2}=30$ Minuten  \n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Stetige Verteilungen", "Header 3": "Normalverteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Die Normalverteilung oder auch oft Gauss-verteilung oder Glockenkurve genannt, ist eines der wichtigsten stetigen Verteilungen.\nDie Normalverteilung besteht aus 2 Parametern, der Erwartungswert $\\mu$ und die Standardabweichung $\\sigma$. Desto Kleiner\n$\\sigma$ desto enger ist die Glockenkurve.  \n- Wir schreiben dann $X \\sim N[\\mu,\\sigma]$\n- Die Dichte von $X$ ist $f_{\\mu,\\sigma}(x)= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$ wobei $a\\leq x\\leq b$\n- $E(X) = \\mu$\n- $V(X) = \\sigma^2$  \n![normalVerteilungGraph](/maths/normalVerteilungGraph.png)  \nIn Matlab haben wir die Funktionen:  \n- Dichte: $normpdf(x,\\mu,\\sigma)$\n- Verteilungsfunktion $normcdf(x,\\mu,\\sigma)$  \nMehr dazu findest du [hier](https://studyflix.de/mathematik/normalverteilung-1089).  \n<Callout type=\"example\" title=\"Beispiel Normalverteilung\">\nDer Intelligenzquotient (IQ) ist normalverteilt und so festgelegt, dass $\\mu=100$ und $\\sigma=15$. Wie hoch ist die Wahrscheinlichkeit,\ndass eine zufällig ausgewählte Person einen IQ zwischen 90 und 110 hat oder grösser als 150.  \nEs sei X der IQ der Person dann ist $X \\sim N(100,15)$.  \n$P(90 \\leq X \\leq 110)=P(X \\leq 110) - P(X \\leq 90) = normcdf(110,100,15)-normcdf(90,100,15) \\approx 50$%  \n$P(X \\geq 150)= 1 - P(X \\< 150) = 1 - normcdf(150,100,15) \\approx 0.04$%\n</Callout>  \n#### Standardisierung der Normalverteilung  \nMit Standardisierung bezeichnen wir die transformation einer normalverteilte Zufallsvariable $X$, zu einer\nZufallsvariable $Z$ welches den Erwartungswert $E(Z)=0$ und die Varianz $V(Z)=1$ besitzt. Dies machen wir damit\nwir verschiedene Zufallsvariablen besser vergleichen können und damit wir auch schneller rechnen können.  \nZuerst zentrieren wir die Zufallsvariable, dies machen wir, indem wir von allen ihre Elementarereignisse den\nErwartungswert $\\mu$ abziehen. Mit Zentrieren ist hier gemeint das wir den Gipfel der funktion $f(x)$ bei der\nNullstelle der x-Achse wollen.  \nDanach Dividieren wir die Differenz $X - \\mu(X)$ durch die Standardabweichung $\\sigma (X)$.  \n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Stetige Verteilungen", "Header 3": "Normalverteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Erwartungswert $\\mu$ abziehen. Mit Zentrieren ist hier gemeint das wir den Gipfel der funktion $f(x)$ bei der\nNullstelle der x-Achse wollen.  \nDanach Dividieren wir die Differenz $X - \\mu(X)$ durch die Standardabweichung $\\sigma (X)$.  \n$$\nZ=\\frac{X-\\mu(X)}{\\sigma(X)}\n$$  \nZ.B. können wir nun $X \\sim N(\\mu,\\sigma)$ zu $Z \\sim N(0,1)$ umwandeln mit $Z = \\frac{X-\\mu(X)}{\\sigma(X)}$.  \nMehr dazu findest du [hier](https://www.youtube.com/watch?v=YmEGmn5C4tg&t=11s).  \n#### Quantile  \nOftmals haben wir einen Wert $\\alpha \\in [0,1]$ gegeben. Und wir suchen nun den Wert $z_\\alpha$ wofür $P(X \\leq z_\\alpha)=\\alpha$.  \n- Wenn $\\alpha = 0.5$ dann reden wir vom Median, auch Zentralwert, **Erwartungswert** und der Modus. Eine\nKennzahl dafür, wo sich die \"Mitte\" einer Wahrscheinlichkeitsverteilung befindet.\n- Wenn $\\alpha = 0.25 \\text{oder} 0.75$ reden wir von einem Quartil\n- Mit dem Perzentil schneiden wir $[0,1]$ in 100 Teile, was equivalent ist zu den Prozentzahlen.  \nIn Matlab haben wir die Funktionen:  \n- $norminv(\\alpha,\\mu,\\sigma)$ wobei $norminv$ English ist und für \"normal inverse\" steht  \n![quantileGraph](/maths/quantileGraph.png)  \nmehr dazu findest du [hier](https://www.youtube.com/watch?v=KdQLNiCOa0U)  \n<Callout type=\"example\" title=\"Beispiel Quantile\">  \nDer Intelligenzquotient (IQ) ist normalverteilt und so festgelegt, dass $\\mu=100$ und $\\sigma=15$. Eine gewisse\nSchulform ist für die tiefsten 5% gedacht. Ab welchem IQ sollte man an diese Schule gehen?  \n$X \\sim N(100,16)$  \nWir suchen also $z_\\alpha$ mit $P(X \\leq z_\\alpha) = 0.05$  \nDies bekommen wir mit der Matlab funktion $norminv(0.05,100,15)=75.33$  \n</Callout>  \n#### Sigma-Regeln  \nFür $X \\sim N(\\mu,\\sigma)$ gilt  \n1. $P(|X-\\mu| \\leq \\sigma) \\approx 68.3$%\n2. $P(|X-\\mu| \\leq 2\\sigma) \\approx 95.5$%\n3. $P(|X-\\mu| \\leq 3\\sigma) \\approx 99.7$%  \nWas bedeutet, dass ein Wert einer normalverteilten Zufallsvariable mit der Wahrscheinlichkeit 68%\nmaximal um $\\pm \\sigma$ vom Erwartungswert $\\mu$ abweicht.  \n![sigmaRegelnGraph](/maths/sigmaRegelnGraph.png)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Stetige Verteilungen", "Header 3": "Normalverteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "3. $P(|X-\\mu| \\leq 3\\sigma) \\approx 99.7$%  \nWas bedeutet, dass ein Wert einer normalverteilten Zufallsvariable mit der Wahrscheinlichkeit 68%\nmaximal um $\\pm \\sigma$ vom Erwartungswert $\\mu$ abweicht.  \n![sigmaRegelnGraph](/maths/sigmaRegelnGraph.png)  \nmehr dazu findest du [hier](https://www.youtube.com/watch?v=OmmODKdYLSI)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Continuous Random Variables", "Header 2": "Stetige Verteilungen", "Header 3": "Exponentialverteilung", "path": "../pages/digitalGarden/maths/probabilityStatistics/continuousRandomVariables.mdx"}, "page_content": "Die Exponentialverteilung beschreibt zufällige Lebensdauern von Geräten oder Wartezeiten auf zufällige Ereignisse.  \n- Lebensdauer einer Glühbirne\n- Wartezeit auf nächstes Erdbeben  \nDie Exponentialverteilung und Poisson-Verteilung haben eine enge Beziehung mit einander.  \n$$\nAnzahl \\sim Poi(\\lambda) \\leftrightarrow Zwischenankunftszeit \\sim Exp(\\lambda)\n$$  \n- Wir schreiben dann $X \\sim Exp(\\lambda)$\n- Die Dichte von $X$ ist $F'(x)= \\lambda e^{-\\lambda x}$\n- Die Verteilung von $X$ ist $F(x)= 1 - e^{-\\lambda x}$\n- $E(X) = \\frac{1}{\\lambda}$\n- $V(X) = \\frac{1}{\\lambda^2}$  \nIn Matlab haben wir die Funktionen:  \n- Dichte: $exppdf(x,1/\\lambda)$\n- Verteilungsfunktion $expcdf(x,1/\\lambda)$  \nMehr dazu findest du [hier](https://studyflix.de/mathematik/exponentialverteilung-1088).  \n<Callout type=\"example\" title=\"Beispiel Exponential-Verteilung\">  \nIn einem Geschäft kommen im Schnitt 20 Kunden pro Stunde.  \n1.  Wie hoch ist die Wahrscheinlichkeit, dass mehr als 30 Kunden in einer Stunde kommen? $X \\sim Poi(20),\\,\nP(X > 30)=1-poisscdf(30,20)$\n2.  Wie hoch ist die Wahrscheinlichkeit, dass man weniger als 5 Minuten auf den ersten Kunden warten muss?\n$T \\sim Exp(20),\\,P(T \\leq \\frac{1}{12})=expcdf(1/12,1/20)$  \n</Callout>  \n#### Gedächtnislosigkeit  \nDie Exponential-Verteilung hat die spezielle Eigenschaft, dass sie kein Gedächtnis hat.  \nWas so viel heisst, wie wenn ein Gerät mit einer exponential verteilten Lebensdauer $X$ während\n$t$ Stunden gelaufen ist, so ist die Wahrscheinlichkeit, dass es weitere $h$ Stunden läuft gleich gross, wie\nwenn ein neues Gerät $h$ Stunden läuft. Dies können wir mit ein wenig Mathematik und bedingte Wahrscheinlichkeiten auch beweisen  \n$$\nP(X \\geq t + h | X \\geq t) = \\frac{P(X \\geq t + h \\cup X \\geq t)}{X \\geq t} = \\frac{P(X \\geq t + h)}{X \\geq t}\n$$  \n$$\n=P(X \\geq h)=\\frac{e^{-\\lambda (t+h)}}{e^{-\\lambda t}}=e^{-\\lambda h}=P(X \\geq h)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Grenzwertsätze", "Header 2": "Ungleichung von Tschebyscheff", "path": "../pages/digitalGarden/maths/probabilityStatistics/randomStuff.mdx"}, "page_content": "Wenn $X$ eine Zufallsvariable ist mit einem Erwartungswert von $\\mu$ und eine Varianz $\\sigma^2$ dann kann man die Wahrscheinlichkeit die folgende Wahrscheinlichkeit abschätzen für jede mögliche Verteilung von $X$  \n$$P(|X-\\mu|\\geq k) \\leq \\frac{\\sigma^2}{k^2}, \\text{ für k > 0}$$  \ndaraus folgt dann auch  \n$$P(|X-\\mu| < k) \\leq 1 - \\frac{\\sigma^2}{k^2}$$  \n:::note Beispiel Ungleichung von Tschebyscheff  \nFür die Grösse einer erwachsenen Personen haben wir einen Erwartungswert von 175cm und eine Standardabweichung von 10cm. Was ist die Wahrscheinlichkeit, dass eine Person kleiner als 160cm oder grösser als 190cm ist.  \nAbschätzung mit Tschebyscheff:  \n$$P(|X -175cm| \\geq 15) \\leq \\frac{100}{15^2} \\approx 44.4 \\%$$  \nTatsächlich mit Normalverteilung:  \n$$P(|X -175cm| \\geq 15) = 1- (normcdf(190,175,10) - normcdf(160,175,10)) \\approx 13.4 \\%$$  \n:::  \nMehr dazu findest du [hier](https://studyflix.de/statistik/tschebyscheff-ungleichung-1546)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Grenzwertsätze", "Header 2": "Gesetz der grossen Zahlen", "path": "../pages/digitalGarden/maths/probabilityStatistics/randomStuff.mdx"}, "page_content": "Wir wissen wenn wir eine Zufallsvariable $X_i$ mit einer Bernoulli Verteilung haben also $X_i \\sim B(p)$ dann ist $\\sum_{i=1}^{n}{X_i} \\sim Bin(n,p)$. Die relative Häufigkeit ist wie wir wissen die Anzahl des Eintreffen eines Ereignis $X$ durch die anzahl unabhängige Ausführungen des Experiments $n$ dann gilt folgendes  \n$$\\lim_{n \\to \\infty}{P(|\\frac{X}{n} - p| \\geq e)} = 0$$  \nWas so viel heisst wie wenn desto mehr unabhängige Experimente wir ausführen desto besser stabilisiert sich die relative Häufigkeit um den Erwartungswert.  \nMehr dazu findest du [hier](https://studyflix.de/statistik/gesetz-der-grosen-zahlen-2053)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Grenzwertsätze", "Header 2": "Zentraler Grenzwertsatz", "path": "../pages/digitalGarden/maths/probabilityStatistics/randomStuff.mdx"}, "page_content": "Der zentrale Grenzwertsatz liefert die Begründung für das Phänomen, dass sich bei der additiven Überlagerung vieler kleiner unabhängiger Zufallsexperiment approximativ zu einer Normalverteilung wird.  \nWenn wir also eine Folge von unabhängigen Zufallsvariablen $X_1,X_2,...$vom gleichen Wahrscheinlichkeitsraum haben welche alle dieselbe Verteilung mit Erwartungswert $\\mu$ und Varianz $\\sigma^2$ haben dann gilt für $n$ die Anzahl Zufallsvariablen und die Summe $S_n=X_1+...+X_n$. Dann hat die Summe approximative die Normalverteilung $N(\\mu n, \\sigma n)$ wobei $\\mu n = n \\cdot \\mu$ und $\\sigma n = \\sqrt{n} \\cdot \\sigma$.  \nDiese Verteilung kann dann natürlich auch noch standardisiert werden.  \n$$\\frac{S_n -\\mu n}{\\sqrt{n}\\cdot \\sigma}\\sim N(0,1)$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Grenzwertsätze", "Header 2": "Satz von Moivre-Laplace", "path": "../pages/digitalGarden/maths/probabilityStatistics/randomStuff.mdx"}, "page_content": "Weil eine Binomialverteilte Zufallsvariable $X \\sim Bin(n,p)$ als Summe von $n$ Bernoulliverteilte Zufallsvariablen interpretiert werden kann und wir sie mit der Normalverteilung annähern können mit $N(np, \\sqrt{np(1-p)})$ können wir ein paar Approximationen machen. Wobei $normcdf(x)$ die Verteilungsfunktion der standardisierten Normalverteilung ist.  \nMit dem Satz können wir für $n > \\frac{9}{p(1-p)}$ folgendes gut approximieren  \n$$P(a \\leq X \\leq b) \\approx normcdf(\\frac{b-np}{\\sqrt{np(1-p)}})-normcdf(\\frac{a-np}{\\sqrt{np(1-p)}})$$  \nGenauer wird es dann mit der Stetigkeitskorrektur  \n$$P(a \\leq X \\leq b) \\approx normcdf(\\frac{b+\\frac{1}{2}-np}{\\sqrt{np(1-p)}})-normcdf(\\frac{a-\\frac{1}{2}-np}{\\sqrt{np(1-p)}})$$  \n:::note Beispiel Satz von Moivre-Laplace  \nEin fairer Würfel wirf 1000 mal geworfen. Wie hoch ist die Wahrscheinlichkeit, dass wir zwischen 150 und 200 sechs würfeln?  \nGenau:  \n$$binocdf(200,1000,1/6)-bincdf(149,1000,1/6)=0.9265$$  \nMit Satz von Moivre-Laplace:  \n$$normcdf(\\frac{200+\\frac{1}{2}-\\frac{1000}{6}}{\\sqrt{1000\\cdot \\frac{5}{36}}}) - normcdf(\\frac{150-\\frac{1}{2}-\\frac{1000}{6}}{\\sqrt{1000\\cdot \\frac{5}{36}}})=0.9253$$  \n:::", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Grenzwertsätze", "Header 2": "Simulation von Zufallsvariablen", "path": "../pages/digitalGarden/maths/probabilityStatistics/randomStuff.mdx"}, "page_content": "Manchmal ist es nur mit grossem Aufwand Wahrscheinlichkeiten zu exakt berechnen. Eine Lösung für dieses Problem ist die Zufallsvariable zu simulieren, also Zahlen zu erzeugen die korrekt verteilt sind und dann schauen, welcher Prozentsatz dieser Zahlen im gesuchten Ereignis liegen. Dank dem Gesetz der grossen Zahlen wird diese Wahrscheinlichkeit genauer mit wachsender Anzahl an Wiederholungen.  \nIn den meisten Programmiersprachen ist der sogenannte lineare Kongruenzgenerator eingebaut welcher ein Pseudozufallszahlengenerator ist. Das heisst er erzeugt die Zahlen nicht wirklich zufällig sondern berechnet sie anhand eines Startwerts, der sogenannte **Seed** (oftmals die Systemzeit).  \nFür kryptographische Zwecke wie Schlüsselerzeugung sind Pseudozufallszahlengenerator nicht geeinigt, weil mit wenigen Werten kann man die verwendete Parameter berechnen und kann dann die Zufallsvariablen voraus sehen.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Grenzwertsätze", "Header 2": "Simulation von Zufallsvariablen", "Header 3": "Linearer Kongruenzgenerator", "path": "../pages/digitalGarden/maths/probabilityStatistics/randomStuff.mdx"}, "page_content": "Beim linearen Kongruenzgenerator haben wir ein sogenanntes modul $m >1$ und  einen Anfangswert (der Seed) $x_0$ und zwei weitere Werte $a$ und $b$. Wichtig dabei ist, dass $a,b,x_0 \\in \\{0,1,...,m-1\\}$ sind. Dann können wir eine Zufallszahl wie gefolgt berechnen  \n$$x_{n+1}=(a\\cdot x_n +b) \\text{ mod } m$$  \nWir erhalten dann einen Wert aus dem endlichen Bereich ${0,1,...m-1}$. Weil der Wertebereich endlich ist gibt es eine Periode. Wenn z.B. $m=12,a=4,b=1,x_0=1$ dann wiederholt sich die Folge schon nach dem dritten Wert.  \nDer Satz von Knuth besagt, damit die Periodenlänge maximal ist, also $m$ muss folgendes gelten:  \n- $b$ ist zum Modul $m$ teilerfremd, also $ggt(b,m)=1$.\n- Jeder Primfaktor von $m$ teilt $a-1$.\n- Wenn $m$ durch 4 teilbar ist, dann muss auch $a-1$ durch 4 teilbar sein.  \nDurch eine Transformation bekommen wir auch nur noch Werte im Intervall $[a,b]$.  \n$$z_n=a+(b-1)\\frac{x_n}{m}$$  \nDie Werte $U$ die wir erhalten sind im Intervall $[a,b]$ gleich verteilt.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Grenzwertsätze", "Header 2": "Simulation von Zufallsvariablen", "Header 3": "Inversionsmethode", "path": "../pages/digitalGarden/maths/probabilityStatistics/randomStuff.mdx"}, "page_content": "Wir können nun unsere Zufallszahlen auf eine bestimmte Verteilung abbilden mit der Inversionsmethode.  \nEs sei $F$ eine streng monoton steigende Verteilungsfunktion und $U$ eine im Intervall $[0,1]$ gleichverteilte Zufallsvariable dann ist Zufallsvariable $F^{-1}(U)$ verteilt mit der Verteilungsfunktion $F$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Real Functions", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Unter einer reellen Funktion $f$ versteht man die Abbildung, die jedem $x \\in D$ mit $D \\subseteq R$ genau eine reelle Zahl $y$ aus einer Wertemenge $W$ zuordnet:  \n$$\nf:x\\mapsto y=f(x), D \\subseteq R \\mapsto W \\subseteq R\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Real Functions", "Header 3": "Nullstelle", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine Funktion $f$ besitzt eine Nullstelle in $x_0$, falls $f(x_0) = 0$ gilt.\nDer Funktionsgraph schneidet die x-Achse in einer Nullstelle der Funktion.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Real Functions", "Header 3": "Gerade", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine Funktion heisst gerade, falls $f(x) = f(−x)$ für alle $x\\in D$ gilt.\nDer Funktionsgraph einer geraden Funktion ist spiegelsymmetrisch zur y−Achse.  \n<Callout type=\"example\">\nDie Funktion $f(x)=x^2$ ist gerade.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Real Functions", "Header 3": "Ungerade", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine Funktion heißt ungerade, falls $f(−x) = −f(x)$ für alle $x\\in D$ gilt.\nDer Funktionsgraph einer ungerade Funktion ist punktsymmetrisch zum Koordinatenursprung.  \n<Callout type=\"example\">\nDie Funktion $f(x)=x^3$ ist ungerade.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Polynomfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine Funktion $f: R \\mapsto R$ der Form:  \n$$\nf(x)=a_nx^n+a_{n-1}x^{n-1}+...+a_1x+a_0\n$$  \nmit $a_n \\neq 0$ heisst **Polynom vom Grad** $n$. Die reelen Zahlen $a_0,a_1,...,a_n$ heissen **Koeffizienten** der Polynoms.  \n<Callout type=\"example\">\n$f_1(x)=x^3-x+2$ ist ein Polynom 3. Grades.\n$f_2(x)=2x^7-4x^5+x^2-3x+2$ ist ein Polynom 7. Grades.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Polynomfunktion", "Header 3": "Linearfaktoren", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Ist $x_0$ eine Nullstelle des Polynoms $n$-ten Grades von $f$, dann wäre ein **Linearfaktor** von $f$:  \n$$\nf(x)=(x-x_0)(b_nx^{n-1}+..+b_2x+b_1)\n$$  \nJedes Polynom $n$-ten Grades hat höchstes n verschiedene Nullstellen.\nBesitzt ein Polynom $n$-ten Grades $n$ Nullstellen $x_1,x_2,..x_n$ dann lässt es sich als Produkt aus $n$ Linearfaktoren darstellen:  \n$$\nf(x)=a_nx^n+a_{n-1}x^{n-1}+a_1x+a_0=a_n(x-x_1)(x-x_2)...(x-x_{n-1})(x-x_n)\n$$  \n#### Zerlegung in Linearfaktoren  \nDie Abspaltung eines Linearfaktors erreicht man am besten mit Polynomdivision.  \n<Callout type=\"example\">\n$f(x)=x^3-7x^2-10x+16$\nDurch einsetzen, dass $x_1 = {\\color{Red}1}$ eine Nullstelle des Polynoms d.h. $f(1) = 0$  \n$(x^3-7x^2-10x+16) : (x-{\\color{Red}1})=x^2-6x-16$\n$\\underline{-(x^3-x^2)}$\n$\\quad -6x^2-10x$\n$\\quad \\underline{-(-6x^2+6x)}$\n$\\quad \\quad -16x+16$\n$\\quad\\quad \\underline{-(-16x+16)}$\n$\\quad\\quad\\quad0$  \nWir erhalten dadurch: $f(x)=(x-1)(x^2-6x-16)$.\n$(x^2-6x-16)$ kann dann weiter mit der Polynomdivision zerteilen um die weiteren Linearfaktoren zu erhalten.  \n$f(x)=x^3-7x^2-10x+16=(x-1)(x+2)(x-8)$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Rationale Funktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine rationale Funktion ist eine Funktion, die sich als Bruch von zwei Polynomfunktion $g(x)$ und $h(x)$ darstellen lässt.  \n$$\nf(x)=\\frac{g(x)}{h(x)}={{a_mx^m+a_{m-1}x^{m-1}+...+a_1x+a_0}\\over {b_nx^n+b_{n-1}x^{n-1}+...+b_1x+b_0}}\n$$  \nEin Polynomfunktion ist eine rationale Funktion wo $n=0$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Rationale Funktion", "Header 3": "Echt rationale Funktionen", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Wenn $m<n$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Rationale Funktion", "Header 3": "Unecht rationale Funktionen", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Wenn $m \\geq n$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Rationale Funktion", "Header 3": "Eigenschaften", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Sei $f(x) = \\frac{g(x)}{h(x)}$ eine rationale Funktion. Mit Zähler und Nenner soweit möglich in Linearfaktoren zerteilt und gemeinsame Linearfaktoren gekürtzt  \n#### Nullstellen  \nDie im Zähler($g(x)$) verbleibenden Linearfaktoren ergeben die **Nullstellen** der Funktion $f(x)$.  \n#### Polstellen  \nDie im Nenner ($h(x)$)verbleibenden Linearfaktoren ergeben die **Polstellen**\nder Funktion $f(x)$.  \n##### Pollstelle $k$-ter Ordnung  \nIst Linearfaktor im gekürzten Nenner in $k$-ter Ordnung $(x − x_0)^k, k \\in N$  dann nennt man die Stelle $x_0$ eine Polstelle $k$−ter Ordnung.  \n##### Pollstelle mit Vorzeichenwechsel  \nEs sei $x_0$ eine Pollstelle $k$-ter Odnung.  \n- Ist $k$ gerade, so handelt es sich um eine **Pollstelle ohne Vorzeichenwechsel**.\n- Ist $k$ ungerade, so handelt es sich um eine **Pollstelle mit Vorzeichenwechsel**.  \n<Callout type=\"example\">\n$f(x)={1 \\over (x+1)}$ hat bei $x=-1$ eine Pollstelle mit Vorzeichenwechsel.\n$f(x)={1 \\over (x-1)^2}$ hat bei $x=1$ eine Pollstelle ohne Vorzeichenwechsel.\n</Callout>  \n#### Defintionslücken  \nVor dem kürzen sind die Nullstellen im Nenner($h(x)$) für rationale Funktionen nicht definiert. Sie müssen explizit aus dem Definitionsbereich der Funktion herausgenommen werden, man spricht von **Definitionslücken**.  \n#### Hebbare Definitionslücken  \nDie vollständig weggekürzten Linearfaktoren im Nenner geben die\n**hebbaren Definitionslücken** der Funktion $f(x)$ an.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Rationale Funktion", "Header 3": "Verhalten rationale Funktionen im Unendlichen", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Genau gleich wie [[2-Folgen#Rationale Folgen]].\nSei $f(x) = {g(x)\\over h(x)}$ eine rationale Funktion, dann gilt für den Grenzwert:  \n$$\n\\lim_{n \\to \\infty}{f(x)} = \\begin{dcases}\n0, grad\\space g < grad \\space h \\\\\n{a_n\\over b_n} , grad \\space g=grad \\space h \\\\\n{a_n\\over b_n} * \\infty , grad \\space g > grad\\space h\n\\end{dcases}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Umkehrfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine Funktion $f: x \\mapsto y, D \\mapsto W$ heisst umkehrbar, wenn aus $x_1 \\neq x_2$ stets folgt $f(x_1)\\neq f(x_2)$\nIst die Funktion umkehrbar, dann gibt es zu jedem $y \\in W$ genau ein $x \\ in D$ $f^{-1}: y \\mapsto f^{-1}(y)=f^{-1}(f(x))=x$ wird **Umkehrfunktion** genannt.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Potenzfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Polynomfunktion der Form $p: x \\mapsto ax^n, R \\mapsto R$ für $a,n \\in R$\nPotenzfunktion haben Wurzelfunktion als Umkehrfunktion und umgekehrt.  \n<Callout type=\"example\">\n$p(x) = x^2$ hat $p^{-1}(x)=\\sqrt{x} = x^{1 \\over 2}$\n$p(x) = x^3$ hat $p^{-1}(x)=\\sqrt[3]{x} = x^{1 \\over 3}$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Wurzelfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Die Funktion$p^{-1}: x \\mapsto\\sqrt[n]{x}$ für n gerade $R^+ \\mapsto R^+$, für n ungerade $R \\mapsto R$ heisst $n$-te Wurzerlfunktion mit $n \\in N$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Exponentialfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "$f: x \\mapsto e^x$ mit $e=2.71828...=$ Eulersche Zahl heisst Exponentialfunktion.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Exponentialfunktion", "Header 3": "Rechenregeln der Exponentialfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "- $e^0=1$\n- $e^{x+y}=e^x*e^y$\n- $e^{-x}=(e^x)^-1={1 \\over e^x}$\n- $e^{nx}=(e^x)^n$\n- $e^{1 \\over n}=\\sqrt[n]{e}$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Logarithmusfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Die Umkehrfunktion zu Exponentialfunktion wird **natürliche Logarithmusfunktion** genannt. $f: x \\mapsto ln(x), R^+ \\mapsto R$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Logarithmusfunktion", "Header 3": "Rechenregeln der Logarithmusfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "- $ln(1)=0$\n- $ln(x*y)=ln(x)+ln(y)$\n- $ln(x^n)= n*ln(x)$\n- $ln(e^x)=x ln(e) = x$ weil $ln(e)=1$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Trigonometrische Funktionen", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Sinus- und Cosinusfunktion sind periodisch mit der Periode $2\\pi$, d.h. es gilt\n$f(x)=f(x+k*2\\pi), k \\in Z$  \nDie Funktionsgraphen von Sinus- und Cosinusfunktion sind kongruent. Durch Verschiebung um $2\\pi$ nach links, geht die Cosinus-Kurve aus der Sinus-Kurve hervor.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Trigonometrische Funktionen", "Header 3": "Trigonometrischer Pythagoras", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "$$\nsin^2(a)+cos^2(a)=1\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Trigonometrische Funktionen", "Header 3": "Sinus", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "$sin: x \\mapsto sin(x), R \\mapsto [-1,1]$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Trigonometrische Funktionen", "Header 3": "Cosinus", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "$cos: x \\mapsto cos(x), R \\mapsto [-1,1]$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Grenzwert einer Funktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "$$\nf(x)={1-x^3-cos(2x)}\\over x^2\n$$  \nist für $x=0$ nicht definiert, hier besteht eine Definitionslücke. Wir können den Funktionswert an der Stelle $x = 0$zwar nicht berechnen, aber mit einer Folge $x_n$ beliebig nahe an die Definitionslücke herantasten.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Grenzwert einer Funktion", "Header 3": "Rechtseitigen Grenzwert", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Die Folge $x_n=1\\over n$ für $n \\to \\infty$ konvergiert gegen 0. Wir können somit die Folgenglieder in die Funktion einsetzen:  \nWir vermuten, dass die Funktionswerte gegen den Grenzwert 2 konvergieren.\nEs gilt also für jede beliebige Folge $x_n \\to 0$, dass $f(x_n) \\to 2$ gilt. Man schreibt daher:  \n$$\n\\lim_{n \\to \\infty}{f(x_n)}=\\lim_{x \\to \\infty,(x>0)}{f(x)}=2\n$$  \nund bezeichnet diesen Wert als den **rechtseitigen Grenzwert** der Funktion $f(x)$ an der Stelle $x=0$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Grenzwert einer Funktion", "Header 3": "Linkseitigen Grenzwert", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Wir wollen nun eine Folge betrachten, die sich von links dem Wert 0 nähert. Z.B $x_n=-1\\over n$. Hier erhalten wie folgende Wertetabelle beim einsetzen:  \nAus der Wertetabelle entnehmen wir auch hier, dass die Folge der Funktionswerte $f(x_n)$ gegen den Wert 2 konvergiert.\nMan schreibt daher:  \n$$\n\\lim_{n \\to \\infty}{f(x_n)}=\\lim_{x \\to \\infty,(x<0)}{f(x)}=2\n$$  \nund bezeichnet diesen Wert als den **linkseitigen Grenzwert** der Funktion $f(x)$ an der Stelle $x=0$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Grenzwert einer Funktion", "Header 3": "Zusammenfassung Grenzwert einer Funktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Betrachtet man bei der Grenzwertbetrachtung einer Funktion $f$ an der Stelle $x_0$ nur Zahlenfolgen $x_n$, die kleinere Werte als $x_0$ enthalten, dann bezeichnet man den Grenzwert als Linkseitigen Grenzwert  \n$$\n\\lim_{x \\to x_0,(x<x_0)}{f(x)}=\\lim_{h \\to 0,(h>0)}{f(x_0-h)}=G_L\n$$  \nZahlenfolgen mit grösseren Werten als $x_0$ erzeugen den Rechtseitigen Grenzwert  \n$$\n\\lim_{x \\to x_0,(x>x_0)}{f(x)}=\\lim_{h \\to 0,(h>0)}{f(x_0+h)}=G_R\n$$  \nStreben für jede gegen $x_0$ konvergente Zahlenfolge $x_n$ die Funktionswerte $f(x_n)$ gegen denselben Wert $G$, dann besitzt die Funktion $f$ an der Stelle $x_0$ den Grenzwert $G$  \n$$\n\\lim_{x \\to x_0}{f(x)}=G=G_R=G_L\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Rechenregeln für Funktionsgrenzwerte", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Seien $f$ und $g$ zwei Funktionen mit dem gleichen Grenzwerten $G,F$ bei $x_0$ dann gilt:  \n- $\\lim_{x \\to x_0}{(f(x)\\pm g(x))}=F\\pm G$\n- $\\lim_{x \\to x_0}{(f(x)*g(x))}=F*G$\n- $\\lim_{x \\to x_0}{(f(x)\\over g(x))}=F\\over G$ für $g(x_0) \\neq 0$ und $G \\neq 0$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Stetigkeit", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine Funktion heisst stetig, wenn ihr Graph kein Loch und keinen Sprung aufweist, d.h. wenn man beim Zeichnen ihres Graphen den Stift nicht absetzen muss.  \nEine Funktion heisst an einer Stelle $x = x_0$ stetig, wenn der Grenzwert von $f(x)$ für $x \\to x_0$ existiert und mit dem Funktionswert an der Stelle $x_0$ übereinstimmt:  \n$$\n\\lim_{x \\to x_0}{f(x)}=f(x_0)\n$$  \nEine Funktion, die an jeder Stelle ihres Definitionsbereiches $D$ stetig ist, nennt man eine **stetige Funktion** (auf $D$).\nExistiert der Grenzwert hingegen nicht oder ist er nicht gleich wie der Funktionswert, so ist die Funktion an dieser Stelle **unstetig**.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Stetigkeit", "Header 3": "Hebbare Unstetigkeitsstelle", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Wenn bei einer Funktion f der linksseitige Grenzwert und der rechtsseitige Grenzwert existieren und gleich sind aber nicht mit dem Funktionswert $f(x_0)$ übereinstimmen oder die funktion an $x_0$ nicht definiert ist, dann kann man eine neue Funktion definieren, die an der Stelle $x_0$ stetig ist. Die Stelle $x_0$ heisst **hebbare Unstetigkeitsstelle**  \n$$\n\\hat{f}=\\begin{dcases}\nf(x), x\\neq x_0 \\\\\nG, x=x_0\n\\end{dcases}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Stetigkeit", "Header 3": "Stetige Funktionen", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "- Polynomfunktion sind für $R$ stetig.\n- Exponentialfunktionen $f(x)=a^x, (a > 0, 1\\neq 0)$ sind für $R$ stetig.\n- Logarithmusfunktionen $f(x)=log_a(x), (a > 0, 1\\neq 0)$ sind für $x>0$ stetig.\n- Trigonometrsiche Funktionen $cos(x), sin(x)$ sind für $R$ stetig.\n- Hyperbelfunktionen $sinh(x), cosh(x), tanh(x))$sind für R stetig.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Stetigkeit", "Header 3": "Rechenregeln für Stetige Funktionen", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Sind die Funktionen $f$ und $g$ auf ihrem ganzen Definitionsbereich stetig, insbesonders an der Stelle $x_0$ gilt:  \n- $f \\pm g$ ist ebenfalls stetig in $x_0$\n- $f * g$ ist ebenfalls stetig in $x_0$\n- $f \\over g$ ist ebenfalls stetig in $x_0$, falls $g(x_0) \\neq 0$\n- Die Komposition von $f \\circ g$ ist ebenfalls an der Stelle $x_0$ stetig", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Steigung", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Die allgemeine Geradengleichung lautet:  \n$$\ng(x)=m*(x-x_0)+y_0\n$$  \n$m$ ist dann die Steigung der Geraden.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Steigung", "Header 3": "Sekante -  Differenzenquotient", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine Gerade durch 2 Punkte $P_0(x_0/f(x_0))$ und $P_1(x_1/f(x_1))$ heisst **Sekante**.\nDie Steigung der Sekante wird **Differenzenquotient** genannt.  \n$$\nm={\\Delta f \\over \\Delta x }={{f(x1)-f(x_0)} \\over {x_1-x_0}}={f(x_0 + \\Delta x)-f(x_0)\\over \\Delta x}\n$$  \nGleichung der Sekante lautet: $g(x)=m*(x-x_0)+f(x_0)$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Steigung", "Header 3": "Tangente - Differenzialkoeffizient", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Eine **Tangente** einer Funktion $f$ im Punkt $x_0$ ist eine Gerade durch einen Punkt $P(x_0/f(x_0))$. Der Grenzwert des [[#Sekante - Differenzenquotient]] wird als **Differnzialkoeffizient**, dafür gibt es verschiedende Schreibweisen:  \n$$\nf'(x_0)=\\lim_{\\Delta x \\to x_0}{f(x_0 + \\Delta x)-f(x_0)\\over \\Delta x}=\\lim_{\\Delta x \\to x_0}{\\Delta f \\over \\Delta x}={df \\over dx}=\\lim_{h \\to x_0}{{f(x+h)-f(x)}\\over h}\n$$  \nGleichung der Tangente lautet: $g(x)=f(x_0)+f'(x_0)*(x-x_0)$  \nExistiert der Grenzwert des Differenzenquotienten dann nennt man die Funktion **differenzierbar** an der Stelle $x_0$. Der Grenzwert wird als **Ableitung** der Funktion $f$ an der Stelle $x_0$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Ableitungsfunktion", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Die Funktion $f': x \\mapsto f'(x)$ heisst die Ableitungsfunktion von $f(x)$ oder kurz **Ableitung von** $f()x$.\nDie Ableitungsfunktion ordnet jedem Wert x die Steigung der Tangente an der Stelle x zu.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Ableitungsfunktion", "Header 3": "Höhere Ableitung", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "Exisitiert zu einer Funktion $f$ Ableitung $f'$ und ist $f'(x)$ wieder differenzierbar, so bezeichnet man deren Ableitung als zweite Ableitung $f''(x)$. Es gilt also $f''=(f')'$\nDie $n$-te Ableitung für $n>3$ schreibt man $f^{(n)}$, die Funktion ist dann $n$-mal differenzierbar.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Function Analysis", "Header 2": "Differenzierbarkeit und Stetigkeit", "path": "../pages/digitalGarden/maths/calculus/functions.mdx"}, "page_content": "- Jede differenzierbare Funktion ist auch stetig und hat an allen Stellen eine eindeutige Steigung.\n- Ist eine Funktion $f$ an der Stelle $x_0$ nicht stetig, dann ist sie dort auch nicht differenzierbar.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Konstantenregel", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "Die Ableitung einer Konstanten ist 0.\n$$f(x)=C \\Rightarrow f'(x)=0, C \\in R$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Faktorregel", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "Beim Ableiten einer Funktion, bleibt ein konstanter Faktor $k \\in R$ vor einer Funktion unverändert erhalten.\n$$g(x)=k*f(x) \\Rightarrow g'(x)=k*f'(x)$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Ableitungen der trigonometrischen Funktionen", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "- $f(x)=sin(x) \\Rightarrow f'(x)=cos(x)$\n- $f(x)=cos(x) \\Rightarrow f'(x)=-sin(x)$\n- $f(x)=tan(x) \\Rightarrow f'(x)={1\\over cos^2(x)}=1+tan^2(x)$ für $x \\neq (2k+1){\\pi \\over 2}$\n- $f(x)=cot(x) \\Rightarrow f'(x)=- {1\\over sin^2(x)}=-1-cot^2(x)$ für $x \\neq k \\pi$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Potenzregel", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "Die [[4-Funktionen#Potenzfunktion]] $f(x) = x^n$ ist für alle $x \\in R$ differenzierbar.\n$$f(x)=x^n \\Rightarrow f'(x)=nx^{n-1}, n \\in Z$$  \n```ad-example\n![[Pasted image 20211024162633.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Summenregel", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "Die Ableitung einer Summe ist gleich der Summe der Ableitungen.\n$$s(x)=f(x)\\pm g(x) \\Rightarrow s'(x)=f'(x) \\pm g'(x)$$  \n```ad-example\n![[Pasted image 20211024162805.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Produktregel", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "$$f(x)=u(x)* v(x) \\Rightarrow f'(x)=u'(x)*v(x) + u(x)*v'(x)$$  \n```ad-example\n![[Pasted image 20211024163316.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Produktregel", "Header 3": "Allgemeine Produktregel", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "Allgemein gilt für die Ableitung eines Produktes aus $n$ Faktoren.\n$$f(x)=u_1*u_2*...*u_n \\Rightarrow f'(x)=u'_1*u_2*...u_n+u_1*u'_2*..*u_n+...+u_1*u_2*...*u'_n$$\nSo wäre: $(uvw)'=u'vw+uv'w+uvw'$ und $(uvwz)'=u'vwz+uv'wz+uvw'z+uvwz'$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Quotientenregel", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "$$f(x)={u(x)\\over v(x)} \\Rightarrow f'(x)={{u'(x)*v(x)-u(x)*v'(x)}\\over v(x)^2}$$  \n```ad-example\n![[Pasted image 20211024164435.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Kettenregel", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "Unter der Verkettung der Funktionen $g$ und $h$ versteht man die Nacheinanderausführung der Funktionen. Man wendet die äussere Funktion $g$ auf das Ergebnis der inneren Funktion $h$. Also von innen nach aussen.\n$f(x)=g(h(x)) \\iff f(x)=(g \\circ h)(x)$  \nMan setzt für die innere Funktion: $z=h(x)$\nso dass sich für die äussere Funktion $f=g(z)=g(h(x))$  \n$$f(x)=g(h(x)) \\Rightarrow f'(x)=g'(z)*h'(x)$$  \n```ad-example\n![[Pasted image 20211024170605.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Ableitung der Exponentialfunktion", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "$$f(x)=e^{g(x)} \\Rightarrow f'(x)= g'(x)* e^{g(x)}$$  \n```ad-example\n![[Pasted image 20211024171328.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Ableitung der Logarithmusfunktion", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "$$f(x)=log_a(x) \\Rightarrow f'(x)= \\frac{1}{x * ln(a)}$$\nwenn $f(x)=ln(x) \\Rightarrow f'(x)= \\frac{1}{x}$  \n```ad-example\n![[Pasted image 20211024171755.png|100]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Differentiation", "Header 2": "Ableitung der Umkehrfunktion", "path": "../pages/digitalGarden/maths/calculus/differentiation.mdx"}, "page_content": "Die Funktion $f(x)$ sei differenzierbar mit der Ableitung $f'(x)$ und besitzt die Umkehrfunktion $x = g(y)$. Die Ableitung der Umkehrfunktion $g(y)$ ist\n$$g'(y)={1\\over f'(x)}$$  \n```ad-example\n![[Pasted image 20211024172038.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Analysis", "Header 2": "Monotonie", "path": "../pages/digitalGarden/maths/calculus/analysis.mdx"}, "page_content": "Die erste Ableitung an der Stelle $x_0$ beschreibt das Steigungsverhalten einer Funktion $f$ in der unmittelbaren Umgebung der Stelle $x_0$  \n$$\nf'(x_0)=\\begin{dcases}\n<0 \\Rightarrow \\text{Funktion fällt, streng monoton fallend} \\\\\n>0 \\Rightarrow \\text{Funktion wächst, streng monoton wachsend}\n\\end{dcases}\n$$  \n![[Pasted image 20211024212936.png]]  \n```ad-example\n![[Pasted image 20211024213308.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Analysis", "Header 2": "Krümmung", "path": "../pages/digitalGarden/maths/calculus/analysis.mdx"}, "page_content": "![[Pasted image 20211024213445.png]]\nDie zweite Ableitung an der Stelle $x_0$ be-schreibt das Krümmungsverhalten einer Funktion $f$ in der unmittelbaren Umgebung der Stelle $x_0$:  \n$$\nf''(x_0)=\\begin{dcases}\n<0 \\Rightarrow \\text{Rechtskrümmung, Steigung nimmt ab} \\\\\n<0 \\Rightarrow \\text{Linkskrümmung, Steigung nimmt zu wachsend}\n\\end{dcases}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Analysis", "Header 2": "Krümmung", "Header 3": "Konkav", "path": "../pages/digitalGarden/maths/calculus/analysis.mdx"}, "page_content": "Ist $f''(x) < 0 \\Rightarrow f'(x)$ ist streng monoton fallend $\\Rightarrow f(x)$ ist konkav (Rechtsgekrümmt).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Analysis", "Header 2": "Krümmung", "Header 3": "Konvex", "path": "../pages/digitalGarden/maths/calculus/analysis.mdx"}, "page_content": "Ist $f''(x) > 0 \\Rightarrow f'(x)$ ist streng monoton wachsend $\\Rightarrow f(x)$ ist konvex (linksgekrümt).", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Analysis", "Header 2": "Extremwerte", "Header 3": "Lokales Maximum", "path": "../pages/digitalGarden/maths/calculus/analysis.mdx"}, "page_content": "Ist $f'(x_0)=0 und f''(x_0)<0$, dann ist x_0 ein lokales Maximum.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Analysis", "Header 2": "Extremwerte", "Header 3": "Lokales Minimum", "path": "../pages/digitalGarden/maths/calculus/analysis.mdx"}, "page_content": "Ist $f'(x_0)=0 und f''(x_0)>0$, dann ist x_0 ein lokales Minimum.  \n```ad-example\n![[Pasted image 20211024220630.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Analysis", "Header 2": "Wendepunkte und Sattelpunkte", "path": "../pages/digitalGarden/maths/calculus/analysis.mdx"}, "page_content": "![[Pasted image 20211024220732.png]]\nIst $f''(x_0)=0 und f'''(x_0)\\neq 0$, dann ist $x_0$ ein Wendepunkt.\nWenn zusätzlich noch $f'(x_0)=0$, dann ist $x_0$ zusätzlich noch ein Sattelpunkt.  \n```ad-example\n![[Pasted image 20211024220936.png]]\n```", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Newton's Methods", "path": "../pages/digitalGarden/maths/calculus/newtonsMethod.mdx"}, "page_content": "Das Newtonverfahren (oder Tangentenverfahren von Newton) ist eine sogenanntes Iterationsverfahren. Das Lösen von Gleichungen gehört zu den wichtigsten Aufgaben der Mathematik. Jedoch können wir schon einfache Gleichungen, wie z. B. $x^3+x=1$ nicht mehr so einfach lösen. Weshalb wir ein Näherungsverfahren zur Lösung von Gleichungen der Form $f(x)=0$ verwenden.\nJede Gleichung kann auf diese Form gebracht werden.  \n$$\nx^3+x=1 \\iff x^3+x-1=0\n$$  \nIst $\\xi$ eine Lösung der Gleichung $f(x)=0$ gilt $f(\\xi)=0$, so kann die Stelle $\\xi$ auch Nullstelle sein.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Taylor Series", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/taylor.mdx"}, "page_content": "A special case of a power series where the coefficients are specifically derived from the function's derivatives at a particular point.  \nA representation of a function as an infinite sum of terms that are calculated from\nthe values of the function's derivatives at a single point.  \nThe Taylor series of a function $f(x)$ about a point $x = a$ is given by:  \n$$\nf(x) = f(a) + f'(a)(x - a) + \\frac{f''(a)}{2!}(x - a)^2 + \\frac{f'''(a)}{3!}(x - a)^3 + ...\n= \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!}(x - a)^n\n$$  \nWhere $f^{(n)}(a)$ is the $n$-th derivative of $f(x)$ evaluated at $x = a$. The higher the order of the derivative,\nthe more accurate the approximation of the function around the point $x = a$.  \nMaclaurin series when the point of consideration is $x = 0$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Power Series", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/power.mdx"}, "page_content": "A power series is a more general mathematical concept, and the Taylor series is a specific type of power series.  \nA power series is a series of the form:  \n$$\n\\sum_{n=0}^{\\infty} a_n(x - c)^n\n$$  \nwhere $a_n$ are the coefficients, $c$ is the center of the series, and $x$ is the variable.\nThis series represents a function of $x$ in terms of powers of $(x - c)$. The\nhigher the power of $(x - c)$, the more accurate the approximation of the function around the point $x = c$.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is a Series?", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/series.mdx"}, "page_content": "A series is the sum of the terms of a sequence.  \nIf the sequence is finite then the series is finite. If the sequence is infinite then the series is infinite.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is a Series?", "Header 2": "Infinite Series", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/series.mdx"}, "page_content": "An infinite series is the sum of the terms of an infinite sequence. If the sequence has the form:  \n$$\na_1, a_2, a_3, a_4, \\ldots\n$$  \nthen the series of the sequence is:  \n$$\nS = a_1 + a_2 + a_3 + a_4 + \\ldots = \\sum_{n=1}^{\\infty} a_n\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "What is a Series?", "Header 2": "Partial Sums", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/series.mdx"}, "page_content": "The sum of the first $n$ terms of a sequence is called the $n$th partial sum. The $n$th partial sum of a sequence is denoted by $s_n$.  \n$$\ns_n = a_1 + a_2 + a_3 + \\ldots + a_n = \\sum_{i=1}^{n} a_i\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sum Operator", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sumOperator.mdx"}, "page_content": "The sum operator or also sometimes called sigma operator or summation is a mathematical operator that represents the addition of a sequence of objects, most commonly numbers but can also be other mathematical objects that support addition.  \nWhen the series of numbers is defined such as $(1,2,3,4,5)$ we commonly write the summation as:  \n$$\n1 + 2 + 3 + 4 + 5 = 15\n$$  \nHowever most often a series is defined by some pattern/formula or function. In this case we use the sum operator to represent the sum of the series.\nThe sum operator is written as a capital sigma $\\sum$ and then usually has a lower and upper bound. The lower bound is the starting index of the series and the upper bound is the ending index of the series. The index is usually a natural number.\nThis index is then used in the formula to represent the current element of the series.  \n$$\n\\sum_{i=1}^{n} i = 1 + 2 + 3 + ... + n\n$$  \nmore generally the sum operator is defined as the following where $a_i$ is the i-th element of the sequence.  \n$$\n\\sum_{i=1}^{n} a_i = a_1 + a_2 + a_3 + ... + a_n\n$$  \nBecause a sequence can be defined by a function, the sum operator can also be used to represent the sum of a function.\nFor computer scientists it can also be useful to think of the sum operator as a loop. The lower bound is the starting index of the loop and the upper bound is the ending index of the loop. The formula inside the sum operator is then the body of the loop.  \n```java\nint sum = 0;\nfor (int i = 1; i <= n; i++) {\nsum += f(i);\n}\n```  \nDepending on the context the upper and/or lower bound are sometimes omitted. If the lower bound is omitted it is assumed to be 1 and if the upper bound is omitted it is assumed to be infinity or the length of the sequence $n$.\nOther common notations are $\\sum_{0 \\leq i \\leq 5} i$ or $\\sum_{s \\in S} s$ where $S$ is a set.  \nWhen a infinite sequence is summed up it is called a [series](/digitalGarden/maths/calculus/series/intro).  \n<Callout type=\"example\">\n- $\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sum Operator", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sumOperator.mdx"}, "page_content": "When a infinite sequence is summed up it is called a [series](/digitalGarden/maths/calculus/series/intro).  \n<Callout type=\"example\">\n- $\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15$\n- $\\sum_{i=1}^{5} i^2 = 1 + 4 + 9 + 16 + 25 = 55$\n- $\\sum_{i=1}^{5} a_i = a_1 + a_2 + a_3 + a_4 + a_5$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sum Operator", "Header 2": "Special Cases", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sumOperator.mdx"}, "page_content": "There are some special cases of the sum operator that are worth mentioning:  \n- **Empty Sum**: The sum of an empty sequence is defined to be 0. This is because there are no elements to sum.\n- **Single Element Sum**: The sum of a sequence with only one element is equal to that element. This is because there is only one element to sum.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sum Operator", "Header 2": "Nested Sums", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sumOperator.mdx"}, "page_content": "The sum operator can also be nested. This means we can sum over multiple indices.\nThis is useful when working with multi-dimensional sequences or functions.  \n$$\n\\sum_{i=1}^{n} \\sum_{j=1}^{m} a_{i,j} = a_{1,1} + a_{1,2} + ... + a_{n,m}\n$$  \nJust like with nested loops in programming languages, the inner sum is executed for each iteration of the outer sum.  \n```java\nint sum = 0;\nfor (int i = 1; i <= n; i++) {\nfor (int j = 1; j <= m; j++) {\nsum += f(i, j);\n}\n}\n```  \n<Callout type=\"example\">\n- $\\sum_{i=1}^{2} \\sum_{j=1}^{2} i*j = 1*1 + 1*2 + 2*1 + 2*2 = 1 + 2 + 2 + 4 = 9$\n- $\\sum_{i=1}^{2} \\sum_{j=1}^{2} i+j = 1+1 + 1+2 + 2+1 + 2+2 = 2 + 3 + 3 + 4 = 12$\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sum Operator", "Header 2": "Properties", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sumOperator.mdx"}, "page_content": "We can derive some properties of the sum operator from the properties of addition and multiplication that can make it easier to work with sums.  \n$$\n\\begin{align*}\n\\sum_{i=l}^{u} C \\cdot f(i) &= C \\cdot \\sum_{i=l}^{u} f(i) & \\text{Distributive property of multiplication} \\\\\n\\sum_{i=l}^{u} (f(i) + g(i)) &= \\sum_{i=l}^{u} f(i) + \\sum_{i=l}^{u} g(i) & \\text{Commutative property of addition} \\\\\n\\sum_{i=l}^{u} f(i) &= \\sum_{i=l+k}^{u+k} f(i-k) & \\text{Shifting the index} \\\\\n\\sum_{i=l}^{u} f(i) &= \\sum_{i=l}^{m} f(i) + \\sum_{i=m+1}^{u} f(i) & \\text{Splitting the sum} \\\\\n\\sum_{i=l}^{u} f(i) &= \\sum_{i=l}^{u} f(u+l-i) & \\text{Reversing the sum order} \\\\\n\\sum_{i=k_0}^{k_1} \\sum_{j=l_0}^{l_1} f(i,j) &= \\sum_{j=l_0}^{l_1} \\sum_{i=k_0}^{k_1} f(i,j) & \\text{Commutative property of addition}\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sum Operator", "Header 2": "Finding Closed Forms", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sumOperator.mdx"}, "page_content": "Story of Gauss and the sum of the first 100 numbers.  \n$$\n\\sum_{i=1}^{n} i = 1 + 2 + 3 + ... + n = \\frac{n(n+1)}{2}\n$$  \n<Callout type=\"proof\">\nBy induction?\n</Callout>  \nThis probably also relates to big O notation and the time complexity of algorithms i think?", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fourier Series", "Header 2": "Reele Darstellung", "Header 3": "Mit Periode 2 Pi", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/fourier.mdx"}, "page_content": "Wir wollen eine periodische Funktion $f(x)$ mit der Periode $T=2\\pi$ mit einer Überlagerung von trigonometrischen Funktionen annähern. Mit einer sog. Fourier-Reihe der folgenden Form  \n$$\n\\hat{f}(x)=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{a_n \\cdot cos(nx)+b_n\\cdot sin(nx)}\n$$  \nDafür müssen die sog. Fourierkoeffizienten so gewählt werden, dass es die Funktion am besten annähert. Diese kann man auch berechnen  \n$$\n\\begin{align*}\na_0&=\\frac{1}{\\pi}\\cdot \\int_0^{2\\pi}{f(x)\\,dx} \\\\\na_n&=\\frac{1}{\\pi}\\cdot \\int_0^{2\\pi}{f(x)\\cdot cos(nx)\\,dx} \\\\\nb_n&=\\frac{1}{\\pi}\\cdot \\int_0^{2\\pi}{f(x)\\cdot sin(nx)\\,dx} \\\\\nn&=1,2,3,...\n\\end{align*}\n$$  \n#### Gerade Funktion  \nWenn die Funktion $f(x)$ die wird approximieren gerade ist, also $f(-x)=f(x)$ so können wir die Berechnung von den Sinusglieder sparen. Die Fourier-Reihe hat dann nurnoch die folgende Form  \n$$\n\\hat{f}(x)=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{a_n \\cdot cos(nx)}\n$$  \n#### Ungerade Funktion  \nBei ungeraden Funktion, also wenn $f(-x)=-f(x)$ können wir ähnlich die Kosinusglieder weglassen.  \n$$\n\\hat{f}(x)=\\sum_{n=1}^{\\infty}{b_n\\cdot sin(nx)}\n$$  \n#### Rechteckkurve  \nWir wollen eine Fourier-Reihe der Rechteckskurve mit der Periode $T=2\\pi$ bilden.  \n$$\nf(x)=\\begin{cases}\n1 &0\\leq x \\leq \\pi\\\\\n-1 &\\pi < x < 2\\pi\n\\end{cases}\n$$  \n![rechteckkurve](/maths/rechteckkurve.png)  \nDie Funktion ist ungerade also können wir uns das Leben einfacher machen. das interessante ist bei der Berechnung das wir das Integral aufspalten können  \n$$\n\\begin{align*}\nb_n &=\\frac{1}{\\pi}\\cdot \\int_0^{2\\pi}{f(x)\\cdot sin(nx)\\,dx} \\\\\n&=\\frac{1}{\\pi}\\left[\\int_0^\\pi{1\\cdot sin(nx)\\,dx}+\\int_\\pi^{2\\pi}{(-1)\\cdot sin(nx)\\,dx}\\right]\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fourier Series", "Header 2": "Reele Darstellung", "Header 3": "Mit Periode T", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/fourier.mdx"}, "page_content": "Nicht immer ist unsere Periode $2\\pi$ deshalb wollen wir eine allgemeine Formulierung für eine Periode mit dem Wert T. Wichtig ist hier das $T=\\frac{2\\pi}{\\omega_0}$ und \\omega_0 die sog. Kreisfrequenz der Schwingung ist.  \n$$\n\\hat{f}(x)=\\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{a_n \\cdot cos(n\\omega_0x)+b_n\\cdot sin(n\\omega_0x)}\n$$  \nDaraus folgt dann  \n$$\n\\begin{align*}\na_0&=\\frac{2}{T}\\cdot \\int_{(T)}{f(x)\\,dx} \\\\\na_n&=\\frac{2}{T}\\cdot \\int_{(T)}{f(x)\\cdot cos(n\\omega_0x)\\,dx} \\\\\nb_n&=\\frac{2}{T}\\cdot \\int_{(T)}{f(x)\\cdot sin(n\\omega_0x)\\,dx} \\\\\nn&=1,2,3,...\n\\end{align*}\n$$  \nWichtig dabei ist zu beachten, dass das Integrationsinterval die Länge der Periode hat.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fourier Series", "Header 2": "Komplexe Darstellung", "Header 3": "Mit Periode 2 Pi", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/fourier.mdx"}, "page_content": "Dank der Euler-Formel können wir die Fourier-Reihe auch in komplexer Form darstellen dafür müssen wir folgendes beachten  \n$$\ncos(nx)=\\frac{1}{2}(e^{inx}+e^{-inx})\n$$  \n$$\nsin(nx)=\\frac{1}{2}i(e^{inx}+e^{-inx})\n$$  \nWir können so dann die Fourier-Reihe und die Koeffizienten Berechnung viel kürzer schreiben.  \n$$\n\\begin{align*}\n\\hat{f}&=\\sum_{n=-\\infty}^{\\infty}{c_n\\cdot e^{inx}} \\\\\nc_n&=\\frac{1}{2\\pi}\\cdot \\int_0^{2\\pi}{f(x)\\cdot e^{-inx}\\,dx} \\\\\nn&=0,\\pm 1,\\pm 2,\\pm 3,...\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fourier Series", "Header 2": "Komplexe Darstellung", "Header 3": "Mit Periode T", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/fourier.mdx"}, "page_content": "Auch hier können wir die Formel umschreiben damit wir eine beliebige Periode $T$ verwenden können.  \n$$\n\\begin{align*}\n\\hat{f}&=\\sum_{n=-\\infty}^{\\infty}{c_n\\cdot e^{in\\omega_0x}} \\\\\nc_n&=\\frac{1}{T}\\cdot \\int_0^T{f(x)\\cdot e^{-in\\omega_0x}\\,dx} \\\\\nn&=0,\\pm 1,\\pm 2,\\pm 3,...\n\\end{align*}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fourier Series", "Header 2": "Zusammenhang reele und komplexe Darstellung", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/fourier.mdx"}, "page_content": "Wir können die Koeffizienten von der einen Darstellung in die andere Darstellung umrechnen mit den folgenden Formeln", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fourier Series", "Header 2": "Zusammenhang reele und komplexe Darstellung", "Header 3": "Reele zu komplexe", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/fourier.mdx"}, "page_content": "$$\nc_0=\\frac{1}{2}a_0, \\quad c_n=\\frac{1}{2}(a_n-ib_n), \\quad c_{-n}=\\frac{1}{2}=(a_n+ib_n)\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Fourier Series", "Header 2": "Zusammenhang reele und komplexe Darstellung", "Header 3": "Komplexe zu reele", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/fourier.mdx"}, "page_content": "$$\na_0=2c_0, \\quad a_n=c_n+c_{-n}, \\quad b_n=i(c_n-c_{-n})\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "A sequence is a list of objects. Unlike a [set](/digitalGarden/maths/discrete/setTheory), the order of the objects in a sequence matters. The objects in a sequence are called elements or terms and are most commonly written between round or square brackets.\nOut of simplicity sequences are often also jut written out and separated by commas.  \n$$\n(a_1, a_2, a_3, ..., a_n) \\text{ or } [a_1, a_2, a_3, ..., a_n] \\text{ or } a_1, a_2, a_3, ..., a_n\n$$  \nA sequence can have no elements at all, in which case it is called an empty sequence and is denoted by $()$. The number of elements in a sequence is called its length. The length of a sequence can be finite or infinite.  \nUnlike a [set](/digitalGarden/maths/discrete/setTheory), the same element can appear multiple times in a sequence.  \n<Callout type=\"example\">\n- $(1, 2, 3, 4, 5)$ is a sequence with 5 elements.\n- $(1, 1, 1, 1, 1)$ is a sequence with 5 elements, all of which are the number 1.\n- $(1, 2, 3, ...)$ is an infinite sequence.\n- $()$ is an empty sequence.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Indexing", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "The elements of a sequence are indexed by natural numbers. The index of an element is the natural number that corresponds to the position of the element in the sequence. The index of the first element is usually 1 or 0, depending on the context.\nAs a computer scientist, I prefer to start counting at 0, but in mathematics, it is more common to start counting at 1.  \nThis also allows for sequences to be defined by a function:  \n$$\nf: N \\to R, n \\mapsto a_n\n$$  \nWhere the function $f$ maps the natural numbers i.e. the index to the real numbers i.e. the elements of the sequence.  \nThe index is often written as a subscript to the variable name, e.g. $a_n$ is the n-th element of the sequence.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Forms of Sequences", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "Sequences can be defined primarly in three ways: Enumeration, explicitly with a formula, or recursively.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Forms of Sequences", "Header 3": "Recursively", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "[recurrenceRelations](/digitalGarden/maths/discrete/relationsFunctions/recurrenceRelations)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Equality", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "Two sequences are equal if they have the same length and each element is equal to the corresponding element in the other sequence. Meaning that for two sequences to be equal, they must have the same elements in the same order.  \n$$\n(a_1, a_2, a_3, ..., a_n) = (b_1, b_2, b_3, ..., b_n) \\iff a_i = b_i \\text{ for all } i \\in N, 1 \\leq i \\leq n\n$$  \n<Callout type=\"example\">\n- $(1, 2, 3) = (1, 2, 3)$ because the elements are the same and in the same order.\n- $(1, 2, 3) \\neq (3, 2, 1)$ because the elements are in a different order.\n- $(1, 2, 3) \\neq (1, 2, 3, 4)$ because the sequences have different lengths.\n</Callout>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Order Pair", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "An ordered pair is a sequence of exactly two elements. This is often used to represent points in a two-dimensional space, i.e the x and y coordinates of a point\nin the Cartesian coordinate system.  \n<Image\nsrc=\"/maths/sequencesOrderPair.png\"\ncaption=\"An ordered pair (x, y) representing a point in a two-dimensional space.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "n-tuple", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "The idea of an ordered pair can be generalized for any finite number of elements a so called n-tuple.\nn-tuples are sequences of exactly n elements and are commonly seen in computer science and mathematics.  \nIn linear algebra a vector is often represented as an n-tuple. For example a 3-dimensional vector in\n3D space can be represented as a 3-tuple. This would correspond to a row-vector which we like to transpose to a column-vector.  \n<Image\nsrc=\"/maths/sequencesNTuple.png\"\ncaption=\"An n-tuple (a1, a2, a3, ..., an) representing a vector in n-dimensional space.\"\nwidth={500}\n/>", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Properties of Sequences", "Header 3": "Monotony", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "Increasing and decreasing  \nstrictly monotonically increasing  \nEine Folge $a_n$ heisst\n**Monoton wachsend**, falls für alle Folgenglieder gilt $a_n \\leq a_{n+1}$\n**Strng Monoton wachsend**, falls für alle Folgenglieder gilt $a_n < a_{n+1}$\n**Monoton fallend**, falls für alle Folgenglieder gilt $a_n \\geq a_{n+1}$\n**Strng Monoton fallend**, falls für alle Folgenglieder gilt $a_n > a_{n+1}$  \n:::note  \n$a_n=n^2$ ist streng monoton wachsend.\n$b_n={1\\over n}$ ist streng monoton fallend.\n$c_n=(-1)^n$ ist weder monoton fallend not wachsend.\n$d_n=1$ ist monoton wachsend und fallend aber nicht streng.  \n:::  \n#### Untersuchung  \nFür die Untersuchung der Monotonie werden oft Ausdrücke der Form $a_{n+1}-a_n$ oder ${a_{n+1} \\over a_n} > 1$ betrachtet.  \nZ.b wenn $a_{n+1} - a_n < 0$ (eigentlich $a_{n+1}<a_n$) gilt, ist die Folge streng monoton fallend. $a_{n+1} - a_n \\leq 0$ wäre nur monoton fallend.  \n:::note  \nZeig das die Folge $a_n={2^{n+1}\\over 3^n}$ streng monoton fallend ist.  \n1. Es muss gelten: $a_{n+1} - a_n < 0 \\iff a_{n+1}<a_n$\n1. Gleichnamig machen: ${2^{n+1}\\over 3^n} - {2^{n+2} \\over 3^{n+1}} = {3*2^{n+1}\\over 3*3^n}  - {2*2^{n+1} \\over 3^{n+1}}$\n2. Vereinfachen: $(3-2)*2^{n+1} \\over 3^{n+1}$\n3. $({2\\over 3})^{n+1}>0$\n2. Oder es muss gelten: ${a_{n+1} \\over a_n} > 1$\n1. ${{2^{n+1}\\over 3^n}\\over {2^{n+2} \\over 3^{n+1}}} = {{2^{n+1}\\over 3^n} *{3^{n+1} \\over 2^{n+2}}}$\n2. Kürzen: ${{2^{n+1}\\over 3^n} *{3*3^{n} \\over 2*2^{n+1}}}={3\\over 2} > 1$  \n:::", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Properties of Sequences", "Header 3": "Boundedness", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "Eine Folge $a_n$ heisst\n**Nach oben beschränkt**, wenn es eine Zahl $S$ gibt, so dass $a_n \\leq S$ für alle $n \\in N$ gilt. $S$ heisst eine obere Schranke der Folge.\n**Nach unten beschränkt**, wenn es eine Zahl $s$ gibt, so dass $a_n \\geq s$ für alle $n \\in N$ gilt. $s$ heisst eine untere Schranke der Folge.  \nHat eine Folge eine obere und untere Schranke ist sie eine **beschränkte** Folge.  \n:::note  \n$a_n=-2n^2+4 = 2,-4,-14,-28,...$ nach oben Beschränkt mit $S=2$\n$a_n=n-5 = -5,-4,-3,...$ nach unten Beschränkt mit $s=5$\n$a_n=(-1)^n = -1,1,-1,1,...$ Beschränkt mit $s=1$ und $S=1$\n$a_n=(-1)^n *n^2= -1,4,-9,16,...$ weder nach oben noch nach unten  \n:::", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Properties of Sequences", "Header 3": "Limits and Convergence", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": ":::note  \nFür $a_n={2n+3 \\over n}$  \n| $a_1$ | $a_2$ | $a_3$ | $a_{10}$ | $a_{1000}$ | $a_{100000}$ |\n| ----- | ----- | ----- | -------- | ---------- | ------------ |\n| 5     | 3.5   | 3     | 2.3      | 2.003      | 2.00003      |  \n**Sprechweise**:\nDie Folge $a_n$ strebt mit wachsendem $n$ gegen den Grenzwert 2.  \n**Schreibweise**:\n$$\\lim_{x \\to \\infty} {2n+3 \\over n} = 2$$\nWird gelesen als \"Limes von ${2n+3 \\over n}$ für n gegen unendlich ist 2.\"  \n:::  \n#### $\\varepsilon$-Umgebung  \n**$\\varepsilon$-Umgebung oder $\\varepsilon$-Streifen** ist ein Streifen mit einem Radius $\\varepsilon$ um den vermuteten Grenzwert. Der Index des Folgengliedes, welches als erstes im Streifen liegt, nennt man **Eintauchzahl**, $N_{\\varepsilon}$.  \n:::note  \n$a_n={2n+3 \\over n}$ mit Streifen $\\varepsilon = {1\\over 2}$\n![[Pasted image 20211016100108.png]]\nIst die Eintauchszahl $N_{1\\over 2} = 7$, weil $a_6$ liegt auf dem Streifenrand, $a_7$ jedoch darin.  \n:::  \n#### Definition  \nEine Folge $a_n$ konvergiert gegen einen **Grenzwert** $g \\in R$ wenn es zu jeder noch so kleinen Zahl $\\varepsilon >0$ eine Zahl $N_{\\varepsilon}$ gibt, so dass $|a_n -g| < \\varepsilon$ für alle $n \\geq N_{\\varepsilon}$.  \nEine Folge die einen Grenzwert $g \\in R$ besitzt, heisst **konvergent**. **Achtung** $\\infty \\notin R$!!!!!\nEine Folge die keinen Grenzwert besitzt, heisst **divergent**.  \n- Jede monoton wachsende (bzw. fallende) Folge, die beschränkt ist, ist immer konvergent.\n- Das Produkt einer beschränkten Folge und einer Nullfolge ist immer eine Nullfolge.  \n#### Untersuchen  \n:::note  \n$a_n={n-1 \\over n+2}$ vermuteter Grenzwert $g=1$\n$|a_n-g|=|{n-1\\over n+2}-1| < \\varepsilon$\n$\\Rightarrow |{n-1\\over n+2}-{n+2\\over n+2}| < \\varepsilon$\n$\\Rightarrow |{-3\\over n+2}|={|-3|\\over|n+2|} < \\varepsilon$\n$\\Rightarrow {3\\over n+2} < \\varepsilon$\n$\\Rightarrow {3\\over \\varepsilon} -2< n$  \nFür $\\varepsilon = 0.2$ erhält man $n>{3\\over 0.2} -2 = 13$ daher ab $a_14$ beträgt Differenz von Folgenglied und Grenzwert weniger als $\\varepsilon$\n$N_{0.2}=14$  \n:::", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Properties of Sequences", "Header 3": "Limits and Convergence", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "$\\Rightarrow {3\\over n+2} < \\varepsilon$\n$\\Rightarrow {3\\over \\varepsilon} -2< n$  \nFür $\\varepsilon = 0.2$ erhält man $n>{3\\over 0.2} -2 = 13$ daher ab $a_14$ beträgt Differenz von Folgenglied und Grenzwert weniger als $\\varepsilon$\n$N_{0.2}=14$  \n:::  \n#### Using Convergent Sequences  \nSind $a_n$ und $b_n$ konvergente Folgen mit den Grenzwerten $a$ bzw. $b$, so ist auch die Folge:  \n- $c*a_n$ konvergent mit $\\lim_{n \\to \\infty} {c*a_n} = c*\\lim_{n \\to \\infty} {a_n} = c*a$ für $c \\in R$\n- $a_n \\pm b_n$ konvergent mit $\\lim_{n \\to \\infty}{a_n \\pm b_n}={{\\lim_{n \\to \\infty}{a_n}} \\pm {\\lim_{n \\to \\infty}{b_n}}}={a \\pm b}$\n- $a_n *b_n$ konvergent mit $\\lim_{n \\to \\infty}{a_n* b_n}={{\\lim_{n \\to \\infty}{a_n}} * {\\lim_{n \\to \\infty}{b_n}}}={a * b}$\n- $a_n \\over b_n$ konvergent mit $\\lim_{n \\to \\infty}{a_n \\over b_n}={{\\lim_{n \\to \\infty}{a_n}} \\over {\\lim_{n \\to \\infty}{b_n}}}={a \\over b}$ falls $b \\neq 0$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Special Sequences", "Header 3": "Harmonic Sequences", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "**Nullfolge** eine Folge die den Grenzwert 0 besitzt.\n**Harmonische Folge** $a_n={1\\over n}$ ist eine Nullfolge", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Geometric Sequences", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "Folgen der Form: $a_n= a_1 *q^{n-1}$ sind geometrische Folgen.\nJedes Glied ist das geometrische Mittel seiner beiden Nachbarglieder $a_n=\\sqrt {a_{n-1}+a_{n+1}}$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Geometric Sequences", "Header 3": "Convergence of Geometric Sequences", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "Eine geometrische Folge $a_n= a_1* q^{n-1}$  \n- mit $|q|>1$ ist divergent\n- mit $|q|<1$ ist konvergent mit Grenzwert 0\n- mit $q=1$ ist eine konstante Folge $a_1$\n- mit $q=-1$ ist divergent, da alternierend.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Sequences", "Header 2": "Rational Sequences", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/sequences.mdx"}, "page_content": "Für eine rationale Folge, die im Zähler aus einem Polynom k-ten Grades\nund im Nenner aus einem Polynom l-ten Grades besteht, gilt:\n$$\n\\lim_{n \\to \\infty}{{a_kn^k+a_{k-1}n^{k-1}+...+a_0}\\over{b_ln^l+b_{l-1}n^{l-1}+...+b_0}} = \\begin{dcases}\n{a_k\\over b_k} *\\infty, falls\\space k >l \\\\\n{a_k\\over b_k} , falls\\space k=l \\\\\n0 , falls\\space k<l\n\\end{dcases}\n$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Convergence of Series", "Header 2": "Reihe einer Folge", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/convergence.mdx"}, "page_content": "Bei einer reellen Folge $a_1, a_2, a_3,...$ wird die zugehoörige Folge der Teilsummen $s_n$ eine Reihe der Folge $a_n$ genannt:\n$$s_n = a_1+a_2+a_3+... +a_n = \\sum^{\\infty}_{k=1}{a_k}$$\n$s_n$ wird auch $n$−te Teilsumme der Folge $a_n$ genannt.  \n**Unendliche Reihe** besitzt unendlich viele Glieder. Also wenn $\\sum^{\\infty}$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Convergence of Series", "Header 2": "Konvergenz", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/convergence.mdx"}, "page_content": "**Konvergent Reihe** = wenn eine Reihe den Grenzwert $\\lim\\limits_{n \\rightarrow \\infty}{s_n}=s=\\sum^{\\infty}_{k=1}{a_k}$\neine nicht-konvergente Reihe heisst divergent.\n**Notwendiges Konvergenzkriterium für Reihen**\nDamit eine Reihe überhaupt konvergent sein kann, muss die entsprechende Folge eine Nullfolge sein. Ist sie nicht eine Nullfolge so it die REihe garantiert divergent.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Convergence of Series", "Header 2": "Rechenregeln für konvergente Reihen", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/convergence.mdx"}, "page_content": "Sind $\\sum^{\\infty}_{k=1}{a_k}$ und $\\sum^{\\infty}_{k=1}{b_k}$ konvergente Reihen so gilt:  \n1. $\\sum^{\\infty}_{k=1}{c*a_k}=c* \\sum^{\\infty}_{k=1}{a_k}$ für $c \\in R$\n2. $\\sum^{\\infty}_{k=1}{a_k\\pm b_k}=\\sum^{\\infty}_{k=1}{a_k}\\pm \\sum^{\\infty}_{k=1}{b_k}$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Convergence of Series", "Header 2": "Geometrische Reihe", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/convergence.mdx"}, "page_content": "Reihe einer [[2-Folgen#Geometrsiche Folge]]] hat die Form:  \n$\\sum^{\\infty}_{k=1}{a_1*q^{k-1}}=a_1*(a+q+q^2+...)= {{a_1*(1-q^n)} \\over 1-q}$  für $q\\in R, q\\neq 0$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Convergence of Series", "Header 2": "Geometrische Reihe", "Header 3": "Konvergenz einer geometrischen Reihe", "path": "../pages/digitalGarden/maths/calculus/sequencesAndSeries/convergence.mdx"}, "page_content": "Eine geometrische Reihe ist für alle $q \\in R$ und $|q| < 1$ konvergent mit dem Grenzwert: $a_1\\over 1-q$. Für $|q|\\geq 1$ ist die geometrsiche Reihe divergent.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration by Partial Fraction Decomposition", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationByPartialFractions.mdx"}, "page_content": "Integration durch Partialbruchzerlegung ist eine spezielle Integrationsmethode die für echt gebrochen rationale Funktionen entwickelt wurde. Ist eine Funktion unecht gebrochen, muss sie zuerst in eine ganzrationale und eine echt gebrochen rationale Funktionen zerlegt werden mit der Polynomdivision. Diese Umwandlung ist immer möglich.  \nEin Video zu der Polynomdivision gibt es [hier](https://www.youtube.com/watch?v=j_f9itXqZHs)  \nIst der Grad $m$ des Nenners größer als der Grad $n$ des Zählers, so heißt die rationale Funktion $f(x)$ echt gebrochen.  \n$$\\text{echt gebrochene Funktion: }f(x)=\\frac{x^3+x^2+x+1}{x^4+3x+3}$$  \n$$\\text{unecht gebrochene Funktion: }f(x)=\\frac{x^3+x^2+x+1}{x^2+5x+1}$$  \nIm Nenner sollte ausserdem eine Linearfaktorzerlegung sein. Ist dies nicht der Fall kann dies schnell erreicht werden, indem man die Nullstellen herausfindet.  \nEin Video zu der Linearfaktorzerlegung gibt es [hier](https://www.youtube.com/watch?v=wRc6AtV7HmY)  \nDanach kann die Partialbruchzerlegung einfach gemacht werden und von jedem partiellen Bruch das Integral berechnen dank der Summenregel.  \nEin Video zu dem ganzen Prozess findest du [hier](https://www.youtube.com/watch?v=E39ne4lX8s8)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration by Substitution", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationBySubstitution.mdx"}, "page_content": "Bei der Integration durch Substitutionen wollen wir mit Hilfe von geeigneten Variabel-Substitutionen das Integral vereinfachen oder wenn möglich sogar zu einem Grundintegral umwandeln.  \nAm besten können wir diese Methode verwenden wenn wir den folgenden Fall haben  \n$$\\int{f(x)\\,dx}=\\int{f(g(x))\\cdot g(x)'\\,dx}$$  \nalso wenn wir eine Verkettung von Funktionen haben und die innere Funktion abgeleitet im Integral vorkommt. Ein häufiges Beispiel ist  \n$$\\int{x\\cdot e^{x^2}}$$  \nweil es nicht normalerweise lösbar ist. Hier ist $g(x)=x^2$ was abgeleitet zu $g'(x)=2x$ wird wir haben aber nur $x$ nicht $2x$. Grund dafür ist die Faktorregel welche besagt das wir die 2 ja herausnehmen können, deshalb können wir Konstanten bei der obigen Voraussetzung ignorieren.  \nDer erste Schritt haben wir schon gemacht wir haben unsere variable zum Substituieren identifiziert $u=x^2$. Wir müssen aber alles was mit der alten Variable zu tun haben ersetzen, inklusive das $dx$. Um dies zu erreichen benutzen wir noch die folgende Formel $dx=\\frac{du}{u'}=\\frac{du}{2x}$.  \nNun können wir in der Formel alles ersetzen  \n$$\\int{x\\cdot e^{x^2}}=\\int{x\\cdot e^u\\, \\frac{du}{2x}}$$  \nDank der obigen Voraussetzung lässt sich das vordere $x$ wegkürzen.  \n$$\\int{\\frac{e^u\\,du}{2}}=\\int{\\frac{1}{2}\\cdot e^u\\,du}=\\frac{1}{2}\\int{e^u\\,du}$$  \nNun haben wir ein Grundintegral und wir wissen das $e^u$ abgeleitet/integriert $e^u$ bleibt können wir das Integral lösen  \n$$\\frac{1}{2}\\int{e^u\\,du}=\\frac{1}{2}e^u +C$$  \nOftmals will man noch die originale Variable beibehalten, dafür macht man dann eine Rücksubstitution.  \n$$\\frac{1}{2}e^u +C=\\frac{1}{2}e^{x^2} +C$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration by Substitution", "Header 2": "Integration durch Substitutionen eines bestimmten Integrals", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationBySubstitution.mdx"}, "page_content": "Bei einem bestimmten Integral gehen wir genau gleich vor wie bei einem unbestimmten jedoch haben wir noch 2 weitere Schritte. Zwar müssen wir die Grenzen auch ersetzen und am Schluss dann das Integral ausrechnen. Dafür verwenden wir das folgende Beispiel  \n$$\\int_{0}^{1}{x\\cdot \\sqrt{1+x^2}}$$  \nWir setzen $u=1+x^2$ und somit auch $dx=\\frac{du}{x'}=\\frac{du}{2x}$  \nNun müssen wir die Grenzen noch ersetzen. Für die untere Grenze ist $x=0$ und somit dann $u=1+0^2=1$. Für die obere Grenze $x=1$ und somit $u=1+1^2=2$. Nun können wir alles ersetzen.  \n$$\\int_{0}^{1}{x\\cdot \\sqrt{1+x^2}}=\\int_{u=1}^{u=2}{x\\cdot \\sqrt{u}\\,\\frac{du}{2x}}=\\int_{1}^{2}{\\frac{1}{2}\\cdot \\sqrt{u}\\,du}=\\frac{1}{2}\\int_{1}^{2}{(u)^{\\frac{1}{2}}\\,du}$$  \nWeil $(\\frac{2}{3}u^{\\frac{3}{2}})'=(u)^{\\frac{1}{2}}$ können wir schreiben  \n$$\\frac{1}{2}\\cdot\\Big|\\frac{2}{3}u^{\\frac{3}{2}}\\Big|_1^2=\\frac{1}{3}\\cdot\\Big|\\sqrt{u^3}\\Big|_1^2=\\frac{1}{3}(\\sqrt{8}-1)$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Definite Integrals", "path": "../pages/digitalGarden/maths/calculus/integrals/definiteIntegrals.mdx"}, "page_content": "[link](https://www.youtube.com/watch?v=wQTOV2zqHFE)\n[link](https://www.youtube.com/watch?v=lP1sALCSxQs)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration Rules", "Header 2": "Faktorregel", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationRules.mdx"}, "page_content": "Ein konstanter Faktor, $C \\in \\mathbb{R}$, darf vor das Integral gezogen werden  \n$$\\int_{a}^{b}{C \\cdot f(x)\\,dx}=C\\cdot \\int_{a}^{b}{f(x)\\,dx}$$  \nDiese Regel gilt auch für unbestimmte Integrale.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration Rules", "Header 2": "Summenregel", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationRules.mdx"}, "page_content": "Eine endliche Summe von Funktionen darf gliedweise integriert werden  \n$$\\int_{a}^{b}{(f_1(x)+...+f_n(x))\\,dx}=\\int_{a}^{b}{f_1(x)\\,dx}\\,+...+ \\int_{a}^{b}{f_n(x)\\,dx}$$  \nDiese Regel gilt auch für unbestimmte Integrale.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration Rules", "Header 2": "Vertauschungsregel", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationRules.mdx"}, "page_content": "Wenn man die beiden Integrationsgrenzen vertauscht bewirkt dies ein Vorzeichenwechsel des Integrals  \n$$\\int_{b}^{a}{f(x)\\,dx}=-\\int_{a}^{b}{f(x)\\,dx}$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration Rules", "Header 2": "Gleiche Integrationsgrenzen", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationRules.mdx"}, "page_content": "Falls die Integrationsgrenzen gleich sind also $a=b$, dann ist der Integralwert gleich 0. Dies macht auch Sinn wenn sich das Integral als Fläche unter der Funktionskurve vorstellt.  \n$$\\int_{a}^{a}{f(x)\\,dx}=0$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration Rules", "Header 2": "Zerlegung des Integrationsintervalls", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationRules.mdx"}, "page_content": "Für jede Stelle $c$ aus dem Integrationsintervall $a\\leq c \\leq b$ gilt  \n$$\\int_{a}^{b}{f(x)\\,dx}=\\int_{a}^{c}{f(x)\\,dx}+\\int_{c}^{b}{f(x)\\,dx}$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Improper Integrals", "path": "../pages/digitalGarden/maths/calculus/integrals/improperIntegrals.mdx"}, "page_content": "[link](https://studyflix.de/mathematik/uneigentliche-integrale-1806)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Antiderivatives", "path": "../pages/digitalGarden/maths/calculus/integrals/antiderivatives.mdx"}, "page_content": "indefinite integral of a function is the antiderivative of that function.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Double Integrals", "path": "../pages/digitalGarden/maths/calculus/integrals/doubleIntegrals.mdx"}, "page_content": "Ein gutes Video zu wie man das macht findest du [hier](https://www.youtube.com/watch?v=n8NDyxOMVEw)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Numerical Integration", "path": "../pages/digitalGarden/maths/calculus/integrals/numericalIntegration.mdx"}, "page_content": "Die numerische Integration wird genutzt, wenn sich eine Stammfunktion nicht durch elementare Funktionen ausdrücken lässt, die numerische Auswertung der Stammfunktion zu komplex oder zeitaufwendig ist. In diesen Fällen verwendet man spezielle Näherungsverfahren, die sog. numerische Integration.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Numerical Integration", "Header 2": "Trapezformel", "path": "../pages/digitalGarden/maths/calculus/integrals/numericalIntegration.mdx"}, "page_content": "Wir setzen voraus, dass die stetige Funktion $f(x)$ im Integrationsintervall $a\\leq x \\leq b$ oberhalb der x-Achse verläuft damit wir es uns besser vorstellen können jedoch muss dies nicht der Fall sein um die Trapezformel zu verwenden.  \nWir zerlegen zuerst das Integrationsintervall in $n$ Teilintervalle gleicher Länge  \n$$h = \\frac{b-a}{n}$$  \nDie Punkte der Teilintervalle auf der x-Achse nennen wir Stützstellen.  \n$$x_0=a,\\,x_1=x_0+h,...,x_k=x_0+k\\cdot h,...,x_n=b$$  \nDie Funktionswerte der Stützstellen nennen wir Stützwerte.  \n$$y_k=f(x_k)=f(x_0+k\\cdot h)=f(a+k\\cdot h)\\text{ mit } k=0,1,...,n$$  \nMit der Formel für den Flächeninhalt eines geometrischen Trapez $A=\\frac{a+b}{2}h$ können wir die Fläche des ersten Trapezes berechnen $A_1=\\frac{y_0+y_1}{2}h$ und genau so dann die restlichen Flächen der Trapeze  \n$$A_n=\\frac{y_{n-1}+y_n}{2}h$$  \n![trapezFlaecheFormel](/maths/trapezFlaecheFormel.png)\n![trapezFormel](/maths/trapezFormel.png)  \nDie Summe aller Trapezflächen geben dann eine gute Näherung für den gesuchten Flächenwert. Welches zu einer ziemlich kurzen Formel zusammen gezogen werden kann indem man die Brüche gleichnamig macht und ein wenig ausklammert und faktorisiert.  \n$$\n\\begin{align*}\n\\int_{a}^{b}{f(x)\\,dx}&\\approx A_1+...+A_n \\\\\n&= \\frac{y_0+y_1}{2}h + \\frac{y_1+y_2}{2}h +... + \\frac{y_{n-1}+y_n}{2}h \\\\\n&= (y_0+2y_1+2y_2+...+2y_{n-1}+y_n)\\frac{h}{2} \\\\\n&= (\\frac{1}{2}\\sum{}_1 + \\sum{}_2)h\n\\end{align*}\n$$  \nwobei $\\sum{}_1=$ Die Summe der beiden äusseren Stützwerte $(y_1,y_n)$\nund $\\sum{}_2=$ Die Summe der inneren Stützwerte $(y_2,...y_{n-1})$.  \nWenn $n \\to \\infty$ dann liefert die Trapezformel den exakten Integralwert.  \nMehr dazu kann man auch [hier](https://www.youtube.com/watch?v=BZbzdsvpc3c) finden.", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Numerical Integration", "Header 2": "Simpsonsche Formel", "path": "../pages/digitalGarden/maths/calculus/integrals/numericalIntegration.mdx"}, "page_content": "[link](https://www.youtube.com/watch?v=N0kFSTDvDcw)\n[link](https://www.youtube.com/watch?v=VDfWERtVN9Y)", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integrals Explained", "path": "../pages/digitalGarden/maths/calculus/integrals/explained.mdx"}, "page_content": "In der Differentialrechnung ist eine Funktion $f$ gegeben und deren Ableitung $f'$ gesucht. In der Integralrechnung ist es genau anders herum. Die Ableitung $f'$ ist gegeben und die Funktion $f$ wird gesucht. Z.B. Wenn $f'(x)=2x$ ist dann wissen wir das $f(x)=x^2$ ist. Die gesuchte Funktion beim integrieren wird auch oft als die Stammfunktion $F$ genannt, beachte hier ist es ein Grossbuchstabe. Wir sehen also, dass das Integrieren die Umkehrfunktion zum Ableiten.  \n$$F(x)=f(x)=\\int{f'(x)\\,dx}$$  \nZu unserem oberen Beispiel können wir auch noch eine Konstante $C$ hinzufügen und es ist immernoch eine Stammfunktion also  \n$$f(x)=x^2+3 \\Rightarrow f'(x)= 2x$$  \nWir können also sehen, dass es zu jeder stetigen Funktion $f(x)$ unendlich viele Stammfunktionen gibt. Das heisst dann wiederum, dass zwei beliebige Stammfunktionen $F_1(x)-F_2()=Konstante$ sich nur um eine Konstante unterscheiden. Wir beschreiben also die Menge aller Stammfunktionen als  \n$$F(x)=F_1(x)+C$$", "type": "Document"}
{"id": null, "metadata": {"Header 1": "Integration by Parts", "path": "../pages/digitalGarden/maths/calculus/integrals/integrationByParts.mdx"}, "page_content": "Die Partielle Integrationsmethode wird auch oft Produkt integration genannt. Wir können diese Integrationsmethode wenn schon aus einem Produkt von 2 Funktionen besteht oder es als Produkt von 2 Funktionen darstellbar ist z.B. $\\int{x\\cdot e^x \\,dx}$  \n$$\\int{f(x)\\,dx}=\\int{u(x)\\cdot v(x)\\,dx}$$  \nWichtig dabei ist, dass auch eines der Faktoren einfach integrierbar ist, wir sehen also schnell das eines der Faktoren eine Ableitung ist.  \nAus der Produktregel der Differentialrechnung können wir folgendes bilden  \n$$\n\\begin{align*}\n&(u(x)\\cdot v(x))'=u'(x)\\cdot v(x)+u(x)\\cdot v'(x) \\\\\n&\\Rightarrow u(x)\\cdot v'(x)=(u(x)\\cdot v(x))'-u'(x)\\cdot v(x)\n\\end{align*}\n$$  \nUnbestimmte Integration auf beiden Seiten führt dann zu  \n$$\n\\begin{align*}\n&\\int{u(x)\\cdot v'(x)\\,dx}= \\int{(u(x)\\cdot v(x))'\\,dx}-\\int{u'(x)\\cdot v(x)\\,dx} \\\\\n&\\Rightarrow \\int{u(x)\\cdot v'(x)\\,dx}= u(x)\\cdot v(x)-\\int{u'(x)\\cdot v(x)\\,dx}\n\\end{align*}\n$$  \nMit dieser Formel kann man dann die Integration lösen wenn man $u(x)$ ableitet und $v(x)$ integriert.  \nGenau so kann man auch vorgehen wenn es ein bestimmtes Integral ist nur ist es dann  \n$$\\int_a^b{u(x)\\cdot v'(x)\\,dx} = \\Big|u(x)\\cdot v(x)\\Big|_a^b-\\int_a^b{u'(x)\\cdot v(x)\\,dx} $$  \nMehr dazu findest du auch [hier](https://studyflix.de/mathematik/partielle-integration-1862)  \n:::note Beispiel Partielle Integration  \nWir wollen das follgende Problem lösen  \n$$\\int{x\\cdot e^x \\, dx}=?$$  \nZuerst zerlegen wir den Integrand wie oben beschrieben.  \n- $u(x)=x$\n- $u'(x)=1$\n- $v'(x)=e^x$\n- $v(x)=e^x$  \nAus der Formel können wir dann folgende berechnen  \n$$\\begin{align*}\n\\int{x\\cdot e^x \\, dx}&=x \\cdot e^x - \\int{1 \\cdot e^x \\,dx} \\\\\n&\\Rightarrow x \\cdot e^x - e^x + C = (x-1) \\cdot e^x + C\n\\end{align*}$$  \n:::", "type": "Document"}
{"id": null, "metadata": {"Header 1": "List of Integrals", "path": "../pages/digitalGarden/maths/calculus/integrals/listOfIntegrals.mdx"}, "page_content": "Hier hast du eine Auflistung von allen [Grundintegrale](https://de.wikipedia.org/wiki/Tabelle_von_Ableitungs-_und_Stammfunktionen)", "type": "Document"}
