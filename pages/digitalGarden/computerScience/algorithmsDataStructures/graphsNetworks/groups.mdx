import Callout from "../../../../../components/Callout/Callout";

## Neighborhoods

## Communities

Communities are subgraphs (subsets or groups of vertices of the original graph), that are better connected to each
other than to the rest of the graph. Communities are very important when analyzing social networks and networks in
general as they often form around a context or a topic such as family, friends, work, hobbies, etc.

These communities can then be further analyzed such as to find out who are the most important people in a community,
what is there impact on the community, and how do they relate to other communities.

![graphCommunities](/compSci/graphCommunities.png)

### Connected Components

Of all the communities, the most basic one with the simplest definition is the connected component. If a connected
component includes a large portion of the graph, then it is commonly referred to as the giant component.

<Callout type="todo">
    Code or algorithm to find connected components
</Callout>

#### Undirected Graphs

In an undirected graph, a connected component is a subset of vertices such that there is a path between every pair of
vertices in the subset. In other words, a connected component is a subgraph of the original graph where all vertices
are connected to each other.

This could be useful for example to find out if a graph is fully connected or not. If the graph has only one connected
component, then it is fully connected. If it has more than one connected component, then it is not fully connected.

If we think of a communication network, then a connected component would be a group of people that can communicate with
each other. If there are multiple connected components, then there are groups of people that cannot communicate with
each other.

![connectedComponents](/compSci/connectedComponents.png)

#### Directed Graphs

In a directed graph the directions of the edges matter. This gives us two types of connected components, weakly
connected components and strongly connected components.

##### Weakly Connected Components

Weakly connected components are the same as connected components in an undirected graphs. So you just ignore the
directions of the edges.

![weaklyConnectedComponents](/compSci/weaklyConnectedComponents.png)

##### Strongly Connected Components

Strongly connected components are a bit more complex. In a directed graph, a strongly connected component is a subset of
vertices such that there is a path between every pair of vertices in the subset, but the path must follow the direction
of the edges.

![stronglyConnectedComponents](/compSci/stronglyConnectedComponents.png)

### Cliques

Cliques focus on undirected graphs. A clique is a complete subgraph of the original graph, i.e a subgraph where all
vertices are connected to each other. Cliques are very important in social networks as they represent groups of people
that all know each other, however in a communication network they would represent a group with redundant connections.

Because cliques are complete subgraphs, they are very easy to find but also happen to be very rare. In the graph below
the two cliques have been highlighted in red and blue.

![cliques](/compSci/cliques.png)

<Callout type="todo">
    Code or algorithm to find cliques
</Callout>

#### k-Core

For a k-Core the rules of a clique are slightly relaxed. A k-Core is a subgraph where all vertices are at least connected
to $k$ other vertices in the subgraph.

<Callout type="todo">
    Visualization of k-Core
</Callout>

Although this is a relaxation of the rules, it is still a very strict rule and
can lead to vertices that don't fulfill the $k$ connections but are only connected to other vertices in a core to not
be included in the core.

<Callout type="todo">
    Show the issue + maybe algorithm to find k-Core
</Callout>

<Callout type="todo">
    Degeneracy of a graph and k-degenerate graphs is exactly this
</Callout>

#### p-Clique

The idea of a p-clique is also to relax the rules of a clique whilst also solving the above-mentioned issue of the
k-core. In a p-clique, the p stands for a percentage in decimal i.e. a ratio. So in a p-clique at least the given
percentage of vertices must be connected to other vertices in the subgraph.

So if we have a 0.5-clique, then at least 50% of the vertices must be connected to other vertices in the subgraph. This
then allows for the vertices that don't fulfill the rule to be included in the subgraph for a k-core but are only
connected to other vertices in the subgraph to be included in the subgraph.

<Callout type="todo">
    Visualization of p-Clique
</Callout>

#### n-Clique

<Callout type="warning">
    Sometimes cliques are named after the number of vertices they contain. For example a clique with 3 vertices is
    called a 3-clique, a clique with 4 vertices is called a 4-clique, etc. this can be generalized to a k-clique. Not an
    n-clique though, that is something else, but when it just says 4-clique it can be ambiguous.
</Callout>

The idea of an n-clique is that we want a maximal subgraph, i.e. with the most vertices, where each pair of vertices can
be connected by a path of length at most n. So a 1-clique is just a normal clique, a 2-clique is a clique where each
pair of vertices can be connected by a path of length at most 2, etc.

<Callout type="warning">
    The path doesn't have to be the shortest path, just a path of length at most n. And the path can go over any vertex,
    not just vertices that are part of the clique.
</Callout>

This can lead to two interesting scenarios:

1. The diameter of the subgraph can actually be longer then n. This is due to the path being able to go over any vertex,
not just vertices that are part of the clique. So in the example below, the diameter of the subgraph is 3 even though it
is a 2-clique.

![nCliqueDiameter](/compSci/nCliqueDiameter.png)

2. The subgraph can be disconnected. In the example below you can see two possible 2-cliques of many for the given graph.
Interestingly, they are both disconnected, because if one of the vertices inbetween is included, then a different vertex
can no longer be included.

![nCliqueDisconnected](/compSci/nCliqueDisconnected.png)


### Clusters

In general clustering is the process of grouping similar objects together. In graph theory, the clustering process can
be seen as a way to group vertices together i.e. to find communities that aren't based on specific rules like cliques
or connected components.

There are two main approaches to clustering graphs:

- bottom-up: start with each vertex in its own cluster and then merge clusters together
- top-down: start with all vertices in one cluster and then split the cluster into smaller clusters

#### Girvan-Newman Clustering

The Girvan-Newman clustering algorithm is a bottom-up approach to clustering which is based on edge betweenness, hence
it is also called edge betweenness clustering. The idea is to iteratively calculate the edge-betweenness of each edge in
the graph and then remove the edge with the highest edge-betweenness.

<Callout type="todo">
    Visualization of Girvan-Newman Clustering & link to edge-betweenness, this chapter then probably needs to be moved
    after the edge-betweenness chapter. Betweenness centrality is the wikipedia article that explains it best.
</Callout>

The thought process behind this is that the edges with the highest edge-betweenness are the edges that have the highest
information flow. So by removing these edges, we are removing the edges that connect two groups/clusters/communities
together. Eventually this will lead to two components, which are then the clusters.

<Callout type="todo">
    Code?
</Callout>

The issue with this approach is that it is very computationally expensive. The edge-betweenness of each edge has to be
calculated, which is $O(|V||E|)$ and then that has to be done iteratively multiple times so the overall complexity can
be summarized to $O(n^3)$ which is not ideal for large graphs.

#### LPA - Label Propagation Algorithm

The LPA is a more general algorithm which doesn't have to just be used for clustering graphs it can also be used to
cluster data in general. However, I will explain it in the context of graph clustering.

<Callout type="todo">
    Maybe one day this can be done in the context of semi-supervised labeling
</Callout>

LPA consists of 2 parts, the preparation and the actual algorithm. In the preparation we do the following:

1. We assign each vertex a unique label from $0$ to $|V| - 1$. The labels in the end will be the clusters, which makes
this a bottom-up approach.

2. We perform graph coloring. I will not go into detail about graph coloring here, but the idea is to color the graph
such that no two connected/neighboring vertices have the same color whilst using the least amount of colors possible.

<Callout type="todo">
    Maybe add a link to the graph coloring chapter if it ever gets written.
</Callout>

Once the preparation is done, we can start the actual algorithm. The algorithm is very simple:

For each color (always in the same order) we go through each vertex (also always in the same order) and check the
labels of its neighbors and count how many times each one occurs. If there is a label that occurs more often than the
others, then we assign that label to the vertex. If there are multiple labels that occur the same amount of times, then
there are two options:

- If the vertexes label is one of the labels that occur the most, then we keep the label.
- If the vertexes label is not one of the labels that occur the most, then we assign it the label with the highest value.
Lowest would also work, as long as it is consistent.

This is repeated until the labels don't change anymore. The labels in the end then represent the clusters. The algorithm
is very simple and fast making it a good choice for large graphs. However, it is not
deterministic, i.e. it can lead to different results depending on the order of the colors and vertices. This can be
mitigated by running the algorithm multiple times and then taking the most common result.

<Callout type="todo">
    Code and visualization, how does it work with directed graphs?
</Callout>

#### Louvain Clustering

The Louvain clustering algorithm is a bottom-up greedy approach to clustering which is based on modularity. So we first
need to understand what modularity is.

##### Modularity

Modularity is a metric that measures the quality of a clustering. The idea is to compare the number of edges within a
cluster with the number of edges between clusters. A good clustering would then have a lot of edges within a cluster
and not many edges between clusters.

<Callout type="todo">
    Finish this seems annoying
</Callout>

##### The Louvain Algorithm

The Louvain algorithm then tries to maximize the modularity of a graph in an iterative process until the modularity
cannot be increased anymore, hence it is a greedy approach.

Initially each vertex is in its own cluster. We then iteratively perform the following steps:

- **Modularity Optimization:** For each vertex we check how the modularity would change if we would
move it to a neighboring cluster. If the modularity would increase, then we move the vertex to the neighboring cluster
which would increase the modularity the most. If the modularity would not increase, then we leave the vertex in its
current cluster. Once we have gone through all vertices, we move on to the next step.
- **Cluster Aggregation:** We then aggregate all vertices in the same cluster into a single vertex. This vertex has a
self-looping edge with a weight equal to the sum of all the edges of the vertices in the cluster. The vertices resembling
the clusters are then connected to each other with edges of weight equal to the sum of all the edges between the
clusters before the aggregation. We then go back to the first step and repeat the process until the modularity cannot be
increased anymore.

#### Clustering Coefficient

## Graph Connectivity

### Bridges

### Cut Vertices


