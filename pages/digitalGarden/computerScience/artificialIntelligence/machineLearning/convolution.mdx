
# Convolution

## Of Vectors

For Deep learning.

Is just cross-correlation. Dot product of the 2 vectors after flattening the matrix (patch and kernel). Theorectically the one vector should be flipped upside down for 
it to not be a cross-correlation but nobody cares.

This is associtiave in regards to applying filters? Where as cross-correlation isnt?

https://www.youtube.com/watch?v=4ERudRAxyGE this connects the vectors with the gradients

for imag eprocessing we just flip the kernel not the image. Has impact on output size hence padding stride etc. IF we use convolution and not cross correlation i.e do flipping
it allows us to do fft? 

## Of Functions

For Signal processing.

Discrete vs continuos is very different.

of 2 functions is then another function where one of them is flipped on y axis and then slowly slid along the x axis over the other function.
Should probably both be scaled to the same units then can interpret it better? But If resulting funciton is same as an input then the two inputs 
are the same??? How to interpret cross correlation.

# Cross Correlation

## Of Vectors

See above.

## Of Functions

Same but the function isnt flipped.

## Of Time Series

How is this interpretetd. Should say somethign about lags no? Because one is slid across the other it is correlation of one time series with the lagged time series of the other.

# Auto Correlation

Is Cross Correlation but the two functions are the same so a cross correlation with itself. How is this interpretetd? Same goes here for time series is this interpreted?
Because it is slid across it is basically the correlation with the lagged version of it self. so each point is correlation of the series with the x lag of itself.

## Partial Auto Correltion

a, b, c. a with b are correlated and b with c, to measure true influence of c on a  need to take out the influence b had on a. Taking out inluence of other timespots on current.

AR model or MA model https://medium.com/@vaibhav1403/ar-model-vs-ma-model-427ee28587a#:~:text=How%20they%20differ%3A,white%20noise%20or%20error%20terms.

1 lag differencing is a way to detrend. subtract lag 1 values.

first order AR model is if lag 1 is used, second order if lag 2 etc. How many points are taken = order, simpler models prefered. Significance threhsold, how to get?

AR model uses partial AC, MA model uses full AC
https://www.youtube.com/watch?v=5Q5p6eVM7zM

Also known as Conditional correlation. Take out all intermediate effects.

https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/

correlogram with Confidence intervals are drawn as a cone. By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this code are very likely a correlation and not a statistical fluke.


The partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.

The autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.

It is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation

how it is calcualted: https://timeseriesreasoning.com/contents/partial-auto-correlation/