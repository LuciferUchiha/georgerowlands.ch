{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ee2235-a728-4a58-992d-2eda1d0e20bd",
   "metadata": {},
   "source": [
    "import Image from \"~/components/Image/Image\";\n",
    "\n",
    "# Eigenvalues and Eigenvectors\n",
    "\n",
    "Before we talk about eigenvalues and eigenvectors let us just remind ourselves that vectors can be transformed using matrices. For example we can rotate a vector using the rotation matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  \\cos\\theta & -\\sin\\theta \\\\\n",
    "  \\sin\\theta &  \\cos\\theta \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "  x \\\\\n",
    "  y \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  x' \\\\\n",
    "  y' \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<Image \n",
    "    src=\"/maths/vectorTransformationRotation2D.png\" \n",
    "    caption=\"Rotating a 2D vector by the angle theta\"\n",
    "    width={400}\n",
    "/>\n",
    "\n",
    "Or we can use a matrix to scale a vector:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  2 & 0 \\\\\n",
    "  0 & 2 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "  4 \\\\\n",
    "  3 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  8 \\\\\n",
    "  6 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<Image \n",
    "    src=\"/maths/vectorTransformationScaling2D.png\" \n",
    "    caption=\"Scaling a 2D vector, in this case doubling its length\"\n",
    "    width={300}\n",
    "/>\n",
    "\n",
    "Now let us go back to eigenvalues and eigenvectors. An eigenvector $\\boldsymbol{v}$ of a square matrix $\\boldsymbol{A}$ is defined as a non-zero vector such that the multiplication with $\\boldsymbol{A}$ only changes the scale of the vector it does not change the direction. The scalar $\\lambda$ is called the eigenvalue.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{Av}=\\lambda \\boldsymbol{v}\n",
    "$$\n",
    "\n",
    "Because there would be an infinite amount of solutions we limit the magnitude of the vector to $\\parallel\\boldsymbol{v}\\parallel_2=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afce862",
   "metadata": {},
   "source": [
    "Let us look at an example of how to calculate the eigenvector and eigenvalue of\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}=\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 \\\\\n",
    "  -2 & -3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For this we can rewrite the problem and solve the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{Av}=\\lambda \\boldsymbol{v} \\\\\n",
    "\\boldsymbol{Av} - \\lambda \\boldsymbol{v} = 0 \\\\\n",
    "\\boldsymbol{Av} - \\lambda \\boldsymbol{Iv} = 0\n",
    "(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For there to be a solution where $\\boldsymbol{v}$ is non-zero then the following must be true and which then must lead to the characteristic polynomial of $\\boldsymbol{A}$. Solving the characteristic polynomial equaling 0 we can get between 0 and $n$ eigenvalues with $n$ being the number of dimensions of $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "det(\\boldsymbol{A}-\\lambda\\boldsymbol{I}) &= 0 \\\\\n",
    "det\\big(\n",
    "    \\begin{bmatrix}\n",
    "      0 & 1 \\\\\n",
    "      -2 & -3 \\\\\n",
    "    \\end{bmatrix}\n",
    "    - \\begin{bmatrix}\n",
    "      \\lambda & 0 \\\\\n",
    "      0 & \\lambda \\\\\n",
    "    \\end{bmatrix}\n",
    "\\big) &= 0 \\\\\n",
    "det\\big(\n",
    "    \\begin{bmatrix}\n",
    "      -\\lambda & 1 \\\\\n",
    "      -2 & -3-\\lambda \\\\\n",
    "    \\end{bmatrix}\n",
    "\\big) &= \\lambda^2+3\\lambda+2=0 \\\\\n",
    "&\\lambda_1 = -1,\\,\\lambda_2 = -2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now that we have the eigenvalues all we need to do is calculate the eigenvectors corresponding to each eigenvalue.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} &= 0 \\\\\n",
    "\\big(\\begin{bmatrix}\n",
    "      0 & 1 \\\\\n",
    "      -2 & -3 \\\\\n",
    "\\end{bmatrix}\n",
    "- \\begin{bmatrix}\n",
    "      -1 & 0 \\\\\n",
    "      0 & -1 \\\\\n",
    "\\end{bmatrix} \\big)\n",
    "\\begin{bmatrix}\n",
    "      v_1 \\\\\n",
    "      v_2 \\\\\n",
    "\\end{bmatrix} &= 0 \\\\\n",
    "\\begin{bmatrix}\n",
    "      1 & 1 \\\\\n",
    "      -2 & -2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "      v_1 \\\\\n",
    "      v_2 \\\\\n",
    "\\end{bmatrix} &= 0 \\\\\n",
    "\\begin{bmatrix}\n",
    "      v_1 + v_2 \\\\\n",
    "      -2v_1 -2v_2 \\\\\n",
    "\\end{bmatrix} &= 0 \\\\\n",
    "&\\Rightarrow v_1 = -v_2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we know $v_1 = -v_2$ since we constrict ourselves to vectors with a magnitude of 1 so $\\sqrt{v_1^2 + (-v_1)^2}=1$ we get for eigenvalue $\\lambda_1=-1$ the eigenvector\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}=\n",
    "\\begin{bmatrix}\n",
    "      0.707107 \\\\\n",
    "      -0.707107 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can also calculate this using the following numpy code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a733566-f096-4539-a5a9-5dec8dbd4e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [-1. -2.]\n",
      "Eigenvectors: [[ 0.70710678 -0.4472136 ]\n",
      " [-0.70710678  0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[0, 1], [-2, -3]])\n",
    "e_values, e_vectors = np.linalg.eig(A)\n",
    "print(f\"Eigenvalues: {e_values}\")\n",
    "print(f\"Eigenvectors: {e_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403f470",
   "metadata": {},
   "source": [
    "## Properties\n",
    "\n",
    "We can use the eigenvalues and eigenvectors of the matrix $\\boldsymbol{A}$ to find out a lot about it\n",
    "\n",
    "- The trace of $\\boldsymbol{A}$ is the sum of its eigenvalues $tr(\\boldsymbol{A})=\\sum_{i=1}^{n}{\\lambda_i}$.\n",
    "- The determinant of $\\boldsymbol{A}$ is the product of its eigenvalues $det(\\boldsymbol{A})=\\prod_{i=1}^{n}{\\lambda_i}$.\n",
    "- The rank of $\\boldsymbol{A}$ is amount of non-zero eigenvalues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "770a638f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace: -3\n",
      "Determinant: 2.0\n",
      "Rank: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trace: {np.trace(A)}\")\n",
    "print(f\"Determinant: {np.linalg.det(A)}\")\n",
    "print(f\"Rank: {np.linalg.matrix_rank(A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d6327",
   "metadata": {},
   "source": [
    "If $\\boldsymbol{A}$ is a diagonal matrix then the eigenvalues are just the diagonal elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27c46681",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [1. 2. 3.]\n",
      "Eigenvectors: [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "D = np.diag([1, 2, 3])\n",
    "e_values, e_vectors = np.linalg.eig(D)\n",
    "print(f\"Eigenvalues: {e_values}\")\n",
    "print(f\"Eigenvectors: {e_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9b3d2",
   "metadata": {},
   "source": [
    "## Trick for 2 by 2 Matrices\n",
    "\n",
    "As presented in this [video by 3Blue1Brown](https://www.youtube.com/watch?v=e50Bj7jn9IQ) there is a cool formula that can be used to calculate the eigenvalues of a $2 \\times 2$ matrix such as $\\boldsymbol{A}=\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}$. It rests upon two properties that have already been mentioned above:\n",
    "\n",
    "- The trace of $\\boldsymbol{A}$ is the sum of its eigenvalues $tr(\\boldsymbol{A})=\\sum_{i=1}^{n}{\\lambda_i}$. So in other words $a + d = \\lambda_1 + \\lambda_2$. We can also reform this to get the mean value of the two eigenvalues: $\\frac{1}{2}tr(\\boldsymbol{A})=\\frac{a+d}{2}=\\frac{\\lambda_1 + \\lambda_2}{2}=m$\n",
    "- The determinant of $\\boldsymbol{A}$ is the product of its eigenvalues $det(\\boldsymbol{A})=\\prod_{i=1}^{n}{\\lambda_i}$. So in other words $ad - bc = \\lambda_1 \\cdot \\lambda_2 = p$.\n",
    "\n",
    "$$\n",
    "\\lambda_1, \\lambda_2 = m \\pm \\sqrt{m^2 - p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6a400",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "The eigendecomposition is a way to split up **square** matrices into 3 matrices which can be useful in many applications. Eigendecomposition can be pretty easily derived from the above since it lead to the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{A}= \\begin{bmatrix}5 & 2 & 0\\\\ 2 & 5 & 0\\\\ 4 & -1 & 4\\end{bmatrix} \\\\\n",
    "    \\boldsymbol{A}\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix} = 7 \\cdot \\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix} \\\\\n",
    "    \\boldsymbol{A}\\begin{bmatrix}0\\\\ 0\\\\ 1\\end{bmatrix} = 4 \\cdot \\begin{bmatrix}0\\\\ 0\\\\ 1\\end{bmatrix} \\\\\n",
    "    \\boldsymbol{A}\\begin{bmatrix}-1\\\\ 1\\\\ 5\\end{bmatrix} = 3 \\cdot \\begin{bmatrix}-1\\\\ 1\\\\ 5\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Instead of holding this information in three separate equations we can combine them to one equation using matrices. We combine the eigenvectors to a matrix where each column is a eigenvector and we create a diagonal matrix with the eigenvalues (by convention in order of small to large):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{A}\\begin{bmatrix}\n",
    "        1 & 0 & -1 \\\\\n",
    "        1 & 0 & 1 \\\\\n",
    "        1 & 1 & 5\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "         1 & 0 & -1 \\\\\n",
    "        1 & 0 & 1 \\\\\n",
    "        1 & 1 & 5\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        7 & 0 & 0 \\\\\n",
    "        0 & 4 & 0 \\\\\n",
    "        0 & 0 & 3\n",
    "    \\end{bmatrix}\n",
    "\\end{align*} \\\\\n",
    "\\boldsymbol{AX}=\\boldsymbol{X}\\Lambda \\\\\n",
    "\\boldsymbol{AXX}^{-1}=\\boldsymbol{X}\\Lambda\\boldsymbol{X}^{-1} \\\\\n",
    "\\boldsymbol{A}=\\boldsymbol{X}\\Lambda\\boldsymbol{X}^{-1}\n",
    "$$\n",
    "\n",
    "If $\\boldsymbol{A}$ is a symmetric matrix then $\\boldsymbol{Q}$ is guaranteed to be an orthogonal matrix because it is the eigenvectors of $\\boldsymbol{A}$ concatenated. Because $\\boldsymbol{Q}$ is orthogonal $\\boldsymbol{Q}^{-1} = \\boldsymbol{Q}^T$ which leads to the formula being simplified to \n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}=\\boldsymbol{X}\\Lambda\\boldsymbol{X}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66320795",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  2,  0],\n",
       "       [ 2,  5,  0],\n",
       "       [ 4, -1,  4]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[5, 2, 0], [2, 5, 0], [4, -1, 4]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48d86fee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  2.,  0.],\n",
       "       [ 2.,  5.,  0.],\n",
       "       [ 4., -1.,  4.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 0, -1], [1, 0, 1], [1, 1, 5]])\n",
    "Lambda = np.diag([7, 4, 3])\n",
    "inverse = np.linalg.inv(X)\n",
    "np.matmul(np.matmul(X, Lambda), inverse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4874554c910ab0571857260f9b3a2c29827abeee32c252ce8c2203a14b689e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
