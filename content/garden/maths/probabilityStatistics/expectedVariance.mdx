import Callout from "@components/Callout/Callout";
import Image from "@components/Image/Image";

# Expected Value, Variance and Covariance

Probability theory doesn't just let us describe which outcomes are possible but we often want to know how a random variable behaves on average. Does it tend to take large values, small values, or something in between? Does it fluctuate wildly, or stick close to a central point? To make these questions precise, we introduce key summary quantities: **expected value**, **variance**, and **covariance**.

## Expected Value

The **expected value** (also called the **mean** or **expectation**) of a random variable is a measure of the central tendency of its distribution. Formally, it is the **weighted average** of all possible values that the random variable can take, with weights given by their respective probabilities.

In other words, the expected value tells us the **long-term average** value of the random variable if we were to repeat the random experiment an infinite number of times. It answers: "On average, what do we expect to observe?"

### Discrete Random Variables

Suppose $X: \Omega \to \mathbb{R}$ is a discrete random variable with values in $W$ (finite or countable). The expected value of $X$ is defined as

```math
\E[X] = \sum_{x \in W} x \cdot \P(X = x)
```

where $W$ is the set of all values $X$ can take, and $\P(X = x)$ is the probability that $X$ takes value $x$. 

This formula is essentially a weighted average: we take each possible value $x$, multiply it by its chance of occurring, and sum over all possibilities. This captures what you would get if you repeated the experiment many times and averaged the results.

Alternatively, you may see the expectation written as a sum over all outcomes $\omega \in \Omega$ in the sample space:

```math
\E[X] = \sum_{\omega \in \Omega} X(\omega) \cdot \P(\{\omega\})
```

Here, $X(\omega)$ is the value of the random variable in outcome $\omega$, and $\P({\omega})$ is the probability of outcome $\omega$.

This definition captures the idea of "average value," where each possible outcome is weighted by its likelihood. So if we perform the random experiment a large number of times, the arithmetic mean of the observed values will be close to $\E[X]$. In particular, the law of large numbers tells us that as the number of trials increases, the sample average converges to the expected value.

<Callout type="example" title="Fair Die">
Let $X$ be the outcome when rolling a fair six-sided die. Then $W = \{1,2,3,4,5,6\}$ and $\P(X = x) = \frac{1}{6}$ for all $x$. The expected value is:

```math
\E[X] = \sum_{x=1}^{6} x \cdot \frac{1}{6} = \frac{1}{6}(1 + 2 + 3 + 4 + 5 + 6) = 3.5
```

So, the "average" roll of a fair die is $3.5$. Of course, $3.5$ is not a possible outcome of a single rollâ€”the expectation is an average, not a possible value! Or in other terms if we kept track of all of our dice rolls whilst playing a game, the average of all rolls would converge to $3.5$ as the number of rolls increases (for example when playing monopoly which takes a long time).
</Callout>

<Callout type="example" title="Gambling Game">
Suppose you win or lose depending on the die outcome, with winnings $X(\omega)$ as follows:

```math
\forall \omega \in \Omega: X(\omega) = \begin{cases}
-1 & \text{ if } \omega = 1, 2, 3 \\
0 & \text{ if } \omega = 4 \\
2 & \text{ if } \omega = 5, 6
\end{cases}
```

Each outcome has probability $\frac{1}{6}$, so

```math
\E[X] = (-1) \cdot \frac{3}{6} + 0 \cdot \frac{1}{6} + 2 \cdot \frac{2}{6} = -\frac{3}{6} + 0 + \frac{4}{6} = \frac{1}{6}
```

So, on average, you expect to win $\frac{1}{6}$ per game in the long run. So in other words, if you had enough cash and played this game many times, you would expect to end up with a profit over time.
</Callout>

<Callout type="example" title="Indicator Function">
Let $A \subseteq \Omega$ be an event, and let $I_A$ be the **indicator random variable** of $A$:

```math
I_A(\omega) = \begin{cases}
1 & \text{ if } \omega \in A \\
0 & \text{ otherwise}
\end{cases}
```

Then,

```math
\E[I_A] = 1 \cdot \P(A) + 0 \cdot \P(\Omega \setminus A) = \P(A)
```

So, the expectation of an indicator random variable is just the probability of the event. 
</Callout>

<Callout type="example" title="Bernoulli Distribution">
If $X \sim \text{Bernoulli}(p)$, so $X$ takes value $1$ with probability $p$ (success) and $0$ with probability $1-p$ (failure):

```math
\E[X] = 1 \cdot p + 0 \cdot (1-p) = p
```

We could also interpret a Bernoulli random variable as an indicator variable where $A$ is the event of success. Then $\E[I_A] = \P(A) = p$.
</Callout>

<Callout type="example" title="Binomial Distribution">
If $X \sim \mathrm{Binomial}(n, p)$, so $X$ counts the number of successes in $n$ independent Bernoulli($p$) trials:

```math
\E[X] = \sum_{k=0}^n k \cdot \binom{n}{k} p^k (1-p)^{n-k} = n p
```

So we iterate over all possible values $k$ from $0$ to $n$, representing the number of successes, and multiply each value by its probability $\binom{n}{k} p^k (1-p)^{n-k}$, which is the probability of getting exactly $k$ successes in $n$ trials. The sum gives us the expected number of successes.
</Callout>

<Callout type="example" title="Poisson Distribution">
If $X \sim \mathrm{Poisson}(\lambda)$, then

```math
\E[X] = \sum_{k=0}^{\infty} k \cdot \frac{\lambda^k e^{-\lambda}}{k!} = \lambda
```

This follows from the definition of the Poisson distribution, where $\lambda$ is the average rate of occurrence. The expected value is simply $\lambda$, which represents the average number of events in a fixed interval.
</Callout>

### Continuous Random Variables

Let's now consider continuous random variables. The intuition remains the same: the expected value is a weighted average, but now with weights given by a probability density function (PDF) $f(x)$. Here, sums are replaced by integrals as we are no longer just summing over discrete outcomes but over a continuous range of values which is why we need to use integrals.

Suppose $X$ is a continuous random variable with PDF $f(x)$ and $X \geq 0$ almost surely (i.e $\P(X < 0) = 0$). Then the expected value is

```math
\E[X] = \int_0^\infty x f(x) \, dx
```

where $f(x) \geq 0$, $\int\_0^\infty f(x) dx = 1$. The reason we say almost surely is that in continuous random variables, the probability of any single point (like $X = 0$) is zero, so we can ignore it without affecting the integral. This definition reflects the same logic as in the discrete case: we weight each possible value $x$ by its likelihood $f(x),dx$ and sum (integrate) over all possible $x$.

For nonnegative random variables, the expected value may be finite or infinite depending on the behavior of the PDF. However, we say it is well defined if the integral converges to a finite value. 

It also follows that if $X$ is a non-negative continuous random variable, then

```math
\E[X] \geq 0
```

Due to the value of $x$ being non-negative and the PDF $f(x)$ being non-negative, the integral will always yield a non-negative result. The equality holds if and only if $f(x) = 0$ for all $x < 0$, meaning $X = 0$ almost surely (i.e., with probability 1). This is because the integral from $0$ to $\infty$ of a non-negative function is zero only if the function is zero almost everywhere in that range.

If $X$ is a non-negative continuous random variable with values in the range from $[a,b]$, then we don't need to integrate from $0$ to $\infty$, but rather from $a$ to $b$:

```math
\E[X] = \int_a^b x f(x) \, dx \text{ if } X(\omega) \in [a,b] \text{ for all } \omega \in \Omega
```

This makes sense as for all values outside $[a,b]$, the PDF $f(x)$ is $0$, so they do not contribute to the integral.

<Callout type="example" title="Uniform Distribution"> 
Let $X \sim \text{Uniform}(0,1)$ so $f(x) = 1$ for $x \in [0,1]$, zero otherwise. 

```math
\E[X] = \int_0^1 x \cdot 1 \, dx = \left[ \frac{x^2}{2} \right]_0^1 = \frac{1^2}{2} - \frac{0^2}{2} = \frac{1}{2}
```

So the average value of a random number in $[0,1]$ is $\frac{1}{2}$. Which is intuitive since the uniform distribution is symmetric around $\frac{1}{2}$.
</Callout>

<Callout type="example" title="Exponential Distribution">
Let $X \sim \mathrm{Exp}(\lambda)$, so $f(x) = \lambda e^{-\lambda x}$ for $x \geq 0$:

```math
\E[X] = \int_0^\infty x \lambda e^{-\lambda x} dx
```

We integrate by parts with $u = x$, $dv = \lambda e^{-\lambda x} dx$, $du = dx$, $v = -e^{-\lambda x}$ then we have:

```math
\begin{align*}
\E[X] &= \int_0^\infty x \lambda e^{-\lambda x} dx \\
&= \left. -x e^{-\lambda x} \right|_0^\infty + \int_0^\infty e^{-\lambda x} dx \\
&= 0 + \int_0^\infty e^{-\lambda x} dx \\
&= \left[ -\frac{1}{\lambda} e^{-\lambda x} \right]_0^\infty \\
&= \frac{1}{\lambda}
\end{align*}
```
</Callout>

If $X$ can take both positive and negative values, we need to ensure the expectation is well-defined. To do this, we decompose $X$ into its positive and negative parts:

```math
X_+(\omega) = \begin{cases}
X(\omega) & \text{if } X(\omega) \geq 0 \\
0 & \text{otherwise}
\end{cases}
\qquad
X_-(\omega) = \begin{cases}
-X(\omega) & \text{if } X(\omega) < 0 \\
0 & \text{otherwise}
\end{cases}
```

We obviously have $X = X_+ - X_-$, but we also have $|X| = X_+ + X_-$ because the negative part $X_-$ is defined as the absolute value of $X$ when $X$ is negative. So if $\E[|X|] = \E[X_+] + \E[X_-] < \infty$, then we say the expectation is well-defined and set:

```math
\E[X] = \E[X_+] - \E[X_-]
```

Because of the condition above that $\E[|X|] = \E[X_+] + \E[X_-] < \infty$ it means that both $\E[X_+]$ and $\E[X_-]$ are finite and well-defined. This is crucial because if either expectation diverges to infinity, the difference would be undefined (as $\infty - \infty$ is not a number). If this holds, $\E[X]$ is well-defined (finite). If $X \geq 0$ then the expectation is always defined, but can be infinite. So we can write

```math
\E[X_+] = \int_{-\infty}^{\infty} X_+(\omega) f(x) dx, \qquad \E[X_-] = \int_{-\infty}^{\infty} X_-(\omega) f(x) dx
```

This means that we can compute the expected value of $X$ as the difference between the expected values of its positive and negative parts so putting it together we have:

```math
\begin{align*}
\E[X] &= \E[X_+] - \E[X_-] \\
&= \int_{-\infty}^{\infty} X_+(\omega) f(x) dx - \int_{-\infty}^{\infty} X_-(\omega) f(x) dx \\
&= \int_{-\infty}^{\infty} (X_+(\omega) - X_-(\omega)) f(x) dx \\
&= \int_{-\infty}^{\infty} X(\omega) f(x) dx 
\end{align*}
```

<Callout type="proof">
Let's check that this matches our intuition:

```math
\E[X] = \int_{-\infty}^\infty X(\omega) f(x) dx
= \int_{-\infty}^0 X(\omega) f(x) dx + \int_0^\infty X(\omega) f(x) dx
```

But for $x<0$, $X(\omega) = x < 0$, so $X_{-(\omega)} = -x > 0$, and for $x \geq 0$, $X_{+(\omega)} = x$. Therefore:

```math
\E[X_+] = \int_0^\infty x f(x) dx, \quad \E[X_-] = \int_{-\infty}^0 (-x) f(x) dx
```

So

```math
\E[X] = \int_0^\infty x f(x) dx - \int_{-\infty}^0 (-x) f(x) dx = \int_{-\infty}^\infty x f(x) dx
```

which is the standard formula for expectation, provided the integral is absolutely convergent. 
</Callout>

<Callout type="example">
Let $X \sim \text{Uniform}(-2,2)$, so $f(x) = \frac{1}{4}$ for $x \in [-2,2]$. Then

```math
\E[X] = \int_{-2}^2 x \cdot \frac{1}{4} dx = \frac{1}{4} \left[ \frac{x^2}{2} \right]_{-2}^2 = \frac{1}{4} \cdot 0 = 0
```

So the average value of a uniformly distributed random variable on $[-2,2]$ is $0$. 
</Callout>

### Linearity of Expectation

One of the most powerful and useful properties of expectation is linearity. The expectation of a sum is the sum of the expectations even if the random variables are dependent! So we have:

```math
\E[\lambda \cdot X] = \lambda \cdot \E[X] \quad \text{and} \quad \E[X + Y] = \E[X] + \E[Y]
```

Putting these together, we get the **linearity of expectation**:

```math
\E\left[\sum_{i=1}^n \lambda_i X_i\right] = \sum_{i=1}^n \lambda_i \E[X_i]
```

<Callout type="proof">
Let's prove this for $n$ random variables by induction. We will focus on the case where the $X_i$ are discrete random variables, but the proof can be adapted for continuous random variables as well. First we show the base case for $n=1$:

```math
\E[\lambda_1 X_1] = \sum_{x_1} \lambda_1 x_1 \cdot \P(X_1 = x_1) = \lambda_1 \sum_{x_1} x_1 \cdot \P(X_1 = x_1) = \lambda_1 \E[X_1]
```

For our **induction hypothesis** we assume it holds for some $k \geq 1$:

```math
\E\left[\sum_{i=1}^k \lambda_i X_i\right] = \sum_{i=1}^k \lambda_i \E[X_i]
```

Now for our induction step, we need to show it holds for $n = k + 1$:

```math
\begin{align*}
\E\left[\sum_{i=1}^{k+1} \lambda_i X_i\right] &= \E\left[\sum_{i=1}^k \lambda_i X_i + \lambda_{k+1} X_{k+1}\right] \\
&= \E\left[\sum_{i=1}^k \lambda_i X_i\right] + \E[\lambda_{k+1} X_{k+1}] \\
&= \sum_{i=1}^k \lambda_i \E[X_i] + \lambda_{k+1} \E[X_{k+1}] \quad \text{(by induction hypothesis)} \\
&= \sum_{i=1}^{k+1} \lambda_i \E[X_i]
\end{align*}
```
</Callout>

<Callout type="example" title="Binomial Distribution by Linearity">
Let $X = X_1 + X_2 + \cdots + X_n$ where $X_i$ are independent (or even just identically distributed) Bernoulli($p$) variables. By linearity we have:

```math
\E[X] = \sum_{i=1}^n \E[X_i] = np
```

Which is much nicer then having to deal with the binomial formula directly! Notice we did not need independence for the sum. The result is true even if the $X_i$ are dependent! 
</Callout>

### Tailsum Formula

Sometimes, computing the expectation directly from the definition can be cumbersome, especially for discrete random variables with many possible values. The **tailsum formula** offers a different perspective that is not only often easier to compute in practice, but also gives an insightful interpretation of expectation.

Suppose $X$ is a non-negative discrete random variable then the **tailsum formula** says:

```math
\E[X] = \sum_{k=1}^\infty \P(X \geq k)
```

The previous definition summed over all possible values $m$ of $X$ and weighted them by their probabilities. The tailsum formula, however, sums over all possible thresholds $k$, counting how many times $X$ is at least $k$.

<Callout type="todo">
Doesnt rly make sense to me why this is the same an intuitively?
</Callout>
<Callout type="proof">
Let $X$ be a non-negative integer-valued random variable. We start with the standard definition:

```math
\E[X] = \sum_{m=0}^\infty m \cdot \P(X = m)
```

But $m$ can be rewritten as a sum:

```math
m = \sum_{k=1}^m 1
```

Therefore,

```math
\E[X] = \sum_{m=0}^\infty \left( \sum_{k=1}^m 1 \right) \P(X = m)
= \sum_{m=0}^\infty \sum_{k=1}^m \P(X = m)
```

Now if we swap the order of summation we get:

```math
= \sum_{k=1}^\infty \sum_{m=k}^\infty \P(X = m)
= \sum_{k=1}^\infty \P(X \geq k)
```
</Callout>

<Callout type="example" title="Geometric Distribution via Tailsum">
Let $X \sim \mathrm{Geom}(p)$, so $\P(X \geq k) = (1-p)^{k-1}$ for $k \geq 1$.

```math
\E[X] = \sum_{k=1}^\infty (1-p)^{k-1}
```

This is a geometric series with first term $1$ and common ratio $1-p$, so we can use the formula for the sum of an infinite geometric series:

```math
\sum_{k=0}^\infty r^k = \frac{1}{1-r} \text{ for } |r| < 1
```

Thus,

```math
\E[X] = \sum_{k=1}^\infty (1-p)^{k-1} = \frac{1}{1 - (1-p)} = \frac{1}{p}
```
</Callout>

If $X$ is a **non-negative continuous random variable** with probability density function $f(x)$, the tailsum formula takes the form of an integral involving the **survival function** (also called the "tail probability function"):

```math
\E[X] = \int_0^\infty \P(X \geq x) dx
```

or, equivalently,

```math
\E[X] = \int_0^\infty (1 - F(x)) dx
```

where $F(x) = \P(X \leq x)$ is the cumulative distribution function (CDF), and $1 - F(x)$ is the **survival function**. The intuition here is similar to the discrete case, but now we are integrating over a continuous range of values. For each value $x \geq 0$, the function $\P(X \geq x)$ measures the "probability mass" that remains above $x$. As $x$ increases, this probability decreases.By integrating this "tail" probability across all $x \geq 0$, we effectively add up all the infinitesimal slices of expected "remaining" value, yielding the overall average (expected) value of $X$.

<Callout type="proof">
To derive the tailsum formula for continuous random variables, we start from the definition of expectation:

```math
\E[X] = \int_0^\infty x f(x) dx
```

But by integration by parts with $u = x$, $dv = f(x) dx$, we have $du = dx$, and $v = F(x)$ (the CDF). Then,

```math
\E[X] = \left[ x F(x) \right]_0^\infty - \int_0^\infty F(x) dx
```

As $x \to \infty$, $F(x) \to 1$ (if $X$ is integrable and nonnegative), so the boundary term vanishes. The lower boundary at $x=0$ gives $0$. Therefore,

```math
\E[X] = \int_0^\infty (1 - F(x)) dx = \int_0^\infty \P(X \geq x) dx
```
</Callout>

<Callout type="example" title="Exponential Distribution via Tailsum">
Let $X \sim \mathrm{Exp}(\lambda)$, so $\P(X \geq x) = e^{-\lambda x}$ for $x \geq 0$.

```math
\E[X] = \int_0^\infty e^{-\lambda x} dx = \left[ -\frac{1}{\lambda} e^{-\lambda x} \right]_0^\infty = 0 - \left( -\frac{1}{\lambda} \right) = \frac{1}{\lambda}
```

So, the expected value of an exponential random variable with parameter $\lambda$ is $\frac{1}{\lambda}$. 
</Callout>

### Characterization of Distributions

When we study random variables, it is tempting to summarize their behavior with a single number such as their expected value. However, the expected value alone does not uniquely determine the distribution of a random variable. That is, many very different random variables can have the same mean, but differ in almost every other respect (such as their spread, skewness, probability of extreme values, and so on).

Think of expectation as the "center of mass" of a probability distribution, but not its shape or spread. Two random variables can have the same mean, but their outcomes may be clustered tightly around that mean (low variance), or spread very far apart (high variance), or distributed in totally different ways.
Expectation does **not** uniquely characterize a distribution. Different random variables can have the same mean but very different shapes (e.g., uniform and normal distributions with the same mean).

This also means that with a transformation of the random variable, we can change its expected value. For a function $g:\mathbb{R} \to \mathbb{R}$ (bounded or integrable), we say:

```math
\E[g(X)] = \int_{-\infty}^{\infty} g(x) f(x) dx \quad \text{or} \quad \E[g(X)] = \sum_{x \in W} g(x) \cdot \P(X = x)
```

This follows from the definition of expectation, where we replace $X$ with $g(X)$, effectively transforming the random variable before computing its expected value. The expectation of a transformed random variable is the average value of the function $g$ applied to the outcomes of $X$, weighted by their probabilities.

<Callout type="example">
Suppose $X$ is a random variable representing the outcome of rolling a fair six-sided die. The expected value of $X$ is $\E[X] = 3.5$. Now, consider the transformation $g(X) = 2X + 1$. The expected value of the transformed random variable is:

```math
\E[g(X)] = \E[2X + 1] = 2\E[X] + 1 = 2 \cdot 3.5 + 1 = 7 + 1 = 8
```

We can also compute this directly:

```math
\begin{align*}
\E[g(X)] = \sum_{x=1}^{6} (2x + 1) \cdot \P(X = x)
&= \sum_{x=1}^{6} (2x + 1) \cdot \frac{1}{6} \\
&= \frac{1}{6} \left(2(1 + 2 + 3 + 4 + 5 + 6) + 6 \right) \\
&= \frac{1}{6} \left(2 \cdot 21 + 6 \right) \\
&= \frac{1}{6} \cdot 48 = 8
\end{align*}
```
</Callout>

### Moments

The $n$-th moment of a random variable $X$ is defined as:

```math
\E[X^n] = \int_{-\infty}^{\infty} x^n f(x) dx \quad \text{(for continuous RVs)}
```

### Product of Expectations

<Callout type="todo">
What does this mean? Why is it important? How does it relate to the expectation? What is the intuition behind it? The mathmatical derivation etc.
</Callout>

If $X$ and $Y$ are independent,

```math
\E[XY] = \E[X] \cdot \E[Y]
```

But the converse is not true: $\E\[XY] = \E\[X] \E\[Y]$ does **not** imply independence.

<Callout type="proof">
Wasn't shown in notes.
</Callout>

A stronger version: For independent random variables and functions $g\_1$, $g\_2$:

```math
\E[g_1(X) \cdot g_2(Y)] = \E[g_1(X)] \cdot \E[g_2(Y)]
```

This generalizes to more than two independent random variables:

```math
\E[g_1(X_1) \cdot \cdots \cdot g_n(X_n)] = \prod_{i=1}^n \E[g_i(X_i)]
```

whenever the $X\_i$ are independent.

<Callout type="example">
Some example where this is useful and then showing that the converse is not true with regards to independence?
</Callout>

## Variance

Also with absolute distance but squaring has nicer properties

```math
\Var(X) = \sigma^2_X = \E[(X - \E[X])^2] = \E[(X - m)^2]
```

where $m = \E[X]$ is the mean/expectation of $X$. 

Taking square root we get the **standard deviation**.

<Callout type="example" title="Variance of a Constant">
constant random variable $X = c$ has variance $0$:
</Callout>

<Callout type="example" title="Uniform Distribution">

</Callout>

If $X$ is a random variable with $\E[X^2] < \infty$, then:

```math
\Var(X) = \E[X^2] - (\E[X])^2
```

If $X$ is a random variable with $\E[|X|] < \infty$, then:

```math
\V(\lambda X) = \lambda^2 \V(X)
```

If $X_1, X_2, \ldots, X_n$ are pairwise independent random variables, then:

```math
\Var\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \Var(X_i)
```

What is difference between pairwise independent and independent?

<Callout type="example" title="Binomial Distribution">

</Callout>

## Approximating Expectation and Probabilities

Some blabla motivation

### Monotonicity

Just like with many other mathematical concepts, we can use the **monotonicity** of expectation to derive inequalities. If $X$ and $Y$ are random variables such that $X \leq Y$ almost surely (why almost surely? Because in continuous random variables, the probability of any single point is zero), then:

```math
\E[X] \leq \E[Y]
```

<Callout type="proof">
Consider the random variable $Z = Y - X$. Since $X \leq Y$ almost surely, we have $Z \geq 0$ almost surely. Therefore, the expectation of $Z$ is non-negative:

```math
\E[Z] = \E[Y - X] = \E[Y] - \E[X] \geq 0
```

This implies that $\E[Y] \geq \E[X]$, proving the monotonicity of expectation.
</Callout>

### Markov's Inequality

If $X$ is a non-negative random variable then for every $a > 0$:

```math
\P(X \geq a) \leq \frac{\E[X]}{a}
```

can also generalize with a transformed random variable?

### Chebyshev's Inequality

Let $X$ be a random variable with finite expected value $\E[X]$ $\E[|X|] < \infty$. Then for any $a > 0$:

```math
\P(|X - \E[X]| \geq k) \leq \frac{\Var(X)}{a^2}
```

Proof uses markovs inquality

### Jensen's Inequality

If $g$ is a convex function and $X$ is a random variable, then:

```math
g(\E[X]) \leq \E[g(X)]
```

If $g(x) = |x|$, then Jensen's inequality gives us the **triangle inequality** for expectations. For every integrable discrete random variable $X$:

```math
|\E[X]| \leq \E[|X|]
```

If we take $g(x) = x^2$, then Jensen's inequality gives us for every integrable discrete random variable $X$:

```math
E[|X|] \leq \sqrt{E[X^2]}
```

Derivation? We also of course have:

```math
(\E[X])^2 \leq \E[X^2]
```

### Chernoff Bounds

## Covariance and Correlation

quantify the dependence between two random variables. How does this relate to my correlation chapter?

Have finite second moment, i.e., $\E[X^2] < \infty$ and $\E[Y^2] < \infty$. This ensures is well defined.

```math
\Cov(X, Y) = \E[XY] - \E[X] \E[Y]
```

or 

```math
\Cov(X, Y) = \E[(X - \E[X])(Y - \E[Y])]
```

Consequnce of finite second moments we have due to elemntary inequality? monotny and lienarity:

```math
\E[|XY|] \leq \frac{1}{2} \left( \E[X^2] + \E[Y^2] \right) < \infty
```

Independence implies covariance zero, but zero covariance does not imply independence.

For eauylity it needs to hold for all transformations $g_1$ and $g_2$ that are piecewise continous and bounded.