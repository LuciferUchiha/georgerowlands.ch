import Callout from '@components/Callout/Callout';
import Image from '@components/Image/Image';

# Vector Spaces

Space is the final frontier. The same goes for linear algebra. Almost all topics in linear algebra come down to understanding vector spaces and how matrices and vectors interact with them. 

## Fields

Before we talk about vector spaces I want to talk about a similar concept you have already been using a lot called fields. Fields are written using upper-case hollow letters such as $\mathbb{Z,R,C}$. Seems familiar? A field is a set of elements for which the basic arithmetic operations are defined:
- Addition and subtraction.
- Multiplication and division.

There are more precise mathematical definitions for fields but this will do for now. The elements of a field are called scalars, hence also the name scalar multiplication when we multiply a vector with a scalar.

## Vector Spaces

A vector space or also called linear space is a set of elements for which addition and scalar multiplication is defined. The elements of a vector space are then called vectors, which we have already gotten to know.

So what are the fields for? We have seen that vectors are sequences of real numbers such as $\mathbb{R}^n$. We also seen that the real numbers are a field. This corresponds the definition that the elements of a vector are from a field. 
This is also the link to the name for scalar multiplication as we for example multiply vectors with a real number from the field of real numbers. 

Vector spaces are usually written using italicized upper-case letters such as $\it{V}$. More specifically the elements of a vector space $\it{V}$ must have the following properties where $\mathbf{o}$ is the null vector, $\mathbf{v}$, $\mathbf{w}, \mathbf{u}$ are vectors and $a, b$ are scalars from the field and $\pm$ is the field addition and $\times$ is the field multiplication:
- **Additive inverse**: $\mathbf{v} + (-\mathbf{v}) = \mathbf{o}$.
- **Additive identity**: $\mathbf{v} + \mathbf{o} = \mathbf{v}$.
- **Addition is commutative**: $\mathbf{v} + \mathbf{w} = \mathbf{w} + \mathbf{v}$.
- **Addition is associative**: $(\mathbf{v} + \mathbf{w}) + \mathbf{u} = \mathbf{v} + (\mathbf{w} + \mathbf{u})$.
- **Scalar multiplication identity**: $1 \cdot \mathbf{v} = \mathbf{v}$.
- **Scalar multiplication is compatible with field multiplication**: $(a \times b) \cdot \mathbf{v} = a \cdot (b \cdot \mathbf{v})$.
- **Scalar multiplication is distributive over vector addition**: $a \cdot (\mathbf{v} + \mathbf{w}) = a \cdot \mathbf{v} + a \cdot \mathbf{w}$.
- **Scalar multiplication is distributive over field addition**: $(a \pm b) \cdot \mathbf{v} = a \cdot \mathbf{v} + b \cdot \mathbf{v}$.

For our "normal" vectors from $\mathbb{R}$ where the elements are real numbers we already know this is the case. Therefore the set of all vectors with our definitions of vector addition and scalar multiplication is a vector space, the so called **real vector space**. However, with proper definitions of these operations this idea can be extended to create a vector space where the elements of the vectors are complex numbers or functions or other mathematical objects. 

We cam quickly show that the additive inverse is unique by assuming that there are two additive inverses $\mathbf{v}_1$ and $\mathbf{v}_2$ for a vector $\mathbf{v}$, this results in the following:

```math
\begin{align*}
\mathbf{v}_1 &= \mathbf{v}_1 + \mathbf{o} \\
&= \mathbf{v}_1 + (\mathbf{v} + \mathbf{v}_2) \\
&= (\mathbf{v}_1 + \mathbf{v}) + \mathbf{v}_2 \\
&= \mathbf{o} + \mathbf{v}_2 \\
&= \mathbf{v}_2
\end{align*}
```

An important consequence of the vector space axioms is that the null vector is always in the vector space. This follows from the fact that for any vector $\mathbf{v} \in \it{V}$ we have that $0 \cdot \mathbf{v} = \mathbf{o}$, so the null vector is always in the vector space. This also means that the vector space is never empty as it always contains at least the null vector.

<Callout type="proof">
```math
\begin{align*}
0 \mathbf{v} &= \mathbf{o} \\
&= 0\mathbf{v} + \mathbf{o} \\
&= 0\mathbf{v} + (0 \mathbf{v} + (-0 \mathbf{v})) \\
&= (0 \mathbf{v} + 0 \mathbf{v}) + (-0 \mathbf{v}) \\
&= 0 \mathbf{v} + (-0 \mathbf{v}) \\
&= \mathbf{o}
\end{align*}
```
</Callout>

Another consequence is that the vector space is closed under addition and scalar multiplication. This means that if we take any two vectors from the vector space and add them together or multiply them with a scalar then the result is also in the vector space. This is important as it means that we can always create new vectors from existing vectors in the vector space. It also means that any linear combination of vectors in the vector space is also in the vector space, so we can never "leave" the vector space by adding or scaling vectors.

<Callout type="proof">
Let $V$ be a vector space over a field $\mathbb{F}$, and let $G \subseteq V$. We want to show that **any linear combination of finitely many vectors from $G$ is in $V$**. In other words, that $V$ is closed under linear combinations (addition and scalar multiplication), as a consequence of the axioms. Let $\mathbf{v}\_1, \ldots, \mathbf{v}\_k \in G$ and $a\_1, \ldots, a\_k \in \mathbb{F}$. A **linear combination** is any expression of the form

```math
\mathbf{w} = a_1 \mathbf{v}_1 + a_2 \mathbf{v}_2 + \cdots + a_k \mathbf{v}_k.
```

We will prove by **induction on $k$** (the number of vectors in the combination) that $\mathbf{w} \in V$. Starting with the base case where $k = 1$ we get:

```math
\mathbf{w} = a_1 \mathbf{v}_1.
```

Which is in $V$ by the **axiom of scalar multiplication**. We then assume that this holds for $k = n$, i.e., any linear combination of $n$ vectors from $V$ is in $V$ as our inductive hypothesis. We then show that it also holds for $k = n+1$ as our inductive step:

```math
\mathbf{w} = a_1 \mathbf{v}_1 + a_2 \mathbf{v}_2 + \cdots + a_n \mathbf{v}_n + a_{n+1} \mathbf{v}_{n+1}.
```

If we define the following vector:

```math
\mathbf{u} = a_1 \mathbf{v}_1 + \cdots + a_n \mathbf{v}_n
```

then by the inductive hypothesis, $\mathbf{u} \in V$. By then again applying the **axiom of scalar multiplication**, we have: $a\_{n+1} \mathbf{v}\_{n+1} \in V$ and by the **axiom of vector addition (closure)**, we have: $\mathbf{u} + a\_{n+1} \mathbf{v}\_{n+1} \in V$. Thus, $\mathbf{w} \in V$ and we have shown that any linear combination of a finite number of vectors from $G$ is in $V$ and that a vector space is closed under linear combinations.
</Callout>

An equivalent definition of a vector space can be given, which is much more concise but uses lots of fancy words from abstract algebra. The first four axioms (related to vector addition) say that a vector space is an abelian/commutative group under addition, and the remaining axioms (related to the scalar multiplication) say that this operation defines a ring homomorphism from a field into the endomorphism ring of this group. Even more concisely, a vector space is a module over a field.


<Callout type="example">
Let us consider the set $P_n$ of all real polynomials of degree at most $n$, i.e.,

```math
P_n = \{ a_0 + a_1x + a_2x^2 + \ldots + a_nx^n \mid a_0, \ldots, a_n \in \mathbb{R} \}.
```

if we define addition and scalar multiplication as usual for some polynomials $p(x)$ and $q(x)$ in $P_n$:

- **Addition**: $(p + q)(x) = p(x) + q(x)$.
- **Scalar multiplication**: $(a \cdot p)(x) = a \cdot p(x)$, where $a \in \mathbb{R}$.

If we check the vector space axioms for $P_n$:

- The sum of two degree $\leq n$ polynomials is still a degree $\leq n$ polynomial.
- The scalar multiple of a degree $\leq n$ polynomial is still degree $\leq n$.
- The zero polynomial $0(x) = 0$ is in $P\_n$.
- All other axioms (commutativity, associativity, etc.) are inherited from the field of real numbers.

we notice that all axioms hold. Thus, $P_n$ is closed under addition and scalar multiplication, contains the zero polynomial, and satisfies all other vector space properties. Therefore, $P_n$ is a vector space over $\mathbb{R}$.
</Callout>
<Callout type="example">
Another example is the set of all real matrices of size $m \times n$, denoted as $M_{m \times n}(\mathbb{R})$. We denote the set of all $m \times n$ matrices with real entries as:

```math
M_{m \times n}(\mathbb{R}) = \{ A = [a_{ij}] \mid a_{ij} \in \mathbb{R} \}.
```

We define addition and scalar multiplication element-wise just like for normal matrix addition and scalar multiplication:

- $(A+B)_{ij} = A_{ij} + B_{ij}$
- $(aA)_{ij} = a \cdot A_{ij}$

Then this space is closed under addition and scalar multiplication, contains the null matrix $\mathbf{0}$, and satisfies all other vector space properties and therefore is a vector space over $\mathbb{R}$.

This space is isomorphic to $\mathbb{R}^{mn}$ (just rearrange all $mn$ entries into a single vector), but we usually treat matrices as $2$-dimensional arrays for convenience and meaning. All the axioms for a vector space hold because the operations are defined entry-wise using the field $\mathbb{R}$, just like with vectors. So the "difference" between $\mathbb{R}^{mn}$ and $M_{m \times n}(\mathbb{R})$ is only the way we organize the entries, not the underlying algebraic structure.
</Callout>

### Subspaces and Ambient Spaces

We often don't actually care about vector spaces, but much more about subspaces. A subspace is a subset of a vector space that is itself a vector space. So if we have a vector space $\it{V}$ and a subspace $\it{S} \subseteq \it{V}$ then we can check if $\it{S}$ is a vector space by checking if the the vector space axioms hold for $\it{S}$. Specifically we also need to check if the subspace is closed under addition and scalar multiplication, meaning that all linear combinations of vectors in the subspace are also in the subspace. So in other words we can say that a subspace is the set of all possible linear combinations of a set of vectors.

The ambient space is the vector space that contains the subspace so the vector space $\it{V}$ in the example above. The ambient space is also called the parent space. If the subset is the whole vector space then it is the ambient space itself.

An intuitive way to think about it for real vectors is that the ambient space is the space we are in, so the 2-dimensional plane or the 3-dimensional room we are in. The subspace is then a smaller or equal space inside the ambient space. So for example a line in the 2-dimensional plane or a plane in the 3-dimensional room. However, the room itself can also be a vector space equal to the ambient space.

Lets look at a real vector subspace as an example: Think of a real vector $\mathbf{v}$ in 2-dimensional space. Now we can obtain a set of vectors that are all multiples of the vector $\mathbf{v}$. If we then think of all the points we can reach we get a infinitely long line through the origin. This line is a 1-dimensional subspace of the 2-dimensional ambient space. We can then then take another vector $\mathbf{w}$ and do the same thing. This will give us another line through the origin and another 1-dimensional subspace. If we now combine these two vectors and the 2 lines are not parallel, i.e the vectors are not multiples of each other and are not collinear, we will get a plane through the origin. This plane is a 2-dimensional subspace of the 2-dimensional ambient space. So it covers the whole space. We also often then say that $\mathbf{v}$ and $\mathbf{w}$ span the subspace. 

<Image src="/maths/vectorSubSpace.png"
       caption="On the left we can see a plane, which is a 2-dimensional subspace in a 3-dimensional ambient space. On the right we can see a line, which is a 1-dimensional subspace in a 2-dimensional ambient space."
       width={800}
/>

So we can formally define a subspace $\it{S}$ of a vector space $\it{V}$ as a subset of $\it{V}$ that satisfies the following properties:

```math
\forall \mathbf{u}, \mathbf{v} \in \it{S} \text{ and } \forall a, b \in \mathbb{R}: a\mathbf{u} + b\mathbf{v} \in \it{S}
```

So if we take any two vectors from the subspace and multiply them with any scalar we get a vector that is also in the subspace (this is the closure under addition and scalar multiplication). Additionally the subspace must contain the null vector $\mathbf{o}$. This is because the null vector is always in the subspace as if we take any vector or linear combination of vectors in the subspace and multiply it with 0 we get the null vector.

<Callout type="example">
For polynomials $p(x)$ of degree at most $n$, the ambient space is $P\_n$.
</Callout>

#### 0-Dimensional Subspace

We saw above that in a real ambient space of dimension $2$ we can have subspaces of dimension $1$, a line, and $2$, a plane. However, we can also have subspaces of dimension $0$. This 0-dimensional subspace can be created by taking the null vector $\mathbf{o}$. No matter how many times we multiply it with a scalar it will always stay the null vector and just a point at the origin. This is a 0-dimensional subspace. 

This means that in an $n$-dimensional vector space we can create $n+1$ subspaces. One for each dimension from $0$ to $n$.

### Operations on Vector Spaces

We can also look at some operations on vector spaces and see if their results are also vector spaces. Let $U$ and $W$ be vector spaces (or subspaces of a common ambient vector space $V$) over the same field $\mathbb{F}$. 

We can then first look at the intersection of the two vector spaces $U \cap W$. The result of the intersection is again a vector space. 

<Callout type="proof">
To show that $U \cap W$ is a vector space, we need to show that it satisfies the vector space axioms:
- **Contains the zero vector:** Since both $U$ and $W$ are vector spaces, $0 \in U$ and $0 \in W$, so $0 \in U \cap W$.
- **Closed under addition:** If $\mathbf{u}\_1, \mathbf{u}\_2 \in U \cap W$, then $\mathbf{u}\_1, \mathbf{u}\_2 \in U$ and $\mathbf{u}\_1, \mathbf{u}\_2 \in W$. Since $U$ and $W$ are closed under addition, $\mathbf{u}\_1 + \mathbf{u}\_2 \in U$ and $\mathbf{u}\_1 + \mathbf{u}\_2 \in W$, so $\mathbf{u}\_1 + \mathbf{u}\_2 \in U \cap W$.
- **Closed under scalar multiplication:** For any scalar $a \in \mathbb{F}$, $a\mathbf{u}\_1 \in U$ and $a\mathbf{u}\_1 \in W$, so $a\mathbf{u}\_1 \in U \cap W$.

Therefore, $U \cap W$ is a vector space.
</Callout>

Next we can look at the union of the two vector spaces $U \cup W$. The result of the union is not necessarily a vector space.

<Callout type="proof">
Consider the following counterexample. Let $U = \text{span}(\begin{bmatrix}1 \ 0\end{bmatrix})$, $W = \text{span}(\begin{bmatrix}0 \ 1\end{bmatrix})$ in $\mathbb{R}^2$. So all vectors in $U$ are of the form $(a, 0)$ and all vectors in $W$ are of the form $(0, b)$ where $a, b \in \mathbb{R}$, which means that $U \cup W$ consists of all vectors of the form $(a, 0)$ or $(0, b)$.

Consider $\mathbf{u}\_1 = (1, 0) \in U$ and $\mathbf{u}\_2 = (0, 1) \in W$. Both are in $U \cup W$, but their sum $(1, 0) + (0, 1) = (1, 1)$ is not in $U \cup W$ (since neither entry is zero).
Therefore, $U \cup W$ is not closed under addition in general, so is not a vector space.
</Callout>

The set difference $U \setminus W$ is never a vector space. This is easily shown as all vector spaces must contain the null vector and the set difference does not contain the null vector as it will always be removed from the set.

Lastly we look at the sum of the two vector spaces $U + W$. The sum of two vector spaces is the set of all vectors that can be formed by adding a vector from $U$ and a vector from $W$. This is also called the direct sum of the two vector spaces:

```math
U + W = \{\mathbf{u} + \mathbf{w} \mid \mathbf{u} \in U, \mathbf{w} \in W\}
```

This is always a vector space.

<Callout type="proof">
To show that $U + W$ is a vector space, we need to show that it satisfies the vector space axioms:
- **Contains the zero vector:** $0 = 0\_U + 0\_W \in U + W$.
- **Closed under addition:** Let $\mathbf{a} = \mathbf{u}\_1 + \mathbf{w}\_1$ and $\mathbf{b} = \mathbf{u}\_2 + \mathbf{w}\_2$ with $\mathbf{u}\_1, \mathbf{u}\_2 \in U$, $\mathbf{w}\_1, \mathbf{w}\_2 \in W$.

```math
\mathbf{a} + \mathbf{b} = (\mathbf{u}_1 + \mathbf{w}_1) + (\mathbf{u}_2 + \mathbf{w}_2)
= (\mathbf{u}_1 + \mathbf{u}_2) + (\mathbf{w}_1 + \mathbf{w}_2)
```

Since $U$ and $W$ are vector spaces, $\mathbf{u}\_1 + \mathbf{u}\_2 \in U$, $\mathbf{w}\_1 + \mathbf{w}\_2 \in W$, so their sum is in $U + W$.
- **Closed under scalar multiplication:** For any scalar $a$,

```math
a\mathbf{a} = a(\mathbf{u}_1 + \mathbf{w}_1) = a\mathbf{u}_1 + a\mathbf{w}_1
```

Again, $a\mathbf{u}\_1 \in U$, $a\mathbf{w}\_1 \in W$, so $a\mathbf{a} \in U + W$. Therefore, $U + W$ is a vector space.
</Callout>

### Span

The span of a set of vectors is the set of all possible linear combinations of the vectors. This is quiet clearly related to subspaces as subspaces are the set of all possible linear combinations of a set of vectors. This is the consequence of them being closed under addition and scalar multiplication. Therefore the span of a set of vectors is a subspace of the vector space. This is why it is often said that a subspaces is spanned by a set of vectors. 

```math
span(\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}) = \{a_1\mathbf{v}_1 + a_2\mathbf{v}_2 + \ldots + a_n\mathbf{v}_n | a_1, a_2, \ldots, a_n \in \mathbb{R}\}
```

<Image
    src="/maths/spansOfVectors.png"
    caption="Different spaces spanned by different sets of vectors."
    width={400}
/>

Because the span of a set of vectors is defined by their linear combinations, vectors that are linear combinations of others do not add any new information to the span. This means that the span of a set of vectors is the same as the span of a subset containg the linearly independent vectors that span the same space.

<Callout type="proof">
Let $S = {\mathbf{v}_1, \ldots, \mathbf{v}_n}$, be a set of linearly independent vectors and let $\mathbf{w} = a_1\mathbf{v}_1 + \cdots + a_n\mathbf{v}_n$. If we then define $T = S \cup {\mathbf{w}}$ we want to show that $\operatorname{span}(S) = \operatorname{span}(T)$.

First we show that $\operatorname{span}(S) \subseteq \operatorname{span}(T)$. This is trivial as any linear combination of the vectors in $S$ is also a linear combination of the vectors in $T$ as we can just use $0$ as the coefficient for $\mathbf{w}$.

To show equality we also need to show that $\operatorname{span}(T) \subseteq \operatorname{span}(S)$. By definition of the span any linear combination of $T$ can be written as:

```math
b_1\mathbf{v}_1 + \cdots + b_n\mathbf{v}_n + c\mathbf{w}
```

Substituting $\mathbf{w} = a_1\mathbf{v}_1 + \cdots + a_n\mathbf{v}_n$ we get:

```math
b_1\mathbf{v}_1 + \cdots + b_n\mathbf{v}_n + c(a_1\mathbf{v}_1 + \cdots + a_n\mathbf{v}_n)
= (b_1 + c a_1)\mathbf{v}_1 + \cdots + (b_n + c a_n)\mathbf{v}_n
```

which is a linear combination of just the vectors in $S$. Thus, $\operatorname{span}(S) = \operatorname{span}(T)$.
</Callout>

<Callout type="example">
- $span(\{\begin{bmatrix} 0 \\ 0 \end{bmatrix}\}) = \{\begin{bmatrix} 0 \\ 0 \end{bmatrix}\}$, a 0-dimensional subspace in a 2-dimensional ambient space. This is the same as $span(\{\})$ because the null vector is always in the vector space.
- $span(\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}) = \{\begin{bmatrix} a \\ b \end{bmatrix} | a, b \in \mathbb{R}\}$, a 2-dimensional subspace (plane) in a 2-dimensional ambient space. We already know that we can create all vectors in $\mathbb{R}^2$ by taking linear combinations of the standard basis vectors, so 
this was expected.
- $span(\{\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \begin{bmatrix} 2 \\ 2 \\ 2 \end{bmatrix}\}) = \{\begin{bmatrix} a \\ b \\ c \end{bmatrix} | a, b, c \in \mathbb{R}\}$, a 2-dimensional subspace in a 3-dimensional ambient space. Here we notice that the third vector is a multiple of the first vector, so it is linearly dependent. Therefore the third vector does not add any new dimension to the spanned space as it can be written as a linear combination of the other vectors.
</Callout>

<Callout type="example">
Let $p(x) = x^3 + x$, $q(x) = x^2 + 1$, $r(x) = x^2 + x + 1$. We could then ask ourselves what is the dimension of the space spanned by these polynomials? So in other words, what polynomials can we create by taking linear combinations of these polynomials? 

We an quickly see that we have the polynomials of degree 0, 1, 2 and 3. So we can create all polynomials of degree at most 3.
</Callout>

### Basis

We have seen that the span of the independent vectors of a vector space is the same as the span of all vectors in the vector space. This means that we can create all vectors in the vector space by taking linear combinations of the independent vectors, so the independent vectors represent the whole vector space. So they form a base for the vector space, which is why there also called called a basis. More formally a basis of some subspace is a set of vectors that are linearly independent and span the entire subspace.

The most common example of a basis is the standard basis $S$ that spans the real vector space. The standard basis is the set of vectors $\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n$ where $\mathbf{e}_i$ is the vector with a 1 at the $i$-th position and 0 elsewhere.

```math
S = \{\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n\} = \{\begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \ldots, \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix}\}
```

A subspace can have many different bases.

<Callout type="example">
Some non-trivial basis for the 2-dimensional real vector space $\mathbb{R}^2$:

- $B_1 = \{\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}$
- $B_2 = \{\begin{bmatrix} 2 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 2 \end{bmatrix}\}$
- $B_3 = \{\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 2 \end{bmatrix}\}$
</Callout>

However, all bases of a subspace have the same number of vectors. This number is called the **dimension of the subspace**. So any linearly independent set of 2 vectors with 2 components will span a 2-dimensional subspace in a 2-dimensional ambient space.

<Callout type="example">
For our vector space of polynomials $P\_n$ the standard basis is ${1, x, x^2, \ldots, x^n}$. This basis has $n+1$ vectors and spans the space of all polynomials of degree at most $n$. Therefore the dimension of this vector space is $n+1$. 
</Callout>
<Callout type="example">
For our vector space of matrices $M\_{m \times n}(\mathbb{R})$ the standard basis consists of the matrices $E\_{ij}$ where $E\_{ij}$ has a $1$ in position $(i,j)$ and zeros elsewhere. The number of such matrices is $m n$, so the dimension of this vector space is $m n$. For example we could construct the following matrix using the standard basis for $M\_{2 \times 3}(\mathbb{R})$:

```math
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix} = 1 \cdot E_{11} + 2 \cdot E_{12} + 3 \cdot E_{13} + 4 \cdot E_{21} + 5 \cdot E_{22} + 6 \cdot E_{23}
```
</Callout>

#### Orthogonal and Orthonormal Basis

Certain bases are better then others as they make calculations easier and have nice properties. One of these categories are orthogonal bases. An orthogonal basis is a basis where all the vectors are orthogonal to each other.
This means that the dot product of any two vectors in the basis is zero. However, this does require that that the vector space has a dot product defined. 

Another category are orthonormal bases. An orthonormal basis is a basis where all the vectors are orthogonal to each other and have a length of 1. This means that the dot product of any two vectors in the basis is zero and the dot product of a vector with itself is 1 because the length of a vector is the square root of the dot product of the vector with itself. An example of an orthonormal basis is the standard basis of the real vector space.

### Coordinate Vectors

We know vectors are identified by their magnitude and direction. Most often it also easiest to think of a vector in its standard position, an arrow pointing from the origin to somewhere in space. In the standard position the point the vector is pointing to in the cartesian coordinate system is the point that matches the vectors components. This sequence of coordinates is called the coordinate vector of the vector. More formally the coordinate vector of a vector $\mathbf{v}$ with respect to a basis $B = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$ is the set of scalars $a_1, a_2, \ldots, a_n$ such that for any vector from the vector space spanned by the basis $\mathbf{v} = a_1\mathbf{v}_1 + a_2\mathbf{v}_2 + \ldots + a_n\mathbf{v}_n$. In the standard position the vector space is spanned by the standard basis vectors $\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n$ which is why the coordinates are just the components of the vector and the coordinate vector is the vector itself. However, we have seen that a vector space can be spanned many different bases, so the coordinate vector of a vector can change depending on the basis.

```math
[\mathbf{v}]_B = \begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix}
```

<Image src="/maths/vectorSpaceCoordinates.png"
       caption="The same vector can have different coordinate vectors depending on the basis."
       width={800}
/>

<Callout type="example">
We have the vector $\mathbf{p} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$ in $\mathbb{R}^2$. We then have the standard basis $S=\{\begin{bmatrix} 1 \\ 0 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \end{bmatrix}\}$. The coordinate vector of $\mathbf{p}$ with respect to the standard basis $S$ is then:

```math
\begin{align*}
\mathbf{p} &= \begin{bmatrix} 2 \\ 3 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 3\begin{bmatrix} 0 \\ 1 \end{bmatrix} \\
[\mathbf{p}]_S &= \begin{bmatrix} 2 \\ 3 \end{bmatrix}
\end{align*}
```

Now if we have the basis $B=\{\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \begin{bmatrix} 0 \\ 2 \end{bmatrix}\}$ the coordinate vector of $\mathbf{p}$ with respect to the basis $B$ is:

```math
\begin{align*}
\mathbf{p} &= \begin{bmatrix} 2 \\ 3 \end{bmatrix} = 2\begin{bmatrix} 1 \\ 1 \end{bmatrix} + \frac{1}{2}\begin{bmatrix} 0 \\ 2 \end{bmatrix} \\
[\mathbf{p}]_B &= \begin{bmatrix} 2 \\ \frac{1}{2} \end{bmatrix}
\end{align*}
```
</Callout>

### Steinitz Exchange Lemma

<Callout type="todo">
Let $V$ be a vector space and $F \subseteq V$ be a finite set of linearly independent vectors and $G \subseteq V$ be a finite set of vectors that spans $V$, so $F \subseteq G$. Then the steinizt excahnge lemma states the following:
- $|F| \leq |G|$, so the number of vectors in $F$ is less than or equal to the number of vectors in $G$.
- There exists a subset $E \subseteq G$ such that $F \cup E$ is a basis of $V$ and $|E| = |F| - |G|$.

Meaning interpretation etc? Can use it to prove that two finite bases of the same vector space have the same number of vectors.
</Callout>

#### Change of Basis

<Callout type="todo">
This is a much more complicated subject.
</Callout>

## Column Space

<Callout type="todo">
If A is square then are the following statements true?
- C(A) = C(2A)
- C(A) = C(A^T)
- C(A) = C(A + I)
- C(A) = C(A^2)

then the same for the null space
</Callout>

A matrix can be thought of as a collection of column vectors. So a matrix with $n$ columns can be thought of as $n$ column vectors stuck together. If we then take the span of the columns we get a subspace. This subspace is called the **Column space of a matrix**. We denote the column space of a matrix $\mathbf{A} \in R^{m \times n}$ as $C(\mathbf{A})$. So in other words the column space is the set of all possible linear combinations of the columns of the matrix. This also means that the independent columns of the matrix $\mathbf{A}$ are the basis of the column space. More formally for a matrix $\mathbf{A} \in R^{m \times n}$:

```math
C(\mathbf{A}) = \{\mathbf{A}\mathbf{x} | \mathbf{x} \in \mathbb{R}^n\} \subseteq \mathbb{R}^m
```

The dimensionality of the column vectors, i.e the number of rows is the dimensionality of the ambient space. The number of linearly independent columns is the dimensionality of the column space as the other columns can be formed as a linear combination of the independent columns. Meaning that the independent columns of the matrix form a basis and and the dimension of the column space is the rank, $r$, of the matrix. More formally the column space of the matrix $\mathbf{A} \in R^{m \times n}$ with the independent columns $\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_r$ is defined as:

```math
\begin{align*}
C(\mathbf{A}) &= \{x_1\mathbf{a}_1 + x_2\mathbf{a}_2 + \ldots + x_n\mathbf{a}_n | x_i \in \mathbb{R}\, \text{and} \, \mathbf{a}_i \in \mathbb{R}^m\} \\
C(\mathbf{A}) &= span(\{\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_n\}) \\
C(\mathbf{A}) &= span(\{\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_r\}) \\
C(\mathbf{A}) &= \mathbb{R}^r
\end{align*}
```

We also know the dimensions of the column space by just looking at the rank of the matrix, say we have a matrix $\mathbf{A} \in R^{m \times n}$ with rank $r$:

```math
dim(C(\mathbf{A})) = r
```

If the rank of the matrix is 0 then the matrix is the null matrix and the column space is the 0-dimensional subspace. If the rank of the matrix is the number of rows then the column space is the whole ambient space. 

<Callout type="example">
If all of the columns of a square matrix are linearly independent then the column space is the whole space. This is the ambient space and the number of linearly independent columns are the same number. So the independent columns span the whole space and are therefore also the basis of the column space.

```math
\C(\begin{bmatrix} 1 & 0 \\ 1 & 2 \end{bmatrix}) = \mathbb{R}^2
```

For other matrices it might be hard to see what the column space is. For this we first figure out what the rank is and which columns are linearly independent using gauss-jordan elimination to get the matrix in reduced row echelon form. Then we can see that the column space is the span of the linearly independent columns.

```math
\mathbf{A} = \begin{bmatrix} 
1 & 2 & 0 & 3 \\
2 & 4 & 1 & 4 \\
3 & 6 & 2 & 5 \\
\end{bmatrix} \rightarrow \text{in row echelon form} \rightarrow \begin{bmatrix}
1 & 2 & 0 & 3 \\
0 & 0 & 1 & -2 \\
0 & 0 & 0 & 0 \\
\end{bmatrix}
```

So we can see that the rank of the matrix is 2 and the first and third columns are linearly independent. Therefore the column space is the span of these two columns.

```math
C(\mathbf{A}) = span(\{\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix}\}) = \mathbb{R}^2
```

Important is that the column space is spanned by the original columns of the matrix and not the columns in the row echelon form. The row echelon form is just a way to find the linearly independent columns!
</Callout>

### Membership of a Vector in the Column Space

If we have a vector $\mathbf{b}$ and we want to know if it is in the column space of a matrix $\mathbf{A}$ we are actually asking the question of whether the vector $\mathbf{b}$ can be written as a linear combination of the columns of $\mathbf{A}$. This turns into our favorite equation to solve:

```math
\mathbf{A}\mathbf{x} = \mathbf{b}
```

Where $\mathbf{x}$ is the vector containing the weights of the linear combination to make $\mathbf{b}$. If we can find a solution for $\mathbf{x}$ then the vector $\mathbf{b}$ is in the column space of $\mathbf{A}$. If there is no solution then the vector $\mathbf{b}$ is not in the column space of $\mathbf{A}$. We have already seen multiple ways to solve this equation, such as the gauss-jordan elimination.

A intuitive way to think about a case where a vector is not in the column space of for example $2 \times 2$ matrix which spans a line in 2D space. If the vector is not on the line then it is not in the column space. If the vector is on the line then it is in the column space.

<Callout type="example">
An example where the vector is in the column space of the matrix:
```math
\begin{align*}
\begin{bmatrix}
2 & 1 \\ 
4 & 4 \\
0 & 0 \\
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\
\end{bmatrix} &= \begin{bmatrix}
4 \\ 12 \\ 0 \\
\end{bmatrix} \\
\begin{bmatrix}
2 & 1 \\
4 & 4 \\
0 & 0 \\
\end{bmatrix} \begin{bmatrix}
1 \\ 2 \\
\end{bmatrix} &= \begin{bmatrix}
4 \\ 12 \\ 0 \\
\end{bmatrix}
\end{align*}
```
</Callout>

#### Augment-rank Algorithm

There is also another way to determine if a vector is in the column space of a matrix. This is the augment-rank algorithm. The idea is very simple and relies on the fact that the rank of a matrix is equivalent to the dimensionality of the spanned space. 

We concatenate the matrix $\mathbf{A}$ with the vector $\mathbf{b}$ and calculate the rank of the augmented matrix:
- If the rank of the augmented matrix is the same as the rank of the original matrix $\mathbf{A}$ without the vector $\mathbf{b}$ then the vector $\mathbf{b}$ is in the column space of the matrix $\mathbf{A}$ as it can be written as a linear combination of the columns of $\mathbf{A}$.
- If the rank increases then the vector $\mathbf{b}$ is not in the column space of the matrix $\mathbf{A}$ and has added a new dimension to the spanned space. 

### Column Space of AA^T

Interestingly the column space of the matrix $\mathbf{A}\mathbf{A}^T$ is the same as the column space of the matrix $\mathbf{A}$. 

Firstly if $\mathbf{A}$ is a $m \times n$ then $\mathbf{A}\mathbf{A}^T$ is a $m \times m$ matrix. So we can see that the ambient spaces are the same, but the number of column vectors is different. The ambient spaces being the same is a however a good precondition for the column spaces being the same. We have also already shown that [the rank of $\mathbf{A}\mathbf{A}^T$ is the same as the rank of $\mathbf{A}$](). So we can also see that the dimensionality of the column spaces is the same.

If we then look at what the matrix multiplication for $\mathbf{A}^T\mathbf{A}$ actually is, we can see that it is 
just a linear combination of the column vectors of $\mathbf{A}$. 

```math
\mathbf{A}\mathbf{A}^T = \begin{bmatrix}
\mathbf{a}_{11} & \mathbf{a}_{12} & \ldots & \mathbf{a}_{1n} \\
\mathbf{a}_{21} & \mathbf{a}_{22} & \ldots & \mathbf{a}_{2n} 
\end{bmatrix} \begin{bmatrix}
\mathbf{a}_{11} & \mathbf{a}_{21} \\
\mathbf{a}_{12} & \mathbf{a}_{22} \\
\vdots & \vdots \\
\mathbf{a}_{1n} & \mathbf{a}_{2n} \\
\end{bmatrix} = \begin{bmatrix}
\mathbf{c}_{11} & \mathbf{c}_{12} \\
\mathbf{c}_{21} & \mathbf{c}_{22} \\
\end{bmatrix}
```

This means that the column space of $\mathbf{A}\mathbf{A}^T$ is the same as the column space of $\mathbf{A}$. Because a vector space is defined by all the possible linear combinations of the vectors that span it. Thus a linear combination of the column vectors of $\mathbf{A}$ can not leave the column space of $\mathbf{A}$ so it must be at least a subset of the vector space. But because we also know that the rank of $\mathbf{A}\mathbf{A}^T$ is the same as the rank of $\mathbf{A}$ we know that the column spaces have the same dimensionality and therefore must be the same. So we can say the following: 

```math
C(\mathbf{A}\mathbf{A}^T) = C(\mathbf{A})
```

<Callout type="example">
If we look at the matrix multiplication we can see the linear combination of the column vectors of $\mathbf{A}$ in the matrix $\mathbf{A}\mathbf{A}^T$.

```math
\begin{align*}
\begin{bmatrix}
0 & 10 \\
3 & 7 \\
5 & 3 \\
\end{bmatrix} \begin{bmatrix}
0 & 3 & 5 \\
10 & 7 & 3 \\
\end{bmatrix} &= \begin{bmatrix}
0 \begin{bmatrix} 0 \\ 3 \\ 5 \end{bmatrix} 
+ 10 \begin{bmatrix} 10 \\ 7 \\ 3 \end{bmatrix}
\quad
3 \begin{bmatrix} 0 \\ 3 \\ 5 \end{bmatrix}
+ 7 \begin{bmatrix} 10 \\ 7 \\ 3 \end{bmatrix}
\quad
5 \begin{bmatrix} 0 \\ 3 \\ 5 \end{bmatrix}
+ 3 \begin{bmatrix} 10 \\ 7 \\ 3 \end{bmatrix}
\end{bmatrix} \\
&= \begin{bmatrix}
100 & 70 & 30 \\
70 & 58 & 34 \\
30 & 34 & 34 \\
\end{bmatrix}
\end{align*}
```
</Callout>

## Row Space

Just like a matrix can be thought of as a collection of column vectors it can also be thought of as a collection of row vectors. If we then take the span of these row vectors we get a subspace of the vector space called the **row space of a matrix**. We denote the row space of a matrix $\mathbf{A} \in R^{m \times n}$ as $R(\mathbf{A})$. So just like for the column space the row space is the set of all possible linear combinations of the rows which again means that the independent rows of the matrix $\mathbf{A}$ are the basis of the row space. Because when we transpose a matrix the rows become the columns we can also say that the row space of a matrix is the same as the column space of the matrix transpose. More formally for a matrix $\mathbf{A} \in R^{m \times n}$:

```math
R(\mathbf{A}) = \{\mathbf{A}^T\mathbf{x} | \mathbf{x} \in \mathbb{R}^m\} = C(\mathbf{A}^T) \subseteq \mathbb{R}^n
```

Notice the important difference that the ambient space is now $\mathbb{R}^n$ because the dimensionality corresponds to the number of columns of the matrix and the vectors are now row vectors so also have a different dimensionality. The number of linearly independent rows is the dimensionality of the row space. More formally the row space of the matrix $\mathbf{A} \in R^{m \times n}$ with the independent rows $\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_r$ is defined as:

```math
\begin{align*}
R(\mathbf{A}) &= \{x_1\mathbf{a}_1 + x_2\mathbf{a}_2 + \ldots + x_m\mathbf{a}_m | x_i \in \mathbb{R}\, \text{and} \, \mathbf{a}_i \in \mathbb{R}^n\} \\
R(\mathbf{A}) &= span(\{\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_m\}) \\
R(\mathbf{A}) &= span(\{\mathbf{a}_1, \mathbf{a}_2, \ldots, \mathbf{a}_r\}) \\
R(\mathbf{A}) &= \mathbb{R}^r
\end{align*}
```

Knowing that the row space is the same as the column space of the matrix transpose show us again that the the column rank of a matrix is the same as the row rank of a matrix. This can also be said in a different way. The dimensionality of the column space is the same as the dimensionality of the row space. This is because the rank of a matrix is the same as the rank of its transpose so we get:

```math
\begin{align*}
rank(\mathbf{A}) &= rank(\mathbf{A}^T) \\
dim(C(\mathbf{A})) &= dim(R(\mathbf{A})) \\
dim(R(\mathbf{A})) &= dim(C(\mathbf{A}^T)) \\
dim(C(\mathbf{A}^T)) &= dim(R(\mathbf{A}^T))
\end{align*}
```

However, very importantly the column space and the row space are not the same. The column space is a subspace of the ambient space $\mathbb{R}^m$ and the row space is a subspace of the ambient space $\mathbb{R}^n$. They also have different bases because the vectors are different. There is however an exception to this rule. If a matrix is symmetric then the row space is the same as the column space. This is because the matrix is the same as its transpose. Also if the matrix is full rank then the row space is the same as the column space. This is because the two spaces then both span the whole space.

<Callout type="todo">
Let A be and mxn matrix and M an invertible mxm matrix then R(A) = R(MA)

We need to prove that C(A^T) = C((MA)^T). Since (MA)^T = A^T M^T 
and N := M^T is also invertible (Lemma 3.10), we set B := A^T and prove C(B) = C(BN)

This then means we can calculate  the row space again using the gauss-jordan elimination and then the row space is the span of the resulting rows, not the original rows like for the column space, instead of transposing the matrix and then calculating the column space.
</Callout>
<Callout type="example">
An example where the row space is the same as the column space of the matrix:

```math
\begin{bmatrix}
1 & 2 & 3 \\
2 & 2 & 2 \\
3 & 2 & 1 \\
\end{bmatrix}
```

The rank of the matrix is 2. We can also see that the matrix is symmetric. Therefore the row space is the same as the column space. They both span a plane in 3D space. For other matrices it can be harder to see what the independent rows are. So we can use gaussian elimination to find the independent rows and then see that the row space is the span of these rows.

```math
\mathbf{A} = \begin{bmatrix} 
1 & 2 & 0 & 3 \\
2 & 4 & 1 & 4 \\
3 & 6 & 2 & 5 \\
\end{bmatrix} \rightarrow \text{in row echelon form} \rightarrow \begin{bmatrix}
1 & 2 & 0 & 3 \\
0 & 0 & 1 & -2 \\
0 & 0 & 0 & 0 \\
\end{bmatrix}
```

So we can see that the rank of the matrix is 2 and the first and second rows are linearly independent as we didn't perform any row swaps when doing the gaussian elimination. We also could've transposed the matrix and then performed gaussian elimination to find the independent columns. Therefore the row space is the span of the first and second rows.

```math
R(\mathbf{A}) = span(\{\begin{bmatrix} 1 & 2 & 0 & 3 \end{bmatrix}, \begin{bmatrix} 2 & 4 & 1 & 4 \end{bmatrix}\}) = \mathbb{R}^2
```
</Callout>

### Row Space of A^TA

Very similarly as for the column space of $\mathbf{A}\mathbf{A}^T$ the row space of $\mathbf{A}^T\mathbf{A}$ is the same as the row space of $\mathbf{A}$ because we can show that the matrix is a linear combination of the row vectors of $\mathbf{A}$.

```math
\mathbf{A}^T\mathbf{A} = \begin{bmatrix}
\mathbf{a}_{11} & \mathbf{a}_{21} & \ldots & \mathbf{a}_{n1} \\
\mathbf{a}_{12} & \mathbf{a}_{22} & \ldots & \mathbf{a}_{n2}
\end{bmatrix} \begin{bmatrix}
\mathbf{a}_{11} & \mathbf{a}_{12} \\
\mathbf{a}_{21} & \mathbf{a}_{22} \\
\vdots & \vdots \\
\mathbf{a}_{n1} & \mathbf{a}_{n2} \\
\end{bmatrix} = \begin{bmatrix}
\mathbf{r}_{11} & \mathbf{r}_{12} \\
\mathbf{r}_{21} & \mathbf{r}_{22} \\
\end{bmatrix}
```

So we formally have the following:

```math
R(\mathbf{A}^T\mathbf{A}) = R(\mathbf{A})
```

So we can say the following:

```math
\begin{align*}
R(\mathbf{A}^T\mathbf{A}) &= R(\mathbf{A}) \\
C(\mathbf{A}^T\mathbf{A}) &= C(\mathbf{A}) \\
\end{align*}
```

And because the matrices $\mathbf{A}^T\mathbf{A}$ and $\mathbf{A}\mathbf{A}^T$ are symmetric we can also say that the column spaces are the same as the row spaces:

```math
\begin{align*}
C(\mathbf{A}^T\mathbf{A}) &= R(\mathbf{A}^T\mathbf{A})  \\
C(\mathbf{A}\mathbf{A}^T) &= R(\mathbf{A}\mathbf{A}^T) \\
\end{align*}
```

However this does not mean that $C(\mathbf{A}) = R(\mathbf{A})$. This is only the case if the matrix is symmetric and full rank.

## Null Space

So far we have seen the column and row space of a matrix. Next we will look at the null space of a matrix, first formally and then how it can actually be found and interpreted. The null space of a matrix $\mathbf{A} \in R^{m \times n}$ is the set of all vectors $\mathbf{x}$ that when multiplied with the matrix $\mathbf{A}$ give the null vector $\mathbf{o}$. We denote the null space of a matrix $\mathbf{A}$ as $N(\mathbf{A})$. So more formally the null space of a matrix $\mathbf{A} \in R^{m \times n}$ is defined as:

```math
N(\mathbf{A}) = \{\mathbf{x} \in \mathbb{R}^n | \mathbf{A}\mathbf{x} = \mathbf{o}\} \subseteq \mathbb{R}^n
```

One obvious vector that is always in the null space of a matrix is the null vector $\mathbf{o}$. This is because then all the elements of the matrix are multiplied with 0 and the result is the null vector. We call this the trivial solution. However, there can also be non-trivial solutions. This means that there are vectors that are not the null vector but when multiplied with the matrix give the null vector. The fact that the null vector is always in the null space also coincides well with our observation that the null vector is in every subspace. The question is now what exactly does the null space of a matrix represent, what does it mean if it only has the trivial solution and what does it mean if it has non-trivial solutions. For this let us go back to our examples of the column and row space. 

First I want to start with a disclaimer. In the above have been making use of the fact that if $\mathbf{M}$ is an invertible matrix then the column and row spaces of the matrix $\mathbf{A}$ are the same as of the matrix $\mathbf{M}\mathbf{A}$. We used this fact to find the independent columns and rows of the matrix because the gaussian elimination can be thought of as performing a specifically designed matrix multiplication. We can also quickly observe that the null space of the row echelon form of a matrix is the same as the null space of the reduced row echelon form of the matrix. This is because the zero rows just have the form $0 = 0$.

We know that two columns are linearly dependent if they are multiples of each other. We can however also say that two columns are linearly dependent if they can combined in a way to give the null vector. So the vectors are dependent if:

```math
\lambda_1 \mathbf{a}_1 + \lambda_2 \mathbf{a}_2 + \ldots + \lambda_n \mathbf{a}_n = \mathbf{o}
```

This looks familiar to the definition of the null space. Just like previously we can look at the row echelon form to find the null space of a matrix. 

```math
\mathbf{A} = \begin{bmatrix} 
1 & 2 & 0 & 3 \\
2 & 4 & 1 & 4 \\
3 & 6 & 2 & 5 \\
\end{bmatrix} \rightarrow \text{in row echelon form} \rightarrow \begin{bmatrix}
1 & 2 & 0 & 3 \\
0 & 0 & 1 & -2 \\
0 & 0 & 0 & 0 \\
\end{bmatrix}
```

So we can now set up the equations that the null space must satisfy. 

```math
\begin{bmatrix}
1 & 2 & 0 & 3 \\
0 & 0 & 1 & -2 \\
0 & 0 & 0 & 0 \\
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4
\end{bmatrix} = \begin{bmatrix}
0 \\ 0 \\ 0
\end{bmatrix}
```

We can see that the zero rows don't contribute anything to the null space which is why we can ignore them and only focus on the reduced row echelon form. We can also see that the columns 1 and 3 are linearly independent with the pivot element, the others are dependent. If we now set up the equations and solve for the independent columns we get:

```math
\begin{align*}
x_1 + 2x_2 + 3x_4 &= 0 & \Rightarrow & x_1 = -2x_2 - 3x_4 \\
x_3 - 2x_4 &= 0 & \Rightarrow & x_3 = 2x_4
\end{align*}
```

So we can see that no matter what the values of $x_2$ and $x_4$ are we can calculate the values of $x_1$ and $x_3$ so that the equation holds. This is why the variables $x_2$ and $x_4$ are also called free variables. So we can see that the pivot variables can be calculated using some linear combination of vectors where the weights are the free variables. 

```math
N(\mathbf{A}) = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \end{bmatrix} = 
\begin{bmatrix} -2x_2 - 3x_4 \\ x_2 \\ 2x_4 \\ x_4 \end{bmatrix} = 
x_2 \begin{bmatrix} -2 \\ 1 \\ 0 \\ 0 \end{bmatrix} + 
x_4 \begin{bmatrix} -3 \\ 0 \\ 2 \\ 1 \end{bmatrix}
```

So we can see for any value we choose for $x_2$ and $x_4$ we can find a vector that when multiplied with the matrix gives the null vector. These vectors therefore span the null space of the matrix and are its basis. We can convince ourselves by setting some values for $x_2$ and $x_4$ and multiplying the resulting vector with the matrix. For example if we set $x_2 = 1$ and $x_4 = 1$ we get: 

```math
\mathbf{A}
 \left( \begin{bmatrix} -2 \\ 1 \\ 0 \\ 0 \end{bmatrix} + \begin{bmatrix} -3 \\ 0 \\ 2 \\ 1 \end{bmatrix} \right) = 
\begin{bmatrix} 
1 & 2 & 0 & 3 \\
2 & 4 & 1 & 4 \\
3 & 6 & 2 & 5 \\
\end{bmatrix} 
\begin{bmatrix} -5 \\ 1 \\ 2 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
```

If we set $x_2$ and $x_4$ we can actually see what the null space is telling us. It is telling us how to combine the two independent columns to get the dependent columns. We can also do the same if we set one of the free variables to 0 and the other to 1 then we only see the relationship between that specific dependent column and the independent columns. 

Just like for the column space and row space the rank of the matrix told us something about the dimensionality of the subspace. The same is true for the null space. Because the null space basis uses the free variables we can see that the number of free variables is the dimensionality of the null space. More formally for a matrix $\mathbf{A} \in R^{m \times n}$:

```math
dim(N(\mathbf{A})) = n - r
```

### Left Null Space

We define the left null space of a matrix $\mathbf{A}$ as the null space of the matrix transpose $\mathbf{A}^T$. We define the left null space of a matrix $\mathbf{A} \in R^{m \times n}$ as follows:

```math
LN(\mathbf{A}) = N(\mathbf{A}^T) = \{\mathbf{x} \in \mathbb{R}^m | \mathbf{A}^T\mathbf{x} = \mathbf{o}\} \subseteq \mathbb{R}^m
```

So it satisfies the following equation:

```math
\mathbf{A}^T\mathbf{x} = \mathbf{o} = \mathbf{xA}
```

So for our running example we get: 

```math
\begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6 \\
0 & 1 & 2 \\
3 & 4 & 5 \\
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix} = 
\begin{bmatrix} 0 & 0 & 0 \end{bmatrix} = \begin{bmatrix}
x_1 & x_2 & x_3 \\
\end{bmatrix} \begin{bmatrix} 
1 & 2 & 0 & 3 \\
2 & 4 & 1 & 4 \\
3 & 6 & 2 & 5 \\
\end{bmatrix}
```

To find the basis of the left null space we can use the same method for the null space. However, we need to transpose the matrix and then perform gaussian elimination on the transposed matrix. So we get:

```math
\begin{align*}
\mathbf{A}^T &= \begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6 \\
0 & 1 & 2 \\
3 & 4 & 5 \\
\end{bmatrix} \rightarrow \text{in row echelon form} \rightarrow \begin{bmatrix}
1 & 0 & -1 \\
0 & 1 & 2 \\
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{bmatrix} \\
\begin{bmatrix}
1 & 0 & -1 \\
0 & 1 & 2 \\
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{bmatrix} \begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix} &= \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} \\
x_1 - x_3 &= 0 \Rightarrow x_1 = x_3 \\
x_2 + 2x_3 &= 0 \Rightarrow x_2 = -2x_3 \\
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} &= x_3 \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}
\end{align*}
```

The dimensionality of the left null space is the number of dependent rows. More formally for a matrix $\mathbf{A} \in R^{m \times n}$ with rank $r$:

```math
dim(LN(\mathbf{A})) = m - r
```

## Solution Space & Number of Solutions

The solution space can be thought of as all the vectors that are a solution to a system of linear equations. So more formally for a matrix $\mathbf{A} \in R^{m \times n}$ and a vector $\mathbf{b} \in \mathbb{R}^m$ the solution space is defined as:

```math
Sol(\mathbf{A}, \mathbf{b}) = \{\mathbf{x} \in \mathbb{R}^n | \mathbf{A}\mathbf{x} = \mathbf{b}\} \subseteq \mathbb{R}^n
```

If the vector $\mathbf{b}$ is the null vector then the solution space is the null space of the matrix. If the vector $\mathbf{b}$ is not the null vector then the solution space isn't actually a subspace for the simple fact that it doesn't contain the null vector. However, we can think of it similarly to a subspace. If we compare the solution space to the null space again we actually notice that it is just a shifted version of the null space. 

<Image src="/maths/lagShiftingNullSpace.png"
       caption="Shifting the null space to get the solution space."
/>

So we can also define the solution space as follows:

```math
Sol(\mathbf{A}, \mathbf{b}) = \mathbf{s} + N(\mathbf{A}) = \{\mathbf{s} + \mathbf{x} | \mathbf{x} \in N(\mathbf{A})\}
```

So the null space actually tells us about the number of solutions to a system of linear equations. If the null space only contains the null vector then the system of linear equations has only one solution. This is done by shifting from the origin to some point. This also matches up with our intuition that a system of linear equations has only one solution if the columns of the matrix are linearly independent or in other words the rank of the matrix is the same as the number of columns. If the null space contains more than just the null vector then the system of linear equations has infinitely many solutions. This can be seen in the image below.

<Image src="/maths/lagSolutionSpace.png"
       caption="A solution space of dimension 0, a point (left); a solution space of dimension 1, a line (middle); a solution space of dimension 2, a plane (right)."
/>

What about when we have no solution? This is the case for when the null space is empty. This can only happen if the vector $\mathbf{b}$ is not in the column space of the matrix. So for example if we have the zero matrix but are looking for a solution that is not the null vector then we can't find a solution. 

## Rank-Nullity Theorem

<Callout type="todo">
One of the most important results in linear algebra relating the rank to the solution space of linear systems is the **Rank-Nullity Theorem**.

The **nullity** of a matrix $\mathbf{A}$, denoted as $\text{nullity}(\mathbf{A})$, is the dimension of its **null space** (also called the kernel). The null space is the set of all vectors $\mathbf{x}$ such that $\mathbf{A}\mathbf{x} = \mathbf{0}$.

* If $\mathbf{A} \in \mathbb{R}^{m \times n}$, then the null space is a subspace of $\mathbb{R}^n$.

For any matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$,

```math
\text{rank}(\mathbf{A}) + \text{nullity}(\mathbf{A}) = n
```

where $n$ is the number of columns.

**Intuition:**

* The rank tells you how many directions in the input space $\mathbb{R}^n$ are mapped to something nonzero (the "image" of the transformation).
* The nullity tells you how many directions are "collapsed" to zero by $\mathbf{A}$.

Example: Nullity and Rank

Let

```math
\mathbf{A} = \begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6
\end{bmatrix}
```

Row 2 is just 2 times row 1, so $\text{rank}(\mathbf{A}) = 1$.

* $n = 3$ (three columns).
* By the rank-nullity theorem, $\text{nullity}(\mathbf{A}) = 3 - 1 = 2$.

Let us find the null space explicitly:
Solve $\mathbf{A}\mathbf{x} = \mathbf{0}$:

```math
\begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0
\end{bmatrix}
```

Gives two equations:

```math
1x_1 + 2x_2 + 3x_3 = 0 \\
2x_1 + 4x_2 + 6x_3 = 0
```

But the second equation is just twice the first, so we only need to solve the first.
Let $x\_2 = s$, $x\_3 = t$ be free parameters. Then

```math
x_1 = -2s - 3t
```

So the general solution is

```math
\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix}
= s \begin{bmatrix}
-2 \\ 1 \\ 0
\end{bmatrix}
+ t \begin{bmatrix}
-3 \\ 0 \\ 1
\end{bmatrix}
```

which forms a 2-dimensional subspace—confirming the nullity is 2.

Given a linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ represented by the matrix $\mathbf{A}$:

* The **image** (or column space) of $T$ is the set of all vectors in $\mathbb{R}^m$ that can be written as $\mathbf{A}\mathbf{x}$ for some $\mathbf{x} \in \mathbb{R}^n$. Its dimension is the **rank**.
* The **kernel** (or null space) of $T$ is the set of all vectors $\mathbf{x} \in \mathbb{R}^n$ such that $\mathbf{A}\mathbf{x} = \mathbf{0}$. Its dimension is the **nullity**.

The rank-nullity theorem says that

> **The dimension of the domain = dimension of the image + dimension of the kernel.**

This is a fundamental result, and you will see it everywhere in linear algebra, from solving systems of equations to understanding decompositions and projections.
</Callout>

## Orthogonal Subspaces and Complements

## Spaces of Functions

We can also define a space of functions. This is a vector space where the elements are functions. The functions must satisfy the properties of a vector space.

For example C[a,b] is the space of continuous functions on the interval [a,b]. This is a vector space because the sum of two continuous functions is continuous and the multiplication of a continuous function with a scalar is continuous.

Can also be extended to C^k[a,b] where the functions are k-times differentiable.

P_m is the space of polynomials of degree m. This is a vector space because the sum of two polynomials is a polynomial and the multiplication of a polynomial with a scalar is a polynomial.
