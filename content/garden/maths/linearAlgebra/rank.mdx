import Callout from '@components/Callout/Callout';
import Image from '@components/Image/Image';

# Rank of a Matrix

The rank of a matrix is a key measurement of a matrix and has many important properties such as that **only full rank matrices can be inverted**. However, for now we will just define it as the number of linearly independent rows or columns in the matrix. The number of linearly independent rows is also often called the **row rank** and the number of linearly independent columns is called the **column rank**. 

This can intuitively be interpreted as the amount of information contained in the matrix, as each linearly independent row or column adds new information that cannot be derived from the others. Later on we will see that the rank also corresponds to the number of pivots in the reduced row echelon form of the matrix, which is a more practical way to compute the rank and that it also corresponds to the dimension of the row space or column space of the matrix. We commonly denote the rank of a matrix $A$ as follows:

```math
r = \text{rank}(\mathbf{A})
```

Determining the rank of a matrix can be done easily by hand by transforming the [matrix into its reduced row echelon form (RREF) with gaussian elimination and counting the number of non-zero rows](). The rank can also be computed using various algorithms or [singular value decomposition (SVD)](), which are more efficient for larger matrices.

Because the rank is defined as the number of linearly independent rows or columns, it is important to note that the rank of a matrix is always less than or equal to the minimum of the number of rows and columns in the matrix. In other words, if a matrix has $m$ rows and $n$ columns, then:

```math
\text{rank}(\mathbf{A}) \leq \min(m, n)
```

For a square matrix with $n$ rows and $n$ columns, the rank can therefore be at most $n$. If the rank is equal to $n$, then the matrix is said to be **full rank**, meaning that all rows and columns are linearly independent. If matrix is tall so that $m > n$, then the rank can be at most $n$, but if all the columns are linearly independent, then the rank is $n$ and the matrix is said to be **column full rank**. If the matrix is wide so that $m < n$, then the rank can be at most $m$, but if all the rows are linearly independent, then the rank is $m$ and the matrix is said to be **row full rank**.

For a matrix $\mathbf{A}$ to have $\text{rank}(\mathbf{A}) = 0$, it must be the zero matrix, i.e. all entries are zero. This is because the only linear combination of the rows or columns that can equal the zero vector is the trivial solution where all scalars are zero as if you have a non-zero row or column, then you automatically have at least one linearly independent row or column.

We have also already seen how to construct a matrix $\mathbf{B}$ with $\text{rank}(\mathbf{B}) = 1$ by taking the outer product of two vectors $\mathbf{u}$ and $\mathbf{v}$, i.e. $\mathbf{B} = \mathbf{u}\mathbf{v}^T$. This is because the resulting matrix will have all rows as scalar multiples of the first row, hence it has only one linearly independent row:

```math
\begin{bmatrix}
u_1 \\
u_2 \\
\vdots \\
u_m
\end{bmatrix}
\begin{bmatrix}
v_1 & v_2 & \ldots & v_n
\end{bmatrix}
=
\begin{bmatrix}
u_1 v_1 & u_1 v_2 & \ldots & u_1 v_n \\
u_2 v_1 & u_2 v_2 & \ldots & u_2 v_n \\
\vdots & \vdots & \ddots & \vdots \\
u_m v_1 & u_m v_2 & \ldots & u_m v_n
\end{bmatrix}
```

Another way of describing the rank of a matrix is as the dimension of the row space or column space of the matrix. The row space is the subspace spanned by the rows of the matrix, while the column space is the subspace spanned by the columns of the matrix. The rank is then equal to the dimension of these subspaces, which is the number of linearly independent vectors needed to span the subspace. This means that the rank of a matrix is also equal to the number of pivots in the reduced row echelon form of the matrix, which is a practical way to compute the rank.

<Callout type="example">
This example will be very simple to give some intuition of the rank as calculating the rank of a matrix is discussed in the other sections. First let us start with some simple matrices:

```math
\begin{align*}
&\begin{bmatrix}
1 \\
2 \\
4
\end{bmatrix}
\,
&\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\,
&\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\, 
&\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1
\end{bmatrix} \\
&r=1
\,
&r=0
\,
&r=3
\,
&r=1
\end{align*}
```

And then some less obvious ones:
```math
\begin{align*}
&\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
\,
&\begin{bmatrix}
1 & 3 \\
2 & 6 \\
4 & 12
\end{bmatrix}
\,
&\begin{bmatrix}
1 & 3.1 \\
2 & 6 \\
4 & 12 
\end{bmatrix}
\,
&\begin{bmatrix}
1 & 3 & 2 \\
6 & 6 & 1 \\
4 & 2 & 0
\end{bmatrix} \\
&r=3
\,
&r=1
\,
&r=2
\,
&r=3
\end{align*}
```
</Callout>

- rank of scalar multiplied matrix

```math
\text{rank}(c\mathbf{A}) = \text{rank}(\mathbf{A}) \quad \text{for } c \in \mathbb{R} \setminus \{0\}
```

- rank of sum of matrices

```math
\text{rank}(\mathbf{A} + \mathbf{B}) \leq \text{rank}(\mathbf{A}) + \text{rank}(\mathbf{B})
```

- rank of product of matrices

```math
\text{rank}(\mathbf{A}\mathbf{B}) \leq \min(\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B}))
```

- rank of $A^T$ this should show that column rank = row rank always

- the rank of the above are also the same as the rank of $A^T A$ and $A A^T$.

## Rank-Nullity Theorem

## Regularization

We can make a matrix full rank by adding some scaled identity matrix to it, this is called regularization. This is often used in machine learning to prevent overfitting, as it adds some noise to the data and makes the model more robust. The regularization parameter $\lambda$ controls the amount of noise added to the data, and can be tuned to achieve the desired level of robustness?

<Callout type="todo">
Show that column rank = row rank always
</Callout>