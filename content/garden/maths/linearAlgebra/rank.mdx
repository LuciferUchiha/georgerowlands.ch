import Callout from '@components/Callout/Callout';
import Image from '@components/Image/Image';

# Linear Independence and Rank

## Linear Independence

Two vectors are linearly independent if neither of them can be written as a linear combination of the other. In other words, two vectors are linearly independent if they are not scalar multiples of each other. It is however easier to define and check for linear dependence. The vectors $\mathbf{a}$ and $\mathbf{b}$ are linearly dependent if they are scalar multiples of each other, i.e. if there exists a scalar $c$ such that:

```math
\mathbf{a} = c\mathbf{b} \quad \text{for some } c \in \mathbb{R}
```

this can also be written as:

```math
\mathbf{a} - c\mathbf{b} = \mathbf{0}
```

where $\mathbf{0}$ is the zero vector. This means that the vectors $\mathbf{a}$ and $\mathbf{b}$ are linearly dependent 
if they are collinear, i.e. they lie on the same line. The two equations above can also be used to define linear independence, we just replace the equal sign with a not equal sign. So the vectors $\mathbf{a}$ and $\mathbf{b}$ are linearly independent if:

```math
\mathbf{a} - c\mathbf{b} \neq \mathbf{0} \quad \text{for all } c \in \mathbb{R}
```

<Image
    src="/maths/vectorLinearDependence2D.png"
    caption="The left two vectors are linearly independent, while the right two vectors are linearly dependent."
    width={400}
    />

<Callout type="example">
If $\mathbf{a}$ and $\mathbf{b}$ are defined as:

```math
\mathbf{a} = \begin{bmatrix}
    1 \\
    2 \\
    3
\end{bmatrix} \quad \text{and} \quad \mathbf{b} = \begin{bmatrix}
    2 \\
    4 \\
    6
\end{bmatrix}
```

then $\mathbf{a}$ and $\mathbf{b}$ are linearly dependent because:

```math
\mathbf{a} = 2\mathbf{b}
```

However, if $\mathbf{a}$ and $\mathbf{b}$ are defined as:

```math
\mathbf{a} = \begin{bmatrix}
    1 \\
    2 \\
    3
\end{bmatrix} \quad \text{and} \quad \mathbf{b} = \begin{bmatrix}
    2 \\
    3 \\
    4
\end{bmatrix}
```

then $\mathbf{a}$ and $\mathbf{b}$ are linearly independent because no scalar multiple of $\mathbf{b}$ can be equal to $\mathbf{a}$.
</Callout>

<Callout type="todo">
Show more why these are equivalent and the derivations
</Callout>

This idea can then be extended to a set of vectors, looking at a sequence of vectors doesn't make a lot of sense as 
the same vector is obviously linearly dependent on itself. 

A set is linearly dependent:
- If one of the vectors in the set can be written as a linear combination of the others
- At least one of the vectors in the set can be written as a linear combination of the previous vectors.
- There is a non-trivial solution to make the linear combination equal to the zero vector 

More formally, a set of vectors $\{\mathbf{v_1}, \mathbf{v_2}, \ldots, \mathbf{v_n}\}$ is linearly dependent if there exist scalars $c_1, c_2, \ldots, c_n$, not all zero, such that:

```math
c_1\mathbf{v_1} + c_2\mathbf{v_2} + \ldots + c_n\mathbf{v_n} = \mathbf{0}
```

If the only solution to this equation is the trivial solution where all scalars are zero, then the set of vectors is linearly independent. From this it obviously follows that the null vector is always linearly dependent as we can creat it by multiplying any vector by zero.

## Rank of a Matrix

The rank of a matrix is a key measurement of a matrix and has many important properties such as that **only full rank matrices can be inverted**. However, for now we will just define it as the number of linearly independent rows or columns in the matrix. This can intuitively be interpreted as the amount of information contained in the matrix, as each linearly independent row or column adds new information that cannot be derived from the others. Later on we will see that the rank also corresponds to the number of pivots in the reduced row echelon form of the matrix, which is a more practical way to compute the rank and that it also corresponds to the dimension of the row space or column space of the matrix. We commonly denote the rank of a matrix $A$ as follows:

```math
r = \text{rank}(\mathbf{A})
```

Determining the rank of a matrix can be done easily by hand by transforming the [matrix into its reduced row echelon form (RREF) with gaussian elimination and counting the number of non-zero rows](). The rank can also be computed using various algorithms or [singular value decomposition (SVD)](), which are more efficient for larger matrices.

Because the rank is defined as the number of linearly independent rows or columns, it is important to note that the rank of a matrix is always less than or equal to the minimum of the number of rows and columns in the matrix. In other words, if a matrix has $m$ rows and $n$ columns, then:

```math
\text{rank}(\mathbf{A}) \leq \min(m, n)
```

For a square matrix with $n$ rows and $n$ columns, the rank can therefore be at most $n$. If the rank is equal to $n$, then the matrix is said to be **full rank**, meaning that all rows and columns are linearly independent. If matrix is tall so that $m > n$, then the rank can be at most $n$, but if all the columns are linearly independent, then the rank is $n$ and the matrix is said to be **column full rank**. If the matrix is wide so that $m < n$, then the rank can be at most $m$, but if all the rows are linearly independent, then the rank is $m$ and the matrix is said to be **row full rank**.

<Callout type="example">
This example will be very simple to give some intuiton of the rank as calculating the rank of a matrix is discussed in the other sections.

First let us start with some simple matrices:

```math
\begin{align*}
&\begin{bmatrix}
1 \\
2 \\
4
\end{bmatrix}
\,
&\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\,
&\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\, 
&\begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1
\end{bmatrix} \\
&r=1
\,
&r=0
\,
&r=3
\,
&r=1
\end{align*}
```

And then some less obvious ones:
```math
\begin{align*}
&\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
\,
&\begin{bmatrix}
1 & 3 \\
2 & 6 \\
4 & 12
\end{bmatrix}
\,
&\begin{bmatrix}
1 & 3.1 \\
2 & 6 \\
4 & 12 
\end{bmatrix}
\,
&\begin{bmatrix}
1 & 3 & 2 \\
6 & 6 & 1 \\
4 & 2 & 0
\end{bmatrix} \\
&r=3
\,
&r=1
\,
&r=2
\,
&r=3
\end{align*}
```
</Callout>

- rank of scalar multiplied matrix

```math
\text{rank}(c\mathbf{A}) = \text{rank}(\mathbf{A}) \quad \text{for } c \in \mathbb{R} \setminus \{0\}
```

- rank of sum of matrices

```math
\text{rank}(\mathbf{A} + \mathbf{B}) \leq \text{rank}(\mathbf{A}) + \text{rank}(\mathbf{B})
```

- rank of product of matrices

```math
\text{rank}(\mathbf{A}\mathbf{B}) \leq \min(\text{rank}(\mathbf{A}), \text{rank}(\mathbf{B}))
```

- rank of $A^T$ this should show that column rank = row rank always

- the rank of the above are also the same as the rank of $A^T A$ and $A A^T$.

rank nullity theorem?

We can make a matrix full rank by adding some scaled identity matrix to it, this is called regularization. This is often used in machine learning to prevent overfitting, as it adds some noise to the data and makes the model more robust. The regularization parameter $\lambda$ controls the amount of noise added to the data, and can be tuned to achieve the desired level of robustness?

<Callout type="todo">
Show that column rank = row rank always
</Callout>