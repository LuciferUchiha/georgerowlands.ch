import Callout from "@components/Callout/Callout";
import Image from "@components/Image/Image";

# Convergent Series

We have seen that a [series](/garden/maths/calculus/sequencesAndSeries/series) is the sum of the terms of a [sequence](
/garden/maths/calculus/sequencesAndSeries/sequences). 

Just like sequences, series can also [converge or diverge](/garden/maths/calculus/sequencesAndSeries/convergentSequences). A series converges if the sequence of partial sums converges. So in other words using the original sequence we calculate a new sequence, where each term is the sum of all terms up to that point. 

```math
\begin{align*}
\text{Sequence} & : (a_1, a_2, a_3, \ldots, a_n) \\
\text{Series} & : S_n = \sum^{n}_{k=1}{a_k} = a_1 + a_2 + a_3 + \ldots + a_n \\
\text{Sequence of partial sums} & : S_1, S_2, S_3, \ldots, S_n
\end{align*}
```

If the sequence of partial sums converges then the series converges. The limit of the sequence of partial sums is called the sum or value of the series. If the sequence of partial sums diverges then the series diverges.

For a series to converge the underlying sequence must be a null sequence, in other words the limit of the original sequence must be zero. This is a necessary but not sufficient condition. There are series that diverge even though the sequence of terms converges to zero.

```math
\sum_{n=1}^{\infty} a_n \text{ converges} \implies \lim_{n \to \infty} a_n = 0
```

There is an intuitive interpretation behind this condition. Imagine you're summing up the terms of a series. For the series to converge, the partial sums need to settle on a finite value as you keep adding more and more terms. If the terms of the sequence do not approach zero, it becomes impossible for the partial sums to settle, and the series will diverge. In short, if the terms are not getting smaller and smaller, the series will keep getting larger and larger and will not converge.

Another important property of convergent series is that the order in which we sum the terms does not matter as the addition of real numbers is associative and commutative. This means that we can rearrange the terms of a convergent series without affecting its convergence or the value of the series. So formally if the function $f$ is a bijective mapping from the natural numbers to the natural numbers, then:

```math
\sum_{n=1}^{\infty} a_n = \sum_{n=1}^{\infty} a_{f(n)}
```

In addition, just like with the convergence of a sequence, what the series does in the beginning does not affect the convergence of the series. For example, if we start summing the terms of a series at a different index, the series will still converge, but the value of the series will change. This is of course only true for real valued series, as if we would add infinity and then add a finite number, the series would diverge and if we skipped adding the infinity term, the series would converge to a finite value. So more formally for any $m \geq N_0$ we have:

```math
\sum_{n=N_0}^{\infty} a_n \text{converges} \iff \sum_{n=m}^{\infty} a_n \text{converges}
```

<Callout type="proof">

</Callout>

<Callout type="example" title="Geometric Series">
Let's look at some example of series and analyze their convergence. We have seen [the geometric sequence](/garden/maths/calculus/convergentSeries) before. The geometric sequence is a sequence where each term is a constant multiple of the previous term. So formally the geometric sequence is defined as follows where $q \in \mathbb{C}$ is a constant:

```math
a_n = q^n
```

Then the geometric sequence has the following properties as $n \to \infty$:
- If $|q| < 1$, then the sequence converges to 0.
- If $q = 1$, then the sequence  converges to 1.
- If $q = -1$, then the sequence oscillates between 1 and -1.
- If $|q| > 1$, then the sequence diverges to infinity.

The geometric series is then just the sum of the terms of the geometric sequence:

```math
\sum^{\infty}_{k=0}{q^k}
```

For the series to converge we need to check if the sequence of partial sums converges. The sequence of partial sums is:

```math
\begin{align*}
S_n &= \sum^{n}_{k=0}{q^k} = 1 + q + q^2 + \ldots + q^n \\ 
q * S_n &= q + q^2 + q^3 + \ldots + q^{n+1} \\
S_n - q * S_n &= 1 - q^{n+1} \\
(1-q) * S_n &= 1 - q^{n+1} \\
S_n &= \frac{1 - q^{n+1}}{1-q}
\end{align*}
```

Now we have a closed form for the sequence of partial sums and therefore we can analyze the convergence of the geometric series. For this we take the limit of the sequence of partial sums to see if the series converges. As $n \to \infty$ the term $q^{n+1}$ goes to zero as $|q| < 1$. So we can assume the limit exists and that it is $\frac{1}{1-q}$. Let's prove it is indeed the limit:

```math
\lim_{n \to \infty}\left|\frac{1 - q^{n+1}}{1-q} - \frac{1}{1-q}\right| = \lim_{n \to \infty}\left|\frac{1 - q^{n+1} - 1}{1-q}\right| = \lim_{n \to \infty}\left|\frac{- q^{n+1}}{1-q}\right| = \lim_{n \to \infty}\left|\frac{q^{n+1}}{1-q}\right| = 0
```

So the sequence of partial sums converges to $\frac{1}{1-q}$ with $|q| < 1$, which means the geometric series converges to $\frac{1}{1-q}$.

Let's look at a concrete example of a geometric series for $q = \frac{1}{2}$:

```math
\begin{align*}
\sum^{\infty}_{k=0}{\left(\frac{1}{2}\right)^k} &= 1 + \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \ldots \\
&= \frac{1}{1-\frac{1}{2}} = \frac{1}{\frac{1}{2}} = 2
\end{align*}
```

But what if the index starts at 1 rather than 0 to see if the starting index has an effect on the convergence of the series and the value of the series. For this let's revisit the closed form for the sequence of partial sums:

```math
\begin{align*}
\sum^{\infty}_{k=1}{q^k} &= q + q^2 + q^3 + \ldots \\
&= \sum^{\infty}_{k=0}{q^k} - q^0 = \frac{1}{1-q} - 1 = \frac{q}{1-q}
\end{align*}
```

We know that this sequence still converges. So we can say that what we do in the first steps does not have an effect on the convergence of the series. However, it does have an effect on the value of the series. So the geometric series for $q = \frac{1}{2}$ starting at 1 converges to $\frac{\frac{1}{2}}{1-\frac{1}{2}} = 1$ instead of 2.

Another way of looking at this is that we know that $\sum^{\infty}_{k=0}{\frac{1}{2^k}} = 2$ and that we can write this also as follows:

```math
\sum^{\infty}_{k=1}{\frac{1}{2^k}} = 2 = 1 + \frac{1}{2} + \frac{1}{4} + \sum^{\infty}_{k=3}{\frac{1}{2^k}} 
```

So therefore we must have:

```math
\sum^{\infty}_{k=3}{\frac{1}{2^k}} = \frac{1}{4}
```
</Callout>

<Callout type="example" title="Harmonic Series">
We have seen an example of a series that converges, now let's look at an example of a series that diverges. Just like with the geometric series that uses the geometric sequence, we can also look at the harmonic series that uses the [harmonic sequence](/garden/maths/calculus/convergentSeries). In this case we will analyze the following harmonic series:

```math
\sum^{\infty}_{n=1}{\frac{1}{n}}
```

We already know that the harmonic sequence converges to zero as $n$ approaches infinity. So the precondition is met. However, this does not automatically mean that the series also converges. In fact, the harmonic series diverges despite the sequence of terms converging to zero, the series diverges. This is a good example to show that the terms of the sequence converging to zero is a necessary but not sufficient condition for the series to converge. 

<Callout type="todo">
This isn't so intuitive so it will need a proof. Maybe later on with cauchy criterion or something like that.
</Callout>

<Image
    src="/maths/harmonicSeries.png"
    alt="Harmonic Series"
    width={500}
/>

However, there is a nice visualization showing that the series is bounded.

<Image
    src="/maths/harmonicSeriesBound.jpg"
    alt="Harmonic Series"
    width={500}
/>
</Callout>

<Callout type="example" title="Telescoping Series">
The telescoping series is another special case of a series that converges. The telescoping series is a series where the terms cancel each other out in a way that the series converges to a finite value. The telescoping series is defined as follows:

```math
\sum^{\infty}_{n=1}{\frac{1}{n(n+1)}}
```

Now you might be wondering why this series is called telescoping series. The reason is that we can rewrite the terms to the following:

```math
\frac{1}{n(n+1)} = \frac{1}{n} - \frac{1}{n+1}
```

So then when we write out the first few terms of the series, we get:

```math
\sum^{\infty}_{n=1}{\left(\frac{1}{n} - \frac{1}{n+1}\right)} = \left(1 - \frac{1}{2}\right) + \left(\frac{1}{2} - \frac{1}{3}\right) + \left(\frac{1}{3} - \frac{1}{4}\right) + \ldots
```

and as we can see the terms cancel each other out in a way that we are left with only the first term and the last term of the series. So we can rewrite the series as follows:

```math
\sum^{\infty}_{n=1}{\frac{1}{n(n+1)}} = \lim_{n \to \infty} \left(1 - \frac{1}{n+1}\right) = 1
```
</Callout>

<Callout type="info" title="Beware of Infinite Sums">
In the case of series we can not just use our usual rules for the [sum operator](/garden/maths/calculus/sequencesAndSeries/sumOperator). The reason is because the sums go to infinity and we can not just use the normal rules of arithmetic. 

To see this let's compare two example previously seen examples. We can rewrite 

```math
\sum^{\infty}_{n=1}{\frac{1}{n(n+1)}} = \sum^{\infty}_{n=1}{\frac{1}{n}} - \sum^{\infty}_{n=1}{\frac{1}{n+1}}$
```

We already know this is the telescoping series that converges to 1. So in this infinity minus infinity case we get 1.
However, if we try to do the same for the value of 0 and the sum over 0. We can do the following split: 

```math
0 = \sum^{\infty}_{n=1}{1} - \sum^{\infty}_{n=1}{1}
```

which is not correct as these then diverge and we suddenly get a value of 1 (infinity minus infinity).

So we can only perform operations with series if we have a series $\sum^{\infty}_{k=1}{a_k}$ and $\sum^{\infty}_{j=1}{b_j}$ and they both converge. If this is the case we can perform the following operations:  
- $\sum^{\infty}_{k=1}{c*a_k}=c* \sum^{\infty}_{k=1}{a_k}$ für $c \in C$
- $\sum^{\infty}_{k=1}{a_k\pm b_k}=\sum^{\infty}_{k=1}{a_k}\pm \sum^{\infty}_{k=1}{b_k}$

<Callout type="todo">
Add a proof for this. Maybe using the sequence of partial sums.
</Callout> 

</Callout>

## Cauchy Criterion

We have seen that for sequences we can use the [Cauchy criterion](/garden/maths/calculus/sequencesAndSeries/convergentSequences#cauchy-criterion) to check if a sequence converges. Specifically we can say that a sequence converges if the following holds:

```math
\forall \epsilon > 0 \quad \exists N_{\epsilon} \in \mathbb{N} : |a_n - a_m| < \epsilon \text{ for all } n, m \geq N_{\epsilon}
```

We can also apply this to series. The Cauchy criterion for series states that a series converges if the following holds:

```math
\forall \epsilon > 0 \exists N_{\epsilon} \in \mathbb{N} : |\sum^{m}_{k=n} a_k| < \epsilon \text{ for all } m \geq n \text{ and } n \geq N_{\epsilon}
```

So after some point $N_{\epsilon}$ the sum of all terms from $n$ to all $m$ is less than $\epsilon$. So in other words after some point the sum of all terms does not change the value of the series anymore. This is a very powerful test to check if a series converges. 

<Callout type="proof">
The proof is rather simple. We know a series converges if the sequence of partial sums converges. So we can just apply the Cauchy criterion for sequences to the sequence of partial sums. So if the partial sums are Cauchy, then the series converges. For the partial sums to be Cauchy, we need to show that the following holds where $S_n = \sum^{n}_{k=1}{a_k}$:

```math
\forall \epsilon > 0 \exists N_{\epsilon} \in \mathbb{N} : |S_m - S_{n-1}| < \epsilon \text{ for all } n, m \geq N_{\epsilon}
```

From this we can derive the following which gives us the Cauchy criterion for series:

```math
|S_m - S_{n-1}| = |\sum^{m}_{k=1}{a_k} - \sum^{n-1}_{k=1}{a_k}| = |\sum^{m}_{k=n}{a_k}| < \epsilon \text{ for all } n, m \geq N_{\epsilon}
```
</Callout>

The cauchy criterion also has a special property that follows from it. Just like with the sequences at some point the difference must go to zero. For the series this means that the terms of the series must go to zero and therefore also the partial sum. So in other words after some point to all $m \geq n$ the section of the series must be zero. More formally:

```math
\sum_{k=1}^{\infty} a_k \text{ converges} \implies \lim_{n \to \infty} \sum_{k=n}^{m} a_k = 0
```

<Callout type="proof">
Using the cauchy criterion we can actually prove that the underlying sequence must be a null sequence. The proof is pretty simple. We assume we have a series $\sum_{n=1}^{\infty} a_n$ that converges. Then from the cauchy criterion we know that the following holds:

```math
\forall \epsilon > 0 \exists N_{\epsilon} \in \mathbb{N} : |\sum^{m}_{k=n} a_k| < \epsilon \text{ for all } m \geq n \text{ and } n \geq N_{\epsilon}
```

Then if we specifically set $m=n$ we get:

```math
\begin{align*}
|\sum^{n}_{k=n} a_k| < \epsilon \\
|a_n| < \epsilon \\
|a_n - 0| < \epsilon
\end{align*}
```

So we can see that the terms of the series must go to zero as $n$ approaches infinity. So we can conclude that the underlying sequence must be a null sequence.
</Callout>

<Callout type="example">
She just shows that the sequence goes to zero and then splits it into the telescoping series and the geometric series to find the value of the series.

```math
\sum^{\infty}_{n=1}{\frac{3^n + n^2 + n}{3^{n+1}(n(n+1))}}
```
</Callout>

## Direct Comparison Test

The direct comparison test is another method like the Cauchy criterion to check if a series converges. The idea comes from the [Monotone Convergence Theorem](/garden/maths/calculus/sequencesAndSeries/convergentSequences#monotone-convergence-theorem) and is in my opinion a lot more intuitive then the cauchy criterion. 

First let's just focus on series that converges where after some point $k$ the terms are always positive. So more formally we focus on series where the following holds:

```math
\text{Let } \sum^{\infty}_{n=1}{a_n} \text{ be a series with } a_n \geq 0 \text{ for all } n \geq k \text{ where } k \geq 1
```

The idea is to use the monoton convergence theorem. First we need to show that the sequence of partial sums is monotone increasing. This is true if the terms of the series are always positive. 

```math
S_{n+1} - S_n = \sum^{n+1}_{k=1}{a_k} - \sum^{n}_{k=1}{a_k} = a_{n+1} \geq 0 \text{ for all } n \geq 1
```

Now because we know that the sequence of partial sums is monotone increasing, and that it converges as otherwise the series would diverge, we can conclude that the sequence of partial sums must also be bounded by the monotone convergence theorem. 

<Callout type="todo">
Add an example of using this and showing that the sequence of partial sums is bounded. 
</Callout>

The direct comparison follows from the idea shown above but in a more general way and kind of mixing it with the [squeeze theorem](/garden/maths/calculus/sequencesAndSeries/convergentSequences#squeeze-theorem). The idea is that we can compare two series and show that one converges if the other converges. The formal definition of the direct comparison test is as follows. If we have two series $\sum^{\infty}_{n=1}{a_n}$ and $\sum^{\infty}_{n=1}{b_n}$ and we know that the following holds for all $k \geq 1$:

```math
0 \leq a_k \leq b_k
```

So the terms of both series are always positive and the terms of the first series are always less than or equal to the terms of the second series. Then we can say that if the second series converges, then the first series also converges. 

```math
\sum^{\infty}_{n=1}{b_n} \text{ converges} \implies \sum^{\infty}_{n=1}{a_n} \text{ converges}
```

The intuition behind this is rather simple. If $b_n$ converges, then the sum of all terms of $b_n$ is finite. If $a_n$ is always less than or equal to $b_n$, then the sum of all terms of $a_n$ must also be finite. Because we are only allowing for positve terms, we also do not have to worry about convergence against negative infinity. 

<Callout type="proof">
The proof follows from the cauchy criterion.
</Callout>

In a similar way we can also show that if the first series diverges, then the second series also diverges. 

```math
\sum^{\infty}_{n=1}{a_n} \text{ diverges} \implies \sum^{\infty}_{n=1}{b_n} \text{ diverges}
```

The idee is again the same, if $a_n$ diverges, then the sum of all terms of $a_n$ is infinite. If $b_n$ is always greater than or equal to $a_n$, then the sum of all terms of $b_n$ must also be infinite and therefore diverges.

We can actually make this test even more powerful. Above we defined that the terms of $b_k$ always had to be greater than or equal to the terms of $a_k$ and that the terms of $a_k$ always had to be positive. It turns out that the test also works if we can show that the conditions are met for all $k \geq m$ for some $m \in \mathbb{N}$. So we can have a finite number of terms that do not meet the conditions. The intuition behind this is that the first $m$ terms of the series do not have an influence on the convergence of the series only on the value of the series. So the test works as long as the following holds:

```math
0 \leq a_k \leq b_k \text{ for all } k \geq m \text{ where } m \geq 1
```


<Callout type="example">
We know that the harmonic series $\sum^{\infty}_{n=1}{\frac{1}{n}}$ diverges. So what if the denominator is squared? Does the series $\sum^{\infty}_{n=1}{\frac{1}{n^2}}$ converge? We can use the direct comparison test to show that it does by comparing it to the telescoping series $\sum^{\infty}_{n=1}{\frac{1}{n(n-1)}}$. We know that the telescope series converges to 1. So we want to show that the other series converges as well. We can do this by showing that the terms of the first series are always less than or equal to the terms of the second series and that they are always positive. The telescope series is not defined for $n=1$ but if we start at $n=2$ we can see that the terms of the first series are always less than or equal to the terms of the second series. 

| $k$ | $\frac{1}{k^2}$  | $\frac{1}{k(k-1)}$ |
|------|-----------------|--------------------|
| 1    | 1               | Undefined          |
| 2    | $\frac{1}{4}$   | $\frac{1}{2}$      |
| 3    | $\frac{1}{9}$   | $\frac{1}{6}$      |
| 4    | $\frac{1}{16}$  | $\frac{1}{12}$     |
| 5    | $\frac{1}{25}$  | $\frac{1}{20}$     |
| 6    | $\frac{1}{36}$  | $\frac{1}{30}$     |

This comes from the following:

```math
k(k-1) \leq k^2 \implies \frac{1}{k(k-1)} \geq \frac{1}{k^2} \text{ for all } k \geq 2
```

So therefore:

```math
1 + \sum^{\infty}_{n=2}{\frac{1}{n^2}} \leq 1 + \sum^{\infty}_{n=2}{\frac{1}{n(n-1)}}
```

and by the direct comparison test because the telescoping series converges we can say that the series $\sum^{\infty}_{n=1}{\frac{1}{n^2}}$ converges as well. 

What about the series $\sum^{\infty}_{n=1}{\frac{1}{n^3}}$? We can use the same idea as above:

```math
k^3 &\geq k^2 \implies \frac{1}{k^3} \leq \frac{1}{k^2} \text{ for all } k \geq 1 \\
```

So then it  follows that:

```math
\sum^{\infty}_{n=1}{\frac{1}{n^3}} \leq \sum^{\infty}_{n=1}{\frac{1}{n^2}}
```

And because we know that the series $\sum^{\infty}_{n=1}{\frac{1}{n^2}}$ converges, we can say that the series $\sum^{\infty}_{n=1}{\frac{1}{n^3}}$ also converges by the direct comparison test. We can actually generalize this to all series of the form $\sum^{\infty}_{n=1}{\frac{1}{n^p}}$ where $p > 1$ converges and for $p \leq 1$ diverges.

To show this we can look at the case where $p = \frac{1}{2}$ so we have the following series:

```math
\sum^{\infty}_{n=1}{\frac{1}{\sqrt{n}}}
```

We can use the direct comparison test to show that this series diverges. We can compare it to the harmonic series $\sum^{\infty}_{n=1}{\frac{1}{n}}$ which we know diverges. We can see that for all $n \geq 1$:

```math
\sqrt{n} \leq n \implies \frac{1}{\sqrt{n}} \geq \frac{1}{n}
```

So we can conclude that:

```math
\sum^{\infty}_{n=1}{\frac{1}{\sqrt{n}}} \geq \sum^{\infty}_{n=1}{\frac{1}{n}}
```

And because the harmonic series diverges, we can say that the series $\sum^{\infty}_{n=1}{\frac{1}{\sqrt{n}}}$ also diverges by the direct comparison test. 
</Callout>
<Callout type="example">
Another series that is actually similar to the series above is the following:

```math
\sum^{\infty}_{n=1}{\frac{1}{n!}}
```

We can use the direct comparison test to show that this series converges. You may know from analysing algorithms that the factorial grows very fast. A simple comparison would just to compare it with the squared series $\sum^{\infty}_{n=1}{\frac{1}{n^2}}$ which we know converges as the following holds:

```math
\frac{1}{n!} \leq \frac{1}{n^2} \text{ for all } n \geq 4
```

So we know that:

```math
\sum^{\infty}_{n=1}{\frac{1}{n!}} \leq \sum^{\infty}_{n=1}{\frac{1}{n^2}}
```

and therefore it converges by the direct comparison test. However, we can also show a tighter bound by observing the following:

```math
\begin{align*}
k! = 1 \cdot 2 \cdot 3 \cdots k \geq 1 \cdot 2 \cdot 2 \cdots 2 = 2^{k-1} \text{ for all } k \geq 2 \\
\frac{1}{k!} \leq \frac{1}{2^{k-1}} \text{ for all } k \geq 2
\end{align*}
```

which means:

```math
\sum^{\infty}_{n=1}{\frac{1}{n!}} \leq \sum^{\infty}_{n=2}{\frac{1}{2^{n-1}}}
```

And we know that the series $\sum^{\infty}_{n=2}{\frac{1}{2^{n-1}}}$ is a geometric series with $q = \frac{1}{2}$ which converges. So we can conclude that the series $\sum^{\infty}_{n=1}{\frac{1}{n!}}$ converges as well.
</Callout>

## Absolute Convergence

We have seen that series can converge to some limit or diverge. But there is also something called absolute convergence. A series is said to be absolutely convergent if the absolute values of the terms of the series converge. In other words, if we take the absolute value of each term in the series and then check if that series converges, then we can say that the original series is absolutely convergent. More formally we define absolute convergence as:

```math
\sum^{\infty}_{n=1}{|a_n|} \text{ converges}
```

If a series is absolutely convergent, then it is also convergent. However, the converse is not true. A series can be convergent without being absolutely convergent. 

```math
\sum^{\infty}_{n=1}{a_n} \text{ converges} \implies \sum^{\infty}_{n=1}{|a_n|} \text{ converges}
```

This is easily proven by the cauchy criterion for series. If the series is absolutely convergent, then the value of the series is finite. If we instead take the absolute value of the value of the series, we get a smaller or equal value as some terms may cancel each other out. So the absolute value of the series is less than or equal to the value of the series. Therefore the value of the series is also finite and converges.   

```math
|\sum^{m}_{k=n} a_k| \leq \sum^{m}_{k=n} |a_k| < \epsilon \text{ for all } m \leq n \text{ and } n \leq N_{\epsilon}
```

If we say that $S_n = \sum^{n}_{k=1}{a_k}$ and $T_n = \sum^{n}_{k=1}{|a_k|}$, then we also get the following from the above inequality:

```math
|S_n| = |\sum^{\infty}_{k=1}{a_k}| = |\lim_{n \to \infty} S_n| = \lim_{n \to \infty} |S_n| \leq \lim_{n \to \infty} T_n = \sum^{\infty}_{k=1}{|a_k|}
```

<Callout type="example">
Let's look at the series $\sum^{\infty}_{n=1}{(-1)^{n+1} \frac{1}{n}}$. The sequence of the terms converge to zero, so our first condition is met. If we analyze the values of the series then we notice that the series converges to $\ln(2)$. So the series converges. 

But if we take the absolute value of the terms of the series, we get the harmonic series $\sum^{\infty}_{n=1}{\frac{1}{n}}$ which diverges. So we can say that the original series is convergent but not absolutely convergent. This is a good example to show that the converse of that absolute convergence implies convergence is not true. 
</Callout>

## Alternating Series Test

Also known as Leibniz test. 

### Alternating Series Estimation Theorem

## Riemann Rearrangement Theorem

A Rearrangement of a series is a new series that is formed by rearranging the terms of the original series. More formally the series $\sum^{\infty}_{n=1}{x_n}$ is a rearrangement of the series $\sum^{\infty}_{n=1}{a_n}$ if there exists a bijective function of the form:

```math
f: \mathbb{N} \to \mathbb{N} \text{ such that } x_n = a_{f(n)} \text{ for all } n \in \mathbb{N}
```

Riemann showed that the series $\sum^{\infty}_{n=1}{(-1)^{n+1} \frac{1}{n}}$ could be rearranged to converge to any real number. 

### Dirchlet's Rearrangement Theorem

Dirchlet showed that if a series converges absolutely, then any rearrangement of the series converges and that it even converges to the same value. 

## Ratio Test

```math
\limsup_{n \to \infty} \left| \frac{a_{n+1}}{a_n} \right| < 1 \implies \sum^{\infty}_{n=1}{a_n} \text{ converges}
```

```math
\liminf_{n \to \infty} \left| \frac{a_{n+1}}{a_n} \right| > 1 \implies \sum^{\infty}_{n=1}{a_n} \text{ diverges}
```

## Root Test

Stronger version of the ratio test. Can do everything the ratio test can do and more. 

```math
\limsup_{n \to \infty} \sqrt[n]{|a_n|} < 1 \implies \sum^{\infty}_{n=1}{a_n} \text{ converges}
```

```math
\limsup_{n \to \infty} \sqrt[n]{|a_n|} > 1 \implies \sum^{\infty}_{n=1}{a_n} \text{ and } \sum^{\infty}_{n=1}{|a_n|} \text{ diverges}
```

## Cauchy Product