import Callout from "@components/Callout/Callout";
import Image from "@components/Image/Image";

# Differentiable Functions

We know that for a linear function with one variable we have the standard equation:

```math
y = mx + b
```

Where $m$ is the slope of the line and $b$ is the y-intercept. For the line defined by the function $f(x)$ finding the slope is easy by reordering the equation to the standard form. And because it is a line the slope is constant, so the same for all points $x_0$ in the domain of the function. However, what if we wanted to find the slope of a non-linear function at some point $x_0$? This is what the derivate is for. 

<Callout type="todo">
Add image of linear function with slope and y-intercept. and a non-linear function with a point $x_0$ and the slope at that point.
</Callout>

First let's just start with an approximation of the slope. For this we can define a **secant line** that goes through two points on the function. The slope of this secant line is then the so called **difference quotient**. So if we want to find the slope of the function $f$ at the point $x_0$, we can take a second point $x_1$ and calculate the slope of the secant line that goes through the points $P_0(x_0, f(x_0))$ and $P_1(x_1, f(x_1))$. We can then define the slope of the secant line as the difference of the function values divided by the difference of the $x$ values:

```math
m = \frac{f(x_1) - f(x_0)}{x_1 - x_0} = \frac{\Delta f}{\Delta x}
```

Hence it is called the difference quotient. 

<Callout type="todo">
Add image of different secant lines with different $x_1$ values.
</Callout>

Depending on the choice of $x_1$ this will give us a different slope. We can see if we choose $x_1$ very close to $x_0$, we get a better approximation of the slope at the point $x_0$. This leads us to the next idea, finding the slope of the tangent line at the point $x_0$. The slope of this tangent line is the derivative of the function at the point $x_0$. We get the tangent line by taking the limit of the difference quotient as $x_1$ approaches $x_0$. A common way of writing this is by defining the point $x_1$ with a small change denoted by $\Delta x$ or $h$. To then approach $x_0$ we can let $\Delta x$ or $h$ approach zero. This gives us the following definition of the derivative:

```math
m = \lim_{\Delta x \to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x} = \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}
```

<Callout type="todo">
Add image of tangent line at point $x_0$ with the slope defined by the limit of the difference quotient. What are actually the origin of sekant and tangent lines? Probably from trigonometry.
</Callout>

Using the derivate which is just the slope of the tangent line we can then find the equation of the tangent line at the point $x_0$, all we are missing the y-intercept $b$. We get this be reordering the equation of the tangent line to the point-slope form:

```math
b = f(x_0) - m \cdot x_0
```

So the equation of the tangent line is:

```math
T(x) = f(x_0) + m \cdot (x - x_0)
```

Where $m$ is the derivative of the function at the point $x_0$. Now we can formally define differentiable functions. We say that a function $f$ is differentiable at the point $x_0$ if $x_0$ is an [accumulation point]() of the domain of $f$ and the limit of the difference quotient exists, i.e. the derivate exists at the point $x_0$:

```math
f \text{ is differentiable at } x_0 \iff \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h} \text{ exists}
```

We denote this limit as $f'(x_0)$ or $\frac{df}{dx}(x_0)$, which is the derivative of the function $f$ at the point $x_0$. We say a function is differentiable on $D$ if it is differentiable at every point in the domain $D$. If this is the case then we can define the derivative function $f'$ as:

```math
f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} \forall x \in D
```

<Callout type="example" title="Constant Function">
Let us look at the derivative of some common functions to get a better understanding of differentiable functions. The constant function is defined as follows:

```math
f(x) = c \quad \forall x \in D
```

We can calculate the derivative of this function at any point $x_0$ in the domain $D$:

```math
\begin{align*}
\lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0} &= \lim_{x \to x_0} \frac{c - c}{x - x_0} \\
&= \lim_{x \to x_0} \frac{0}{x - x_0} \\
&= 0
\end{align*}
```

Therefore the derivative of the constant function for any $c \in \mathbb{R}$ and any point $x_0 \in D$ is:

```math
f'(x_0) = 0
```
</Callout>
<Callout type="example" title="Linear Function">

A (non-constant) linear function has the form  

```math
f(x) = mx + b \quad \forall x \in D
````

We can find the derivative at an arbitrary point $x_0 \in D$ as follows:

```math
\begin{align*}
\lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0}
&= \lim_{x \to x_0} \frac{m x + b - (m x_0 + b)}{x - x_0} \\
&= \lim_{x \to x_0} \frac{m(x - x_0)}{x - x_0} \\
&= \lim_{x \to x_0} m = m.
\end{align*}
```

Hence the derivative of any linear function is the slope itself:

```math
f'(x_0) = m.
```

Because the slope is constant, the function is differentiable everywhere and $f'(x)=m$ for all $x \in D$.
</Callout>
<Callout type="example" title="Quadratic Function">

Next we consider the simple quadratic (squared) function:

```math
f(x) = x^{2} \quad \forall x \in D.
````

To find the derivative at an arbitrary point $x_{0}\in D$ we apply the limit definition:

```math
\begin{align*}
\lim_{x \to x_{0}} \frac{f(x) - f(x_{0})}{x - x_{0}}
&= \lim_{x \to x_{0}} \frac{x^{2} - x_{0}^{2}}{x - x_{0}} \\
&= \lim_{x \to x_{0}} \frac{(x - x_{0})(x + x_{0})}{x - x_{0}} \\
&= \lim_{x \to x_{0}} (x + x_{0}) \\
&= 2x_{0}.
\end{align*}
```

Hence the derivative of $f(x)=x^{2}$ is for any point $x \in D$:

```math
f'(x) = 2x.
```

Because the tangent at any point has slope $2x$ the function is differentiable everywhere on its domain. An alternative approach would've been to use the formulation with $h$ instead of $x_0$:

```math
\begin{align*}
\lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}
&= \lim_{h \to 0} \frac{(x_0 + h)^{2} - x_{0}^{2}}{h} \\
&= \lim_{h \to 0} \frac{x_0^{2} + 2x_0 h + h^{2} - x_{0}^{2}}{h} \\
&= \lim_{h \to 0} \frac{2x_0 h + h^{2}}{h} \\
&= \lim_{h \to 0} (2x_0 + h) \\
&= 2x_0.
\end{align*}
```
</Callout>
<Callout type="example" title="Absolute Value Function">
The absolute-value function is defined piecewise:

```math
f(x)=
\begin{cases}
x & \text{if } x \geq 0,\\
-x & \text{if } x < 0.
\end{cases}
```

So to find the derivative we need to look at the two cases separately. We start with the case where $x_0>0$:

```math
\begin{align*}
\lim_{x \to x_0} \frac{f(x)-f(x_0)}{x-x_0}
  &= \lim_{x \to x_0} \frac{x - x_0}{x - x_0} = 1.
\end{align*}
```

So $f'(x_0)=1$. If we look at the case where $x_0<0$:

```math
\begin{align*}
\lim_{x \to x_0} \frac{f(x)-f(x_0)}{x-x_0}
  &= \lim_{x \to x_0} \frac{-x + x_0}{x - x_0}
  = \lim_{x \to x_0} \frac{-(x - x_0)}{x - x_0} = -1.
\end{align*}
```

So $f'(x_0)=-1$. Lastly we need to look at the case where $x_0=0$. In this case we have to look at the left and right limits separately as 

```math
\lim_{x \to 0^{+}} \frac{f(x)-f(0)}{x}
 = 1,
\qquad
\lim_{x \to 0^{-}} \frac{f(x)-f(0)}{x}
 = -1,
```

and because the two one-sided limits are not equal the limit does not exist as the difference approaches 0, so the derivative also does not exist at $x=0$. Putting this together we get the following piecewise definition of the derivative of the absolute value function:

```math
f'(x)=
\begin{cases}
 1, & x>0,\\
-1, & x<0,\\
\text{undefined}, & x=0.
\end{cases}
```

Thus $|x|$ is differentiable everywhere except for $x=0$.
</Callout>
<Callout type="example" title="Van der Waerden Function">

Van der Waerden function. Everywhere continous but nowhere differentiable.
</Callout>

## Tangent as an Approximation

It turns out that the tangent line is actually a pretty good approximation of the function in the area around the point $x_0$. This is useful because the tangent line is a simple linear function that we can easily work with and calculate with, whereas the original function might be more complex and expensive to compute. So we can approximate the function as follows:

```math
f(x) = T(x) + R_{x_0}(x)
```

Where $T(x)$ is the tangent line at the point $x_0$ and $R_{x_0}(x)$ is the remainder term. If $x \neq x_0$ Then we can rewrite this as:

```math
\begin{align*}
f(x) &= T(x) + R_{x_0}(x) \\
f(x) &= (f(x_0) + f'(x_0)(x - x_0)) + R_{x_0}(x) \\
f(x) - f(x_0) &= f'(x_0)(x - x_0) + R_{x_0}(x) \\
\frac{f(x) - f(x_0)}{x - x_0} &= f'(x_0) + \frac{R_{x_0}(x)}{x - x_0} \\
\lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0} &= f'(x_0) + \lim_{x \to x_0} \frac{R_{x_0}(x)}{x - x_0} \\
f'(x_0) &= f'(x_0) + \lim_{x \to x_0} \frac{R_{x_0}(x)}{x - x_0} \\
\lim_{x \to x_0} \frac{R_{x_0}(x)}{x - x_0} &= 0
\end{align*}
```

This means that as $x$ approaches $x_0$, the remainder term approaches zero faster than the difference of the points on the function. So for the limit to be zero the remainder term is always a small value compared to the difference of the points on the function. This is a very useful property of differentiable functions and allows us to use the tangent line as an approximation of the function in the area around the point $x_0$. If we define the remainder term as follows:

```math
r(x) = \begin{cases}
\frac{R_{x_0}(x)}{x - x_0} & \text{if } x \neq x_0 \\
0 & \text{if } x = x_0
\end{cases}
```

Then because we have $\lim_{x \to x_0} r(x) = 0 = r(x_0)$, we can say that the remainder term is continuous at the point $x_0$ because the limit is equal to the value, "filling" the original discontinuity hole. Because it is continuous it follows that if the function is also differentiable at $x_0$, then by the definition there exists some $m = f'(x_0)$ such that:

```math
f(x) = f(x_0) + f'(x_0)(x - x_0) + r(x)(x - x_0)
```

where $r(x_0) = 0$ and $f(x_0) + f'(x_0)(x - x_0) = mx + b$ is the equation of the tangent line at the point $x_0$.
This leads to the so called Weierstrass' differentiability criterion theorem which states that if a function is differentiable at a point and there exists some $c \in \mathbb{R}$ and function $r(x)$ that is continuous at $x_0$ such that the following holds:

```math
f(x) = f(x_0) + c (x - x_0) + r(x)(x - x_0)
```

Then the derivative $f'(x_0) = c$ is unique. This can be shown by assuming that there is another derivative $c$ and matching remainder term $s(x)$.

## Continuity of Differentiable Functions

Intuitively, differentiability is the "stronger" property then continuity, if you can zoom in far enough that a function looks perfectly linear, then its graph also can't have any jumps or holes at that point. However, we can also formally show this by using the definition of differentiability. Specifically, we can show that if a function $f$ is differentiable at a point $x_0$, then it is also continous at that point because then there exists some function $g$ that is continuous at $x_0$ such that the following holds:

```math
f(x) = f(x_0) + g(x)(x - x_0)
```

Where $g(x)=f'(x_0)$ is the derivative of $f$ at the point $x_0$. So for example $g$ could be for the linear function $f(x) = mx + b$ the slope $m$ of the tangent line at the point $x_0$. From this it follows that if $x_0$ is an accumulation point of the domain and the derivative exists, then the function is continuous at that point. 

```math
f \text{ is differentiable at } x_0 \implies f \text{ is continuous at } x_0
```

Then if a function is differentiable for all $x \in D$, then it is also continuous for all $x \in D$:

```math
f \text{ is differentiable on } D \implies f \text{ is continuous on } D
```

However, the converse is not true, i.e. a function can be continuous at a point but not differentiable at that point. A common example of this is the absolute value function which is continuous everywhere as $\lim_{x \to 0} |x| = 0 = |0|$ but not differentiable at $x_0 = 0$ because the left and right limits of the derivative at that point are not equal:

```math 
\lim_{x \to 0^{+}} \frac{|x| - |0|}{x - 0} = 1 \neq -1 = \lim_{x \to 0^{-}} \frac{|x| - |0|}{x - 0}
```

<Callout type="example" title="Exponential Function">
We already know that the exponential function is continuous everywhere. However, if we didn't know this, we could show that it is differentiable at any point $x_0$ in its domain which would then imply that it is also continuous at that point. We can find the derivative of the exponential function $f(x) = e^x = \exp(x)$ at an arbitrary point $x_0$ as follows. First we look at the difference:

```math
\begin{align*}
f(x_0 + h) - f(x_0) &= \exp(x_0 + h) - \exp(x_0) \\
&= \exp(x_0) \cdot \exp(h) - \exp(x_0) \\
&= \exp(x_0) (\exp(h) - 1)
\end{align*}
```

Then we can divide this by $h$ and take the limit as $h$ approaches zero:

```math
\begin{align*}
\lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h} &= \lim_{h \to 0} \frac{\exp(x_0) (\exp(h) - 1)}{h} \\
&= \exp(x_0) \cdot \lim_{h \to 0} \frac{\exp(h) - 1}{h} \\
&= \exp(x_0) \cdot 1 = \exp(x_0)
\end{align*}
```

So the derivative of the exponential function at any point $x_0$ is the exponential function itself:

```math
\exp'(x_0) = \exp(x_0)
```

The tricky part here is showing that the limit $\lim_{h \to 0} \frac{\exp(h) - 1}{h} = 1$ exists. This can be shown using the definition of the exponential function as a power series:

```math
\begin{align*}
\exp(h) = \sum_{n=0}^{\infty} \frac{h^n}{n!} = 1 + h + \frac{h^2}{2!} + \frac{h^3}{3!} + \ldots \\
\exp(h) - 1 = h + \frac{h^2}{2!} + \frac{h^3}{3!} + \ldots \\
\frac{\exp(h) - 1}{h} = 1 + \frac{h}{2!} + \frac{h^2}{3!} + \ldots
\end{align*}
```
</Callout>
<Callout type="example" title="Sine and Cosine Functions">
We can also look at the sine and cosine functions which are also continuous everywhere. We can find the derivative of the sine function at an arbitrary point $x_0$ in its domain by using the useful addition formula $\sin(a+b)=\sin a\cos b+\cos a\sin b$:

```math
\begin{align*}
\sin'(x_{0})
&=\lim_{h\to 0}\frac{\sin(x_{0}+h)-\sin x_{0}}{h} \\
&=\lim_{h\to 0}\frac{\sin x_{0}\cos h+\cos x_{0}\sin h-\sin x_{0}}{h}\\
&=\sin x_{0}\underbrace{\lim_{h\to 0}\frac{\cos h-1}{h}}_{=0}
  +\cos x_{0}\underbrace{\lim_{h\to 0}\frac{\sin h}{h}}_{=1}\\
&=\cos x_{0}
\end{align*}
```

Key here is that $\lim_{h\to 0}\frac{\sin h}{h}=1$ and $\lim_{h\to 0}\frac{\cos h-1}{h}=0$, which again can be shown using the there underlying power series definitions of the sine and cosine functions:

```math
\begin{align*}
\sin(h) = h - \frac{h^3}{3!} + \frac{h^5}{5!} - \ldots \\
\frac{\sin(h)}{h} = 1 - \frac{h^2}{3!} + \frac{h^4}{5!} - \ldots \\
\lim_{h \to 0} \frac{\sin(h)}{h} = 1
\text{ and } \\
\cos(h) = 1 - \frac{h^2}{2!} + \frac{h^4}{4!} - \ldots \\
\frac{\cos(h) - 1}{h} = -\frac{h}{2!} + \frac{h^3}{4!} - \ldots \\
\lim_{h \to 0} \frac{\cos(h) - 1}{h} = 0
\end{align*}
```

Therefore we have:

```math
\sin'(x) = \cos x
```

Because the cosine function is defined for all $x \in \mathbb{R}$, the sine function is differentiable everywhere on $\mathbb{R}$ and thus continuous everywhere on $\mathbb{R}$ as well.

Proceeding analogously for the cosine function we have:

```math
\begin{aligned}
\cos'(x_{0})
&=\lim_{h\to 0}\frac{\cos(x_{0}+h)-\cos x_{0}}{h}\\
&=\lim_{h\to 0}\frac{\cos x_{0}\cos h-\sin x_{0}\sin h-\cos x_{0}}{h}\\
&=\cos x_{0}\underbrace{\lim_{h\to 0}\frac{\cos h-1}{h}}_{=0}
  -\sin x_{0}\underbrace{\lim_{h\to 0}\frac{\sin h}{h}}_{=1}\\
&=-\sin x_{0}
\end{aligned}
```

Therefore we have:

```math
\cos'(x) = -\sin x
```
</Callout>

## Constant Rule

We have already seen in one of the examples that the derivative of a constant function is zero. This is a fundamental rule in calculus and is known as the **constant rule**. 

```math
f(x)=c \quad \Longrightarrow \quad f'(x)=0,\qquad \forall c \in\mathbb R
```

## Factor Rule

Another rule that is very useful is the **factor rule**. It states that if we have a function that is a constant multiple of another function, then the derivative of that function is just the constant multiplied by the derivative of the other function. More formally:

```math
f(x)=c\cdot g(x) \quad \Longrightarrow \quad f'(x)=c \cdot g'(x),\qquad \forall c \in\mathbb R
```

We can see this by looking at the limit definition of the derivative at any point $x_0$ in the domain of $g$,

```math
\begin{align*}
f'(x_0)
&=\lim_{h\to 0}\frac{f(x_0+h)-f(x_0)}{h} \\
&=\lim_{h\to 0}\frac{c g(x_0+h)-c g(x_0)}{h} \\
&=c \lim_{h\to 0}\frac{g(x_0+h)-g(x_0)}{h} \\
&=c g'(x_0)
\end{align*}
```

<Callout type="example">
As an example, let's look at the derivative of the following function:

```math
f(x) = 5\sin x
```

Because 5 is a constant it just gets factored out of the limit and we are left with the derivative of the sine function, which we already know is $\cos x$. So we can write:

```math
\begin{align*}
f'(x)
&= \lim_{h\to 0}\frac{5\sin(x+h)-5\sin x}{h} \\
&= 5\lim_{h\to 0}\frac{\sin(x+h)-\sin x}{h} \\
&= 5\cos x
\end{align*}
```
</Callout>

## Summation Rule

The **summation rule** states that the derivative of the sum of two functions is the sum of the derivatives of those functions. More formally, if we have two functions $a$ and $b$, then:

```math
f(x)=a(x)\pm b(x)\quad\Longrightarrow\quad f'(x)=a'(x)\pm b'(x)
```

Again, we can see this by looking at the limit definition of the derivative at any point $x_0$ in the domain of $a$ and $b$:

```math
\begin{align*}
f'(x_0)
&=\lim_{h\to 0}\frac{a(x_0+h)\!\pm\! b(x_0+h)-[a(x_0)\!\pm\! b(x_0)]}{h}\\
&=\lim_{h\to 0}\left[\frac{a(x_0+h)-a(x_0)}{h}\right]
 \pm \lim_{h\to 0}\left[\frac{b(x_0+h)-b(x_0)}{h}\right]\\
&=a'(x_0)\pm b'(x_0)
\end{align*}
```

<Callout type="example">
Let us look at the following function:

```math
f(x)=3x^{2}+5x-7
```

We already know that the derivative of $x^2$ is $2x$ and the derivative of $x$ is $1$. So we can apply the summation rule along with the factor rule to find the derivative of $f$:

```math
\begin{align*}
f'(x)
&= 3\cdot 2x + 5\cdot 1 + 0 \\
&= 6x + 5
\end{align*}
```
</Callout>

## Product Rule

We often need to differentiate the product of two functions. The **product rule** states that if $f(x)$ is the product of two functions $a(x)$ and $b(x)$, then the derivative of $f$ is given by:

```math
f(x)=a(x)b(x) \quad\Longrightarrow\quad 
f'(x)=a'(x)b(x)+a(x)b'(x)
```

Again we can derive this by looking at the limit definition of the derivative at any point $x_0$ in the domain of $a$ and $b$:

```math
\begin{align*}
f'(x_0)
&=\lim_{h\to 0}\frac{a(x_0+h)b(x_0+h)-a(x_0)b(x_0)}{h}\\
&=\lim_{h\to 0}\frac{a(x_0+h)b(x_0+h)-a(x_0)b(x_0+h)
                 +a(x_0)b(x_0+h)-a(x_0)b(x_0)}{h}\\
&=\lim_{h\to 0}\bigg[
     b(x_0+h)\frac{a(x_0+h)-a(x_0)}{h}
   +a(x_0)\frac{b(x_0+h)-b(x_0)}{h}\bigg]\\
&=b(x_0)a'(x_0)+a(x_0)b'(x_0)
\end{align*}
```

<Callout type="example">
We can apply the product rule to find the derivative of the following function:

```math
f(x)=x^{2}\sin x.
```

We can identify $a(x)=x^{2}$ and $b(x)=\sin x$. We already know that the derivative of $x^{2}$ is $2x$ and the derivative of $\sin x$ is $\cos x$. So we can apply the product rule to get the following:

```math
\begin{align*}
f'(x)
&= a'(x)b(x) + a(x)b'(x) \\
&= 2x\sin x + x^{2}\cos x.
\end{align*}
```
</Callout>

## Quotient Rule

Just like we have the product rule for the product of two functions, we also have a **quotient rule** for the quotient of two functions. So if $f(x)$ is the quotient of two functions $a(x)$ and $b(x)$ and $b(x) \neq 0$ then the derivative of $f$ is given by:

```math
f(x)=\frac{a(x)}{b(x)} \quad\Longrightarrow\quad
f'(x)=\frac{a'(x)b(x)-a(x)b'(x)}{[b(x)]^{2}} 
```

Again we can derive this by looking at the limit definition of the derivative at any point $x_0$ in the domain of $a$ and $b$ where $b(x_0) \neq 0$. Importantly, we use the product rule and $b^{-1}(x)$ for the derivative of the reciprocal function:

```math
\begin{align*
f'(x)
&=a'(x)b^{-1}(x)+a(x)(b^{-1}(x))'\\
&=a'(x)b^{-1}(x)-a(x)b^{-2}(x)b'(x)\\
&=\frac{a'(x)}{b(x)}-\frac{a(x)b'(x)}{[b(x)]^{2}}\\
&=\frac{a'(x)b(x)-a(x)b'(x)}{[b(x)]^{2}}
\end{align*}
```

<Callout type="example" title="Derivative of Tangent and Cotangent Functions">
The tangent function is defined as the quotient of the sine and cosine functions:

```math
f(x)=\tan x=\frac{\sin x}{\cos x}.
```

So we can apply the quotient rule to find the derivative of the tangent function with our known derivatives of the sine and cosine functions:

```math
\begin{align*}
f'(x)
&=\frac{\cos x \cdot \cos x - \sin x \cdot (-\sin x)}{(\cos x)^{2}} \\
&=\frac{\cos^{2} x + \sin^{2} x}{(\cos x)^{2}} \\
&=\frac{1}{(\cos x)^{2}} = \sec^{2} x.
\end{align*}
```

Note that we used the Pythagorean identity $\sin^{2} x + \cos^{2} x = 1$ to simplify the expression and that the tangent function is not defined for $x = (2k + 1)\frac{\pi}{2}$ where $k \in \mathbb{Z}$ because the cosine function is zero at these points, making the denominator zero.

We can then do the same for the cotangent function which is defined as the quotient of the cosine and sine functions. Here again we can apply the quotient rule with our known derivatives of the sine and cosine functions and note that the cotangent function is not defined for $x = k\pi$ where $k \in \mathbb{Z}$ because the sine function is zero at these points, making the denominator zero:

```math
f(x)=\cot x=\frac{\cos x}{\sin x}.
```

Which gives us:

```math
\begin{align*}
f'(x)
&=\frac{\sin x \cdot (-\sin x) - \cos x \cdot \cos x}{(\sin x)^{2}} \\
&=\frac{-\sin^{2} x - \cos^{2} x}{(\sin x)^{2}} \\
&=-\frac{1}{(\sin x)^{2}} = -\csc^{2} x.
\end{align*}
```
</Callout>

## Power Rule

We have already seen the power rule in effect when we looked at the derivative of the quadratic function $f(x) = x^2$. The **power rule** states that if we have a function that is a power of $x$, i.e. $f(x) = x^n$ where $n$ is a positive integer, then the derivative of that function is as follows:

```math
f(x)=x^{n} \quad \Longrightarrow \quad f'(x)=nx^{n-1} \quad \forall n \in \mathbb{N}
```

You can remember this rule by thinking of the exponent as a coefficient that gets multiplied in front of the term and then the exponent gets reduced by one. To show this, we can use a proof by induction on the positive integer $n$. We have already seen that it holds for the base cases where $n=0$, $n=1$ and $n=2$. So next we assume that the rule holds for some positive integer $n=k$, i.e. $f(x) = x^k$ and $f'(x) = kx^{k-1}$. Then we need to show that it also holds for $n=k+1$:

```math
\begin{align*}
x^k = x \cdot x^{k-1} \\
(x^k)' &= (x \cdot x^{k-1})' \\
&= x' \cdot x^{k-1} + x \cdot (x^{k-1})' \\
&= 1 \cdot x^{k-1} + x \cdot (k - 1)x^{k-2} \\
&= x^{k-1} + kx^{k-1} \\
&= (k+1)x^{k}.
\end{align*}
```

Thus the rule holds for all positive integers $n \in \mathbb N$. Once we have seen the chain rule, we can extend this to all real numbers $n \in \mathbb R$ using the exponential function.

<Callout type="example">
Let us look at the following function:

```math
f(x) = x^{5}.
```
We can apply the power rule to find the derivative of this function to get:

```math
f'(x) = 5x^{4}.
```
</Callout>

## Inverse Function Rule

We have already seen some examples of inverse functions such as a fraction. However, we can also generalize this to any invertible function. The **inverse function rule** states that if $f$ is a bijective function and $f^{-1}$ is its inverse, then if the derivative of $f$ exists at a point $x_0$ in its domain, and $f'(x_0) \neq 0$ and lastly if $f^{-1}(y_0$ where $y_0 = f(x_0)$ is continous and therefore an accumulation point of the codomain, then the derivative of the inverse function $f^{-1}$ at the point $y_0$ exists and is given by:

```math
f^{-1}(y_0) = x_0 \quad \Longrightarrow \quad (f^{-1})'(y_0) = \frac{1}{f'(x_0)}.
```

<Callout type="example">
We have already seen the derivative of $f(x) = x^2$, which is $f'(x) = 2x$. Now let's find the derivative of its inverse function $f^{-1}(y) = \sqrt{y}$:

```math
\begin{align*}
f^{-1}(y) &= \sqrt{y} \\
(f^{-1})'(y) &= \frac{1}{f'(x_0)} \\
&= \frac{1}{2\sqrt{y}}.
\end{align*}
```
</Callout>

<Callout type="example">
We have seen the derivative of the exponential function $f(x) = e^x$, which is $f'(x) = e^x$. Now let's find the derivative of its inverse function $f^{-1}(y) = \ln(y)$:

```math
\begin{align*}
f^{-1}(y) &= \ln(y) \\
(f^{-1})'(y) &= \frac{1}{f'(x_0)} \\
&= \frac{1}{e^{\ln(y)}} \\
&= \frac{1}{y}.
\end{align*}
```
</Callout>

## Chain Rule

The chain rule is probably the most important but also the most complex rule for differentiation. It allows us to differentiate composite functions, i.e. functions that are composed of other functions. This has many applications and is for example key to the [backpropagation algorithm]() for machine learning. The **chain rule** states that if we have a function $f: D \to E$ and $g: E to \mathbb{R}$ such that $x_0$ is an accumulation point of $D$ and $f(x_0)$ an accumulation point in $E$ then the derivative of the composition of the two functions $g \circ f: D \to \mathbb{R}$ is given by:

```math
f(x)=g(h(x)) \quad \Longrightarrow \quad f'(x)=g'(h(x))\cdot h'(x)
```

or in other words:

```math
(g \circ h)'(x) = g'(h(x)) \cdot h'(x)
```

<Callout type="todo">
Prooving this seems rather complicated. Also what about intuitive notation with dx, dy etc. just multiplied.
</Callout>

<Callout type="example">
Let's look at the following function:

```math
f(x) = \exp(x^3 + \sin x).
```

Then we can set $g(x) = \exp(x)$ and $h(x) = x^3 + \sin x$. We can then apply the chain rule to find the derivative of $f$:

```math
\begin{align*}
g(x) &= \exp(x) \\
g'(x) &= \exp(x) \\
h(x) &= x^3 + \sin x \\
h'(x) &= 3x^2 + \cos x \\
f'(x) &= g'(h(x)) \cdot h'(x) \\
&= \exp(x^3 + \sin x) \cdot (3x^2 + \cos x).
\end{align*}
```

Another example would be the following function:

```math
f(x) = (17x^7 + x^5 + 2 + e^x)^{2025}.
```

We can set $g(x) = x^{2025}$ and $h(x) = 17x^7 + x^5 + 2 + e^x$. Then we can apply the chain rule to find the derivative of $f$:

```math
\begin{align*}
g(x) &= x^{2025} \\
g'(x) &= 2025x^{2024} \\
h(x) &= 17x^7 + x^5 + 2 + e^x \\
h'(x) &= 119x^6 + 5x^4 + e^x \\
f'(x) &= g'(h(x)) \cdot h'(x) \\
&= 2025(17x^7 + x^5 + 2 + e^x)^{2024} \cdot (119x^6 + 5x^4 + e^x).
\end{align*}
```
</Callout>

### Power Rule with Chain Rule

The chain rule can be used show that the power rule also holds for all real numbers $n \in \mathbb R$. We can rewrite the power function as follows:

```math
f(x) = x^n = \exp(n \cdot \ln(x)).
```

Then we can apply the chain rule to find the derivative of $f$. We can set $g(x) = \exp(x)$ and $h(x) = n \cdot \ln(x)$. Then we can apply the chain rule to find the derivative of $f$:

```math
\begin{align*}
g(x) &= \exp(x) \\
g'(x) &= \exp(x) \\
h(x) &= n \cdot \ln(x) \\
h'(x) &= n \cdot \frac{1}{x} \\
f'(x) &= g'(h(x)) \cdot h'(x) \\
&= \exp(n \cdot \ln(x)) \cdot n \cdot \frac{1}{x} \\
&= x^n \cdot n \cdot \frac{1}{x} \\
&= n \cdot x^{n-1}.
\end{align*}
```

<Callout type="example" title="Negative Exponents">
So we can now use the power rule also for negative integers and real numbers. For example, if we have $f(x) = \frac{1}{x^2}$, we can rewrite this as $f(x) = x^{-2}$. First let's find the derivative using the quotient rule:

```math
\begin{align*}
f'(x) = \frac{a'(x)b(x) - a(x)b'(x)}{[b(x)]^2} \\
&= \frac{0 \cdot x^2 - 1 \cdot (2^x)}{(x^2)^2} \\
&= \frac{-2x}{x^4} \\
&= -\frac{2}{x^3}.
\end{align*}
```

If we use the power rule instead, we can rewrite the function as $f(x) = x^{-2}$ and then apply the power rule:

```math
\begin{align*}
f'(x) &= -2 \cdot x^{-3} \\
&= -2 \cdot \frac{1}{x^3}.
\end{align*}
```
</Callout>

<Callout type="example" title="Fractional Exponents">
We can also use the power rule for fractional exponents in other words for roots of $x$. For example, if we have $f(x) = \sqrt[3]{x} = x^{1/3}$, we can apply the power rule to find the derivative:

```math
\begin{align*}
f(x) &= x^{1/3} \\
f'(x) &= \frac{1}{3} \cdot x^{1/3 - 1} \\
&= \frac{1}{3} \cdot x^{-2/3} \\
&= \frac{1}{3} \cdot \frac{1}{\sqrt[3]{x^2}}.
\end{align*}
```
</Callout>

## L'Hospital's Rule

## Higher Order Derivatives

## Analyzing Real Functions

### Extrema

A critical point is where it gets interesting, because it is where the function changes its behavior. A critical point is a point where the derivative is either zero or does not exist. 

definition of local maxima and minima in some delta neighborhood.

f has local extrema if it either has a local maximum or local minimum.

meaning of positive derivative, then there is a delta such that the function is increasing in that neighborhood to the right of the point, and decreasing to the left of the point.
If the derivative is negative, then there is a delta such that the function is decreasing in that neighborhood to the right of the point, and increasing to the left of the point.

Then it logically follows that if the derivative is zero, then the function is neither increasing nor decreasing in that neighborhood, which means that it is either a local maximum or a local minimum. what about constant function?

Proof comes from the continiouty bit showing that the value can not just drastically change at that point, so it must be increasing or decreasing.

#### Rolle's Theorem

Rolle's Theorem states that if a function $f$ is continuous on the closed interval $[a, b]$ and differentiable on the open interval $(a, b)$, and if $f(a) = f(b)$, then there exists at least one point $c \in (a, b)$ such that $f'(c) = 0$. In other words, if a function has the same value at two points and is smooth (differentiable) in between, then it must have a flat spot i.e either a local maximum or minimum, at some point in between.

Applying this with the Mean Value Theorem, we can say that if a function is continuous on the closed interval $[a, b]$ and differentiable on the open interval $(a, b)$, then there exists at least one point $c \in (a, b)$ such that:

```math
\frac{f(b) - f(a)}{b - a} = f'(c).
```

This doesnt make a lot of intuitive sense. From this it follows that if f'(c) = 0 for all $c \in (a, b)$, then the function is constant on the interval $[a, b]$. 

It also follows that if $f'(c) = g'(c) = 0$ for all $c \in (a, b)$, then there exists at least one point $d \in (a, b)$ such that $f(x) = g(x) + d for all $x \in [a, b]$. So if the have the same derivative at all points in the interval, then they are just shifted versions of each other.

We also get that if if the derivate is positive at all points in the interval, then the function is increasing on the interval, and if the derivative is negative at all points in the interval, then the function is decreasing on the interval. This can also be adapted to the strict monotonicity. but this isnt just simply looking at the strict inequality. Counterexample would be x^3. 

If there exists a bound $M$ such that $|f'(x)| \leq M$ then it follows that the function is Lipschitz continuous on the interval $[a, b]$. This means that the function does not change too rapidly and is bounded by a linear function. In other words, there exists a constant $M$ such that for all $x, y \in [a, b]$:

```math
|f(x) - f(y)| \leq M |x - y|.
```
